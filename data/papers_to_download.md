# ArXiv Papers to Download (20-30 papers on RAG/LLMs)

**Created**: 2025-11-23 (Day 27)
**Target**: 20-30 papers for RAG Q&A system corpus

---

## Core RAG Papers (Must-have)

1. **Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks** (Lewis et al., 2020)
   - ArXiv: https://arxiv.org/abs/2005.11401
   - ID: 2005.11401
   - Foundational RAG paper from Meta

2. **FiD: Fusion-in-Decoder** (Izacard & Grave, 2021)
   - ArXiv: https://arxiv.org/abs/2007.01282
   - ID: 2007.01282
   - Encoder-decoder RAG architecture

3. **Lost in the Middle** (Liu et al., 2023)
   - ArXiv: https://arxiv.org/abs/2307.03172
   - ID: 2307.03172
   - Long-context position bias in RAG

4. **Self-RAG** (Asai et al., 2023)
   - ArXiv: https://arxiv.org/abs/2310.11511
   - ID: 2310.11511
   - Self-reflective RAG with critique tokens

5. **RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval** (Sarthi et al., 2024)
   - ArXiv: https://arxiv.org/abs/2401.18059
   - ID: 2401.18059
   - Hierarchical RAG with recursive summarization

6. **RAFT: Adapting Language Model to Domain Specific RAG** (Zhang et al., 2024)
   - ArXiv: https://arxiv.org/abs/2403.10131
   - ID: 2403.10131
   - Fine-tuning for RAG with distractor documents

7. **GraphRAG: Unlocking LLM discovery on narrative private data** (Edge et al., 2024)
   - ArXiv: https://arxiv.org/abs/2404.16130
   - ID: 2404.16130
   - Microsoft's graph-based RAG

---

## Retrieval Methods

8. **ColBERT: Efficient and Effective Passage Search** (Khattab & Zaharia, 2020)
   - ArXiv: https://arxiv.org/abs/2004.12832
   - ID: 2004.12832
   - Late interaction, token-level embeddings

9. **Dense Passage Retrieval (DPR)** (Karpukhin et al., 2020)
   - ArXiv: https://arxiv.org/abs/2004.04906
   - ID: 2004.04906
   - Dual-encoder dense retrieval

10. **SPLADE: Sparse Lexical and Expansion Model** (Formal et al., 2021)
    - ArXiv: https://arxiv.org/abs/2107.05720
    - ID: 2107.05720
    - Learned sparse retrieval

---

## Evaluation & Metrics

12. **RAGAS: Automated Evaluation of RAG** (Es et al., 2023)
    - ArXiv: https://arxiv.org/abs/2309.15217
    - ID: 2309.15217
    - Faithfulness, context precision/recall metrics

13. **ARES: An Automated Evaluation Framework for RAG** (Saad-Falcon et al., 2023)
    - ArXiv: https://arxiv.org/abs/2311.09476
    - ID: 2311.09476
    - LLM-as-judge for RAG evaluation

14. **Benchmarking Large Language Models in Retrieval-Augmented Generation** (Chen et al., 2023)
    - ArXiv: https://arxiv.org/abs/2309.01431
    - ID: 2309.01431
    - RGB benchmark for RAG systems

---

## Query Processing

15. **Query Rewriting for Retrieval-Augmented LLMs** (Ma et al., 2023)
    - ArXiv: https://arxiv.org/abs/2305.14283
    - ID: 2305.14283
    - Query decomposition and rewriting

16. **Step-Back Prompting** (Zheng et al., 2023)
    - ArXiv: https://arxiv.org/abs/2310.06117
    - ID: 2310.06117
    - High-level reasoning before retrieval

17. **HyDE: Hypothetical Document Embeddings** (Gao et al., 2022)
    - ArXiv: https://arxiv.org/abs/2212.10496
    - ID: 2212.10496
    - Generate hypothetical answers for retrieval

---

## Advanced RAG Techniques

18. **Active Retrieval Augmented Generation** (Jiang et al., 2023)
    - ArXiv: https://arxiv.org/abs/2305.06983
    - ID: 2305.06983
    - Adaptive retrieval frequency

19. **Interleaving Retrieval with Chain-of-Thought** (Trivedi et al., 2022)
    - ArXiv: https://arxiv.org/abs/2212.10509
    - ID: 2212.10509
    - Retrieval during reasoning steps

20. **Re-ranking for RAG** (Nogueira et al., 2020)
    - ArXiv: https://arxiv.org/abs/2007.15651
    - ID: 2007.15651
    - Cross-encoder reranking strategies

---

## Long Context vs RAG

21. **Long-Context Language Models Meet RAG** (Xu et al., 2024)
    - ArXiv: https://arxiv.org/abs/2402.13116
    - ID: 2402.13116
    - When to use long context vs RAG

22. **The Needle in a Haystack Test** (Anthropic, 2023)
    - Blog: https://www.anthropic.com/news/claude-2-1-needle-in-haystack
    - Note: No ArXiv paper, but referenced in many RAG papers

---

## Survey Papers (High-level overviews)

23. **Retrieval-Augmented Generation: A Survey** (Gao et al., 2023)
    - ArXiv: https://arxiv.org/abs/2312.10997
    - ID: 2312.10997
    - Comprehensive RAG survey (Dec 2023)

24. **A Survey on Retrieval-Augmented Text Generation** (Li et al., 2023)
    - ArXiv: https://arxiv.org/abs/2202.01110
    - ID: 2202.01110
    - Early survey covering fundamentals

25. **RAG for NLP: A Survey and Roadmap** (Asai et al., 2023)
    - ArXiv: https://arxiv.org/abs/2312.00752
    - ID: 2312.00752
    - Recent survey with taxonomy

---

## Multi-hop & Complex Reasoning

26. **Multi-hop Retrieval for Open-Domain QA** (Qi et al., 2020)
    - ArXiv: https://arxiv.org/abs/2009.12756
    - ID: 2009.12756
    - Multi-hop reasoning patterns

27. **ReAct: Synergizing Reasoning and Acting in Language Models** (Yao et al., 2022)
    - ArXiv: https://arxiv.org/abs/2210.03629
    - ID: 2210.03629
    - Agentic RAG with tool use

---

## Chunking & Document Processing

28. **Semantic Chunking for RAG** (Kamradt, 2023)
    - Note: Blog post, not ArXiv. Search for "semantic chunking RAG" papers
    - Placeholder: Look for papers on optimal chunk size

29. **Contextual Document Embeddings** (Nussbaum et al., 2024)
    - ArXiv: https://arxiv.org/abs/2410.02525
    - ID: 2410.02525
    - Anthropic's contextual retrieval approach

---

## Hallucination & Faithfulness

30. **Measuring and Reducing Hallucination in RAG** (Manakul et al., 2023)
    - ArXiv: https://arxiv.org/abs/2305.14552
    - ID: 2305.14552
    - Faithfulness metrics and mitigation

---

## Download Instructions

### Option A: Manual Download (Browser)
1. Visit each ArXiv link above
2. Click "Download PDF" button
3. Save to `data/raw/` folder
4. Rename files (optional): `2005.11401_rag_lewis.pdf`

### Option B: Automated Download (Python Script)
See `download_papers.py` script (to be created in next step)

**Target**: Download 20-25 papers (prioritize Core RAG + Retrieval + Evaluation sections)

**Estimated time**: 15-20 minutes for manual download, 5 minutes for script
