{"user_input": "Who are the authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering'?", "reference_contexts": ["Dense Passage Retrieval for Open-Domain Question Answering\nVladimir Karpukhin∗, Barlas O˘guz∗, Sewon Min†, Patrick Lewis,\nLedell Wu, Sergey Edunov, Danqi Chen‡, Wen-tau Yih\nFacebook AI\n†University of Washington\n‡Princeton University\n{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\nAbstract\nOpen-domain question answering relies on ef-\nﬁcient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method.\nIn this work, we show that\nretrieval can be practically implemented us-\ning dense representations alone, where em-\nbeddings are learned from a small number\nof questions and passages by a simple dual-\nencoder framework.\nWhen evaluated on a\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong Lucene-\nBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks.1\n1\n"], "reference": "The authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering' are Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.", "persona_name": "Research Scientist specializing in Natural Language Processing", "query_style": "PERFECT_GRAMMAR", "query_length": "LONG", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is BM25 used for in open-domain QA?", "reference_contexts": ["Introduction\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions us-\ning a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), inter alia), the advances of reading\ncomprehension models suggest a much simpliﬁed\ntwo-stage framework: (1) a context retriever ﬁrst\nselects a small subset of passages where some\nof them contain the answer to the question, and\nthen (2) a machine reader can thoroughly exam-\nine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\nopen-domain QA to machine reading is a very rea-\nsonable strategy, a huge performance degradation\nis often observed in practice2, indicating the needs\nof improving retrieval.\n∗Equal contribution\n1The code and trained models have been released at\nhttps://github.com/facebookresearch/DPR.\n2For instance, the exact match score on SQuAD v1.1 drops\nfrom above 80% to less than 40% (Yang et al., 2019a).\nRetrieval in open-domain QA is usually imple-\nmented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords efﬁ-\nciently with an inverted index and can be seen\nas representing the question and context in high-\ndimensional, sparse vectors (with weighting). Con-\nversely, the dense, latent semantic encoding is com-\nplementary to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n“Who is the bad guy in lord of the rings?”, which can\nbe answered from the context “Sala Baker is best\nknown for portraying the villain Sauron in the Lord\nof the Rings trilogy.” A term-based system would\nhave difﬁculty retrieving such a context, while\na dense retrieval system would be able to better\nmatch “bad guy” with “villain” and fetch the cor-\nrect context. Dense encodings are also learnable\nby adjusting the embedding functions, which pro-\nvides additional ﬂexibility to have a task-speciﬁc\nrepresentation. With special in-memory data struc-\ntures and indexing schemes, retrieval can be done\nefﬁciently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)).\nHowever, it is generally believed that learn-\ning a good dense vector representation needs a\nlarge number of labeled pairs of question and con-\ntexts. Dense retrieval methods have thus never\nbe shown to outperform TF-IDF/BM25 for open-\ndomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\nobjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then ﬁne-\ntuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain\narXiv:2004.04906v3  [cs.CL]  30 Sep 2020\n\fQA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot ﬁne-tuned using pairs of questions and answers,\nthe corresponding representations could be subop-\ntimal.\nIn this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), with-\nout additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\nof question and passage pairs. Through a series\nof careful ablation studies, our ﬁnal solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective compar-\ning all pairs of questions and passages in a batch.\nOur "], "reference": "BM25 is used for retrieval in open-domain QA, usually implemented using TF-IDF or BM25, which matches keywords efficiently with an inverted index and can be seen as representing the question and context in high-dimensional, sparse vectors (with weighting).", "persona_name": "Research Scientist", "query_style": "POOR_GRAMMAR", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the purpose of using Wikipedia in open-domain question answering?", "reference_contexts": ["Dense Passage Retriever (DPR) is exception- ally strong. It not only outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy), but also results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs. 33.3%) in the open Natural Questions setting (Lee et al., 2019; Kwiatkowski et al., 2019). Our contributions are twofold. First, we demon- strate that with the proper training setup, sim- ply ﬁne-tuning the question and passage encoders on existing question-passage pairs is sufﬁcient to greatly outperform BM25. Our empirical results also suggest that additional pretraining may not be needed. Second, we verify that, in the context of open-domain question answering, a higher retrieval precision indeed translates to a higher end-to-end QA accuracy. By applying a modern reader model to the top retrieved passages, we achieve compara- ble or better results on multiple QA datasets in the open-retrieval setting, compared to several, much complicated systems. 2 Background The problem of open-domain QA studied in this paper can be described as follows. Given a factoid question, such as “Who ﬁrst voiced Meg on Family Guy?” or “Where was the 8th Dalai Lama born?”, a system is required to answer it using a large corpus of diversiﬁed topics. More speciﬁcally, we assume the extractive QA setting, in which the answer is restricted to a span appearing in one or more pas- sages in the corpus. Assume that our collection contains D documents, d1, d2, · · · , dD. We ﬁrst split each of the documents into text passages of equal lengths as the basic retrieval units3 and get M total passages in our corpus C = {p1, p2, . . . , pM}, where each passage pi can be viewed as a sequence of tokens w(i) 1 , w(i) 2 , · · · , w(i) |pi|. Given a question q, the task is to ﬁnd a span w(i) s , w(i) s+1, · · · , w(i) e from one of the passages pi that can answer the question. Notice that to cover a wide variety of domains, the corpus size can easily range from millions of docu- ments (e.g., Wikipedia) to billions (e.g., the Web). As a result, any open-domain QA system needs to include an efﬁcient retriever component that can se- lect a small set of relevant texts, before applying the reader to extract the answer (Chen et al., 2017).4 Formally speaking, a retriever R : (q, C) →CF is a function that takes as input a question q and a corpus C and returns a much smaller ﬁlter set of texts CF ⊂C, where |CF| = k ≪|C|. For a ﬁxed k, a retriever can be evaluated in isolation on top-k retrieval accuracy, which is the fraction of ques- tions for which CF contains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passages in a low-dimensional and continuous space, such that it can retrieve efﬁciently the top k passages relevant to the input question for the reader at run-time. Note that M can be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and k is usually small, such as 20–100. 3.1 Overview Our dense passage retriever (DPR) uses a dense encoder EP (·) which maps any text passage to a d- dimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval. 3The ideal size and boundary of a text passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using ﬁxed-length passages performs better in both retrieval and ﬁnal QA accuracy, as observed by Wang et al. (2019). 4Exceptions include (Seo et al., 2019) and"], "reference": "Wikipedia is a large corpus of diversified topics that can be used to answer factoid questions in an extractive QA setting.", "persona_name": "Research Scientist in Information Retrieval", "query_style": "MISSPELLED", "query_length": "LONG", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What method does Roberts et al. use to measure the similarity between questions and passages?", "reference_contexts": ["(Roberts et al., 2020), which retrieves and generates the answers, respectively. At run-time, DPR applies a different encoder EQ(·) that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector. We deﬁne the similarity between the question and the passage using the dot product of their vectors: sim(q, p) = EQ(q)⊺EP (p). (1) Although more expressive model forms for measur- ing the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the similarity function needs to be decomposable so that the represen- tations of the collection of passages can be pre- computed. Most decomposable similarity functions are some transformations of Euclidean distance (L2). For instance, cosine is equivalent to inner product for unit vectors and the Mahalanobis dis- tance is equivalent to L2 distance in a transformed space. Inner product search has been widely used and studied, as well as its connection to cosine similarity and L2 distance (Mussmann and Ermon, 2016; Ram and Gray, 2012). As our ablation study ﬁnds other similarity functions perform compara- bly (Section 5.2; Appendix B), we thus choose the simpler inner product function and improve the dense passage retriever by learning better encoders. Encoders Although in principle the question and passage encoders can be implemented by any neu- ral networks, in this work we use two independent BERT (Devlin et al., 2019) networks (base, un- cased) and take the representation at the [CLS] token as the output, so d = 768. Inference During inference time, we apply the passage encoder EP to all the passages and index them using FAISS (Johnson et al., 2017) ofﬂine. FAISS is an extremely efﬁcient, open-source li- brary for similarity search and clustering of dense vectors, which can easily be applied to billions of vectors. Given a question q at run-time, we derive its embedding vq = EQ(q) and retrieve the top k passages with embeddings closest to vq. 3.2 Training Training the encoders so that the dot-product sim- ilarity (Eq. (1)) becomes a good ranking function for retrieval is essentially a metric learning prob- lem (Kulis, 2013). The goal is to create a vector space such that relevant pairs of questions and pas- sages will have smaller distance (i.e., higher simi- larity) than the irrelevant ones, by learning a better embedding function. Let D = {⟨qi, p+ i , p− i,1, · · · , p− i,n⟩}m i=1 be the training data that consists of m instances. Each instance contains one question qi and one relevant (positive) passage p+ i , along with n irrelevant (neg- ative) passages p− i,j. We optimize the loss function as the negative log likelihood of the positive pas- sage: L(qi, p+ i , p− i,1, · · · , p− i,n) (2) = −log esim(qi,p+ i ) esim(qi,p+ i ) + Pn j=1 esim(qi,p− i,j) . Positive and negative passages For retrieval problems, it is often the case that positive examples are available explicitly, while negative examples need to be selected from an extremely large pool. For instance, passages relevant to a question may be given in a QA dataset, or can be found using the answer. All other passages in the collection, while not speciﬁed explicitly, can be viewed as irrelevant by default. In practice, how to select negative ex- amples is often overlooked but could be decisive for learning a high-quality encoder. We consider three different types of negatives: (1) Random: any random passage from the corpus; (2) BM25: top passages returned by BM25 which don’t contain the answer but match most question tokens; (3) Gold: positive passages paired with other questions which appear in the training set. We will discuss the impact of different types of negative passages and training schemes in Section 5.2. Our best model uses gold passages from the same mini-batch and one BM25 negative passage. In particular, re-using gold passages from the same batch as negatives can make the computation efﬁcient while achiev- ing great performance. We discuss this approach below. In-batch negatives Assume that we have B questions in a mini-batch and each one is asso- ciated with a relevant passage. Let Q and P be the (B×d) matrix of question and"], "reference": "Roberts et al. define the similarity between a question and a passage using the dot product of their vectors: sim(q, p) = EQ(q)⊺EP (p).", "persona_name": "Research Scientist specializing in Natural Language Processing", "query_style": "PERFECT_GRAMMAR", "query_length": "LONG", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the purpose of passage embeddings in machine learning?", "reference_contexts": ["passage embeddings in a batch of size B. S = QPT is a (B × B) ma- trix of similarity scores, where each row of which corresponds to a question, paired with B passages. In this way, we reuse computation and effectively train on B2 (qi, pj) question/passage pairs in each batch. Any (qi, pj) pair is a positive example when i = j, and negative otherwise. This creates B train- ing instances in each batch, where there are B −1 negative passages for each question. The trick of in-batch negatives has been used in the full batch setting (Yih et al., 2011) and more recently for mini-batch (Henderson et al., 2017; Gillick et al., 2019). It has been shown to be an effective strategy for learning a dual-encoder model that boosts the number of training examples. 4"], "reference": "Passage embeddings are used to represent text passages as dense vectors, which can then be compared using similarity scores. This allows for efficient training on large batches of question-passage pairs by reusing computation and creating multiple positive and negative examples within each batch.", "persona_name": "Research Scientist", "query_style": "MISSPELLED", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is BM25 used for in the context of question answering datasets?", "reference_contexts": ["Experimental Setup\nIn this section, we describe the data we used for\nexperiments and the basic setup.\n4.1\nWikipedia Data Pre-processing\nFollowing (Lee et al., 2019), we use the English\nWikipedia dump from Dec. 20, 2018 as the source\ndocuments for answering questions. We ﬁrst apply\nthe pre-processing code released in DrQA (Chen\net al., 2017) to extract the clean, text-portion of\narticles from the Wikipedia dump. This step re-\nmoves semi-structured data, such as tables, info-\nboxes, lists, as well as the disambiguation pages.\nWe then split each article into multiple, disjoint text\nblocks of 100 words as passages, serving as our\nbasic retrieval units, following (Wang et al., 2019),\nwhich results in 21,015,324 passages in the end.5\nEach passage is also prepended with the title of the\nWikipedia article where the passage is from, along\nwith an [SEP] token.\n4.2\nQuestion Answering Datasets\nWe use the same ﬁve QA datasets and train-\ning/dev/testing splitting method as in previous\nwork (Lee et al., 2019). Below we brieﬂy describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\nNatural Questions (NQ) (Kwiatkowski et al.,\n2019) was designed for end-to-end question an-\nswering.\nThe questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identiﬁed by annotators.\nTriviaQA (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web.\nWebQuestions (WQ) (Berant et al., 2013) consists\nof questions selected using Google Suggest API,\nwhere the answers are entities in Freebase.\nCuratedTREC (TREC) (Baudiˇs and ˇSediv`y,\n2015) sources questions from TREC QA tracks\n5However, Wang et al. (2019) also propose splitting docu-\nments into overlapping passages, which we do not ﬁnd advan-\ntageous compared to the non-overlapping version.\nDataset\nTrain\nDev\nTest\nNatural Questions\n79,168\n58,880\n8,757\n3,610\nTriviaQA\n78,785\n60,413\n8,837\n11,313\nWebQuestions\n3,417\n2,474\n361\n2,032\nCuratedTREC\n1,353\n1,125\n133\n694\nSQuAD\n78,713\n70,096\n8,886\n10,570\nTable 1: Number of questions in each QA dataset. The\ntwo columns of Train denote the original training ex-\namples in the dataset and the actual questions used for\ntraining DPR after ﬁltering. See text for more details.\nas well as various Web sources and is intended for\nopen-domain QA from unstructured corpora.\nSQuAD v1.1 (Rajpurkar et al., 2016) is a popu-\nlar benchmark dataset for reading comprehension.\nAnnotators were presented with a Wikipedia para-\ngraph, and asked to write questions that could be\nanswered from the given text. Although SQuAD\nhas been used previously for open-domain QA re-\nsearch, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\nSelection of positive passages\nBecause only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA6, we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the ques-\ntion will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool.7 We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the num-\nber of questions in training/dev/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n5\n"], "reference": "BM25 is used to select positive passages by ranking them based on their relevance to the questions. The highest-ranked passage containing the answer is selected as the positive passage.", "persona_name": "Research Scientist", "query_style": "WEB_SEARCH_LIKE", "query_length": "LONG", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is TREC used for in the context of passage retrieval experiments?", "reference_contexts": ["Experiments: Passage Retrieval In this section, we evaluate the retrieval perfor- mance of our Dense Passage Retriever (DPR), along with analysis on how its output differs from 6We use the unﬁltered TriviaQA version and discard the noisy evidence documents mined from Bing. 7The improvement of using gold contexts over passages that contain answers is small. See Section 5.2 and Ap- pendix A. Training Retriever Top-20 Top-100 NQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD None BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0 Single DPR 78.4 79.4 73.2 79.8 63.2 85.4 85.0 81.4 89.1 77.2 BM25 + DPR 76.6 79.8 71.0 85.2 71.5 83.8 84.5 80.5 92.7 81.3 Multi DPR 79.4 78.8 75.0 89.1 51.6 86.0 84.7 82.9 93.9 67.6 BM25 + DPR 78.0 79.9 74.7 88.5 66.2 83.9 84.4 82.3 94.1 78.6 Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved passages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details. traditional retrieval methods, the effects of different training schemes and the run-time efﬁciency. The DPR model used in our main experiments is trained using the in-batch negative setting (Sec- tion 3.2) with a batch size of 128 and one additional BM25 negative passage per question. We trained the question and passage encoders for up to 40 epochs for large datasets (NQ, TriviaQA, SQuAD) and 100 epochs for small datasets (TREC, WQ), with a learning rate of 10−5 using Adam, linear scheduling with warm-up and dropout rate 0.1. While it is good to have the ﬂexibility to adapt the retriever to each dataset, it would also be de- sirable to obtain a single retriever that works well across the board. To this end, we train a multi- dataset encoder by combining training data from all datasets excluding SQuAD.8 In addition to DPR, we also present the results of BM25, the traditional retrieval method9 and BM25+DPR, using a linear combination of their scores as the new ranking function. Speciﬁcally, we obtain two initial sets of top-2000 passages based on BM25 and DPR, respectively, and rerank the union of them using BM25(q,p) + λ · sim(q, p) as the ranking function. We used λ = 1.1 based on the retrieval accuracy in the development set. 5.1 Main Results Table 2 compares different passage retrieval sys- tems on ﬁve QA datasets, using the top-k accuracy (k ∈{20, 100}). With the exception of SQuAD, DPR performs consistently better than BM25 on all datasets. The gap is especially large when k is small (e.g., 78.4% vs. 59.1% for top-20 accuracy on Natural Questions). When training with mul- 8SQuAD is limited to a small set of Wikipedia documents and thus introduces unwanted bias. We will discuss this issue more in Section 5.1. 9Lucene implementation. BM25 parameters b = 0.4 (doc- ument length normalization) and k1 = 0.9 (term frequency scaling) are tuned using development sets. 20 40 60 80 100 k: # of retrieved passages 40 50 60 70 80 90 Top-k accuracy (%) BM25 # Train: 1k # Train: 10k # Train: 20k # Train: 40k # Train: all (59k) Figure 1: Retriever top-k accuracy with different num- bers of training examples used in our dense passage re- triever vs BM25. The results are measured on the de- velopment set of Natural Questions. Our DPR trained using 1,000 examples already outperforms BM25. tiple datasets,"], "reference": "TREC is used as one of the QA datasets to evaluate the retrieval performance of the Dense Passage Retriever (DPR) model.", "persona_name": "Research Scientist specializing in Natural Language Processing", "query_style": "PERFECT_GRAMMAR", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the reason for the lower performance of SQuAD according to Lee et al.?", "reference_contexts": ["TREC, the smallest dataset of the ﬁve, beneﬁts greatly from more training examples. In contrast, Natural Questions and WebQuestions improve modestly and TriviaQA degrades slightly. Results can be improved further in some cases by combining DPR with BM25 in both single- and multi-dataset settings. We conjecture that the lower performance on SQuAD is due to two reasons. First, the annota- tors wrote questions after seeing the passage. As a result, there is a high lexical overlap between passages and questions, which gives BM25 a clear advantage. Second, the data was collected from only 500+ Wikipedia articles and thus the distribu- tion of training examples is extremely biased, as argued previously by Lee et al. (2019). 5.2 Ablation Study on Model Training To understand further how different model training options affect the results, we conduct several addi- tional experiments and discuss our ﬁndings below. Sample efﬁciency We explore how many train- ing examples are needed to achieve good passage retrieval performance. Figure 1 illustrates the top-k retrieval accuracy with respect to different num- bers of training examples, measured on the devel- opment set of Natural Questions. As is shown, a dense passage retriever trained using only 1,000 ex- amples already outperforms BM25. This suggests that with a general pretrained language model, it is possible to train a high-quality dense retriever with a small number of question–passage pairs. Adding more training examples (from 1k to 59k) further improves the retrieval accuracy consistently. In-batch negative training We test different training schemes on the development set of Natural Questions and summarize the results in Table 3. The top block is the standard 1-of-N training set- ting, where each question in the batch is paired with a positive passage and its own set of n neg- ative passages (Eq. (2)). We ﬁnd that the choice of negatives — random, BM25 or gold passages (positive passages from other questions) — does not impact the top-k accuracy much in this setting when k ≥20. The middle bock is the in-batch negative training (Section 3.2) setting. We ﬁnd that using a similar conﬁguration (7 gold negative passages), in-batch negative training improves the results substantially. The key difference between the two is whether the gold negative passages come from the same batch or from the whole training set. Effectively, in-batch negative training is an easy and memory-efﬁcient way to reuse the negative examples already in the batch rather than creating new ones. It produces more pairs and thus increases the number of train- ing examples, which might contribute to the good model performance. As a result, accuracy consis- tently improves as the batch size grows. Finally, we explore in-batch negative training with additional “hard” negative passages that have high BM25 scores given the question, but do not contain the answer string (the bottom block). These additional passages are used as negative passages for all questions in the same batch. We ﬁnd that adding a single BM25 negative passage improves the result substantially while adding two does not help further. Impact of gold passages We use passages that match the gold contexts in the original datasets (when available) as positive examples (Section 4.2). Type #N IB Top-5 Top-20 Top-100 Random 7 \u0017 47.0 64.3 77.8 BM25 7 \u0017 50.0 63.3 74.8 Gold 7 \u0017 42.6 63.1 78.3 Gold 7 \u0013 51.1 69.1 80.8 Gold 31 \u0013 52.1 70.8 82.1 Gold 127 \u0013 55.8 73.0 83.1 G.+BM25(1) 31+32 \u0013 65.0 77.3 84.4 G.+BM25(2) 31+64 \u0013 64.5 76.4 84.0 G.+BM25(1) 127+128 \u0013 65.8 78.0 84.9 Table 3: Comparison of different training schemes, measured as top-k retrieval accuracy on Natural Ques- tions (development set). #N: number of negative examples, IB: in-batch training. G.+BM25(1) and G.+BM25(2) denote in-batch training with 1 or 2 ad- ditional BM25 negatives, which serve as negative pas- sages for all questions in the batch. Our experiments on Natural Questions show that switching to distantly-supervised passages (using the highest-ranked BM25 passage that contains the answer), has only"], "reference": "The lower performance on SQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage, resulting in high lexical overlap between passages and questions, which gives BM25 an advantage. Second, the data was collected from only 500+ Wikipedia articles, leading to a biased distribution of training examples.", "persona_name": "Research Scientist specializing in Natural Language Processing", "query_style": "MISSPELLED", "query_length": "LONG", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What CPU model was used to profile the passage retrieval speed for DPR?", "reference_contexts": ["a small impact: 1 point lower top-k accuracy for retrieval. Appendix A contains more details. Similarity and loss Besides dot product, cosine and Euclidean L2 distance are also commonly used as decomposable similarity functions. We test these alternatives and ﬁnd that L2 performs compara- ble to dot product, and both of them are superior to cosine. Similarly, in addition to negative log- likelihood, a popular option for ranking is triplet loss, which compares a positive passage and a nega- tive one directly with respect to a question (Burges et al., 2005). Our experiments show that using triplet loss does not affect the results much. More details can be found in Appendix B. Cross-dataset generalization One interesting question regarding DPR’s discriminative training is how much performance degradation it may suf- fer from a non-iid setting. In other words, can it still generalize well when directly applied to a different dataset without additional ﬁne-tuning? To test the cross-dataset generalization, we train DPR on Natural Questions only and test it directly on the smaller WebQuestions and CuratedTREC datasets. We ﬁnd that DPR generalizes well, with 3-5 points loss from the best performing ﬁne-tuned model in top-20 retrieval accuracy (69.9/86.3 vs. 75.0/89.1 for WebQuestions and TREC, respec- tively), while still greatly outperforming the BM25 baseline (55.0/70.9). 5.3 Qualitative Analysis Although DPR performs better than BM25 in gen- eral, passages retrieved by these two methods dif- fer qualitatively. Term-matching methods like BM25 are sensitive to highly selective keywords and phrases, while DPR captures lexical variations or semantic relationships better. See Appendix C for examples and more discussion. 5.4 Run-time Efﬁciency The main reason that we require a retrieval compo- nent for open-domain QA is to reduce the number of candidate passages that the reader needs to con- sider, which is crucial for answering user’s ques- tions in real-time. We proﬁled the passage retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20GHz and 512GB memory. With the help of FAISS in-memory index for real-valued vectors10, DPR can be made incredibly efﬁcient, processing 995.0 questions per second, returning top 100 pas- sages per question. In contrast, BM25/Lucene (im- plemented in Java, using ﬁle index) processes 23.7 questions per second per CPU thread. On the other hand, the time required for building an index for dense vectors is much longer. Com- puting dense embeddings on 21-million passages is resource intensive, but can be easily parallelized, taking roughly 8.8 hours on 8 GPUs. However, building the FAISS index on 21-million vectors on a single server takes 8.5 hours. In comparison, building an inverted index using Lucene is much cheaper and takes only about 30 minutes in total. 6"], "reference": "The Intel Xeon CPU E5-2698 v4 @ 2.20GHz was used to profile the passage retrieval speed for DPR.", "persona_name": "Research Scientist specializing in Natural Language Processing", "query_style": "PERFECT_GRAMMAR", "query_length": "MEDIUM", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is the purpose of a passage selection score in an end-to-end QA system?", "reference_contexts": ["Experiments: Question Answering In this section, we experiment with how different passage retrievers affect the ﬁnal QA accuracy. 6.1 End-to-end QA System We implement an end-to-end question answering system in which we can plug different retriever systems directly. Besides the retriever, our QA sys- tem consists of a neural reader that outputs the answer to the question. Given the top k retrieved passages (up to 100 in our experiments), the reader assigns a passage selection score to each passage. In addition, it extracts an answer span from each passage and assigns a span score. The best span from the passage with the highest passage selection 10FAISS conﬁguration: we used HNSW index type on CPU, neighbors to store per node = 512, construction time search depth = 200, search depth = 128. score is chosen as the ﬁnal answer. The passage selection model serves as a reranker through cross- attention between the question and the passage. Al- though cross-attention is not feasible for retrieving relevant passages in a large corpus due to its non- decomposable nature, it has more capacity than the dual-encoder model sim(q, p) as in Eq. (1). Apply- ing it to selecting the passage from a small number of retrieved candidates has been shown to work well (Wang et al., 2019, 2018; Lin et al., 2018). Speciﬁcally, let Pi ∈RL×h (1 ≤i ≤k) be a BERT (base, uncased in our experiments) rep- resentation for the i-th passage, where L is the maximum length of the passage and h the hidden dimension. The probabilities of a token being the starting/ending positions of an answer span and a passage being selected are deﬁned as: Pstart,i(s) = softmax \u0000Piwstart \u0001 s, (3) Pend,i(t) = softmax \u0000Piwend \u0001 t, (4) Pselected(i) = softmax \u0000ˆP⊺wselected \u0001 i, (5) where ˆP = [P[CLS] 1 , . . . , P[CLS] k ] ∈Rh×k and wstart, wend, wselected ∈Rh are learnable vectors. We compute a span score of the s-th to t-th words from the i-th passage as Pstart,i(s) × Pend,i(t), and a passage selection score of the i-th passage as Pselected(i). During training, we sample one positive and ˜m−1 negative passages from the top 100 passages returned by the retrieval system (BM25 or DPR) for each question. ˜m is a hyper-parameter and we use ˜m = 24 in all the experiments. The training ob- jective is to maximize the marginal log-likelihood of all the correct answer spans in the positive pas- sage (the answer string may appear multiple times in one passage), combined with the log-likelihood of the positive passage being selected. We use the batch size of 16 for large (NQ, TriviaQA, SQuAD) and 4 for small (TREC, WQ) datasets, and tune k on the development set. For experiments on small datasets under the Multi setting, in which using other datasets is allowed, we ﬁne-tune the reader trained on Natural Questions to the target dataset. All experiments were done on eight 32GB GPUs. 6.2 Results Table 4 summarizes our ﬁnal end-to-end QA re- sults, measured by exact match with the reference answer after minor normalization as in (Chen et al., 2017; Lee et al., 2019). From the table, we can Training Model NQ TriviaQA WQ TREC SQuAD Single BM25+BERT (Lee et al., 2019) 26.5 47.1 17.7 21.3 33.2 Single ORQA (Lee et al., 2019) 33.3 45.0 36.4 30.1 20.2 Single HardEM (Min et al., 2019a) 28.1 50.9 - - - Single GraphRetriever (Min et al., 2019b) 34.5 56.0 36.4 - - Single PathRetriever (Asai et al., 2020) 32.6 - - - 56.5 Single REALMWiki (Guu et al., 2020) 39.2 - 40.2 46.8 - Single REALMNews (Guu et al., 2020) 40.4 - 40.7 42.9 - Single BM25 32.6 52.4 29.9"], "reference": "In an end-to-end QA system, a passage selection score assigns a value to each retrieved passage based on its relevance to the question. This score helps determine which passage is most likely to contain the correct answer.", "persona_name": "Research Scientist specializing in Natural Language Processing", "query_style": "MISSPELLED", "query_length": "LONG", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What is REALMWiki?", "reference_contexts": ["24.9 38.1 DPR 41.5 56.8 34.6 25.9 29.8 BM25+DPR 39.0 57.0 35.2 28.0 36.7 Multi DPR 41.5 56.8 42.4 49.4 24.1 BM25+DPR 38.8 57.9 41.1 50.6 35.8 Table 4: End-to-end QA (Exact Match) Accuracy. The ﬁrst block of results are copied from their cited papers. REALMWiki and REALMNews are the same model but pretrained on Wikipedia and CC-News, respectively. Single and Multi denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets (all except SQuAD). For WQ and TREC in the Multi setting, we ﬁne-tune the reader trained on NQ. see that higher retriever accuracy typically leads to better ﬁnal QA results: in all cases except SQuAD, answers extracted from the passages retrieved by DPR are more likely to be correct, compared to those from BM25. For large datasets like NQ and TriviaQA, models trained using multiple datasets (Multi) perform comparably to those trained using the individual training set (Single). Conversely, on smaller datasets like WQ and TREC, the multi- dataset setting has a clear advantage. Overall, our DPR-based models outperform the previous state- of-the-art results on four out of the ﬁve datasets, with 1% to 12% absolute differences in exact match accuracy. It is interesting to contrast our results to those of ORQA (Lee et al., 2019) and also the concurrently developed approach, REALM (Guu et al., 2020). While both methods include addi- tional pretraining tasks and employ an expensive end-to-end training regime, DPR manages to out- perform them on both NQ and TriviaQA, simply by focusing on learning a strong passage retrieval model using pairs of questions and answers. The additional pretraining tasks are likely more useful only when the target training sets are small. Al- though the results of DPR on WQ and TREC in the single-dataset setting are less competitive, adding more question–answer pairs helps boost the perfor- mance, achieving the new state of the art. To compare our pipeline training approach with joint learning, we run an ablation on Natural Ques- tions where the retriever and reader are jointly trained, following Lee et al. (2019). This approach obtains a score of 39.8 EM, which suggests that our strategy of training a strong retriever and reader in isolation can leverage effectively available supervi- sion, while outperforming a comparable joint train- ing approach with a simpler design (Appendix D). One thing worth noticing is that our reader does consider more passages compared to ORQA, al- though it is not completely clear how much more time it takes for inference. While DPR processes up to 100 passages for each question, the reader is able to ﬁt all of them into one batch on a sin- gle 32GB GPU, thus the latency remains almost identical to the single passage case (around 20ms). The exact impact on throughput is harder to mea- sure: ORQA uses 2-3x longer passages compared to DPR (288 word pieces compared to our 100 tokens) and the computational complexity is super- linear in passage length. We also note that we found k = 50 to be optimal for NQ, and k = 10 leads to only marginal loss in exact match accu- racy (40.8 vs. 41.5 EM on NQ), which should be roughly comparable to ORQA’s 5-passage setup. 7 Related Work Passage retrieval has been an important compo- nent for open-domain QA (Voorhees, 1999). It not only effectively reduces the search space for answer extraction, but also identiﬁes the support context for users to verify the answer. Strong sparse vector space models like TF-IDF or BM25 have been used as the standard method applied broadly to various QA tasks (e.g., Chen et al., 2017; Yang et al., 2019a,b; Nie et al., 2019; Min et al., 2019a; Wolfson et al., 2020). Augmenting text-based re- trieval with external structured information, such as knowledge graph and Wikipedia hyperlinks, has also been explored recently (Min et al., 2019b; Asai et al., 2020). The use of dense vector representations for re- trieval has a long history since Latent Semantic Analysis (Deerwester et al., 1990). Using labeled pairs of queries and documents, discriminatively trained dense encoders have become"], "reference": "REALMWiki is a model pretrained on Wikipedia.", "persona_name": "Research Scientist in Information Retrieval", "query_style": "POOR_GRAMMAR", "query_length": "SHORT", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "What significant contributions did Lee et al. make to the field of information retrieval, particularly in the context of dense retrieval models?", "reference_contexts": ["popular re- cently (Yih et al., 2011; Huang et al., 2013; Gillick et al., 2019), with applications to cross-lingual document retrieval, ad relevance prediction, Web search and entity retrieval. Such approaches com- plement the sparse vector methods as they can po- tentially give high similarity scores to semantically relevant text pairs, even without exact token match- ing. The dense representation alone, however, is typically inferior to the sparse one. While not the focus of this work, dense representations from pre- trained models, along with cross-attention mecha- nisms, have also been shown effective in passage or dialogue re-ranking tasks (Nogueira and Cho, 2019; Humeau et al., 2020). Finally, a concurrent work (Khattab and Zaharia, 2020) demonstrates the feasibility of full dense retrieval in IR tasks. Instead of employing the dual-encoder framework, they introduced a late-interaction operator on top of the BERT encoders. Dense retrieval for open-domain QA has been explored by Das et al. (2019), who propose to re- trieve relevant passages iteratively using reformu- lated question vectors. As an alternative approach that skips passage retrieval, Seo et al. (2019) pro- pose to encode candidate answer phrases as vectors and directly retrieve the answers to the input ques- tions efﬁciently. Using additional pretraining with the objective that matches surrogates of questions and relevant passages, Lee et al. (2019) jointly train the question encoder and reader. Their approach outperforms the BM25 plus reader paradigm on multiple open-domain QA datasets in QA accuracy, and is further extended by REALM (Guu et al., 2020), which includes tuning the passage encoder asynchronously by re-indexing the passages dur- ing training. The pretraining objective has also recently been improved by Xiong et al. (2020b). In contrast, our model provides a simple and yet effective solution that shows stronger empirical per- formance, without relying on additional pretraining or complex joint training schemes. DPR has also been used as an important mod- ule in very recent work. For instance, extending the idea of leveraging hard negatives, Xiong et al. (2020a) use the retrieval model trained in the pre- vious iteration to discover new negatives and con- struct a different set of examples in each training iteration. Starting from our trained DPR model, they show that the retrieval performance can be further improved. Recent work (Izacard and Grave, 2020; Lewis et al., 2020b) have also shown that DPR can be combined with generation models such as BART (Lewis et al., 2020a) and T5 (Raf- fel et al., 2019), achieving good performance on open-domain QA and other knowledge-intensive tasks. 8 Conclusion In this work, we demonstrated that dense retrieval can outperform and potentially replace the tradi- tional sparse retrieval component in open-domain question answering. While a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully. Moreover, our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional val- ues. As a result of improved retrieval performance, we obtained new state-of-the-art results on multiple open-domain question answering benchmarks. Acknowledgments We thank the anonymous reviewers for their helpful comments and suggestions."], "reference": "Lee et al. made a significant contribution by jointly training the question encoder and reader, which outperformed the BM25 plus reader paradigm on multiple open-domain QA datasets in terms of QA accuracy. Their approach was further extended by REALM (Guu et al., 2020), which included tuning the passage encoder asynchronously by re-indexing the passages during training.", "persona_name": "Research Scientist in Information Retrieval", "query_style": "PERFECT_GRAMMAR", "query_length": "LONG", "synthesizer_name": "single_hop_specific_query_synthesizer"}
{"user_input": "How does the sufficient context autorater help in reducing hallucinations in RAG systems?", "reference_contexts": ["<1-hop>\n\nSUFFICIENT CONTEXT: A NEW LENS ON RETRIEVAL AUGMENTED GENERATION SYSTEMS Hailey Joren∗ UC San Diego hjoren@ucsd.edu Jianyi Zhang† Duke University jianyi.zhang@duke.edu Chun-Sung Ferng Google csferng@google.com Da-Cheng Juan Google dacheng@google.com Ankur Taly Google ataly@google.com Cyrus Rashtchian Google cyroid@google.com ABSTRACT Augmenting LLMs with context leads to improved performance across many applications. Despite much research on Retrieval Augmented Generation (RAG) systems, an open question is whether errors arise because LLMs fail to utilize the context from retrieval or the context itself is insufficient to answer the query. To shed light on this, we develop a new notion of sufficient context, along with a method to classify instances that have enough information to answer the query. We then use sufficient context to analyze several models and datasets. By stratifying errors based on context sufficiency, we find that larger models with higher baseline performance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when the context is sufficient, but often output incorrect answers instead of abstaining when the context is not. On the other hand, smaller models with lower baseline performance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient context. We further categorize cases when the context is useful, and improves accuracy, even though it does not fully answer the query and the model errs without the context. Building on our findings, we explore ways to reduce hallucinations in RAG systems, including a new selective generation method that leverages sufficient context information for guided abstention. Our method improves the fraction of correct answers among times where the model responds by 2–10% for Gemini, GPT, and Gemma. Key findings and the prompts used in our autorater analysis are available on our github. 1 INTRODUCTION Providing Large Language Models (LLMs) with additional context, such as in Retrieval Augmented Generation (RAG) systems, has led to major improvements in LLM factuality and verifiability when adapting to new domains (Lewis et al., 2020). In the case of open-domain question answering, a retrieval model provides context at inference time in the form of snippets or long-form text (Zhu et al., 2021). Then, the model synthesizes the query along with this added context to generate the answer. Unfortunately, current RAG-based LLMs exhibit many undesirable traits, such as confidently predicting the incorrect answer with retrieved evidence (Mishra et al., 2024; Niu et al., 2024; Ru et al., 2024), being distracted by unrelated information (Cuconasu et al., 2024; Yoran et al., 2024), and failing to properly extract answers from long text snippets (Hsieh et al., 2024; Liu et al., 2024). The ideal outcome is for the LLM to output the correct answer if the provided context contains enough information to answer the question when combined with the model’s parametric knowledge. Otherwise, the model should abstain from answering and/or ask for more information. One core challenge in achieving this ideal outcome is building models that can use the provided context only when it helps answer the question correctly. Several works have investigated this issue by evaluating ∗Work done during an internship at Google. †Work done during an internship at Google. 1 arXiv:2411.06037v3 [cs.CL] 23 Apr 2025 Published as a conference paper at ICLR 2025 models in the presence of irrelevant information in the context (discussed in Section 2). However, “relevant information” can range from directly containing the answer to simply being topically related to the question. Even “golden” or oracle documents in datasets vary in how much information they provide about the query, and whether they directly inform the ground truth answer or not. In other words, while the goal seems to be to understand how LLMs behave when they do or do not have sufficient information to answer the query, prior work fails to address this head-on. As our first contribution, we put forth a new notion of sufficient context. We divide instances into two categories based on whether the context provides enough information to construct an answer to the query. The sufficient context designation is a function of an input pair consisting of one question and the associated context. Crucially, it does not require a ground truth", "<2-hop>\n\nanswer. Figure 1 shows examples and a breakdown of model responses after splitting the data based on sufficient vs. insufficient context. To divide the dataset, we use an LLM-based autorater to classify instances as sufficient or not. Here, an autorater is a model that evaluates instances based on a property, e.g., a sufficient context autorater. Using our sufficient context autorater, we uncover new insights into LLM behavior and into existing benchmark datasets. First, we find models generate incorrect answers on a non-trivial fraction of instances that have sufficient context to answer the query. In other words, open-book QA cannot be solved by improving retrieval alone. Second, when given instances without sufficient context, models tend to hallucinate more than they abstain, especially for multi-hop questions. This finding complements prior work, which shows that LLMs are not robust to noisy retrieval (Yoran et al., 2024; Wu et al., 2024). Third, models generate correct answers in many cases, even when the provided context is insufficient. Surprisingly, this remains true after we filter out questions that the model answers correctly in a closed book (w/o RAG) setting. Together, our analysis deepens our understanding of RAG systems by revealing nuances in how models generate responses with retrieval. As a final contribution, we explore ways to use sufficient context labels to reduce model hallucinations. We implement a new selective generation framework that improves accuracy. We use a smaller, intervention model to determine when the model generates or abstains, providing a controllable trade-off. Moreover, we can combine our method with any LLM, including proprietary models like Gemini and GPT. Our main result is that using sufficient context as an additional signal leads to much higher accuracy over the fraction of answered queries, for most coverage levels and across multiple models/datasets. We also find that fine-tuning open-source models with sufficient context information does not easily reduce the hallucination rate. Instead, for Mistral 3, fine-tuning can lead to a higher abstention rate at the cost of fewer correct answers. Key findings and the prompts used in our autorater analysis are available on our github. To summarize, our main contributions are 1. We define the notion of sufficient context, unifying existing work on relevance for RAG systems. Then, we design a sufficient context autorater (achieving 93% accuracy), enabling us to label instances scalably and to analyze model responses with or without sufficient context. 2. Our analysis leads to several new findings about retrieval-augmented model performance. One takeaway is that SOTA LLMs output correct responses 35–62% of the time with insufficient context. Hence, intervention strategies to increase accuracy should not solely rely on sufficiency. 3. Building on our findings above, we develop an efficient and general method for selective generation, using both confidence and sufficient context signals. Our method improves the fraction of correct answers (among total model responses) by up to 2–10% for Gemini, GPT, and Gemma. 2 RELATED WORK Many papers have shown that reaping the benefits of RAG (e.g., better factuality) will require a deeper understanding of how LLMs respond to variations in the queries and provided context (Asai et al., 2024; Fan et al., 2024; Ram et al., 2023; Rau et al., 2024). We review two main areas. First, much work has evaluated RAG systems with poor retrieval, uncovering cases where LLMs are led astray by irrelevant context. Another line of study aims to reduce LLM hallucinations in RAG settings. (Ir)relevant Context. Prior studies uncover a lack of robustness to imperfect retrieval. However, these studies vary in terms of how they evaluate retrieval quality, without anchoring to a precise “relevance” definition. Shi et al. (2023a) adds sentences to math questions (based on GSM8K) which 2 Published as a conference paper at ICLR 2025 Question: Who is Lya L. married to? Insufficient Context Lya L. married Tom in 2006… They divorced in 2014… Lya went on dates with Paul in 2018… Context C Sufficient Context Lya L. married Paul in 2020… They looked happy together at the recent event. Context A Examples: sufficient context Insufficient Context Lya L. is an astronaut, born in Ohio…. Lya has two children… Lya’s parents are lawyers… Context D Sufficient Context Lya L. – Wikipedia Born: October 1, 1980 Spouse: Paul (m. 2020) Context B Categorizing Model Responses (Musique Dataset) Figure 1: New insights into RAG systems", "<3-hop>\n\nTECHNIQUES TO REDUCE HALLUCINATIONS WITH RAG From our previous analysis, we have seen that models may hallucinate rather than abstain and that this happens more with RAG than in a closed-book setting. A natural next question is whether we can prompt or fine-tune a model to perform closer to the ideal case. Can we steer the model to either output the correct answer or abstain, while hallucinating an incorrect answer as little as possible? 5.1 SELECTIVE RAG USING SUFFICIENT CONTEXT SIGNAL One simple solution to improving RAG performance would be to use the sufficient context autorater to abstain given insufficient context. However, this heavy-handed approach can lower overall performance, since all models answer some questions correctly even with insufficient context, as described in Table 2 and demonstrated in Figure 3. Instead, we propose a method for combining 8 Published as a conference paper at ICLR 2025 the sufficient context autorater outputs with model self-rated confidence scores to tune a selective accuracy-coverage trade-off, where “coverage” denotes the portion of inputs on which the model does not abstain. Specifically, we use these signals to train a simple linear model to predict hallucinations, and then use it to set coverage-accuracy trade-off thresholds. This mechanism differs from other strategies for improving abstention in two key ways. First, because it operates independently from generation, it mitigates unintended downstream effects, whereas strategies like fine-tuning to improve abstention can inadvertently worsen performance on certain inputs (see Section 5.2). Second, it offers a controllable mechanism for tuning abstention, which allows for different operating settings in differing applications, such as strict accuracy compliance in medical domains or maximal coverage on creative generation tasks. Abstention Signals We utilize two main signals for abstention: the self-rated probabilities as in Li et al. (2024); Kadavath et al. (2022) and the sufficient context autorater. For the self-rated probabilities, we use two strategies: P(True) and P(Correct). P(True) requires sampling answers from the model multiple times, and then prompting the model multiple times to label each model as correct or incorrect, resulting in a final probability of correctness associated with each question as in Kadavath et al. (2022). For proprietary models, where extensive querying is prohibitively expensive, we use P(Correct) instead. We adapt the probability-generating prompt from Li et al. (2024) to obtain the model’s response and its estimated probability of correctness. For the sufficient context signal, we use the binary label from an autorater. Our hypothesis is that combining these signals should lead to more effective abstention, particularly in cases where the context is insufficient. Methods. We calculate P(True) by sampling 20 responses for each question and querying the model 5 times to evaluate whether the answer is correct or incorrect (without using the ground truth) as in Kadavath et al. (2022). For P(Correct), the prompt requests the most likely and second most likely answers along with their probabilities. We use string matching to extract the response and self-predicted probability, keeping the one with the highest probability. To determine sufficient context, we use FLAMe, a small and efficient model for determining the sufficient context label. We divide the retrievals into chunks of 1600 tokens to fit in the context window and label the context as sufficient if any of these chunks are sufficient. We combine the binary sufficient context label with the self-rated answer probability (P(True) for open-source models or P(Correct) for proprietary models) in a simple logistic regression model to predict hallucinations with 100 iterations of random hyperparameter search. At inference time, we use the logistic regression model scores to threshold the outputs, abstaining when the score is below a chosen threshold as in Joren et al. (2024). We measure the added value for selective accuracy of the sufficient context signal (purple line in Figure 4) by comparing it with the model self-rated confidence alone (gray line). Results. We find that our approach leads to a better selective accuracy-coverage trade-off compared to using model confidence alone. In particular, see gains of over 10% for Gemma 27B on HotpotQA in the highest accuracy regions, and gains of over 5% for Gemini 1.5 Pro on the same dataset near the 70% coverage region. These", "<4-hop>\n\nCONCLUSION\nOur work provided a new lens on LLM responses in RAG systems centered around our notion of\nsufficient context. We constructed a sufficient context autorater, which enabled scalable insights into\nmodel performance on different types of instances. Our analysis revealed that even with sufficient\ncontext, LLMs frequently hallucinate answers. We also found, surprisingly, many cases where\na model will output a correct answer with access to only insufficient context. Qualitatively, we\ncategorized such instances, leading to a fuller picture of ways context can be useful. Finally, we\ndemonstrated a general-purpose selective generation method, which applies to Gemini, GPT, and\nGemma, and can reduce hallucinations by 2–10% on queries that the model answers.\nLimitations. Our analysis focuses on QA datasets, but summarization tasks also utilize context,\nwhich may or may not be sufficient. For example, models may behave differently on the prompt\n“Summarize the reviews of 5-star hotels in Mallorca” depending on whether the context mentions the\nhotel reviews, whether they are for 5-star hotels, etc. Another shortcoming is an exploration of how\noften different retrieval methods lead to sufficient context. Also to achieve the best performance, we\ncould have used our autorater to iteratively judge whether to retrieve more or answer the question.\nFuture Work. One direction is a fine-grained sufficient context autorater, which outputs a score\ninstead of a binary label. This could be useful for ranking contexts after the retrieval step. Another\ndirection is to extend the definition of sufficient context to multi-modal RAG settings, such as for\nvisual QA (images) or document QA (pdf files). Finally, our selective generation results suggest that\nthere is room for improvement in reducing hallucinations by using auxiliary signals from the inputs.\n10\n\fPublished as a conference paper at ICLR 2025\nACKNOWLEDGMENTS\nWe thank Hrishikesh Garud, Vikram Gopali, Xun Sun, and Bruce Wang for annotating data. We\nthank Ranjay Krishna and Jacob Eisenstein for helpful discussions. We also thank Alyshia Olsen for\nhelp with the figure design and color palette. We thank the anonymous reviewers for suggestions to\nimprove the presentation.\n"], "reference": "The sufficient context autorater helps in reducing hallucinations in RAG systems by providing a binary label for whether the context is sufficient to answer the query. This label is then combined with model self-rated confidence scores to tune a selective accuracy-coverage trade-off, where 'coverage' denotes the portion of inputs on which the model does not abstain. By using these signals to train a simple logistic regression model, the approach leads to a better selective accuracy-coverage trade-off compared to using model confidence alone.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How does query rewriting improve the performance of retrieval-augmented language models in handling factual information?", "reference_contexts": ["<1-hop>\n\nIntroduction Large Language Models (LLMs) have shown re- markable abilities for human language processing and extraordinary scalability and adaptability in few- or zero-shot settings.(Ouyang et al., 2022; Brown et al., 2020; Chowdhery et al., 2022). How- ever, the training process depends on large-scale high-quality corpora but without the perception ∗Work done during an internship at 3Microsoft Research Asia. # Equal contribution. †Corresponding author. This paper was partially supported by Joint Research Project of Yangtze River Delta Science and Technology Inno- vation Community (No. 2022CSJGG1400). 1https://github.com/xbmxb/RAG-query-rewriting of the real world. Thus, LLMs still have to face the issue of hallucination (Yao et al., 2023; Bang et al., 2023) and temporal misalignment (Röttger and Pierrehumbert, 2021; Luu et al., 2022; Jang et al., 2022). This affects the reliability of LLMs and hinders wider practical application, because the consistency between the LLM responses with the real world needs further validation. Exist- ing work has proved that incorporating external knowledge (i.e., non-parametric knowledge) with internal knowledge (i.e., parametric knowledge) can effectively alleviate hallucination, especially for knowledge-intensive tasks. In fact, retrieval- augmented LLMs have been shown so effective that they have been regarded as a standard solu- tion to alleviate the factuality drawbacks in naive LLM generations. Retrieval augmentation is ap- plied to select relative passages as external contexts for the language model, which is retrieve-then-read framework (Lewis et al., 2020b; Karpukhin et al., 2020; Izacard et al., 2022). Take the open-domain Question-Answering task (open-domain QA) as an example, a retriever first searches for related documents for a question. Then the LLM receives the question and the documents, then predicts an answer. As most LLMs are only accessible through infer- ence APIs, they play the part of black-box frozen readers in the pipeline. This makes previous re- trieval augmentation methods that require complete access (Lewis et al., 2020b; Guu et al., 2020; Izac- ard et al., 2022) no longer feasible. Recent studies on retrieval-augmented language models lean more on the LLM-oriented adaptation. An idea is to train a dense retrieval model to cater to the frozen lan- guage model (Shi et al., 2023). By using feedback from the LLM as a training objective, the retrieval model is tuned for better LLM input contexts. An- other research line focuses on the design of inter- actions between the retriever and the reader (Yao et al., 2023; Khattab et al., 2022), where both the arXiv:2305.14283v3 [cs.CL] 23 Oct 2023 retriever and the reader are usually frozen. The idea is to trigger the emergent ability through carefully crafted prompts or a sophisticated prompt pipeline. Multiple interactions with external knowledge al- low the LLM to approach the correct answer step by step. However, there are still problems remaining to be solved. Existing approaches overlook the adap- tation of the query, i.e., the input of the retrieve- then-read pipeline. The retrieval query is either original from datasets or directly determined by the black-box generation, thus is always fixed. How- ever, there is inevitably a gap between the input text and the knowledge that is really needed to query. This limits performance and places a burden on retrieval capability enhancement and prompt engineering. In consideration of this issue, this paper pro- poses Rewrite-Retrieve-Read, a new framework for retrieval augmentation, which can be further tuned for adapting to LLMs. In front of the retriever, a step of rewriting the input is added, filling the gap between the given input and retrieval need, as is shown in Figure 1. We adopt the off-the-shelf tool, an internet search engine, as the retriever, which avoids the maintenance of the search index and can access up-to-date knowledge (Lazaridou et al., 2022). Different from", "<2-hop>\n\nprevious studies (Khattab et al., 2022; Yao et al., 2023) that require the mem- ory of multiple interaction rounds between the re- triever and the LLM for each sample, the motiva- tion of our rewriting step is to clarify the retrieval need from the input text. We also propose a trainable scheme for our rewrite-retrieve-read framework (Figure 1 (c)). The black-box retriever and the reader form a frozen system. To further smooth the steps of our pipeline, we apply a small, trainable language model to perform the rewriting step, denoted as the rewriter. The rewriter is trained by reinforcement learning using the LLM performance as a reward, learning to adapt the retrieval query to improve the reader on downstream tasks. Our proposed methods are evaluated on knowledge-intensive downstream tasks including open-domain QA (HotpoQA (Yang et al., 2018), AmbigNQ (Min et al., 2020), PopQA (Mallen et al., 2022)) and multiple choice QA (MMLU (Hendrycks et al., 2021)). The experiments are implemented on T5-large (Raffel et al., 2020) as the rewriter, ChatGPT (Ouyang et al., 2022) and Vicuna-13B (Chiang et al., 2023) as the LLM reader. The results show that query rewriting con- sistently improves the retrieve-augmented LLM performance. The results also indicate that the smaller language model can be competent for query rewriting. To sum up, our proposed novel retrieval- augmentation method, rewrite-retrieve-read is the first framework where the input text is adapted for the frozen retriever and LLM reader. We introduce a tuneable scheme with a small, trainable model, achieving performance gains with less resource consumption. 2 Related Work 2.1 Retrieval Augmentation Language models require external knowledge to al- leviate the factuality drawbacks. Retrieval augmen- tation has been regarded as the standard effective solution. With a retrieval module, related passages are provided to the language model as the context of the original input. Thus factual information like common sense or real-time news helps with output prediction through contextualized reading compre- hension. Earlier studies use sparse retriever (Chen et al., 2017) or dense retriever (Karpukhin et al., 2020) in front of a pre-trained language model (PrLM). The neural retriever and reader are both PrLMs of trainable size like BERT (Devlin et al., 2019) or BART (Lewis et al., 2020a). Hence, the whole retrieve-then-reader framework is a tuneable end- to-end system, where the retrieved contexts can be regarded as the intermediate results (Karpukhin et al., 2020; Lewis et al., 2020b). Approaches to smooth the two-step framework are proposed to op- timize the retrieval and the reading comprehension (Sachan et al., 2021; Lee et al., 2022; Jiang et al., 2022). More recently, retrieval remains a powerful enhancement as the size of models and data scales rapidly (Mallen et al., 2022; Shi et al., 2023; Brown et al., 2020). On the other hand, retrieval enhance- ment can compensate for the shortfall in parameter size, compared to large-scale language models. For example, by jointly training the retriever and the reader, Atlas (Izacard et al., 2022) shows few-shot performance on par with 540B PalM (Chowdhery et al., 2022) but be of 50× smaller size. The Internet as a knowledge base More related to our work, the search engine can assume the role of the retriever and use the Internet as the source of Input Retriever Output Documents Input Web Search Documents Black-box LLM Query Input Documents Query Output Output Reward Input: What profession does Nicholas Ray and Elia Kazan have in common? Query: Nicholas Ray profession Nicholas Ray American author and director, original name Raymond Nicholas Kienzle, born August 7, 1911, Galesville, Wisconsin, U.S...... director Rewriter Retriever Black-box LLM Reader Black-box LLM Reader (a) Retrieve-then-read (b)Rewrite-retrieve-read (c) Trainable rewrite-retrieve-read Black-box LLM Reader Web Search Retriever Rewriter Small PrLM Example Query: Elia Kazan profession Elia Kazan was an American film and theatre director, producer, screenwriter and actor, described ...... Correct (reader ) Hit (retriever )✅ ✅ Figure 1: Overview of our proposed pipeline. From left to right,", "<3-hop>\n\nneed, as is shown in Figure 1. We adopt the off-the-shelf tool, an internet search engine, as the retriever, which avoids the maintenance of the search index and can access up-to-date knowledge (Lazaridou et al., 2022). Different from previous studies (Khattab et al., 2022; Yao et al., 2023) that require the mem- ory of multiple interaction rounds between the re- triever and the LLM for each sample, the motiva- tion of our rewriting step is to clarify the retrieval need from the input text. We also propose a trainable scheme for our rewrite-retrieve-read framework (Figure 1 (c)). The black-box retriever and the reader form a frozen system. To further smooth the steps of our pipeline, we apply a small, trainable language model to perform the rewriting step, denoted as the rewriter. The rewriter is trained by reinforcement learning using the LLM performance as a reward, learning to adapt the retrieval query to improve the reader on downstream tasks. Our proposed methods are evaluated on knowledge-intensive downstream tasks including open-domain QA (HotpoQA (Yang et al., 2018), AmbigNQ (Min et al., 2020), PopQA (Mallen et al., 2022)) and multiple choice QA (MMLU (Hendrycks et al., 2021)). The experiments are implemented on T5-large (Raffel et al., 2020) as the rewriter, ChatGPT (Ouyang et al., 2022) and Vicuna-13B (Chiang et al., 2023) as the LLM reader. The results show that query rewriting con- sistently improves the retrieve-augmented LLM performance. The results also indicate that the smaller language model can be competent for query rewriting. To sum up, our proposed novel retrieval- augmentation method, rewrite-retrieve-read is the first framework where the input text is adapted for the frozen retriever and LLM reader. We introduce a tuneable scheme with a small, trainable model, achieving performance gains with less resource consumption. 2 Related Work 2.1 Retrieval Augmentation Language models require external knowledge to al- leviate the factuality drawbacks. Retrieval augmen- tation has been regarded as the standard effective solution. With a retrieval module, related passages are provided to the language model as the context of the original input. Thus factual information like common sense or real-time news helps with output prediction through contextualized reading compre- hension. Earlier studies use sparse retriever (Chen et al., 2017) or dense retriever (Karpukhin et al., 2020) in front of a pre-trained language model (PrLM). The neural retriever and reader are both PrLMs of trainable size like BERT (Devlin et al., 2019) or BART (Lewis et al., 2020a). Hence, the whole retrieve-then-reader framework is a tuneable end- to-end system, where the retrieved contexts can be regarded as the intermediate results (Karpukhin et al., 2020; Lewis et al., 2020b). Approaches to smooth the two-step framework are proposed to op- timize the retrieval and the reading comprehension (Sachan et al., 2021; Lee et al., 2022; Jiang et al., 2022). More recently, retrieval remains a powerful enhancement as the size of models and data scales rapidly (Mallen et al., 2022; Shi et al., 2023; Brown et al., 2020). On the other hand, retrieval enhance- ment can compensate for the shortfall in parameter size, compared to large-scale language models. For example, by jointly training the retriever and the reader, Atlas (Izacard et al., 2022) shows few-shot performance on par with 540B PalM (Chowdhery et al., 2022) but be of 50× smaller size. The Internet as a knowledge base More related to our work, the search engine can assume the role of the retriever and use the Internet as the source of Input Retriever Output Documents Input Web Search Documents Black-box LLM Query Input Documents Query Output Output Reward Input: What profession does Nicholas Ray and Elia Kazan have in common? Query: Nicholas Ray profession Nicholas Ray American author and director, original name Raymond Nicholas Kienzle, born August 7, 1911, Galesville, Wisconsin, U.S...... director Rewriter Retriever Black-box LLM Reader Black-box", "<4-hop>\n\nsetting). The scores show the approximate upper bound ability of the reader with retrieval aug- mentation, abbreviated as the “upper bound” score. The effectiveness of retrieval is proved compared to the no retrieval setting (the first line). For each retrieval method, two settings are presented: (i) collecting Bing snippets, (ii) selecting from URLs by BM25. The metrics show that content selection with BM25 recalls better documents than snippets, 2Our trainable rewriter is adapted to the retriever using BM25 during RL training. Using the output queries of the test set after training, the snippet hit rate is 73.4%. Example 1: multi-hop question Q0: The youngest daughter of Lady Mary-Gaye Curzon stars with Douglas Smith and Lucien Laviscount in what 2017 film? Q1: the youngest daughter of Lady Mary-Gaye Curzon; 2017 film stars Douglas Smith and Lucien Laviscount Q2: Lady Mary-Gaye Curzon youngest daughter 2017 film with Douglas Smith and Lucien Laviscount Example 2: Q1: movie \"All Star\" 2000 Example 3: multiple choice Hit Correct Q0: A car-manufacturing factory is considering a new site for its next plant. Which of the following would community planners be most concerned with before allowing the plant to be built? Options: A. The amount of materials stored in the plant B. The hours of operations of the new plant C. The effect the plant will have on the environment D. The work environment for the employees at the plant Q1: What would community planners be most concerned with before allowing a car- manufacturing factory to be built? Q2: 2000 movie \"All Star\" song Q0: What 2000 movie does the song \"All Star\" appear in? ✅ ✅ ✅ ✅ ✅ ✅ ✅ ✅ ❌ ❌ ❌ ❌ ❌ ❌ ❌ ❌ Figure 3: Examples for intuitive illustration. Q0 denotes original input, Q1 is from the LLM rewriter, and Q2 is from the trained T5 rewriter. Hit means retriever recall the answer, while Correct is for the reader output. while query rewriting makes progress on both set- tings. We also observed that the improvement in the hit rate of the retriever is more significant than the improvement in the reader. This is consistent with the findings in related search (Mallen et al., 2022; Liu et al., 2023). 6.3 Case Study To intuitively show how the query rewriting makes a difference in the retrieved contexts and prediction performance, we present examples in Figure 3 to compare the original questions and the queries. In example 1, the original question asks for a film that the youngest daughter of Lady Mary-Gaye Curzon co-stars with two certain actors. Both query 1 and query 2 put the keyword film forward, closely fol- lowing the youngest daughter of Lady Mary-Gaye Curzon. With both, the actress Charlotte Calthorpe and her movie information can be retrieved and the answer is included. The second is an example where the query from the LLM rewriter failed but the query from T5 gets the correct answer. The number 2000 is misunderstood in query 1, while query 2 keeps 200 movie together, avoiding mean- ingless retrieval. Example 3 is for multiple choice. The query simplifies the background and enhances the keyword community planner. The retrieve con- texts are mainly about Introduction to Community Planning where the answer environment appears several times. 7 Conclusion This paper introduces the Rewrite-Retrieve-Read pipeline, where a query rewriting step is added for the retrieval-augmented LLM. This approach is applicable for adopting a frozen large language model as the reader and a real-time web search engine as the retriever. Further, we propose to ap- ply a tuneable small language model the rewriter, which can be trained to cater to the frozen retriever and reader. The training implementation consists of two stages, warm-up and reinforcement learn- ing. Evaluation and analyses on open-domain QA and multiple-choice QA show the effectiveness of query rewriting. Our work proposes a novel retrieval-augmented black-box LLM framework, proves that the retrieval augmentation can be en- hanced from the aspect of query rewriting, and provides a new method for integrating trainable modules into black-box LLMs. Limitations We acknowledge the limitations of this work. (i) There is still a trade-off between generalization and specialization among downstream tasks. Adding a training process, the scalability to direct transfer is compromised, compared to few-shot in-context learning. (ii) The research line of LLM agent has shown impressive performance but relies on mul- tiple calls to the LLM for each sample (Khattab et al.,", "<5-hop>\n\nBenchmarking Large Language Models in Retrieval-Augmented Generation\nJiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2\n1Chinese Information Processing Laboratory\n2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n{jiawei2020,hongyu,xianpei,sunle}@iscas.ac.cn\nAbstract\nRetrieval-Augmented Generation (RAG) is a promising ap-\nproach for mitigating the hallucination of large language\nmodels (LLMs). However, existing research lacks rigorous\nevaluation of the impact of retrieval-augmented generation\non different large language models, which make it challeng-\ning to identify the potential bottlenecks in the capabilities\nof RAG for different LLMs. In this paper, we systemati-\ncally investigate the impact of Retrieval-Augmented Gener-\nation on large language models. We analyze the performance\nof different large language models in 4 fundamental abili-\nties required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robust-\nness. To this end, we establish Retrieval-Augmented Genera-\ntion Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the afore-\nmentioned fundamental abilities required to resolve the case.\nThen we evaluate 6 representative LLMs on RGB to diag-\nnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\nof noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to ef-\nfectively apply RAG to LLMs.\n Introduction\nRecently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges includ-\ning factual hallucination (Cao et al. 2020; Raunak, Menezes,\nand Junczys-Dowmunt 2021; Ji et al. 2023), knowledge out-\ndating (He, Zhang, and Roth 2022), and the lack of domain-\nspecific expertise (Li et al. 2023c; Shen et al. 2023).\nIncorporating external knowledge via information re-\ntrieval, i.e., Retrieval-Augmented Generation (RAG), has\nbeen regarded as a promising way to resolve the above chal-\nlenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\n* Corresponding authors.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n"], "reference": "Query rewriting improves the performance of retrieval-augmented language models by adapting the retrieval query to better match the input text, thus enhancing the reader's ability to retrieve relevant and accurate information. This leads to consistent improvements in the model's performance on downstream tasks such as open-domain QA and multiple-choice QA.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How does the combination of parametric memory with non-parametric memory in RAG models improve their performance on knowledge-intensive NLP tasks?", "reference_contexts": ["<1-hop>\n\nIntroduction Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results, arXiv:2005.11401v4 [cs.CL] 12 Apr 2021 The Divine Comedy (x) q Query Encoder q(x) MIPS pθ Generator pθ (Parametric) Margin- alize This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\" (y) End-to-End Backprop through q and pθ Barack Obama was born in Hawaii.(x) Fact Veriﬁcation: Fact Query supports (y) Question Generation Fact Veriﬁcation: Label Generation Document Index Define \"middle ear\"(x) Question Answering: Question Query The middle ear includes the tympanic cavity and the three ossicles. (y) Question Answering: Answer Generation Retriever pη (Non-Parametric) z4 z3 z2 z1 d(z) Jeopardy Question Generation: Answer Query Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For ﬁnal prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack- augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera- tion for knowledge-intensive tasks—tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent", "<2-hop>\n\nRetrieval-Augmented Generation for\nKnowledge-Intensive NLP Tasks\nPatrick Lewis†‡, Ethan Perez⋆,\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n†Facebook AI Research; ‡University College London; ⋆New York University;\nplewis@fb.com\nAbstract\nLarge pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\nedge is still limited, and hence on knowledge-intensive tasks, their performance\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\ndecisions and updating their world knowledge remain open research problems. Pre-\ntrained models with a differentiable access mechanism to explicit non-parametric\nmemory have so far been only investigated for extractive downstream tasks. We\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n(RAG) — models which combine pre-trained parametric and non-parametric mem-\nory for language generation. We introduce RAG models where the parametric\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\npare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, and another which can use different passages\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\nseq2seq baseline.\n1\n approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1 2", "<3-hop>\n\ndata. While retrieval-augmented generation has been widely explored in the NLP community, we sug- gest that future research could extend this approach to tasks that involve data from multiple modali- ties. For instance, with recent advancements in image-text retrieval (Jia et al., 2021; Radford et al., 2021), the structural gap between images and texts is largely bridged. Some early studies (Zhang et al., 2020) have shown that information retrieved from images could improve the performance of neural machine translation model. Naturally, such meth- ods could be extended to other multi-modal tasks, such as image captioning (Karpathy and Li, 2015). A similar idea could also be applied to tasks be- yond images, such as speech-to-text transcription (Gales and Young, 2007). 6 Future Directions Despite the current success of retrieval augmented text generation, there is still a long way to go as discussed in previous sections. We highlight some directions to facilitate the future research as fol- lows: Retrieval Sensitivity The performance of re- trieval augmented text generation is very sensitive to the retrieval quality, i.e., the similarity between the query and the retrieved examples. Currently, re- trieval augmented text generation models perform well when the retrieved examples are very simi- lar to the query. However, they are even worse than the generation models without retrieval when the retrieval examples are less similar. Therefore, it would be important to exploit new methods to address such an issue on similarity. Retrieval Efﬁciency Generally, if one enlarges the retrieval memory to some extent, it would be possible to retrieve an example which is very simi- lar to the query.Unfortunately, the downside is that the overall inference for the retrieval augmented generation models is less efﬁcient due the consid- erable retrieval overhead. In this sense, it is urgent to consider some methods to trade off the retrieval memory size and retrieval efﬁciency, for example, data compression for the retrieval memory. Local vs. Global Optimization Theoretically, it seems promising to jointly learn retrieval metrics and generation models. However, in practice, there is an essential gap about the retrieval metric be- tween the training and inference phrases. In the training phase, the loss is locally back-propagated to only a few retrieved examples while in the infer- ence phase the metric is globally conducted among all examples in the memory. It would be interesting to narrow such a gap when learning a better metric for generation tasks. Multi-Modalities With recent advancement in image-text retrieval, directly associating images with relevant text becomes possible. This urges researchers to investigate the possibility of retrieval- based text generation in tasks that involve data from different modalities. One typical task is image captioning. Beyond images, other tasks like speech- to-text transcription could potentially beneﬁt from retrieval-based generation methods as well. Diverse & Controllable Retrieval Most of the existing approaches adopt a universal metric for retrieval, such as lexical similarities of sentences. Future work should explore how to use customized metrics for retrieval. This can be beneﬁcial for more controlled text generation. For example, in- stances with emotions and styles may be more de- sirable in the personalized dialogue generation, par- allel data that contains speciﬁc terminologies is more helpful in machine translation, and so on. On the other hand, using a universal metric for retrieval may lead to the lack of diversity of the retrieval re- sults. Collecting a diverse set of retrieval results can improve the coverage of useful information. Thus, considering multiple different metrics for re- trieval may lead to generation with higher quality in the future. 7 Conclusion In this paper, we surveyed recent approaches for retrieval-augmented text generation. We reviewed and summarized the development of different com- ponents of retrieval-augmented text generation in- cluding retrieval metrics, retrieval sources, and in- tegration paradigms. We gave in-depth discussions when retrieval-augmented text generation comes to different applications including dialogue response generation, machine translation, and other genera- tion tasks. We also pointed out some future direc- tions for retrieval-augmented text generation."], "reference": "The combination of parametric memory with non-parametric memory in RAG models improves their performance on knowledge-intensive NLP tasks by allowing for the expansion and revision of knowledge, as well as direct access to and inspection of accessed knowledge. This hybrid approach addresses some limitations of pre-trained neural language models, which cannot easily expand or revise their memory and may produce 'hallucinations'. RAG models achieve state-of-the-art results on open-domain extractive question answering tasks such as Natural Questions, WebQuestions, and CuratedTrec.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "What are some notable approaches for retrieval-augmented text generation tasks and how do they leverage deep learning technology?", "reference_contexts": ["<1-hop>\n\nA Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\n", "<2-hop>\n\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-\ngrating translation memory to NMT models (Gu\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\nHe et al., 2021). We also review the applications\nof retrieval-augmented generation in other genera-\ntion tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n2\n"], "reference": "Notable approaches for retrieval-augmented text generation tasks include exemplar/template retrieval for dialogue response generation, integrating translation memory into NMT models for machine translation, and applying retrieval-augmented generation in other tasks such as abstractive summarization, code generation, paraphrase, and knowledge-intensive generation. These approaches leverage deep learning technology to enhance the scalability and effectiveness of text generation.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How does VisRAG enhance the retrieval and generation stages of RAG by utilizing vision-language models?", "reference_contexts": ["<1-hop>\n\nI. INTRODUCTION L ARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing “hallucinations” [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown Corresponding Author.Email:haofen.wang@tongji.edu.cn 1Resources are available at https://github.com/Tongji-KGLLM/ RAG-Survey in Figure 1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG’s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]–[5].The subsequent arrival of ChatGPT [6] marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more com- plex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques. The burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of “Retrieval,” “Generation,” and “Augmentation.” On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: • In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, arXiv:2312.10997v5 [cs.CL] 27 Mar 2024 2 Fig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. This review contex- tualizes the broader scope of RAG research within the landscape of LLMs. • We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of “Retrieval”, “Generation” and “Augmentation”, and delve into their synergies, elucidating how these com- ponents intricately collaborate to form a cohesive and effective RAG framework. • We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives", "<2-hop>\n\ndownstream tasks. The retriever is fine-tuned with two types of supervised signals: hard labels for the dataset and soft rewards from the LLMs. This dual-signal approach fosters a more effective fine-tuning process, tailoring the embedding model to diverse downstream applications. REPLUG [72] utilizes a retriever and an LLM to calculate the probability distributions of the retrieved documents and then performs supervised training by computing the KL divergence. This straightforward and effective training method enhances the performance of the retrieval model by using an LM as the supervisory signal, eliminating the need for specific cross-attention mechanisms. Moreover, inspired by RLHF (Reinforcement Learning from Human Feedback), utilizing LM-based feedback to reinforce the retriever through reinforcement learning. E. Adapter Fine-tuning models may present challenges, such as in- tegrating functionality through an API or addressing con- straints arising from limited local computational resources. Consequently, some approaches opt to incorporate an external adapter to aid in alignment. To optimize the multi-task capabilities of LLM, UP- RISE [20] trained a lightweight prompt retriever that can automatically retrieve prompts from a pre-built prompt pool that are suitable for a given zero-shot task input. AAR (Augmentation-Adapted Retriver) [47] introduces a universal adapter designed to accommodate multiple downstream tasks. While PRCA [69] add a pluggable reward-driven contextual adapter to enhance performance on specific tasks. BGM [26] keeps the retriever and LLM fixed,and trains a bridge Seq2Seq model in between. The bridge model aims to transform the retrieved information into a format that LLMs can work with effectively, allowing it to not only rerank but also dynami- cally select passages for each query, and potentially employ more advanced strategies like repetition. Furthermore, PKG 10 introduces an innovative method for integrating knowledge into white-box models via directive fine-tuning [75]. In this approach, the retriever module is directly substituted to gen- erate relevant documents according to a query. This method assists in addressing the difficulties encountered during the fine-tuning process and enhances model performance. preferences of fine-tuned models and retrievers [79]. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align pref- erences. A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence.", "<3-hop>\n\nMethods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by θ that generates a current token based on a context of the previous i −1 tokens y1:i−1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) ≈ X z∈top-k(p(·|x)) pη(z|x)pθ(y|x, z) = X z∈top-k(p(·|x)) pη(z|x) N Y i pθ(yi|x, z, y1:i−1) RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne: pRAG-Token(y|x) ≈ N Y i X z∈top-k(p(·|x)) pη(z|x)pθ(yi|x, z, y1:i−1) Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pη(z|x) ∝exp \u0000d(z)⊤q(x) \u0001 d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a", "<4-hop>\n\nINTRODUCTION Trained on massive data, large language models (LLMs) have shown strong abilities in common NLP tasks using their parametric knowledge (Wei et al., 2022; Zhao et al., 2023; Achiam et al., 2023). However, the issue of hallucination (Ji et al., 2023; Bang et al., 2023) and the challenge of updating the parametric knowledge limit their real-world application in specific domains. Retrieval- augmented generation (RAG) alleviates this problem by supplying the LLM with information re- trieved from a custom outer knowledge base (Guu et al., 2020; Lewis et al., 2020; Yu et al., 2023). Open-source RAG frameworks like llamaindex (Liu, 2022) have been developed to facilitate the research and deployment of RAG. Typical retrieval-augmented generation (RAG) pipelines are text-based, operating on segmented texts as retrieval units (Yu et al., 2023; Asai et al., 2024; Yan et al., 2024), which we refer to as TextRAG. In real-world scenarios, knowledge is often presented in multi-modality documents such as textbooks and manuals, which may have texts and figures intersected together. To acquire texts from such data sources, a parsing stage is required, which typically involves a cascade of processes, including layout recognition, optical character recognition (OCR), and post-processing steps like text joining (Zhang et al., 2024; Liu, 2022). While effective in most scenarios, the parsing process inevitably introduces errors, which can negatively impact the retrieval and generation phases. More- over, TextRAG utilizes only textual information, overlooking potential information present in other modalities like images. Although research has been conducted on image retrieval and multi-modal ∗Equal contribution. †Corresponding authors. 1 arXiv:2410.10594v2 [cs.IR] 2 Mar 2025 Published as a conference paper at ICLR 2025 RAG, these approaches primarily focus on predefined scenarios wherein images and descriptive texts are properly extracted and paired (Wei et al., 2023; Sharifymoghaddam et al., 2024; Zhou et al., 2024), differing from real-world scenarios where texts and images (including figures) are often interleaved within a single document page. The recent development of vision-language models (VLMs) has introduced a promising approach to understanding complex visual cues in images and documents (OpenBMB, 2024b; Wang et al., 2024). By integrating a language model with a vision encoder, VLMs demonstrate superior abil- ities in applications such as describing pictures (Alayrac et al., 2022), explaining figures (Bavishi et al., 2023), and transcribing (printed and handwritten) text from document images (Laurenc¸on et al., 2024). Given the robust capabilities of VLMs in capturing multi-modal information present in images, an intriguing question arises: can the basic language model in the retrieval and generation components of TextRAG be substituted with a VLM, thus the parsing stage is bypassed and all the information of the document is preserved? In this paper, we present Vision-based Retrieval-augmented Generation (VisRAG), to study the fea- sibility of building a pure-vision RAG pipeline using VLMs. VisRAG is built with a VLM-based retriever VisRAG-Ret and generator VisRAG-Gen. Inherited the bi-encoder of text-based dense re- triever (Karpukhin et al., 2020), VisRAG-Ret maps the query and the document into an embedding space, but utilizing the document’s image directly instead of relying on extracted textual content. The embedding is obtained by applying weighted mean pooling on the final hidden states of the in- put text or vision tokens. After retrieving top-k document images, VisRAG processes these images to generate the answer. While it is straightforward to use a VLM that supports multi-image input for generation, for VLMs that can only accept one single image, we propose page concatenation and weighted selection techniques to enable the handling of multiple documents. Throughout the pro- cess, VisRAG preserves all information in its original visual format, thereby preventing the potential information loss or distortion that", "<5-hop>\n\nmight occur in traditional RAG pipelines. TextRAG VisRAG 10 20 30 40 50 Accuracy (%) 37.97 53.32 43.54 52.44 MiniCPM-V Generator GPT-4o Generator Figure 1: TextRAG vs. VisRAG on final gen- eration accuracy. In TextRAG, parsed text serves as the basis for both retrieval and gen- eration processes. In contrast, VisRAG lever- ages the original document image directly by using a VLM-based retriever and generator. Details can be found in Sec. 5.1. To evaluate VisRAG on real-world multi-modal doc- uments, we construct datasets from open-source vi- sual question answering (VQA) datasets and syn- thetic query-document pairs derived from web- crawled PDFs. In terms of retrieval, VisRAG- Ret outperforms state-of-the-art text- and vision- centric retrievers and achieves better results than solely relying on its constituent vision encoder or language model under identical training con- ditions. For generation, VisRAG-Gen surpasses traditional text-based generators with open-source VLMs. With VLMs capable of handling mul- tiple images, VisRAG shows increasing perfor- mance gains with more retrieved documents, indi- cating the potential for multi-page reasoning. As depicted in Figure 1, in a direct comparison of pipeline performances, VisRAG achieves a 40% rel- ative improvement over TextRAG using MiniCPM- V 2.6 (OpenBMB, 2024b) as the generator and a 20% relative improvement with GPT-4o (OpenAI, 2024) as the generator, attributed to the cascade ef- fect. Further analysis reveals that VisRAG possesses better training data efficiency and generalization ability than baseline models, and demonstrates ro- bustness across both text-centric and vision-centric documents. VisRAG shows great promise in replacing TextRAG as the next-generation standard for RAG pipelines. 2 RELATED WORK Retrieval-augmented Generation (RAG). RAG enhances large language models (LLMs) by incorporating retrieved information from external knowledge bases, which assists in addressing knowledge-intensive tasks (Guu et al., 2020), reducing hallucinations (Semnani et al., 2023), and 2 Published as a conference paper at ICLR 2025 acquiring new knowledge (Vu et al., 2023). An RAG pipeline typically comprises a text-based retriever that fetches relevant information from the knowledge base given the user query, and an LLM-based generator that reads the query along with the retrieved information to generate an an- swer (Shi et al., 2024b; Yu et al., 2023). Prior research on RAG primarily focuses on: a) improving the retriever, which is typically a text encoder producing text embeddings, through generator feed- back (Yu et al., 2023; Shi et al., 2024b); b) enhancing the generator via supervised fine-tuning (Lin et al., 2024; Xu et al., 2024a), in-context pre-training (Shi et al., 2024a), or advanced prompting (Xu et al., 2024c); and c) developing advanced RAG pipelines to handle long-form or multi-hop ques- tion answering (Jiang et al., 2023; Asai et al., 2024). However, research on RAG has predominantly targeted cleaned text corpora like Wikipedia from an academic standpoint. Building effective RAG pipelines for real-world, multi-modal documents remains a challenge. Vision-language Models. Recent advancements in vision-language models (VLMs) have greatly improved fine-grained multi-modal understanding. Since CLIP (Radford et al., 2021) pioneered contrastive visual-text alignment, models like Flamingo (Alayrac et al., 2022), LLaVA (Liu et al., 2023b), and BLIP (Li et al., 2022) have expanded LLMs to process visual inputs by connecting languages models with a CLIP-style vision encoder. Research has then shifted towards more ad- vanced multi-task and multi-stage pre-training paradigms, enabling models to generalize across a wide range of vision-language tasks (Liu et al., 2024a; Bai et al., 2023; Wang et al., 2023; Dai et al., 2023). This is followed by notable advancements in high-resolution visual understanding (Xu et al., 2024b; Bavishi et al., 2023; Lin et al., 2023) and OCR capabilities (Kim et al., 2022; Lee et al., 2023; Hong et al., 2024; Chen et al.,", "<6-hop>\n\nPublished as a conference paper at ICLR 2025\nVISRAG:\nVISION-BASED\nRETRIEVAL-AUGMENTED\nGENERATION ON MULTI-MODALITY DOCUMENTS\nShi Yu1∗, Chaoyue Tang2∗, Bokai Xu2∗, Junbo Cui2∗, Junhao Ran3, Yukun Yan1†,\nZhenghao Liu4, Shuo Wang1, Xu Han1, Zhiyuan Liu1†, Maosong Sun1\n1Department of Computer Science and Technology, Tsinghua University\n2ModelBest Inc. 3Rice University 4Northeastern University\nyus21@mails.tsinghua.edu.cn\nABSTRACT\nRetrieval-augmented generation (RAG) is an effective technique that enables large\nlanguage models (LLMs) to utilize external knowledge sources for generation.\nHowever, current RAG systems are solely based on text, rendering it impossible\nto utilize vision information like layout and images that play crucial roles in real-\nworld multi-modality documents. In this paper, we introduce VisRAG, which\ntackles this issue by establishing a vision-language model (VLM)-based RAG\npipeline. In this pipeline, instead of first parsing the document to obtain text,\nthe document is directly embedded using a VLM as an image and then retrieved\nto enhance the generation of a VLM. Compared to traditional text-based RAG,\nVisRAG maximizes the retention and utilization of the data information in the\noriginal documents, eliminating the information loss introduced during the pars-\ning process. We collect both open-source and synthetic data to train the retriever in\nVisRAG and explore a variety of generation methods. Experiments demonstrate\nthat VisRAG outperforms traditional RAG in both the retrieval and generation\nstages, achieving a 20–40% end-to-end performance gain over traditional text-\nbased RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing\ntraining data and demonstrates strong generalization capability, positioning it as a\npromising solution for RAG on multi-modality documents. Our code and data are\navailable at https://github.com/openbmb/visrag.\n1\n", "<7-hop>\n\nMETHODOLOGY In this section, we first recap the typical RAG pipeline (Sec. 3.1), then present our VisRAG frame- work (Sec. 3.2) and the construction of our training and evaluation data (Sec. 3.3). 3.1 PRELIMINARY: RETRIEVAL-AUGMENTED GENERATION A typical retrieval-augmented generation (RAG) pipeline consists of a retriever and a generator, both built on large language models (LLMs)1. This pipeline operates on a knowledge corpus D, 1In many cases, the retriever uses language models smaller than 1B parameters, which may not be consid- ered “large”, but we use the term LLM for simplicity. 3 Published as a conference paper at ICLR 2025 Figure 2: TextRAG (left) vs. VisRAG (right). Traditional text-based RAG (TextRAG) relies on parsed texts for retrieval and generation, losing visual information in multi-modal documents. Our vision-based RAG (VisRAG) employs a VLM-based retriever and generator to directly process the document page’s image, thereby preserving all information in the original page. which is processed into units for retrieval and generation, denoted as D = {d1, . . . , dn}, where n is the number of retrieval units. Given a text query q and the retrieval corpus D, the retriever functions as R : (q, D) →DR, taking q and D as inputs and producing a candidate set DR ⊂D. To enable efficient search, the units in the knowledge corpus D are pre-encoded into embeddings. During RAG pipeline inference, approximate nearest neighbor (ANN) search is applied to retrieve DR, which serves as the knowledge source for generation. The generation process can be defined as a function G : (q, DR) →a, where a represents the answer and G denotes the LLM generator. This is achieved by prompting the LLM with the query and the retrieved units DR to generate an answer. As shown in Figure 2 (left), traditional RAG frameworks (TextRAG) typically utilize text-based units for retrieval and generation. However, in real-world scenarios, data often appear in complex, multi-modal documents, requiring an additional parsing step to obtain text. In this paper, we propose to use the page as the fundamental unit for retrieval and generation, which is directly processed by vision language models (VLMs) as an image without further processing during retrieval and generation. In subsequent sections, we use the terms “page” and “document” interchangeably. 3.2 VISRAG: VISION-BASED RETRIEVAL-AUGMENTED GENERATION In this section, we present Vision-based Retrieval-augmented Generation (VisRAG), as shown in Figure 2 (right). In contrast to traditional RAG frameworks which use text segments for both re- trieval and generation, VisRAG leverages the image of the document to preserve all information. 3.2.1 RETRIEVAL The first stage of VisRAG, VisRAG-Ret, aims to retrieve a set of pages from the corpus D given q. We follow the dual-encoder paradigm in text-based dense retrieval models (Karpukhin et al., 2020) but employ a VLM rather than an LLM to encode the query and page. Specifically, the query and page are encoded separately as text and image in the VLM, producing in a sequence of hidden states. To derive the final embedding, and given that we use generative VLMs with causual attention, we adopt the position-weighted mean pooling over the last-layer VLM hidden states (Muennighoff, 2022), giving higher weights to later tokens: v = S X i=1 wihi, (1) where hi is the i-th hidden state, S is the sequence length, wi = i PS j=1 j is the i-th weight, and v is the query or page embedding. The similarity score is calculated by the cosine similarity of the query 4 Published as a conference paper at ICLR 2025 and page embedding. VisRAG-Ret is optimized using the InfoNCE loss: l(q, d+, D−) = −log exp(s(q, d+)/τ) exp(s(q, d+)/τ) + P d−∈D−exp(s(q, d−)/τ), (2) where d+, D−are positive document and"], "reference": "VisRAG enhances the retrieval and generation stages of RAG by employing a vision-language model (VLM)-based retriever and generator. Instead of parsing documents to obtain text, VisRAG directly embeds the document page as an image using a VLM and retrieves it to enhance the generation process. This approach maximizes the retention and utilization of data information in the original documents, eliminating information loss introduced during the parsing process.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How does the SPLADE model address the limitations of previous sparse lexical representation models in terms of efficiency and effectiveness?", "reference_contexts": ["<1-hop>\n\nINTRODUCTION The release of large pre-trained language models like BERT [7] has shaken-up Natural Language Processing and Information Re- trieval. These models have shown a strong ability to adapt to various tasks by simple fine-tuning. At the beginning of 2019, Nogueira and Cho [17] achieved state-of-the-art results – by a large margin – on the MS MARCO passage re-ranking task, paving the way for LM-based neural ranking models. Because of strict efficiency re- quirements, these models have initially been used as re-rankers in a two-stage ranking pipeline, where first-stage retrieval – or candidate generation – is conducted with bag-of-words models (e.g. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference’17, July 2017, Washington, DC, USA © 2021 Association for Computing Machinery. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn BM25) that rely on inverted indexes. While BOW models remain strong baselines [27], they suffer from the long standing vocabulary mismatch problem, where relevant documents might not contain terms that appear in the query. Thus, there have been attempts to substitute standard BOW approaches by learned (neural) rankers. Designing such models poses several challenges regarding effi- ciency and scalability: therefore there is a need for methods where most of the computation can be done offline and online inference is fast. Dense retrieval with approximate nearest neighbors search has shown impressive results [8, 15, 26], but is still combined with BOW models because of its inability to explicitly model term match- ing. Hence, there has recently been a growing interest in learning sparse representations for queries and documents [1, 4, 19, 28, 29]. By doing so, models can inherit from the desirable properties of BOW models like exact-match of (possibly latent) terms, efficiency of inverted indexes and interpretability. Additionally, by modeling implicit or explicit (latent, contextualized) expansion mechanisms – similarly to standard expansion models in IR – these models can reduce the vocabulary mismatch. The contributions of this paper are threefold: (1) we build upon SparTerm [1], and show that a mild tuning of hyperparameters brings improvements that largely outperform the results reported in the original paper; (2) we propose the SParse Lexical AnD Ex- pansion (SPLADE) model, based on a logarithmic activation and sparse regularization. SPLADE performs an efficient document ex- pansion [1, 16], with competitive results with respect to complex training pipelines for dense models like ANCE [26]; (3) finally, we show how the sparsity regularization can be controlled to influ- ence the trade-off between efficiency (in terms of the number of floating-point operations) and effectiveness. 2 RELATED WORKS Dense retrieval based on BERT Siamese models [22] has become the standard approach for candidate generation in Question An- swering and IR [8, 10, 12, 15, 25]. While the backbone of these mod- els remains the same, recent works highlight the critical aspects of the training strategy to obtain state-of-the-art results, ranging", "<2-hop>\n\nfrom improved negative sampling [8, 25] to distillation [11, 15]. ColBERT [13] pushes things further: the postponed token-level interactions allow to efficiently apply the model for first-stage re- trieval, benefiting of the effectiveness of modeling fine-grained interactions, at the cost of storing embeddings for each (sub)term – raising concerns about the actual scalability of the approach for large collections. To the best of our knowledge, very few studies have discussed the impact of using approximate nearest neighbors arXiv:2107.05720v1 [cs.IR] 12 Jul 2021 (ANN) search on IR metrics [2, 23]. Due to the moderate size of the MS MARCO collection, results are usually reported with an exact, brute-force search, therefore giving no indication on the effective computing cost. An alternative to dense indexes is term-based ones. Building on standard BOW models, Zamani et al. first introduced SNRM [28]: the model embeds documents and queries in a sparse high-dimensional latent space by means of ℓ1 regularization on representations. How- ever, SNRM effectiveness remains limited and its efficiency has been questioned [20]. More recently, there have been attempts to transfer the knowledge from pre-trained LM to sparse approaches. Based on BERT, DeepCT [4–6] focused on learning contextualized term weights in the full vocabulary space – akin to BOW term weights. However, as the vocabulary associated with a document remains the same, this type of approach does not solve the vocabu- lary mismatch, as acknowledged by the use of query expansion for retrieval [4]. A first solution to this problem consists in expanding documents using generative approaches such as doc2query [19] and docTTTTTquery [18] to predict expansion words for documents. The document expansion adds new terms to documents – hence fighting the vocabulary mismatch – as well as repeats existing terms, implicitly performing re-weighting by boosting important terms. These methods are however limited by the way they are trained (predicting queries), which is indirect in nature and limit their progress. A second solution to this problem, that has been chosen by recent works such as [1, 16, 29], is to estimate the im- portance of each term of the vocabulary implied by each term of the document, i.e. to compute an interaction matrix between the document or query tokens and all the tokens from the vocabulary. This is followed by an aggregation mechanism (roughly sum for SparTerm [1], max for EPIC [16] and SPARTA [29]), that allows to compute an importance weight for each term of the vocabulary, for the full document or query. However, EPIC and SPARTA (document) representations are not sparse enough by construction – unless resorting on top-𝑘pooling – contrary to SparTerm, for which fast retrieval is thus possible. Furthermore, the latter does not include (like SNRM) an explicit sparsity regularization, which hinders its performance. Our SPLADE model relies on such regularization, as well as other key changes, that boost both the efficiency and the effectiveness of this type of models. 3 SPARSE LEXICAL REPRESENTATIONS FOR FIRST-STAGE RANKING In this section, we first describe in details the SparTerm model [1], before presenting our model named SPLADE. 3.1 SparTerm SparTerm predicts term importance – in BERT WordPiece vocab- ulary (|𝑉| = 30522) – based on the logits of the Masked Lan- guage Model (MLM) layer. More precisely, let us consider an input query or document sequence (after WordPiece tokenization) 𝑡= (𝑡1,𝑡2, ...,𝑡𝑁), and its"], "reference": "The SPLADE model addresses the limitations of previous sparse lexical representation models by incorporating sparsity regularization, which boosts both the efficiency (in terms of the number of floating-point operations) and effectiveness of these types of models. Additionally, it relies on a logarithmic activation function and efficient document expansion, achieving competitive results with respect to complex training pipelines for dense models like ANCE.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How does ARES utilize prediction-powered inference (PPI) to improve model-based evaluation accuracy?", "reference_contexts": ["<1-hop>\n\nARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com Abstract Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to re- trieve, and responses to generate. We intro- duce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By cre- ating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES uti- lizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or docu- ments used in the evaluated RAG systems. We make our code and datasets publicly available on Github. 1 Introduction Retrieval-augmented generation (RAG) has be- come a prominent approach for building user- facing NLP applications, such as systems for ques- tion answering (QA), fact-checking, and customer support (Petroni et al., 2021; Wang et al., 2019). Typically, a RAG system consists of a retriever and a downstream language model (LM). Given a user question, the retriever finds relevant passages from a corpus and the LM uses these passages to gener- ate a response. This formulation admits a multitude of choices: what retrieval model to use, how to di- vide the documents into retrieval chunks, and how to prompt or finetune the LM to use the retrieved information, to name only a few of the simplest design decisions. ∗Project started during research internship at Databricks The best design for a RAG system is not neces- sarily universal across data domains, corpus sizes, and cost/latency budgets. To tune their own RAG systems, practitioners traditionally need hand an- notations for test questions, passages to retrieve (to assess the retriever), and responses to generate, labeled specifically for their target domain. Alter- natively, they may evaluate different approaches in production by collecting human preferences that compare the candidate systems. Unfortunately, both of these strategies demand high expertise and impose considerable annotation costs. Model-based evaluation is an inexpensive strat- egy to test generative output quality (Zheng et al., 2023). For instance, the open-source RAGAS framework (James and Es, 2023) prompts an LM for evaluating the relevance of retrieved informa- tion and the faithfulness and accuracy of generated responses. Unfortunately, such strategies currently rely for evaluation on a fixed set of heuristically hand-written prompts, offering little adaptability to various evaluation contexts and no guarantees about quality. To evaluate RAG systems rapidly and accu- rately, we propose ARES, the Automated RAG Evaluation System. ARES is the first automated RAG evaluation system to generate tailored LLM judges for each component of a RAG pipeline, lead- ing to substantial boosts in evaluation precision and accuracy compared to existing approaches like RA- GAS. Furthermore, unlike existing RAG evaluation systems, ARES provides confidence intervals for its scoring by leveraging prediction-powered in- ference (PPI; Angelopoulos et al. 2023). Given a corpus of documents and a RAG system, ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response gen- erated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question). A good arXiv:2311.09476v2 [cs.CL] 31 Mar 2024 RAG system finds relevant contexts and generates answers that are both faithful and relevant. Many existing RAG evaluation frameworks re- quire substantial human annotations for scoring. ARES significantly improves data efficiency dur- ing evaluation by only requiring three inputs: an in- domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers", "<2-hop>\n\n(e.g. five examples or more), which are used for prompting LLMs in synthetic data gen- eration. Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus. Sec- ond, it defines three separate judge models to per- form three classification tasks (context relevance, answer faithfulness, and answer relevance). These judges are lightweight models fine-tuned against a contrastive learning objective. Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation ac- curacy and provide statistical confidence intervals for RAG scoring. PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our hu- man preference validation set, which is composed of approximately 150 annotated datapoints or more that designate both positive and negative examples for context relevance, answer faithfulness, and an- swer relevance. We conduct extensive empirical evaluations, demonstrating that ARES accurately scores RAG systems across the six knowledge-intensive datasets in KILT and SuperGLUE, beating exist- ing automated evaluation approaches like RAGAS by 59.3 and 14.4 percentage points on average across context relevance and answer relevance eval- uation accuracy, respectively. Additionally, ARES accurately calculates answer hallucination occur- rences in the AIS attribution dataset (Rashkin et al., 2022), predicting within 2.5 percentage points of the ground truth average for answer hallucinations. Compared to annotation-based evaluation methods, ARES is substantially more accurate and efficient, requiring 78% less annotations than the baseline approach. We also find that ARES consistently distinguishes competitive RAG systems that are only a few points apart in ground-truth metrics. This precision enables ARES to guide the develop- ment and comparison of competitive approaches and configurations. We make the ARES code and datasets publicly available on Github. 2 Related Work RAG (Guu et al., 2020; Lewis et al., 2020; Khat- tab et al., 2021; Izacard et al., 2022)) is now a common strategy for bolstering LLMs by combin- ing them with retrieval systems. Through retrieval, RAG helps LM systems gather domain-specific knowledge, ground generations in factual informa- tion (Shuster et al., 2021; Huo et al., 2023), and offer a degree of transparency or interpretability via citing sources (Mialon et al., 2023). Multiple LLM-based evaluation techniques have emerged for gauging LLM systems. This is essen- tial for rapid deployment in new settings, where it is difficult to build a traditional benchmark dataset from scratch. Early attempts at this use LLMs out of the box, as in MT-Bench and Chatbot Arena (Zheng et al., 2023). AutoCalibrate (Liu et al., 2023b) seeks to align an LLM-judge with human preferences, leveraging a self-refinement prompt to iteratively improve the LLM judge. How- ever, AutoCalibrate does not offer any statistical guarantees for the accuracy of its predictions. Other work has used LLM prompting to evaluate system quality across natural language generation tasks, such as translation, summarization, and dialogue (Kocmi and Federmann, 2023; Fu et al., 2023; Liu et al., 2023a; Wang et al., 2023). In the context of knowledge-intensive NLP tasks, LLMs have been explored for assessing attribution and factuality in LLMs (Min et al., 2023; Gekhman et al., 2023; Yue et al., 2023). New guidelines like LongEval (Krishna et al., 2023) and datasets like Hagrid and ALCE (Kamalloo et al., 2023; Gao et al., 2023) provide resources for analyzing knowledge-intensive LLM pipelines. The two most-closely related projects to ARES are EXAM (Sander and Dietz, 2021) and RA- GAS (James and Es, 2023). To evaluate RAG sys- tems, the EXAM metric estimates how many exam questions a reader (simulated as a QA system) can answer correctly based on the generated response. This requires a set of queries with several asso- ciated sub-questions each, which adds a burden that ARES does not bring. RAGAS is based on a handful of heuristic hand-written prompts. These offer little adaptability to new RAG evaluation set- tings (e.g., new corpora) and, as we show in our evaluation, substantially underperform ARES. 3 ARES ARES proceeds in three stages (Figure 1). There are three required inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints (or more), and few-shot examples of in-domain queries and answers", "<3-hop>\n\nARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating\nretrieval-augmented\ngeneration\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1\n ", "<4-hop>\n\nConclusion\nIn this work, we present ARES, a novel automated\nevaluation framework for retrieval-augmented gen-\neration (RAG). ARES offers a novel training\npipeline for fine-tuning lightweight LLM judges\non synthetically generated queries and answers.\nARES can evaluate each component of a RAG sys-\ntem separately to help improve system understand-\ning and create targeted solutions, and it requires\nonly minimal human annotations. For the eight dif-\nferent datasets in KILT, SuperGLUE, and AIS re-\nquiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing\n\flogits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n7\nLimitations\nARES relies on a small set of annotations in the\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\noften require an annotator familiar with the RAG\nsystem’s domain application. While these annota-\ntions can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\n", "<5-hop>\n\nI. INTRODUCTION L ARGE language models (LLMs) have achieved remark- able success, though they still face significant limitations, especially in domain-specific or knowledge-intensive tasks [1], notably producing “hallucinations” [2] when handling queries beyond their training data or requiring current information. To overcome challenges, Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant document chunks from external knowledge base through semantic similarity calcu- lation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content. Its integration into LLMs has resulted in widespread adoption, establishing RAG as a key technology in advancing chatbots and enhancing the suitability of LLMs for real-world applica- tions. RAG technology has rapidly developed in recent years, and the technology tree summarizing related research is shown Corresponding Author.Email:haofen.wang@tongji.edu.cn 1Resources are available at https://github.com/Tongji-KGLLM/ RAG-Survey in Figure 1. The development trajectory of RAG in the era of large models exhibits several distinct stage characteristics. Initially, RAG’s inception coincided with the rise of the Transformer architecture, focusing on enhancing language models by incorporating additional knowledge through Pre- Training Models (PTM). This early stage was characterized by foundational work aimed at refining pre-training techniques [3]–[5].The subsequent arrival of ChatGPT [6] marked a pivotal moment, with LLM demonstrating powerful in context learning (ICL) capabilities. RAG research shifted towards providing better information for LLMs to answer more com- plex and knowledge-intensive tasks during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques. The burgeoning field of RAG has experienced swift growth, yet it has not been accompanied by a systematic synthesis that could clarify its broader trajectory. This survey endeavors to fill this gap by mapping out the RAG process and charting its evolution and anticipated future paths, with a focus on the integration of RAG within LLMs. This paper considers both technical paradigms and research methods, summarizing three main research paradigms from over 100 RAG studies, and analyzing key technologies in the core stages of “Retrieval,” “Generation,” and “Augmentation.” On the other hand, current research tends to focus more on methods, lacking analysis and summarization of how to evaluate RAG. This paper compre- hensively reviews the downstream tasks, datasets, benchmarks, and evaluation methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations. Our contributions are as follows: • In this survey, we present a thorough and systematic review of the state-of-the-art RAG methods, delineating its evolution through paradigms including naive RAG, arXiv:2312.10997v5 [cs.CL] 27 Mar 2024 2 Fig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs, research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models in the pre-training stage through retrieval-augmented techniques. advanced RAG, and modular RAG. This review contex- tualizes the broader scope of RAG research within the landscape of LLMs. • We identify and discuss the central technologies integral to the RAG process, specifically focusing on the aspects of “Retrieval”, “Generation” and “Augmentation”, and delve into their synergies, elucidating how these com- ponents intricately collaborate to form a cohesive and effective RAG framework. • We have summarized the current assessment methods of RAG, covering 26 tasks, nearly 50 datasets, outlining the evaluation objectives", "<6-hop>\n\ndownstream tasks. The retriever is fine-tuned with two types of supervised signals: hard labels for the dataset and soft rewards from the LLMs. This dual-signal approach fosters a more effective fine-tuning process, tailoring the embedding model to diverse downstream applications. REPLUG [72] utilizes a retriever and an LLM to calculate the probability distributions of the retrieved documents and then performs supervised training by computing the KL divergence. This straightforward and effective training method enhances the performance of the retrieval model by using an LM as the supervisory signal, eliminating the need for specific cross-attention mechanisms. Moreover, inspired by RLHF (Reinforcement Learning from Human Feedback), utilizing LM-based feedback to reinforce the retriever through reinforcement learning. E. Adapter Fine-tuning models may present challenges, such as in- tegrating functionality through an API or addressing con- straints arising from limited local computational resources. Consequently, some approaches opt to incorporate an external adapter to aid in alignment. To optimize the multi-task capabilities of LLM, UP- RISE [20] trained a lightweight prompt retriever that can automatically retrieve prompts from a pre-built prompt pool that are suitable for a given zero-shot task input. AAR (Augmentation-Adapted Retriver) [47] introduces a universal adapter designed to accommodate multiple downstream tasks. While PRCA [69] add a pluggable reward-driven contextual adapter to enhance performance on specific tasks. BGM [26] keeps the retriever and LLM fixed,and trains a bridge Seq2Seq model in between. The bridge model aims to transform the retrieved information into a format that LLMs can work with effectively, allowing it to not only rerank but also dynami- cally select passages for each query, and potentially employ more advanced strategies like repetition. Furthermore, PKG 10 introduces an innovative method for integrating knowledge into white-box models via directive fine-tuning [75]. In this approach, the retriever module is directly substituted to gen- erate relevant documents according to a query. This method assists in addressing the difficulties encountered during the fine-tuning process and enhances model performance. preferences of fine-tuned models and retrievers [79]. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align pref- erences. A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence.", "<7-hop>\n\nMethods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y. As shown in Figure 1, our models leverage two components: (i) a retriever pη(z|x) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator pθ(yi|x, z, y1:i−1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by θ that generates a current token based on a context of the previous i −1 tokens y1:i−1, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the pη and pθ components, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) ≈ X z∈top-k(p(·|x)) pη(z|x)pθ(y|x, z) = X z∈top-k(p(·|x)) pη(z|x) N Y i pθ(yi|x, z, y1:i−1) RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne: pRAG-Token(y|x) ≈ N Y i X z∈top-k(p(·|x)) pη(z|x)pθ(yi|x, z, y1:i−1) Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pη(z|x) ∝exp \u0000d(z)⊤q(x) \u0001 d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(pη(·|x)), the list of k documents z with highest prior probability pη(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component pθ(yi|x, z, y1:i−1) could be modelled using any encoder-decoder. We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a"], "reference": "ARES utilizes prediction-powered inference (PPI) to improve model-based evaluation accuracy by leveraging a small set of human-annotated datapoints for computing its confidence intervals. This approach allows ARES to provide statistical confidence intervals for RAG scoring, enhancing the precision and reliability of its evaluations.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "How did the introduction of dense passage retrieval improve open-domain question answering systems?", "reference_contexts": ["<1-hop>\n\nDense Passage Retrieval for Open-Domain Question Answering\nVladimir Karpukhin∗, Barlas O˘guz∗, Sewon Min†, Patrick Lewis,\nLedell Wu, Sergey Edunov, Danqi Chen‡, Wen-tau Yih\nFacebook AI\n†University of Washington\n‡Princeton University\n{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\nAbstract\nOpen-domain question answering relies on ef-\nﬁcient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method.\nIn this work, we show that\nretrieval can be practically implemented us-\ning dense representations alone, where em-\nbeddings are learned from a small number\nof questions and passages by a simple dual-\nencoder framework.\nWhen evaluated on a\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong Lucene-\nBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks.1\n1\n", "<2-hop>\n\nIntroduction\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions us-\ning a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), inter alia), the advances of reading\ncomprehension models suggest a much simpliﬁed\ntwo-stage framework: (1) a context retriever ﬁrst\nselects a small subset of passages where some\nof them contain the answer to the question, and\nthen (2) a machine reader can thoroughly exam-\nine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\nopen-domain QA to machine reading is a very rea-\nsonable strategy, a huge performance degradation\nis often observed in practice2, indicating the needs\nof improving retrieval.\n∗Equal contribution\n1The code and trained models have been released at\nhttps://github.com/facebookresearch/DPR.\n2For instance, the exact match score on SQuAD v1.1 drops\nfrom above 80% to less than 40% (Yang et al., 2019a).\nRetrieval in open-domain QA is usually imple-\nmented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords efﬁ-\nciently with an inverted index and can be seen\nas representing the question and context in high-\ndimensional, sparse vectors (with weighting). Con-\nversely, the dense, latent semantic encoding is com-\nplementary to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n“Who is the bad guy in lord of the rings?”, which can\nbe answered from the context “Sala Baker is best\nknown for portraying the villain Sauron in the Lord\nof the Rings trilogy.” A term-based system would\nhave difﬁculty retrieving such a context, while\na dense retrieval system would be able to better\nmatch “bad guy” with “villain” and fetch the cor-\nrect context. Dense encodings are also learnable\nby adjusting the embedding functions, which pro-\nvides additional ﬂexibility to have a task-speciﬁc\nrepresentation. With special in-memory data struc-\ntures and indexing schemes, retrieval can be done\nefﬁciently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)).\nHowever, it is generally believed that learn-\ning a good dense vector representation needs a\nlarge number of labeled pairs of question and con-\ntexts. Dense retrieval methods have thus never\nbe shown to outperform TF-IDF/BM25 for open-\ndomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\nobjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then ﬁne-\ntuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain\narXiv:2004.04906v3  [cs.CL]  30 Sep 2020\n\fQA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot ﬁne-tuned using pairs of questions and answers,\nthe corresponding representations could be subop-\ntimal.\nIn this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), with-\nout additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\nof question and passage pairs. Through a series\nof careful ablation studies, our ﬁnal solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective compar-\ning all pairs of questions and passages in a batch.\nOur ", "<3-hop>\n\nDense Passage Retriever (DPR) is exception- ally strong. It not only outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy), but also results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs. 33.3%) in the open Natural Questions setting (Lee et al., 2019; Kwiatkowski et al., 2019). Our contributions are twofold. First, we demon- strate that with the proper training setup, sim- ply ﬁne-tuning the question and passage encoders on existing question-passage pairs is sufﬁcient to greatly outperform BM25. Our empirical results also suggest that additional pretraining may not be needed. Second, we verify that, in the context of open-domain question answering, a higher retrieval precision indeed translates to a higher end-to-end QA accuracy. By applying a modern reader model to the top retrieved passages, we achieve compara- ble or better results on multiple QA datasets in the open-retrieval setting, compared to several, much complicated systems. 2 Background The problem of open-domain QA studied in this paper can be described as follows. Given a factoid question, such as “Who ﬁrst voiced Meg on Family Guy?” or “Where was the 8th Dalai Lama born?”, a system is required to answer it using a large corpus of diversiﬁed topics. More speciﬁcally, we assume the extractive QA setting, in which the answer is restricted to a span appearing in one or more pas- sages in the corpus. Assume that our collection contains D documents, d1, d2, · · · , dD. We ﬁrst split each of the documents into text passages of equal lengths as the basic retrieval units3 and get M total passages in our corpus C = {p1, p2, . . . , pM}, where each passage pi can be viewed as a sequence of tokens w(i) 1 , w(i) 2 , · · · , w(i) |pi|. Given a question q, the task is to ﬁnd a span w(i) s , w(i) s+1, · · · , w(i) e from one of the passages pi that can answer the question. Notice that to cover a wide variety of domains, the corpus size can easily range from millions of docu- ments (e.g., Wikipedia) to billions (e.g., the Web). As a result, any open-domain QA system needs to include an efﬁcient retriever component that can se- lect a small set of relevant texts, before applying the reader to extract the answer (Chen et al., 2017).4 Formally speaking, a retriever R : (q, C) →CF is a function that takes as input a question q and a corpus C and returns a much smaller ﬁlter set of texts CF ⊂C, where |CF| = k ≪|C|. For a ﬁxed k, a retriever can be evaluated in isolation on top-k retrieval accuracy, which is the fraction of ques- tions for which CF contains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improv- ing the retrieval component in open-domain QA. Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passages in a low-dimensional and continuous space, such that it can retrieve efﬁciently the top k passages relevant to the input question for the reader at run-time. Note that M can be very large (e.g., 21 million passages in our experiments, de- scribed in Section 4.1) and k is usually small, such as 20–100. 3.1 Overview Our dense passage retriever (DPR) uses a dense encoder EP (·) which maps any text passage to a d- dimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval. 3The ideal size and boundary of a text passage are func- tions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using ﬁxed-length passages performs better in both retrieval and ﬁnal QA accuracy, as observed by Wang et al. (2019). 4Exceptions include (Seo et al., 2019) and", "<4-hop>\n\nExperiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k ∈{5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow best practice and use", "<5-hop>\n\nResults 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 - /50.1 37.4 - T5-11B+SSM[52] 36.6 - /60.5 44.7 - Open Book REALM [20] 40.4 - / - 40.7 46.8 DPR [26] 41.5 57.9/ - 41.1 50.6 RAG-Token 44.1 55.2/66.1 45.5 50.0 RAG-Seq. 44.5 56.8/68.0 45.2 52.2 Table 2: Generation and classiﬁcation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2* BART 15.1 19.7 38.2 41.6 64.0 81.1 RAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5 RAG-Seq. 14.7 21.4 40.8 44.2 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each", "<6-hop>\n\ndocuments at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence. 10 20 30 40 50 K Retrieved Docs 39 40 41 42 43 44 NQ Exact Match RAG-Tok RAG-Seq 10 20 30 40 50 K Retrieved Docs 40 50 60 70 80 NQ Answer Recall @ K RAG-Tok RAG-Seq Fixed DPR BM25 10 20 30 40 50 K Retrieved Docs 48 50 52 54 56 Bleu-1 / Rouge-L score RAG-Tok R-L RAG-Tok B-1 RAG-Seq R-L RAG-Seq B-1 Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor- mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved. 5 Related Work Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks. 8 General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench- marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed architecture, by learning a retrieval module to augment pre-trained, generative language models. Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be ﬁne-tuned for strong performance on a variety of tasks. Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9]. Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a ﬁnal output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces"], "reference": "Dense Passage Retrieval for Open-Domain Question Answering introduced a method that uses dense representations alone to perform efficient passage retrieval, outperforming traditional sparse vector space models like TF-IDF or BM25. This was achieved by learning embeddings from a small number of questions and passages using a simple dual-encoder framework. When evaluated on various open-domain QA datasets, the dense retriever demonstrated significant improvements in terms of top-20 passage retrieval accuracy compared to strong Lucene-BM25 systems.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "What is the core idea behind HyDE and how does it improve zero-shot dense retrieval?", "reference_contexts": ["<1-hop>\n\nto follow instructions. With these ingredients, we propose to pivot through Hypothetical Document Embeddings (HyDE), and decompose dense retrieval into two tasks, a generative task per- formed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder’s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are ﬁltered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks. HyDE appears unsupervised. No model is trained in HyDE: both the generative model and the con- trastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM. In our experiments, we show HyDE using Instruct- GPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models signiﬁcantly out- performs the previous state-of-the-art Contriever- only zero-shot no-relevance system on 11 queries sets, covering tasks like Web Search, Question Answering, Fact Veriﬁcation and languages like Swahili, Korean, Japanese. 2 Related Works Dense Retrieval (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019). Researchers stud- ied the metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sam- pling (Xiong et al., 2021; Qu et al., 2021), and also introduced distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021). Later works studied the second stage pre-training of language model speciﬁcally for retrieval (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022). The popularity of dense retrieval can be partially attributed to the rich and successful research in very efﬁcient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2017). Instructions-Following Language Models Soon after the emergence of LLMs, several groups of researchers discover that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). This can be done by standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied “Task-aware Retrieval with Instructions”. They ﬁne-tuned dense encoders that can also encode task-speciﬁc instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an instruction following generative LLM, as described above. Zero-Shot Dense Retrieval The tasks of zero- shot (dense) retrieval are arguably empirically de- ﬁned by Thakur et al. (2021) for the neural re- trieval community. Their BEIR benchmark con- sists of diverse retrieval tasks. The paper and many follow-up research generally consider the Transfer Learning setup where the dense re- triever is ﬁrst learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021; Wang et al., 2022; Yu et al., 2022). However, as stated by Izacard et al. (2021), such a large collection can rarely be assumed. In this paper, therefore, we study the problem of building effective dense retrieval", "<2-hop>\n\nPrecise Zero-Shot Dense Retrieval without Relevance Labels\nLuyu Gao∗†\nXueguang Ma∗‡\nJimmy Lin‡\nJamie Callan†\n†Language Technologies Institute, Carnegie Mellon University\n‡David R. Cheriton School of Computer Science, University of Waterloo\n{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca\nAbstract\nWhile dense retrieval has been shown effec-\ntive and efﬁcient across tasks and languages,\nit remains difﬁcult to create effective fully\nzero-shot dense retrieval systems when no rel-\nevance label is available.\nIn this paper, we\nrecognize the difﬁculty of zero-shot learning\nand encoding relevance.\nInstead, we pro-\npose to pivot through Hypothetical Document\nEmbeddings (HyDE). Given a query, HyDE ﬁrst\nzero-shot instructs an instruction-following\nlanguage model (e.g. InstructGPT) to gen-\nerate a hypothetical document.\nThe docu-\nment captures relevance patterns but is unreal\nand may contain false details. Then, an un-\nsupervised contrastively learned encoder (e.g.\nContriever) encodes the document into an\nembedding vector.\nThis vector identiﬁes a\nneighborhood in the corpus embedding space,\nwhere similar real documents are retrieved\nbased on vector similarity. This second step\nground the generated document to the actual\ncorpus, with the encoder’s dense bottleneck\nﬁltering out the incorrect details. Our exper-\niments show that HyDE signiﬁcantly outper-\nforms the state-of-the-art unsupervised dense\nretriever Contriever and shows strong per-\nformance comparable to ﬁne-tuned retrievers,\nacross various tasks (e.g. web search, QA, fact\nveriﬁcation) and languages (e.g. sw, ko, ja).1\n1\n systems without relevance labels. Similar to Izacard et al. (2021), we also do not assume access to the test time corpora for training. This is a more realistic setup and prevents over-engineering on the test corpora. By the deﬁnition in Sachan et al. (2022), our setup can be roughly considered as “unsuper- vised”. Strictly, as with Sachan et al. (2022), the only supervision resides in the LLM, in the pro- cessing of learning to follow instructions. Generative Retrieval Generative search is a new class of retrieval methods that use neural generative models as search indices (Metzler et al., 2021; Tay et al., 2022; Bevilacqua et al., 2022; Lee et al., 2022). These models use (constrained) decoding to generate document identiﬁers, such as id and sub-string, which map directly to real documents. They have to go through special training procedures over relevance data; effective search may also need to use novel forms of search indices (Bevilacqua et al., 2022; Lee et al., 2022). In comparison, our method uses the standard MIPS index and requires no training or training data. Our generative model produces an intermediate hypothetical document to be fed into a dense encoder, instead of a real document. 3", "<3-hop>\n\nRELATED WORK\nOpen-domain QA with Dense Retrieval\nIn contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto ﬁnetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved signiﬁcant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\nof questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model’s hidden states. However, their method requires an initial round\nof TF-IDF/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\nQuery Expansion Techniques in IR\nAs our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\nexpansion techniques (Rocchio, 1971; Voorhees, 1994; Ruthven & Lalmas, 2003) that are widely used\nin traditional IR systems. In particular, our system is similar in spirit to pseudo-relevance feedback\ntechniques (Croft & Harper, 1979; Cao et al., 2008; Lv & Zhai, 2010), where no additional user\ninteraction is required at the query reformulation stage. Existing studies mainly focus on alleviating\nthe uncertainty of the user query (Collins-Thompson & Callan, 2007) by adding relevant terms from\nthe ﬁrst round of retrieval, where the retrieval target remains the same throughout the iterative process.\nIn contrast, the query reformulation in our approach aims to follow the multi-hop reasoning chain and\neffectively retrieves different targets at each step. Furthermore, instead of explicitly selecting terms\nto expand the query, we simply concatenate the whole passage and rely on the pretrained encoder to\nchoose useful information from the last retrieved passage.\nOther Multi-hop QA Work\nApart from HotpotQA, other multi-hop QA datasets (Welbl et al.,\n2018; Talmor & Berant, 2018; Zhang et al., 2018) are mostly built from knowledge bases (KBs).\nCompared to questions in HotpotQA, questions in these datasets are rather synthetic and less diverse.\nAs multi-hop relations in KBs could be mentioned together in a single text piece, these datasets are\nnot designed for an open-domain setting which necessitates multi-hop retrieval. Existing methods on\nthese datasets either retrieve passages from a small passage pool pruned based on the the speciﬁc\ndataset (Sun et al., 2019; Dhingra et al., 2020), or focus on a non-retrieval setting where a compact\ndocuments set is already given (De Cao et al., 2018; Zhong et al., 2019; Tu et al., 2019; Beltagy et al.,\n2020). Compared to these research, our work aims at building an efﬁcient multi-hop retrieval model\nthat easily scales to large real-world corpora that include millions of open-domain documents.\n5\n"], "reference": "The core idea behind Hypothetical Document Embeddings (HyDE) is to pivot through generating a hypothetical document that captures relevance patterns but may contain false details. This document is then encoded into an embedding vector by an unsupervised contrastively learned encoder, which identifies a neighborhood in the corpus embedding space where similar real documents are retrieved based on vector similarity. HyDE improves zero-shot dense retrieval by grounding the generated document to the actual corpus and filtering out incorrect details with the encoder's dense bottleneck.", "synthesizer_name": "multi_hop_abstract_query_synthesizer"}
{"user_input": "What is the purpose of constructing a Retrieval-Augmented Generation Benchmark (RGB) and what specific abilities does it evaluate?", "reference_contexts": ["<1-hop>\n\nbuild Retrieval- Augmented Generation Benchmark (RGB) to evaluate the retrieval-augmented generation of LLMs, and we concern about 4 specific abilities: Noise Robustness is the robustness of LLMs in noisy documents. As retrievers are not perfect, the external knowl- edge they retrieve often contains a significant amount of noise, i.e., documents which are relevant to the question but do not contain any information about the answer. To effec- tively answer user questions, LLMs must be able to extract the necessary information from documents despite there are noisy documents. Negative Rejection is a measure of whether LLMs can decline to answer a question when none of the contexts pro- vide useful information. In real-world situations, the search engine often fails to retrieve documents containing the an- swers. In these cases, it is important for the model to have the capability to reject recognition and avoid generating mis- leading content. Information Integration is a capacity to integrate an- swers from multiple documents. In many cases, the an- swer to a question may be contained in multiple documents. For example, for the question “Who are the champions of the U.S. Open 2022 men’s and women’s singles?”, the two champions may be mentioned in different documents. In or- der to provide better answers to complex questions, it is nec- essary for LLMs to have the ability to integrate information. Counterfactual Robustness refers to a capacity to han- dle errors in external knowledge. In the real world, there is an abundance of false information on the internet. Please note that we only evaluate the situation that LLMs are given warnings about potential risks in the retrieved information through instruction. In real-world scenarios, it is not possible to obtain per- fect documents with all the necessary external knowledge. Therefore, evaluating these four abilities of the model be- comes essential in order to measure the RAG of LLMs. Data construction Inspired by previous benchmarks for LLMs, RGB utilizes a question-answering format for evaluation. We evaluate the LLMs by judging the retrieval-augmented responses of them to the questions. To simulate real-world scenarios, we con- struct question and answer data using actual news articles. Due to the abundance of knowledge contained within the LLMs there is a potential for bias when measuring the first three abilities. To mitigate this, the instances of RGB are constructed by latest news articles. Additionally, we retrieve external documents from Internet through search engines. Finally, we expand the corpus and divided it into 4 testbeds to evaluate the above basic abilities of LLMs. The overall procedure of our data construction is illustrated in Figure 2. QA instances generation. We first collect latest news ar- ticles and use prompts to make ChatGPT generate events, questions, and answers for each articles. For example, as shown in the Figure 2, for a report about “The 2022 Nobel Prize”, ChatGPT will generate corresponding event, ques- tion and provide key information for answering it. By gen- erating events, the model is able to preliminarily filter out news articles that do not contain any events. After genera- tion, we manually check the answer and filter out data that is difficult to retrieve through search engines. Retrieve using search engine. For each query, we use Google’s API to fetch 10 relevant web pages and extract corresponding snippets of text from them. Simultaneously, we read these web pages and convert their textual content into text chunks with a maximum length of 300 tokens. Us- ing an existing dense retrieval model 2, we select the top-30 text chunks that match the query most effectively. These re- trieved text chunks, along with the snippets provided by the search API, will serve as our external documents. These doc- uments will be divided into positive documents and negative documents based on whether they contain the answer. Testbeds construction for each ability. We expand the corpus and divided it into 4 testbeds to evaluate the above basic abilities of LLMs. To evaluate the noise robustness, we"], "reference": "The purpose of constructing a Retrieval-Augmented Generation Benchmark (RGB) is to evaluate the retrieval-augmented generation of LLMs. It evaluates four specific abilities: Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What challenges do language models face when retrieving information from their input contexts in open-domain question answering?", "reference_contexts": ["<1-hop>\n\ndegrades when models must access and use information in the middle of their input con- text (§2.3). For example, when relevant infor- mation is placed in the middle of its input con- text, GPT-3.5-Turbo’s performance on the multi- document question task is lower than its perfor- mance when predicting without any documents (i.e., the closed-book setting; 56.1%). Furthermore, we find that models often have identical performance to their extended-context counterparts, indicating that extended-context models are not necessarily better at using their input context (§2.3). Given that language models struggle to retrieve and use relevant information in the multi-document question answering task, to what extent can lan- guage models even retrieve from their input con- texts? We study this question with a synthetic key- value retrieval task, which is designed to be a mini- mal testbed for the basic ability to retrieve matching tokens from the input context. In this task, models are given a collection of JSON-formatted key-value pairs and must return the value associated with a specific key. Similar to the multi-document QA task, the key-value retrieval task admits controlled changes to the input context length (adding more key-value pairs) and the position of relevant in- formation. Although some models perform the synthetic key-value retrieval task perfectly, other models struggle to simply retrieve matching tokens that occur in the middle of their input context and continue to exhibit a U-shaped performance curve. To better understand why language models strug- gle to robustly access and use information in their input contexts, we study the role of model archi- tecture (decoder-only vs. encoder-decoder), query- aware contextualization, and instruction fine-tuning (§4). We find that: • Encoder-decoder models are relatively robust to changes in the position of relevant informa- tion within their input context, but only when evaluated on sequences within its training- time sequence length. When evaluated on sequences longer than those seen during train- ing, we observe a U-shaped performance curve (§4.1). • Query-aware contextualization (placing the query before and after the documents or key- value pairs) enables near-perfect performance on the synthetic key-value task, but minimally changes trends in multi-document QA (§4.2). • Even base language models (i.e., without in- struction fine-tuning) show a U-shaped per- formance curve as we vary the position of relevant information in the input context. Our results indicate that prompting language models with longer input contexts is a trade-off— providing the language model with more informa- tion may help it perform the downstream task, but it also increases the amount of content that the model must reason over, potentially decreasing ac- curacy. To better understand this trade-off in prac- tice, we perform a case study with retriever-reader models on open-domain question answering (§5). In contrast to our controlled multi-document QA task, where the context always contains exactly one document that answers the question, none or many of the top k documents may contain the an- swer in the open-domain QA setting. When re- trieving from Wikipedia to answer queries from NaturalQuestions-Open, we find that model perfor- mance saturates long before retriever recall satu- rates, indicating that current models fail to effec- tively use additional retrieved documents—using 50 documents instead of 20 retrieved documents only marginally improves performance (∼1.5% for GPT-3.5-Turbo and ∼1% for claude-1.3). Our analysis provides a better understanding of how language models use their input context and introduces new evaluation protocols for future long- context models; to claim that a language model can robustly use information within long input con- texts, it is necessary to show that its performance is minimally affected by the position of the rele- vant information in the input context (e.g., minimal difference in best- and worst-case performance). To facilitate further work on understanding and improving how language models use their input context, we release our code and evaluation data.1 2"], "reference": "Language models struggle to retrieve and use relevant information in the multi-document question answering task, as evidenced by their performance on a synthetic key-value retrieval task. This challenge is particularly pronounced for models that rely on decoder-only architectures or do not employ query-aware contextualization techniques.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What is the trade-off between providing more context and its impact on language model performance in open-domain QA?", "reference_contexts": ["<1-hop>\n\nIs More Context Is Always Better? A Case Study With Open-Domain QA Our results indicate that prompting language mod- els with longer input contexts is a trade-off— providing the language model with more informa- tion may help it perform the downstream task, but it also increases the amount of content that the model must reason over, potentially decreasing accuracy. Even if a language model can take in 16K tokens, is it actually beneficial to provide 16K tokens of context? The answer to this question is ultimately downstream task-specific since it de- pends on the marginal value of the added context and the model’s ability to effectively use long input contexts, but we perform a case study with open- domain question answering on NaturalQuestions- Open to better understand this trade-off in existing language models. We use language models in a standard retriever- reader setup. A retrieval system (Contriever, fine- tuned on MS-MARCO) takes an input query from NaturalQuestions-Open and returns the k docu- ments from Wikipedia with the highest relevance score. To condition language models on these re- trieved documents, we simply include them in the prompt. We evaluate retriever recall and reader accuracy (whether any of the annotated answers appear in the predicted output) as a function of the number of retrieved documents k. We use a subset of NaturalQuestions-Open where the long answer is a paragraph (as opposed to a table or a list). Figure 11 presents retriever recall and open- 5 10 20 30 40 50 Number of Retrieved Docs 50 60 70 80 90 Metric claude-1.3 claude-1.3-100k gpt-3.5-turbo-0613 gpt-3.5-turbo-16k-0613 mpt-30b-instruct longchat-13b-16k contriever recall Figure 11: Retriever recall and model performance as a function of the number of retrieved documents. Model performance saturates long before retriever recall, indi- cating that the models have difficulty making use of the extra retrieved documents. domain QA results. We see that reader model performance saturates long before retriever per- formance saturates, indicating that readers are not effectively using the extra context. Using more than 20 retrieved documents only marginally im- proves reader performance (∼1.5% for GPT-3.5- Turbo and ∼1% for Claude-1.3), while significantly increasing the input context length (and thus la- tency and cost). These results, coupled with the observation that models are often better at retriev- ing and using information at the start or end of the input contexts, suggest that effective rerank- ing of retrieved documents (pushing relevant infor- mation closer to the start of the input context) or ranked list truncation (retrieving fewer documents when appropriate; Arampatzis et al., 2009) may be promising directions for improving how language- model-based readers use retrieved context. 6 Related Work 6.1 Long-Context Language Models There is much prior work in designing performant language models with cheaper scaling than Trans- formers in the context length. Many lines of work pursue Transformer variants with attention modi- fications like recurrence (Dai et al., 2019), factor- izing attention into computationally less intensive approximations (Beltagy et al., 2020; Zaheer et al., 2020), or low-rank approximations (Wang et al., 2020; Peng et al., 2021). Dao et al. (2022) in- stead provide a faster exact attention by a carefully- crafted IO-aware CUDA kernel. Separately, there are attempts to do away with attention entirely to remove quadratic sequence length complexity, of- ten through convolution and/or linear RNNs, e.g., in RWKV (Peng, 2023), S4 (Gu et al., 2022), or Hyena (Poli et al., 2023). Many prior efforts evalu- ate perplexity on a diverse web corpus as a proxy for the ability to process long contexts; this work shows that precise knowledge access on long con- texts may be an added challenge. 6.2 How Do Language Models Use Context? The pioneering work of Khandelwal et al. (2018) showed that small LSTM language models make increasingly coarse use of longer-term context; Sankar et al. (2019) found similar results in di- alogue models. In a similar vein, Daniluk et al. (2017) find that attentive LSTM"], "reference": "The trade-off between providing more context and its impact on language model performance in open-domain QA is that while longer input contexts may help improve downstream task performance, they also increase the amount of content the model must reason over, potentially decreasing accuracy. The study found that reader model performance saturates long before retriever performance saturates, indicating that readers are not effectively using the extra context. Using more than 20 retrieved documents only marginally improves reader performance (∼1.5% for GPT-3.5-Turbo and ∼1% for Claude-1.3), while significantly increasing the input context length (and thus latency and cost).", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What model did ARES use as its in-context learning baseline?", "reference_contexts": ["<1-hop>\n\nExperiments 4.1 Models For our fine-tuned judges, ARES relies on generat- ing cheap but quality synthetic queries and answers using LLMs. For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We se- lected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge. Our fine-tuned LLM judges allow us to rank RAG systems without relying on external APIs, solely using few-shot prompts and deployable LLMs on commercial GPUs. For our in-context learning baseline, we use Ope- nAI’s gpt-3.5-turbo-16k, version 10/23, (Brown et al., 2020) in a zero/few-shot setting. For similar- ity search over in-domain passages, we use FAISS IndexFlatL2 for indexing (Johnson et al., 2019) and OpenAI’s text-embedding-ada-002 for gener- ating embeddings. We use simlarity search over in-domain passages to filter our synthetic queries that cannot retrieve the passage from which they were generated. We use version 0.0.18 of RAGAS in our experiments (James and Es, 2023). 4.2 Datasets Our core experimental goal is to provide a rich picture of where ARES can be applied effectively. To test across multiple types of queries, documents, and answers, we selected all the datasets from the widely-used KILT and SuperGLUE benchmarks for which RAG is appropriate. From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW) (Kwiatkowski et al., 2019; Yang et al., 2018; Akhtar et al., 2023; Dinan et al., 2018). Each dataset uses Wikipedia passages but the queries and answers offer a range of applica- tions. Both NQ and HotpotQA feature direct ques- tions and expect short answers, but NQ uses single passages for reasoning while HotpotQA requires multiple passages for reasoning. Furthermore, FEVER focuses on fact-verification, determining if a passage supports or refutes a given statement, and expects an output of “SUPPORTS” or “REFUTES”. WoW seeks to evaluate dialogue agents by mapping user dialogue to relevant Wikipedia passages be- fore a chatbot generates a paragraph-length chat response incorporating passage knowledge. From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD (Khashabi et al., 2018; Zhang et al., 2018). MultiRC focuses on di- rect questions for seven different domains (News, Wikipedia articles, articles on society/law/justice, articles on history/anthropology, elementary school science textbooks, 9/11 reports, and fiction). ReCoRD focuses on determining the placeholder entity in a statement, focusing on news articles from CNN and the Daily Mail. For MultiRC and ReCoRD, we create open-domain versions of their tasks. For MultiRC, we perform retrieval over its seven sets of domain passages. For ReCoRD, we perform retrieval over its news article passages. The efficacy of ARES relies on its ability to rank different RAG systems while only using a human preference validation set and domain-targeted LLM judges. To test the limits of ARES, we need to sim- ulate the existence of many RAG systems that are separated by small accuracy margins on our eval- uation metrics. For this, we create systems using artificial query-passage-answer triples, in which we empirically know the positive and negative ex- amples of the mock RAG system. We generate these mock splits of the given datasets by select- ing (1) The positive and negative query-passage matches for context relevance, and (2) the positive and negative query-passage-answer matches for an- swer relevance. We include positive and negative examples from our evaluation sets in Table 7. For our positive triples, we can simply use the KILT and SuperGLUE examples without any al- teration. For gathering negative query-passage pairs and query-passage-answer triples, we ran- domly sample passages and answers from either: the same Wikipedia document or an entirely ran- dom Wikipedia document. This sampling allows us to artificially create mock RAG systems for test- ing ARES. By sampling both related and unrelated documents/answers, we hope to better gauge the efficacy of ARES in judging RAG outputs. We do not evaluate answer faithfulness for KILT and SuperGLUE datasets since we do not have human-annotated hallucinated"], "reference": "ARES used OpenAI’s gpt-3.5-turbo-16k, version 10/23, (Brown et al., 2020) as its in-context learning baseline.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What are some limitations of using FLAN-T5-XXL in the ARES evaluation framework?", "reference_contexts": ["<1-hop>\n\nConclusion\nIn this work, we present ARES, a novel automated\nevaluation framework for retrieval-augmented gen-\neration (RAG). ARES offers a novel training\npipeline for fine-tuning lightweight LLM judges\non synthetically generated queries and answers.\nARES can evaluate each component of a RAG sys-\ntem separately to help improve system understand-\ning and create targeted solutions, and it requires\nonly minimal human annotations. For the eight dif-\nferent datasets in KILT, SuperGLUE, and AIS re-\nquiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing\n\flogits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n7\nLimitations\nARES relies on a small set of annotations in the\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\noften require an annotator familiar with the RAG\nsystem’s domain application. While these annota-\ntions can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\n"], "reference": "The use of FLAN-T5-XXL in the ARES evaluation framework has several limitations. It requires substantial GPU-based hardware with about 32GB of memory to run, taking several hours for fine-tuning and generation. Additionally, all of the datasets used in the evaluation are in English, which may limit its applicability to other languages.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What figure shows the overall performance of FLARE and baseline methods across all tasks/datasets?", "reference_contexts": ["<1-hop>\n\nExperimental Results We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task’s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Methods EM F1 Prec. Rec. No retrieval 28.2 36.8 36.5 38.6 Single-time retrieval 39.4 48.8 48.6 51.5 Multi-time retrieval Previous-window 43.2 52.3 51.7 54.5 Previous-sentence 39.0 49.2 48.9 51.8 Question decomposition 47.8 56.4 56.1 58.6 FLAREinstruct (ours) 42.4 49.8 49.1 52.5 FLAREdirect (ours) 51.0 59.7 59.1 62.6 Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets StrategyQA ASQA ASQA-hint WikiAsp Metrics EM EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L No retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4 Single-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9 Multi-time retrieval Previous-window 71.2 39.9 27.0 34.3 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3 Previous-sentence 71.0"], "reference": "Figure 4 shows the overall performance of FLARE and baseline methods across all tasks/datasets, demonstrating that FLARE outperforms all baselines on all tasks/datasets.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What is GraphRAG and how does it differ from vector RAG in terms of its approach to knowledge graph extraction?", "reference_contexts": ["<1-hop>\n\nResults show GraphRAG strongly outperforms vector RAG when using GPT-4 as the LLM. GraphRAG is available as open-source software at https://github.com/microsoft/graphrag. In ad- dition, versions of the GraphRAG approach are also available as extensions to multiple open- source libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), Nebu- laGraph (NebulaGraph, 2024), and Neo4J (Neo4J, 2024). 2 Background 2.1 RAG Approaches and Systems RAG generally refers to any system where a user query is used to retrieve relevant information from external data sources, whereupon this information is incorporated into the generation of a response to the query by an LLM (or other generative AI model, such as a multi-media model). The query and retrieved records populate a prompt template, which is then passed to the LLM (Ram et al., 2023). RAG is ideal when the total number of records in a data source is too large to include in a single prompt to the LLM, i.e. the amount of text in the data source exceeds the LLM’s context window. In canonical RAG approaches, the retrieval process returns a set number of records that are seman- tically similar to the query and the generated answer uses only the information in those retrieved records. A common approach to conventional RAG is to use text embeddings, retrieving records closest to the query in vector space where closeness corresponds to semantic similarity (Gao et al., 2023). While some RAG approaches may use alternative retrieval mechanisms, we collectively refer to the family of conventional approaches as vector RAG. GraphRAG contrasts with vector RAG in its ability to answer queries that require global sensemaking over the entire data corpus. 2 GraphRAG builds upon prior work on advanced RAG strategies. GraphRAG leverages summaries over large sections of the data source as a form of ”self-memory” (described in Cheng et al. 2024), which are later used to answer queries as in Mao et al. 2020). These summaries are generated in parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al., 2023; Gao et al., 2023; Khattab et al., 2022; Shao et al., 2023; Su et al., 2020; Trivedi et al., 2022; Wang et al., 2024). In particular, GraphRAG is similar to other approaches that use hierarchical indexing to create summaries (similar to Kim et al. 2023; Sarthi et al. 2024). GraphRAG contrasts with these approaches by generating a graph index from the source data, then applying graph-based community detection to create a thematic partitioning of the data. 2.2 Using Knowledge Graphs with LLMs and RAG Approaches to knowledge graph extraction from natural language text corpora include rule- matching, statistical pattern recognition, clustering, and embeddings (Etzioni et al., 2004; Kim et al., 2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of research that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022; OpenAI, 2023; Tan et al., 2017; Trajanoska et al., 2023; Yao et al., 2023; Yates et al., 2007; Zhang et al., 2024a). It also adds to a growing body of RAG approaches that use a knowledge graph as an index (Gao et al., 2023). Some techniques use subgraphs, elements of the graph, or properties of the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023) or as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other techniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time an LLM-based agent dynamically traverses a graph with nodes representing document elements (e.g., passages, tables) and edges encoding lexical and semantical similarity or structural relation- ships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of graphs in this context: their inherent modularity (Newman, 2006) and the ability to partition graphs into nested modular communities of closely related nodes (e.g., Louvain, Blondel et al. 2008; Lei- den, Traag et al."], "reference": "GraphRAG is a method for knowledge graph extraction that uses advanced machine learning models, specifically leveraging summaries over large sections of the data source as a form of 'self-memory'. It contrasts with vector RAG by generating a graph index from the source data and applying graph-based community detection to create a thematic partitioning of the data. This approach allows GraphRAG to answer queries that require global sensemaking over the entire data corpus, making it more effective for complex knowledge extraction tasks.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "How can the retrieval module be optimized to aid in a specific downstream task such as question answering?", "reference_contexts": ["<1-hop>\n\ndocuments at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence. 10 20 30 40 50 K Retrieved Docs 39 40 41 42 43 44 NQ Exact Match RAG-Tok RAG-Seq 10 20 30 40 50 K Retrieved Docs 40 50 60 70 80 NQ Answer Recall @ K RAG-Tok RAG-Seq Fixed DPR BM25 10 20 30 40 50 K Retrieved Docs 48 50 52 54 56 Bleu-1 / Rouge-L score RAG-Tok R-L RAG-Tok B-1 RAG-Seq R-L RAG-Seq B-1 Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor- mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved. 5 Related Work Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks. 8 General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench- marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed architecture, by learning a retrieval module to augment pre-trained, generative language models. Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be ﬁne-tuned for strong performance on a variety of tasks. Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9]. Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a ﬁnal output. These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces"], "reference": "Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What training methodology did the researchers use for their Dense Passage Retriever model?", "reference_contexts": ["<1-hop>\n\nExperiments: Passage Retrieval In this section, we evaluate the retrieval perfor- mance of our Dense Passage Retriever (DPR), along with analysis on how its output differs from 6We use the unﬁltered TriviaQA version and discard the noisy evidence documents mined from Bing. 7The improvement of using gold contexts over passages that contain answers is small. See Section 5.2 and Ap- pendix A. Training Retriever Top-20 Top-100 NQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD None BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0 Single DPR 78.4 79.4 73.2 79.8 63.2 85.4 85.0 81.4 89.1 77.2 BM25 + DPR 76.6 79.8 71.0 85.2 71.5 83.8 84.5 80.5 92.7 81.3 Multi DPR 79.4 78.8 75.0 89.1 51.6 86.0 84.7 82.9 93.9 67.6 BM25 + DPR 78.0 79.9 74.7 88.5 66.2 83.9 84.4 82.3 94.1 78.6 Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved passages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details. traditional retrieval methods, the effects of different training schemes and the run-time efﬁciency. The DPR model used in our main experiments is trained using the in-batch negative setting (Sec- tion 3.2) with a batch size of 128 and one additional BM25 negative passage per question. We trained the question and passage encoders for up to 40 epochs for large datasets (NQ, TriviaQA, SQuAD) and 100 epochs for small datasets (TREC, WQ), with a learning rate of 10−5 using Adam, linear scheduling with warm-up and dropout rate 0.1. While it is good to have the ﬂexibility to adapt the retriever to each dataset, it would also be de- sirable to obtain a single retriever that works well across the board. To this end, we train a multi- dataset encoder by combining training data from all datasets excluding SQuAD.8 In addition to DPR, we also present the results of BM25, the traditional retrieval method9 and BM25+DPR, using a linear combination of their scores as the new ranking function. Speciﬁcally, we obtain two initial sets of top-2000 passages based on BM25 and DPR, respectively, and rerank the union of them using BM25(q,p) + λ · sim(q, p) as the ranking function. We used λ = 1.1 based on the retrieval accuracy in the development set. 5.1 Main Results Table 2 compares different passage retrieval sys- tems on ﬁve QA datasets, using the top-k accuracy (k ∈{20, 100}). With the exception of SQuAD, DPR performs consistently better than BM25 on all datasets. The gap is especially large when k is small (e.g., 78.4% vs. 59.1% for top-20 accuracy on Natural Questions). When training with mul- 8SQuAD is limited to a small set of Wikipedia documents and thus introduces unwanted bias. We will discuss this issue more in Section 5.1. 9Lucene implementation. BM25 parameters b = 0.4 (doc- ument length normalization) and k1 = 0.9 (term frequency scaling) are tuned using development sets. 20 40 60 80 100 k: # of retrieved passages 40 50 60 70 80 90 Top-k accuracy (%) BM25 # Train: 1k # Train: 10k # Train: 20k # Train: 40k # Train: all (59k) Figure 1: Retriever top-k accuracy with different num- bers of training examples used in our dense passage re- triever vs BM25. The results are measured on the de- velopment set of Natural Questions. Our DPR trained using 1,000 examples already outperforms BM25. tiple datasets,"], "reference": "The researchers used the in-batch negative setting for training their Dense Passage Retriever model, with a batch size of 128 and one additional BM25 negative passage per question.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "What method did Gao et al. introduce to improve LLMs' generation quality without hurting their versatility?", "reference_contexts": ["<1-hop>\n\nEXPERIMENTS 4.1 TASKS AND DATASETS We conduct evaluations of our SELF-RAG and diverse baselines on a range of downstream tasks, holistically evaluating outputs with metrics designed to assess overall correctness, factuality, and fluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instruc- tions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of our experiments’ settings, including test-time instructions, are available in the Appendix Section B.1. Closed-set tasks include two datasets, i.e., a fact verification dataset about public health (PubHealth; Zhang et al. 2023) and a multiple-choice reasoning dataset created from scientific exams (ARC- 6 Preprint. Challenge; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We aggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2). Short-form generations tasks include two open-domain question answering (QA) datasets, PopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need to answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset, consisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. As the TriviaQA-unfiltered (open) test set is not publicly available, we follow prior work’s validation and test split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate performance based on whether gold answers are included in the model generations instead of strictly requiring exact matching, following Mallen et al. (2023); Schick et al. (2023). Long-form generation tasks include a biography generation task (Min et al., 2023) and a long-form QA task ALCE-ASQA Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al., 2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on MAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA. 5 4.2 BASELINES Baselines without retrievals. We evaluate strong publicly available pre-trained LLMs, Llama27B,13B (Touvron et al., 2023), instruction-tuned models, Alpaca7B,13B (Dubois et al., 2023) (our replication based on Llama2); and models trained and reinforced using private data, Chat- GPT (Ouyang et al., 2022) and Llama2-chat13B. For instruction-tuned LMs, we use the official system prompt or instruction format used during training if publicly available. We also compare our method to concurrent work, CoVE65B (Dhuliawala et al., 2023), which introduces iterative prompt engineering to improve the factuality of LLM generations. Baselines with retrievals. We evaluate models augmented with retrieval at test time or during training. The first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output given the query prepended with the top retrieved documents using the same retriever as in our system. It also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the reflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines with LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same augmentation technique above, as well as perplexity.ai, an InstructGPT-based production search system. The second category includes concurrent methods that are trained with retrieved text passages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning data with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023) to pre-train an LM with API calls (e.g., Wikipedia APIs).6 4.3 EXPERIMENTAL SETTINGS", "<2-hop>\n\nINTRODUCTION State-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023) despite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation (RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs with relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al., 2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce unnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they retrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover, the output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since the models are not explicitly trained to leverage and follow facts from provided passages. This work introduces Self-Reflective Retrieval-augmented Generation (SELF-RAG) to improve an LLM’s generation quality, including its factual accuracy without hurting its versatility, via on-demand retrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on its own generation process given a task input by generating both task output and intermittent special tokens (i.e., reflection tokens). Reflection tokens are categorized into retrieval and critique tokens to indicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular, given an input prompt and preceding generations, SELF-RAG first determines if augmenting the continued generation with retrieved passages would be helpful. If so, it outputs a retrieval token that calls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple retrieved passages, evaluating their relevance and then generating corresponding task outputs (Step 2). It then generates critique tokens to criticize its own output and choose best one (Step 3) in terms of factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which 1Our code and trained models are available at https://selfrag.github.io/. 1 arXiv:2310.11511v1 [cs.CL] 17 Oct 2023 Preprint. Step 1: Retrieve K documents California was named after a fictional island in a Spanish book. Prompt How did US states get their names? US states got their names from a variety of sources. Eleven states are named after an individual person (e.g, California was named after Christopher Columbus). Some states including Texas and Utah, are named after Native American tribe. Retrieval-Augmented Generation (RAG) Ours: Self-reflective Retrieval-Augmented Generation (Self-RAG) Popular names by states. In Texas, Emma is a popular baby name. Of the fifty states, eleven are named after an individual person. Prompt How did US states get their names? + Step 2: Prompt LM with K docs and generate Retriever LM Prompt How did US states get their names? US states got their names from a variety of sources. Retrieve Step 1: Retrieve on demand Prompt + 11 of 50 state names Relevant Step 2: Generate segment in parallel come from persons. Supported Irrelevant Texas is named after a Native American tribe. Step 3: Critique outputs and select best segment origins in a 16th-century novel Las Sergas de Esplandián. California's name has its Relevant Partially US states got their names from a variety of sources. 11 of 50 states names are come from persons. 26 states are named after Native Americans, including Utah. Prompt: Write an essay of your best summer vacation Prompt: Write an essay of your best summer vacation No Retrieval My best summer vacation is when my family and I embarked on a road trip along … My best… > Repeat.… No information in passages Contradictory > Prompt + Prompt + Retrieve Figure 1: Overview of SELF-RAG. SELF-RAG learns to retrieve, critique, and generate text passages to enhance overall generation quality, factuality, and verifiability. consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity (e.g., the bottom figure example does not require factual knowledge) and never second visits the generation quality. Moreover, SELF-RAG provides citations for each segment with its self-assessment of whether the output is supported by the passage, leading"], "reference": "Gao et al. introduced Self-Reflective Retrieval-augmented Generation (SELF-RAG) to improve an LLM's generation quality, including its factual accuracy without hurting its versatility, via on-demand retrieval and self-reflection.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
{"user_input": "How does the effectiveness of FLARE in multihop QA tasks compare to other baselines, and what factors contribute to its performance?", "reference_contexts": ["<1-hop>\n\nExperimental Results We first report overall results across 4 tasks/datasets and compare the performance of FLARE with all the baselines introduced in section 4. We then run ablation experiments to study the efficacy of various design choices of our method. 6.1 Comparison with Baselines Overall results. The overall performance of FLARE and baseline across all tasks/datasets are reported in Figure 4. FLARE outperforms all base- line on all tasks/datasets, indicating that FLARE is a generic method that can effectively retrieve additional information throughout the generation. Among various tasks, multihop QA shows the most significant improvement. This is largely due to the task’s clear definition and specific objective of producing the final answer through a 2-hop rea- soning process, which makes it easier for LMs to generate on-topic output. In contrast, ASQA and WikiAsp are more open-ended, which increases the difficulty of both generation and evaluation. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs to stay on topic. Thorough comparisons with baselines. The per- formance of all baselines on 2WikiMultihopQA are reported in Table 1. FLARE outperforms all baselines by a large margin, which confirms that forward-looking active retrieval is highly effective. Most multi-time retrieval augmented approaches outperform single-time retrieval but with different Methods EM F1 Prec. Rec. No retrieval 28.2 36.8 36.5 38.6 Single-time retrieval 39.4 48.8 48.6 51.5 Multi-time retrieval Previous-window 43.2 52.3 51.7 54.5 Previous-sentence 39.0 49.2 48.9 51.8 Question decomposition 47.8 56.4 56.1 58.6 FLAREinstruct (ours) 42.4 49.8 49.1 52.5 FLAREdirect (ours) 51.0 59.7 59.1 62.6 Table 1: FLARE and baselines on 2WikiMultihopQA. Previous-window (Borgeaud et al., 2022; Ram et al., 2023), previous-sentence (Trivedi et al., 2022), and ques- tion decomposition (Press et al., 2022; Yao et al., 2022) methods are reimplemented for fair comparisons. margins. The improvement of retrieving using the previous sentence is relatively small which we hy- pothesize is mainly because the previous sentence often describes entities or relations different from those in the next sentence in 2WikiMultihopQA. While the previous-window approach might use the first half of a sentence to retrieve information potentially helpful for generating the second half. Among all baselines, the question decomposition approach (Press et al., 2022) achieves the best per- formance. which is not surprising since the in- context exemplars manually annotated with decom- posed sub-questions (Prompt D.2) guide LMs to generate sub-questions that align with the topic/in- tent of future generations. FLARE outperforms this baseline, indicating that manual exemplar an- notation is not necessary for effective future-aware retrieval. The gap between FLAREinstruct and ques- tion decomposition is large, indicating that teaching LMs to generate search queries using task-generic retrieval instructions and exemplars is challenging. We report all metrics for the other datasets in Table 2. FLARE outperforms baselines with re- spect to all metrics. Retrieval using the previ- Datasets StrategyQA ASQA ASQA-hint WikiAsp Metrics EM EM D-F1 R-L DR EM D-F1 R-L DR UniEval E-F1 R-L No retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4 Single-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9 Multi-time retrieval Previous-window 71.2 39.9 27.0 34.3 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3 Previous-sentence 71.0"], "reference": "FLARE outperforms all baselines on multihop QA tasks by a large margin. This is largely due to the task's clear definition and specific objective of producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to generate on-topic output. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs stay on topic.", "synthesizer_name": "multi_hop_specific_query_synthesizer"}
