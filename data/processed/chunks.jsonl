{"chunk_id": "2004.04906_dpr_karpukhin:chunk_0", "chunk_text": "## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_450", "chunk_text": "_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [2], indicating the needs\n\n- f improving retrieval.\n\n\n_\u2217_ Equal contribution\n1The code and trained models have been released at\n[https://github.com/facebookresearch/DPR.](https://github.com/facebookresearch/DPR)\n2For instance, the exact match score on SQuAD v1.1 drops\nfrom above 80% to less than 40% (Yang et al., 2019a).\n\n\n\nRetrieval in open-domain QA is usually implemented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords efficiently with an inverted index and can be seen\nas representing the question and context in highdimensional, sparse vectors (with weighting). Conversely, the _dense_, latent semantic encoding is _com-_\n_plementary_ to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n_\u201cWho is the bad guy in lord of the rings?\u201d_, which can\n\nbe answered from the context _\u201cSala Baker is best_\n\n_known for portraying the villain Sauron in the Lord_\n\n_of the Rings trilogy.\u201d_ A term-based system would\nhave difficulty retrieving such a context, while\na dense retrieval system would be able to better\nmatch _\u201cbad guy\u201d_ with _\u201cvillain\u201d_ and fetch the correct context. Dense encodings are also _learnable_\nby adjusting the embedding functions, which provides additional flexibility to have a task-specific\nrepresentation. With special in-memory data structures and indexing schemes, retrieval can be done\nefficiently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)).\n\nHowever, it is generally believed that learning a good dense vector representation needs a\nlarge number of labeled pairs of question and contexts. Dense retrieval methods have thus never\n\nbe shown to outperform TF-IDF/BM25 for opendomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_900", "chunk_text": " question and contexts. Dense retrieval methods have thus never\n\nbe shown to outperform TF-IDF/BM25 for opendomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\n\n - bjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then finetuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain\n\n\nQA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot fine-tuned using pairs of questions and answers,\nthe corresponding representations could be suboptimal.\n\nIn this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), _with-_\n\n_out_ additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\n\n- f question and passage pairs. Through a series\n\n- f careful ablation studies, our final solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective comparing all pairs of questions and passages in a batch.\nOur _Dense Passage Retriever_ (DPR) is exceptionally strong. It not only outperforms BM25 by a\nlarge margin (65.2% vs. 42.9% in Top-5 accuracy),\nbut also results in a substantial improvement on\nthe end-to-end QA accuracy compared to ORQA\n(41.5% vs. 33.3%) in the open Natural Questions\nsetting (Lee et al., 2019; Kwiatkowski et al., 2019).\n\nOur contributions are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_1350", "chunk_text": " are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\n\n- pen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve comparable or better results on multiple QA datasets in the\n\n- pen-retrieval setting, compared to several, much\ncomplicated systems.\n\n\n**2** **Background**\n\n\nThe problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \u201c _Who first voiced Meg on Family_\n_Guy?_ \u201d or \u201c _Where was the 8th Dalai Lama born?_ \u201d, a\nsystem is required to answer it using a large corpus\n\n- f diversified topics. More specifically, we assume\n\n\n\nthe extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more passages in the corpus. Assume that our collection\ncontains _D_ documents, _d_ 1 _, d_ 2 _, \u00b7 \u00b7 \u00b7, dD_ . We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units [3] and get _M_\ntotal passages in our corpus _C_ = _{p_ 1 _, p_ 2 _, . . ., pM_ _}_,\nwhere each passage _pi_ can be viewed as a sequence\n\n- f tokens _w_ [(] _[i]_ [)]\n1 _[, w]_ 2 [(] _[i]_ [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _|_ [(] _p_ _[i]_ [)] _i|_ [. Given a question] _[ q]_ [,]\n\nthe task is to find a span _ws_ [(] _[i]_ [)] _[, w]_ _s_ [(] _[i]_ +1 [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _e_ [(] _[i]_ [)] from\n\n- ne of the passages _pi_ that can", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_1800", "chunk_text": "_ _s_ [(] _[i]_ +1 [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _e_ [(] _[i]_ [)] from\n\n- ne of the passages _pi_ that can answer the question.\nNotice that to cover a wide variety of domains, the\ncorpus size can easily range from millions of documents (e.g., Wikipedia) to billions (e.g., the Web).\nAs a result, any open-domain QA system needs to\ninclude an efficient _retriever_ component that can select a small set of relevant texts, before applying the\nreader to extract the answer (Chen et al., 2017). [4]\n\nFormally speaking, a retriever _R_ : ( _q, C_ ) _\u2192CF_\nis a function that takes as input a question _q_ and a\ncorpus _C_ and returns a much smaller _filter set_ - f\ntexts _CF \u2282C_, where _|CF_ _|_ = _k \u226a|C|_ . For a fixed\n_k_, a _retriever_ can be evaluated in isolation on _top-k_\n_retrieval accuracy_, which is the fraction of questions for which _CF_ contains a span that answers the\nquestion.\n\n\n**3** **Dense Passage Retriever (DPR)**\n\n\nWe focus our research in this work on improving the _retrieval_ component in open-domain QA.\nGiven a collection of _M_ text passages, the goal of\n\n- ur dense passage retriever (DPR) is to index all\nthe passages in a low-dimensional and continuous\nspace, such that it can retrieve efficiently the top\n_k_ passages relevant to the input question for the\nreader at run-time. Note that _M_ can be very large\n(e.g., 21 million passages in our experiments, described in Section 4.1) and _k_ is usually small, such\n\nas 20\u2013100.\n\n\n**3.1** **Overview**\n\n\nOur dense passage retriever (DPR) uses a dense\nencoder _EP_ ( _\u00b7_ ) which maps any text passage to a _d_ dimensional real-valued vectors and builds an index\n\nfor all the _M_ passages that we will use for retrieval.\n\n\n3The ideal size and boundary of a text passage are functions of both the retriever and reader. We", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_2250", "chunk_text": " to a _d_ dimensional real-valued vectors and builds an index\n\nfor all the _M_ passages that we will use for retrieval.\n\n\n3The ideal size and boundary of a text passage are functions of both the retriever and reader. We also experimented\nwith natural paragraphs in our preliminary trials and found that\nusing fixed-length passages performs better in both retrieval\nand final QA accuracy, as observed by Wang et al. (2019).\n4Exceptions include (Seo et al., 2019) and (Roberts et al.,\n2020), which _retrieves_ and _generates_ the answers, respectively.\n\n\nAt run-time, DPR applies a different encoder _EQ_ ( _\u00b7_ )\nthat maps the input question to a _d_ - dimensional\nvector, and retrieves _k_ passages of which vectors\nare the closest to the question vector. We define\nthe similarity between the question and the passage\nusing the dot product of their vectors:\n\n\nsim( _q, p_ ) = _EQ_ ( _q_ ) [\u22ba] _EP_ ( _p_ ) _._ (1)\n\n\nAlthough more expressive model forms for measuring the similarity between a question and a passage\ndo exist, such as networks consisting of multiple\nlayers of cross attentions, the similarity function\nneeds to be decomposable so that the representations of the collection of passages can be precomputed. Most decomposable similarity functions\nare some transformations of Euclidean distance\n\n(L2). For instance, cosine is equivalent to inner\nproduct for unit vectors and the Mahalanobis distance is equivalent to L2 distance in a transformed\nspace. Inner product search has been widely used\nand studied, as well as its connection to cosine\nsimilarity and L2 distance (Mussmann and Ermon,\n2016; Ram and Gray, 2012). As our ablation study\nfinds other similarity functions perform comparably (Section 5.2; Appendix B), we thus choose\nthe simpler inner product function and improve the\ndense passage retriever by learning better encoders.\n\n\n**Encoders** Although in principle the question and\npassage encoders can be implemented by any neural networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, uncased) and take the representation at the [CLS]\ntoken as the output, so _d_ = 768.\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_2700", "chunk_text": " networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, uncased) and take the representation at the [CLS]\ntoken as the output, so _d_ = 768.\n\n\n**Inference** During inference time, we apply the\npassage encoder _EP_ to all the passages and index\nthem using FAISS (Johnson et al., 2017) offline.\nFAISS is an extremely efficient, open-source library for similarity search and clustering of dense\nvectors, which can easily be applied to billions of\nvectors. Given a question _q_ at run-time, we derive\nits embedding _vq_ = _EQ_ ( _q_ ) and retrieve the top _k_\npassages with embeddings closest to _vq_ .\n\n\n**3.2** **Training**\n\n\nTraining the encoders so that the dot-product similarity (Eq. (1)) becomes a good ranking function\nfor retrieval is essentially a _metric learning_ problem (Kulis, 2013). The goal is to create a vector\nspace such that relevant pairs of questions and passages will have smaller distance (i.e., higher simi\n\n\nlarity) than the irrelevant ones, by learning a better\nembedding function.\nLet _D_ = _{\u27e8qi, p_ [+] _i_ _[, p][\u2212]_ _i,_ 1 _[,][ \u00b7 \u00b7 \u00b7][, p][\u2212]_ _i,n_ _[\u27e9}]_ _i_ _[m]_ =1 [be the]\ntraining data that consists of _m_ instances. Each\ninstance contains one question _qi_ and one relevant\n(positive) passage _p_ [+] _i_ [, along with] _[ n]_ [ irrelevant (neg-]\native) passages _p_ _[\u2212]_ _i,j_ [. We optimize the loss function]\nas the negative log likelihood of the positive pas\nsage:\n\n\n_L_ ( _qi, p_ [+] _i_ _[, p][\u2212]_ _i,_ 1 _[,][ \u00b7 \u00b7 \u00b7][, p][\u2212]_ _i,n_ [)] (2)\n\n_e_ [sim(] _[q][i][,p]_ _i_ [+][)]\n\n= _\u2212_\nlog\n\n_e_ [sim(] _[q][", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_3150", "chunk_text": "\u2212]_ _i,n_ [)] (2)\n\n_e_ [sim(] _[q][i][,p]_ _i_ [+][)]\n\n= _\u2212_\nlog\n\n_e_ [sim(] _[q][i][,p]_ _i_ [+][)] + ~~[\ufffd]~~ _[n]_ _j_ =1 _[e]_ [sim(] _[q][i][,p]_ _i,j_ _[\u2212]_ [)] _[.]_\n\n\n**Positive and negative passages** For retrieval\nproblems, it is often the case that positive examples\nare available explicitly, while negative examples\nneed to be selected from an extremely large pool.\nFor instance, passages relevant to a question may\nbe given in a QA dataset, or can be found using the\nanswer. All other passages in the collection, while\nnot specified explicitly, can be viewed as irrelevant\nby default. In practice, how to select negative examples is often overlooked but could be decisive\nfor learning a high-quality encoder. We consider\nthree different types of negatives: (1) Random: any\nrandom passage from the corpus; (2) BM25: top\npassages returned by BM25 which don\u2019t contain\nthe answer but match most question tokens; (3)\nGold: positive passages paired with other questions\nwhich appear in the training set. We will discuss the\nimpact of different types of negative passages and\ntraining schemes in Section 5.2. Our best model\nuses gold passages from the same mini-batch and\n\n- ne BM25 negative passage. In particular, re-using\ngold passages from the same batch as negatives\ncan make the computation efficient while achieving great performance. We discuss this approach\nbelow.\n\n\n**In-batch negatives** Assume that we have _B_\nquestions in a mini-batch and each one is associated with a relevant passage. Let **Q** and **P** be the\n( _B_ _\u00d7d_ ) matrix of question and passage embeddings\nin a batch of size _B_ . **S** = **QP** _[T]_ is a ( _B \u00d7 B_ ) matrix of similarity scores, where each row of which\ncorresponds to a question, paired with _B_ passages.\nIn this way, we reuse computation and effectively\ntrain on _B_ [2] ( _qi_,", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_3600", "chunk_text": " B_ ) matrix of similarity scores, where each row of which\ncorresponds to a question, paired with _B_ passages.\nIn this way, we reuse computation and effectively\ntrain on _B_ [2] ( _qi_, _pj_ ) question/passage pairs in each\nbatch. Any ( _qi_, _pj_ ) pair is a positive example when\n_i_ = _j_, and negative otherwise. This creates _B_ training instances in each batch, where there are _B \u2212_ 1\n\n\nnegative passages for each question.\nThe trick of in-batch negatives has been used in\nthe full batch setting (Yih et al., 2011) and more\nrecently for mini-batch (Henderson et al., 2017;\nGillick et al., 2019). It has been shown to be an\neffective strategy for learning a dual-encoder model\nthat boosts the number of training examples.\n\n\n**4** **Experimental Setup**\n\n\nIn this section, we describe the data we used for\nexperiments and the basic setup.\n\n\n**4.1** **Wikipedia Data Pre-processing**\n\n\nFollowing (Lee et al., 2019), we use the English\nWikipedia dump from Dec. 20, 2018 as the source\ndocuments for answering questions. We first apply\nthe pre-processing code released in DrQA (Chen\net al., 2017) to extract the clean, text-portion of\narticles from the Wikipedia dump. This step removes semi-structured data, such as tables, infoboxes, lists, as well as the disambiguation pages.\nWe then split each article into multiple, disjoint text\nblocks of 100 words as _passages_, serving as our\nbasic retrieval units, following (Wang et al., 2019),\nwhich results in 21,015,324 passages in the end. [5]\n\nEach passage is also prepended with the title of the\nWikipedia article where the passage is from, along\nwith an [SEP] token.\n\n\n**4.2** **Question Answering Datasets**\n\n\nWe use the same five QA datasets and training/dev/testing splitting method as in previous\nwork (Lee et al., 2019). Below we briefly describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\n**Natural Questions (NQ)** (Kwiatkowski", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_4050", "chunk_text": "/testing splitting method as in previous\nwork (Lee et al., 2019). Below we briefly describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\n**Natural Questions (NQ)** (Kwiatkowski et al.,\n2019) was designed for end-to-end question answering. The questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identified by annotators.\n**TriviaQA** (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web.\n\n**WebQuestions (WQ)** (Berant et al., 2013) consists\n\n- f questions selected using Google Suggest API,\nwhere the answers are entities in Freebase.\n**CuratedTREC (TREC)** (Baudis and\u02c7 Sediv [\u02c7] y`,\n2015) sources questions from TREC QA tracks\n\n\n5However, Wang et al. (2019) also propose splitting documents into overlapping passages, which we do not find advantageous compared to the non-overlapping version.\n\n\n\n**Dataset** **Train** **Dev** **Test**\n\n\nNatural Questions 79,168 58,880 8,757 3,610\nTriviaQA 78,785 60,413 8,837 11,313\nWebQuestions 3,417 2,474 361 2,032\nCuratedTREC 1,353 1,125 133 694\nSQuAD 78,713 70,096 8,886 10,570\n\n\nTable 1: Number of questions in each QA dataset. The\ntwo columns of **Train** denote the original training examples in the dataset and the actual questions used for\ntraining DPR after filtering. See text for more details.\n\n\nas well as various Web sources and is intended for\n\n- pen-domain QA from unstructured corpora.\n**SQuAD v1.1** (Rajpurkar et al., 2016) is a popular benchmark dataset for reading comprehension.\nAnnotators were presented with a Wikipedia paragraph, and asked to write questions that could be\nanswered from the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_4500", "chunk_text": " the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n\n\n**Selection of positive passages** Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA [6], we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the question will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. [7] We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the number of questions in training/dev/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n\n\n**5** **Experiments: Passage Retrieval**\n\n\nIn this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n\n\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Appendix A.\n\n\n**Training** **Retriever** **Top-20** **Top-100**\nNQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD\n\n\nNone BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0\n\n\nDPR 78.4 79.4 73.2 79.8 63.2 85.4 **85.0** 81.4 89.1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_4950", "chunk_text": ".1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7 **81.3**\n\n\nDPR **79.4** 78.8 **75.0** **89.1** 51.6 **86.0** 84.7 **82.9** 93.9 67.6\nMulti\nBM25 + DPR 78.0 **79.9** 74.7 88.5 66.2 83.9 84.4 82.3 **94.1** 78.6\n\n\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved\npassages that contain the answer. _Single_ and _Multi_ denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.\n\n\n\ntraditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10 _[\u2212]_ [5] using Adam, linear\nscheduling with warm-up and dropout rate 0 _._ 1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be desirable to obtain a single retriever that works well\nacross the board. To this end, we train a _multi_ dataset encoder by combining training data from\nall datasets excluding SQuAD. [8] In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_5400", "chunk_text": " BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25( _q_, _p_ ) + _\u03bb \u00b7_ sim( _q, p_ ) as the ranking function.\nWe used _\u03bb_ = 1 _._ 1 based on the retrieval accuracy in\nthe development set.\n\n\n**5.1** **Main Results**\n\n\nTable 2 compares different passage retrieval systems on five QA datasets, using the top- _k_ accuracy\n( _k \u2208{_ 20 _,_ 100 _}_ ). With the exception of SQuAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when _k_ is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\n\n- n Natural Questions). When training with mul\n\n8SQuAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\n\nmore in Section 5.1.\n\n9\n[Lucene implementation. BM25 parameters](https://lucene.apache.org/) _b_ = 0 _._ 4 (document length normalization) and _k_ 1 = 0 _._ 9 (term frequency\nscaling) are tuned using development sets.\n\n\n\nFigure 1: Retriever top- _k_ accuracy with different numbers of training examples used in our dense passage retriever vs BM25. The results are measured on the de\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n\n\ntiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_5850", "chunk_text": "-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\n\n- nly 500+ Wikipedia articles and thus the distribution of training examples is extremely biased, as\nargued previously by Lee et al. (2019).\n\n\n**5.2** **Ablation Study on Model Training**\n\n\nTo understand further how different model training\n\n- ptions affect the results, we conduct several additional experiments and discuss our findings below.\n\n\n\n\n\n\n**Sample efficiency** We explore how many training examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top- _k_\nretrieval accuracy with respect to different numbers of training examples, measured on the devel\n- pment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 examples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question\u2013passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently.\n\n\n**In-batch negative training** We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3.\nThe top block is the standard 1-of- _N_ training setting, where each question in the batch is paired\nwith a positive passage and its own set of _n_ negative passages (Eq. (2)). We find that the choice\n\n- f negatives \u2014 random, BM25 or gold passages\n(positive passages from other questions) \u2014 does\nnot impact the top- _k_ accuracy much in this setting\nwhen _k \u2265_ 20.\n\nThe middle bock is the in-batch negative training\n(Section 3.2) setting. We find that using a similar\nconfiguration (7 gold negative passages), in-batch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\n\n- r from the whole training set. Effectively, in-batch\nnegative training is an easy and memory", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_6300", "chunk_text": "atch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\n\n- r from the whole training set. Effectively, in-batch\nnegative training is an easy and memory-efficient\nway to reuse the negative examples already in the\nbatch rather than creating new ones. It produces\nmore pairs and thus increases the number of training examples, which might contribute to the good\nmodel performance. As a result, accuracy consistently improves as the batch size grows.\nFinally, we explore in-batch negative training\nwith additional \u201chard\u201d negative passages that have\nhigh BM25 scores given the question, but do not\ncontain the answer string (the bottom block). These\nadditional passages are used as negative passages\nfor all questions in the same batch. We find that\nadding a single BM25 negative passage improves\nthe result substantially while adding two does not\nhelp further.\n\n\n**Impact of gold passages** We use passages that\nmatch the gold contexts in the original datasets\n(when available) as positive examples (Section 4.2).\n\n\n\n**Type** **#N** **IB** **Top-5** **Top-20** **Top-100**\n\n\nRandom 7 47.0 64.3 77.8\n\nBM25 7 50.0 63.3 74.8\n\nGold 7 42.6 63.1 78.3\n\n\nGold 7 51.1 69.1 80.8\n\nGold 31 52.1 70.8 82.1\n\nGold 127 55.8 73.0 83.1\n\n\nG.+BM25 [(1)] 31+32 65.0 77.3 84.4\nG.+BM25 [(2)] 31+64 64.5 76.4 84.0\nG.+BM25 [(1)] 127+128 **65.8** **78.0** **84.9**\n\n\nTable 3: Comparison of different training schemes,\nmeasured as top- _k_ retrieval accuracy on Natural Questions (development set). #N: number of negative\nexamples, IB: in-batch training. G.+BM25 [(1)] and\nG.+BM25 [(2)] denote in-batch training with 1 or 2 additional BM25 negatives, which serve as", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_6750", "chunk_text": "N: number of negative\nexamples, IB: in-batch training. G.+BM25 [(1)] and\nG.+BM25 [(2)] denote in-batch training with 1 or 2 additional BM25 negatives, which serve as negative passages for all questions in the batch.\n\n\nOur experiments on Natural Questions show that\nswitching to distantly-supervised passages (using\nthe highest-ranked BM25 passage that contains the\nanswer), has only a small impact: 1 point lower\ntop- _k_ accuracy for retrieval. Appendix A contains\nmore details.\n\n\n**Similarity and loss** Besides dot product, cosine\nand Euclidean L2 distance are also commonly used\nas decomposable similarity functions. We test these\nalternatives and find that L2 performs comparable to dot product, and both of them are superior\nto cosine. Similarly, in addition to negative loglikelihood, a popular option for ranking is triplet\nloss, which compares a positive passage and a negative one directly with respect to a question (Burges\net al., 2005). Our experiments show that using\ntriplet loss does not affect the results much. More\ndetails can be found in Appendix B.\n\n\n**Cross-dataset generalization** One interesting\nquestion regarding DPR\u2019s discriminative training\nis how much performance degradation it may suffer from a non-iid setting. In other words, can\nit still generalize well when directly applied to\na different dataset without additional fine-tuning?\nTo test the cross-dataset generalization, we train\nDPR on Natural Questions only and test it directly\n\n- n the smaller WebQuestions and CuratedTREC\ndatasets. We find that DPR generalizes well, with\n3-5 points loss from the best performing fine-tuned\nmodel in top-20 retrieval accuracy (69.9/86.3 vs.\n75.0/89.1 for WebQuestions and TREC, respectively), while still greatly outperforming the BM25\nbaseline (55.0/70.9).\n\n\n**5.3** **Qualitative Analysis**\n\n\nAlthough DPR performs better than BM25 in general, passages retrieved by these two methods differ qualitatively. Term-matching methods like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_7200", "chunk_text": " like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_7650", "chunk_text": " highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer. The passage\nselection model serves as a reranker through crossattention between the question and the passage. Although cross-attention is not feasible for retrieving\nrelevant passages in a large corpus due to its nondecomposable nature, it has more capacity than the\ndual-encoder model sim( _q, p_ ) as in Eq. (1). Applying it to selecting the passage from a small number\n\n- f retrieved candidates has been shown to work\n\nwell (Wang et al., 2019, 2018; Lin et al., 2018).\nSpecifically, let **P** _i \u2208_ R _[L][\u00d7][h]_ (1 _\u2264_ _i \u2264_ _k_ ) be\na BERT (base, uncased in our experiments) representation for the _i_ - th passage, where _L_ is the\nmaximum length of the passage and _h_ the hidden\ndimension. The probabilities of a token being the\nstarting/ending positions of an answer span and a\npassage being selected are defined as:\n\n\n\n_P_ start _,i_ ( _s_ ) = softmax\ufffd **P** _i_ **w** start\ufffd\n\n\n\n_s_ _[,]_ (3)\n\n_t_ _[,]_ (4)\n\n\n\n_P_ end _,i_ ( _t_ ) = softmax\ufffd **P** _i_ **w** end\ufffd\n\n\n\n_P_ selected( _i_ ) = softmax\ufffd **\u02c6P** \u22ba **w** selected\ufffd\n\n\n\n_i_ _[,]_ [ (5)]\n\n\n\nwhere **P** **[\u02c6]** = [ **P** [[CLS]] 1 _, . . .,_ **P** [[CLS]] _k_ ] _\u2208_ R _[h][\u00d7][k]_ and\n**w** start _,_ **w** end _,_ **w** selected _\u2208_ R _[h]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_8100", "chunk_text": "]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7 P_ end _,i_ ( _t_ ), and\na passage selection score of the _i_ - th passage as\n_P_ selected( _i_ ).\nDuring training, we sample one positive and\n\n\u02dc\n_m_ _\u2212_ 1 negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. \u02dc _m_ is a hyper-parameter and we\nuse \u02dc _m_ = 24 in all the experiments. The training objective is to maximize the marginal log-likelihood\n\n- f all the correct answer spans in the positive passage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\n\n- f the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQuAD)\nand 4 for small (TREC, WQ) datasets, and tune _k_\n\n- n the development set. For experiments on small\ndatasets under the _Multi_ setting, in which using\n\n- ther datasets is allowed, we fine-tune the reader\ntrained on Natural Questions to the target dataset.\nAll experiments were done on eight 32GB GPUs.\n\n\n**6.2** **Results**\n\n\nTable 4 summarizes our final end-to-end QA results, measured by _exact match_ with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can\n\n\n**Training** **Model** **NQ** **TriviaQA** **WQ** **TREC** **SQuAD**\n\n\nSingle BM25+BERT (Lee et al., 2019) 26.5 47.1 17.7 21.3 33.2\nSingle ORQA (Lee et al., 2019) 33.3 45.0 36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9    -    -    Single GraphRetriever (Min et al., 201", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_8550", "chunk_text": "36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9    -    -    Single GraphRetriever (Min et al., 2019b) 34.5 56.0 36.4    -    Single PathRetriever (Asai et al., 2020) 32.6    -    -    - **56.5**\nSingle REALMWiki (Guu et al., 2020) 39.2    - 40.2 46.8    Single REALMNews (Guu et al., 2020) 40.4    - 40.7 42.9    \n\nBM25 32.6 52.4 29.9 24.9 38.1\n\nSingle DPR **41.5** 56.8 34.6 25.9 29.8\nBM25+DPR 39.0 57.0 35.2 28.0 36.7\n\n\nDPR **41.5** 56.8 **42.4** 49.4 24.1\nMulti\nBM25+DPR 38.8 **57.9** 41.1 **50.6** 35.8\n\n\nTable 4: End-to-end QA (Exact Match) Accuracy. The first block of results are copied from their cited papers.\nREALMWiki and REALMNews are the same model but pretrained on Wikipedia and CC-News, respectively. _Single_\nand _Multi_ denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets\n(all except SQuAD). For WQ and TREC in the _Multi_ setting, we fine-tune the reader trained on NQ.\n\n\n\nsee that higher retriever accuracy typically leads to\nbetter final QA results: in all cases except SQuAD,\nanswers extracted from the passages retrieved by\nDPR are more likely to be correct, compared to\nthose from BM25. For large datasets like NQ and\nTriviaQA, models trained using multiple datasets\n(Multi) perform comparably to those trained using\nthe individual training set (Single). Conversely,\n\n- n smaller datasets like WQ and TREC, the multidataset setting has a clear advantage. Overall, our\nD", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_9000", "chunk_text": " trained using multiple datasets\n(Multi) perform comparably to those trained using\nthe individual training set (Single). Conversely,\n\n- n smaller datasets like WQ and TREC, the multidataset setting has a clear advantage. Overall, our\nDPR-based models outperform the previous state\n- f-the-art results on four out of the five datasets,\nwith 1% to 12% absolute differences in exact match\n\naccuracy. It is interesting to contrast our results to\nthose of ORQA (Lee et al., 2019) and also the\nconcurrently developed approach, REALM (Guu\net al., 2020). While both methods include additional pretraining tasks and employ an expensive\nend-to-end training regime, DPR manages to outperform them on both NQ and TriviaQA, simply\nby focusing on learning a strong passage retrieval\nmodel using pairs of questions and answers. The\nadditional pretraining tasks are likely more useful\n\n- nly when the target training sets are small. Although the results of DPR on WQ and TREC in the\nsingle-dataset setting are less competitive, adding\nmore question\u2013answer pairs helps boost the performance, achieving the new state of the art.\n\n\nTo compare our pipeline training approach with\njoint learning, we run an ablation on Natural Questions where the retriever and reader are jointly\n\n\n\ntrained, following Lee et al. (2019). This approach\n\n- btains a score of 39.8 EM, which suggests that our\nstrategy of training a strong retriever and reader in\nisolation can leverage effectively available supervision, while outperforming a comparable joint training approach with a simpler design (Appendix D).\nOne thing worth noticing is that our reader does\nconsider more passages compared to ORQA, although it is not completely clear how much more\ntime it takes for inference. While DPR processes\nup to 100 passages for each question, the reader\nis able to fit all of them into one batch on a single 32GB GPU, thus the latency remains almost\nidentical to the single passage case (around 20ms).\nThe exact impact on throughput is harder to measure: ORQA uses 2-3x longer passages compared\nto DPR (288 word pieces compared to our 100\ntokens) and the computational complexity is superlinear in passage length. We also note that we\nfound _k_ = 50 to be optimal for NQ, and", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_9450", "chunk_text": " longer passages compared\nto DPR (288 word pieces compared to our 100\ntokens) and the computational complexity is superlinear in passage length. We also note that we\nfound _k_ = 50 to be optimal for NQ, and _k_ = 10\nleads to only marginal loss in exact match accuracy (40.8 vs. 41.5 EM on NQ), which should be\nroughly comparable to ORQA\u2019s 5-passage setup.\n\n\n**7** **Related Work**\n\n\nPassage retrieval has been an important component for open-domain QA (Voorhees, 1999). It\nnot only effectively reduces the search space for\nanswer extraction, but also identifies the support\ncontext for users to verify the answer. Strong sparse\nvector space models like TF-IDF or BM25 have\n\n\nbeen used as the standard method applied broadly\nto various QA tasks (e.g., Chen et al., 2017; Yang\net al., 2019a,b; Nie et al., 2019; Min et al., 2019a;\nWolfson et al., 2020). Augmenting text-based retrieval with external structured information, such\nas knowledge graph and Wikipedia hyperlinks, has\nalso been explored recently (Min et al., 2019b; Asai\net al., 2020).\n\n\nThe use of dense vector representations for retrieval has a long history since Latent Semantic\nAnalysis (Deerwester et al., 1990). Using labeled\npairs of queries and documents, discriminatively\ntrained dense encoders have become popular recently (Yih et al., 2011; Huang et al., 2013; Gillick\net al., 2019), with applications to cross-lingual\ndocument retrieval, ad relevance prediction, Web\nsearch and entity retrieval. Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically\nrelevant text pairs, even without exact token matching. The dense representation alone, however, is\ntypically inferior to the sparse one. While not the\nfocus of this work, dense representations from pretrained models, along with cross-attention mechanisms, have also been shown effective in passage\n\n- r dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_9900", "chunk_text": " shown effective in passage\n\n- r dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates\nthe feasibility of full dense retrieval in IR tasks.\nInstead of employing the dual-encoder framework,\nthey introduced a late-interaction operator on top\n\n- f the BERT encoders.\n\n\nDense retrieval for open-domain QA has been\nexplored by Das et al. (2019), who propose to retrieve relevant passages iteratively using reformulated question vectors. As an alternative approach\nthat skips passage retrieval, Seo et al. (2019) propose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input questions efficiently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al. (2019) jointly train\nthe question encoder and reader. Their approach\n\n- utperforms the BM25 plus reader paradigm on\nmultiple open-domain QA datasets in QA accuracy,\nand is further extended by REALM (Guu et al.,\n2020), which includes tuning the passage encoder\nasynchronously by re-indexing the passages during training. The pretraining objective has also\nrecently been improved by Xiong et al. (2020b).\nIn contrast, our model provides a simple and yet\n\n\n\neffective solution that shows stronger empirical performance, without relying on additional pretraining\n\n- r complex joint training schemes.\nDPR has also been used as an important module in very recent work. For instance, extending\nthe idea of leveraging hard negatives, Xiong et al.\n(2020a) use the retrieval model trained in the previous iteration to discover new negatives and construct a different set of examples in each training\niteration. Starting from our trained DPR model,\nthey show that the retrieval performance can be\nfurther improved. Recent work (Izacard and Grave,\n2020; Lewis et al., 2020b) have also shown that\nDPR can be combined with generation models\nsuch as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2019), achieving good performance on\n\n- pen-domain QA and other knowledge-intensive\ntasks.\n\n\n**8** **Conclusion**\n\n\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_10350", "chunk_text": "Raffel et al., 2019), achieving good performance on\n\n- pen-domain QA and other knowledge-intensive\ntasks.\n\n\n**8** **Conclusion**\n\n\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the traditional sparse retrieval component in open-domain\nquestion answering. While a simple dual-encoder\napproach can be made to work surprisingly well,\nwe showed that there are some critical ingredients\nto training a dense retriever successfully. Moreover,\n\n- ur empirical analysis and ablation studies indicate\nthat more complex model frameworks or similarity\nfunctions do not necessarily provide additional values. As a result of improved retrieval performance,\nwe obtained new state-of-the-art results on multiple\n\n- pen-domain question answering benchmarks.\n\n\n**Acknowledgments**\n\n\nWe thank the anonymous reviewers for their helpful\ncomments and suggestions.\n\n\n**References**\n\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over Wikipedia graph\nfor question answering. In _International Conference_\n\n_on Learning Representations (ICLR)_ .\n\n\nPetr Baudi\u02c7s and Jan Sediv`y. 2015. [\u02c7] Modeling of the\nquestion answering task in the yodaqa system. In _In-_\n_ternational Conference of the Cross-Language Eval-_\n_uation Forum for European Languages_, pages 222\u2013\n228. Springer.\n\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\n\n\nquestion-answer pairs. In _Empirical Methods in Nat-_\n_ural Language Processing (EMNLP)_ .\n\n\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard\nS\u00a8ackinger, and Roopak Shah. 1994. Signature verification using a \u201cSiamese\u201d time delay neural network.\nIn _NIPS_, pages 737\u2013744.\n\n\nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,\nMatt Deeds, Nicole Hamilton, and Greg Hullender.\n2005. Learning to rank using gradient descent. In\n_Proceedings of the 22nd international conference on_\n_Machine learning_, pages 89\u201396.\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_10800", "chunk_text": " descent. In\n_Proceedings of the 22nd international conference on_\n_Machine learning_, pages 89\u201396.\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer opendomain questions. In _Association for Computa-_\n_tional Linguistics (ACL)_, pages 1870\u20131879.\n\n\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nand Andrew McCallum. 2019. Multi-step retrieverreader interaction for scalable open-domain question\nanswering. In _International Conference on Learn-_\n_ing Representations (ICLR)_ .\n\n\nScott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman.\n1990. Indexing by latent semantic analysis. _Jour-_\n_nal of the American society for information science_,\n41(6):391\u2013407.\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In _North American Association for Com-_\n_putational Linguistics (NAACL)_ .\n\n\nDavid A Ferrucci. 2012. Introduction to \u201cThis is Watson\u201d. _IBM Journal of Research and Development_,\n56(3.4):1\u20131.\n\n\nDaniel Gillick, Sayali Kulkarni, Larry Lansing,\nAlessandro Presta, Jason Baldridge, Eugene Ie, and\nDiego Garcia-Olano. 2019. Learning dense representations for entity retrieval. In _Computational Nat-_\n_ural Language Learning (CoNLL)_ .\n\n\nRuiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, and\nDavid Simcha. 2016. Quantization based fast inner\nproduct search. In _Artificial Intelligence and Statis-_\n_tics_, pages 482\u2013490.\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM:\nRetrieval-augmented language model pre-training.\n_ArXiv_, abs/2002.08909.\n\n\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yunhsuan Sung, L", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_11250", "chunk_text": "0. REALM:\nRetrieval-augmented language model pre-training.\n_ArXiv_, abs/2002.08909.\n\n\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yunhsuan Sung, L\u00b4aszl\u00b4o Luk\u00b4acs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart\nreply. _ArXiv_, abs/1705.00652.\n\n\nPo-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,\nAlex Acero, and Larry Heck. 2013. Learning deep\nstructured semantic models for Web search using\n\n\n\nclickthrough data. In _ACM International Confer-_\n_ence on Information and Knowledge Management_\n_(CIKM)_, pages 2333\u20132338.\n\n\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architectures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In _International Conference_\n\n_on Learning Representations (ICLR)_ .\n\n\nGautier Izacard and Edouard Grave. 2020. Leveraging\npassage retrieval with generative models for open domain question answering. _ArXiv_, abs/2007.01282.\n\n\nJeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou. 2017.\nBillion-scale similarity search with GPUs. _ArXiv_,\nabs/1702.08734.\n\n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In _Association for Computational Lin-_\n_guistics (ACL)_, pages 1601\u20131611.\n\n\nOmar Khattab and Matei Zaharia. 2020. ColBERT:\nEfficient and effective passage search via contextualized late interaction over BERT. In _ACM SIGIR_\n_Conference on Research and Development in Infor-_\n_mation Retrieval (SIGIR)_, pages 39\u201348.\n\n\nBrian Kulis. 2013. Metric learning: A survey. _Foun-_\n_dations and Trends in Machine Learning_, 5(4):287\u2013", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_13050", "chunk_text": "AAAI)_ .\n\n\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. 2019. Multi-passage BERT:\nA globally normalized bert model for open-domain\nquestion answering. In _Empirical Methods in Natu-_\n_ral Language Processing (EMNLP)_ .\n\n\nTomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question understanding benchmark. _Transactions of the Associa-_\n_tion of Computational Linguistics (TACL)_ .\n\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020a. Approximate nearest neighbor\nnegative contrastive learning for dense text retrieval.\n_ArXiv_, abs/2007.00808.\n\n\nWenhan Xiong, Hankang Wang, and William Yang\nWang. 2020b. Progressively pretrained dense corpus\nindex for open-domain question answering. _ArXiv_,\nabs/2005.00038.\n\n\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.\nEnd-to-end open-domain question answering with\nbertserini. In _North American Association for Com-_\n_putational Linguistics (NAACL)_, pages 72\u201377.\n\n\nWei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming\nLi, and Jimmy Lin. 2019b. Data augmentation for\nbert fine-tuning in open-domain question answering.\n_ArXiv_, abs/1904.06652.\n\n\nWen-tau Yih, Kristina Toutanova, John C Platt, and\nChristopher Meek. 2011. Learning discriminative\nprojections for text similarity measures. In _Com-_\n_putational Natural Language Learning (CoNLL)_,\npages 247\u2013256.\n\n\n**A** **Distant Supervision**\n\n\nWhen training our final DPR model using Natural\nQuestions, we use the passages in our collection\nthat best match the gold context as the positive\npassages. As some QA datasets contain only the\nquestion", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_13500", "chunk_text": "A** **Distant Supervision**\n\n\nWhen training our final DPR model using Natural\nQuestions, we use the passages in our collection\nthat best match the gold context as the positive\npassages. As some QA datasets contain only the\nquestion and answer pairs, it is thus interesting\nto see when using the passages that contain the\nanswers as positives (i.e., the distant supervision\nsetting), whether there is a significant performance\ndegradation. Using the question and answer together as the query, we run Lucene-BM25 and pick\nthe top passage that contains the answer as the positive passage. Table 5 shows the performance of\nDPR when trained using the original setting and\nthe distant supervision setting.\n\n\n**B** **Alternative Similarity Functions &**\n**Triplet Loss**\n\n\nIn addition to dot product (DP) and negative loglikelihood based on softmax (NLL), we also experiment with Euclidean distance (L2) and the triplet\nloss. We negate L2 similarity scores before applying softmax and change signs of question-topositive and question-to-negative similarities when\napplying the triplet loss on dot product scores. The\nmargin value of the triplet loss is set to 1. Table 6 summarizes the results. All these additional\n\nexperiments are conducted using the same hyperparameters tuned for the baseline (DP, NLL).\nNote that the retrieval accuracy for our \u201cbaseline\u201d\nsettings reported in Table 5 (Gold) and Table 6\n(DP, NLL) is slightly better than those reported in\nTable 3. This is due to a better hyper-parameter\nsetting used in these analysis experiments, which\nis documented in our code release.\n\n\n**C** **Qualitative Analysis**\n\n\nAlthough DPR performs better than BM25 in general, the retrieved passages of these two retrievers\nactually differ qualitatively. Methods like BM25\nare sensitive to highly selective keywords and\nphrases, but cannot capture lexical variations or semantic relationships well. In contrast, DPR excels\nat semantic representation, but might lack sufficient\ncapacity to represent salient phrases which appear\nrarely. Table 7 illustrates this phenomenon with\ntwo examples. In the first example, the top scoring passage from BM25 is irrelevant, even though\nkeywords such as _England_ and _Ireland_ appear multiple times. In comparison, DPR is able to return\n\n\n\n**Top-1** **Top-5** **Top-20", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_13950", "chunk_text": " top scoring passage from BM25 is irrelevant, even though\nkeywords such as _England_ and _Ireland_ appear multiple times. In comparison, DPR is able to return\n\n\n\n**Top-1** **Top-5** **Top-20** **Top-100**\n\n\nGold 44.9 66.8 78.1 85.0\n\nDist. Sup. 43.9 65.3 77.1 84.4\n\n\nTable 5: Retrieval accuracy on the development set of\nNatural Questions, trained on passages that match the\ngold context (Gold) or the top BM25 passage that contains the answer (Dist. Sup.).\n\n\n**Sim Loss** **Retrieval Accuracy**\nTop-1 Top-5 Top-20 Top-100\n\n\nNLL **44.9** **66.8** **78.1** **85.0**\nDP\nTriplet 41.6 65.0 77.2 84.5\n\n\nNLL 43.5 64.7 76.1 83.1\nL2\nTriplet 42.2 66.0 **78.1** 84.9\n\n\nTable 6: Retrieval Top- _k_ accuracy on the development\nset of Natural Questions using different similarity and\nloss functions.\n\n\nthe correct answer, presumably by matching _\u201cbody_\n\n_of water\u201d_ with semantic neighbors such as _sea_ and\n_channel_, even though no lexical overlap exists. The\nsecond example is one where BM25 does better.\nThe salient phrase _\u201cThoros of Myr\u201d_ is critical, and\nDPR is unable to capture it.\n\n\n**D** **Joint Training of Retriever and**\n**Reader**\n\n\nWe fix the passage encoder in our joint-training\nscheme while allowing only the question encoder\nto receive backpropagation signal from the combined (retriever + reader) loss function. This allows\nus to leverage the HNSW-based FAISS index for\nefficient low-latency retrieving, without reindexing\nthe passages during model updates. Our loss function largely follows ORQA\u2019s approach, which uses\nlog probabilities of positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_14400", "chunk_text": " positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically, we get top 100 passages for each\nquestion in a mini-batch and use the method similar\nto in-batch negative training: all retrieved passages\u2019\nvectors participate in the loss calculation for _all_\nquestions in a batch. Our training batch size is set\nto 16, which effectively gives 1,600 passages per\nquestion to calculate retriever loss. The reader still\nuses 24 passages per question, which are selected\n\n\n**Question** **Passage received by BM25** **Passage retrieved by DPR**\n\n\n\nWhat is the body of water\nbetween England and Ireland?\n\n\nWho plays Thoros of Myr in\nGame of Thrones?\n\n\n\nTitle:British Cycling Title: Irish Sea\n... **England** is not recognised as a region by the UCI, and . . . Annual traffic between Great Britain and **Ireland** amounts\nthere is no English cycling team outside the Commonwealth to over 12 million passengers and of traded goods. **The Irish**\nGames. For those occasions, British Cycling selects and sup- **Sea** is connected to the North Atlantic at both its northern\nports the **England** team. Cycling is represented on the Isle and southern ends. To the north, the connection is through\n\n- f Man by the Isle of Man Cycling Association. Cycling in the North Channel between Scotland and Northern **Ireland**\nNorthern **Ireland** is organised under Cycling Ulster, part of and the Malin Sea. The southern end is linked to the Atlantic\nthe all-Ireland governing **body** Cycling **Ireland** . Until 2006, through the St George\u2019s Channel between Ireland and Pema rival governing **body** existed, . . . brokeshire, and the Celtic Sea. . . .\n\n\n\nTitle: No One (Game of Thrones) Title: P\u02daal Sverre Hagen\n...He may be \u201dno one,\u201d but there\u2019s still enough of a person P\u02daal Sverre Valheim Hagen (born 6 November 1980) is a Norleft in him to respect, and admire who this girl is and what wegian stage and screen actor. He appeared in the Norweshe\u2019s become.", "token_count": 500, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2004.04906_dpr_karpukhin:chunk_14850", "chunk_text": "verre Valheim Hagen (born 6 November 1980) is a Norleft in him to respect, and admire who this girl is and what wegian stage and screen actor. He appeared in the Norweshe\u2019s become. Arya finally tells us something that we\u2019ve kind gian film \u201dMax Manus\u201d and played Thor Heyerdahl in the\n\n- f known all along, that she\u2019s not no one, she\u2019s Arya Stark Oscar-nominated 2012 film \u201dKon-Tiki\u201d. Pl Hagen was born\n\n- f Winterfell.\u201d \u201dNo One\u201d saw the reintroduction of Richard in Stavanger, Norway, the son of Roar Hagen, a Norwegian\nDormer and **Paul Kaye**, who portrayed Beric Dondarrion and cartoonist who has long been associated with Norway\u00b4s largest\n**Thoros** - f **Myr**, respectively, in the third season, . . . daily, \u201dVG\u201d. He lived in Jtten, a neighborhood in the city of\n\nStavanger in south-western Norway. . . .\n\n\n\nTable 7: Examples of passages returned from BM25 and DPR. Correct answers are written in **blue** and the content\nwords in the question are written in bold.\n\n\nfrom the top 5 positive and top 30 negative passages\n(from the set of top 100 passages retrieved from\nthe same question). The question encoder\u2019s initial\nstate is taken from a DPR model previously trained\n\n- n the NQ dataset. The reader\u2019s initial state is a\nBERT-base model. In terms of the end-to-end QA\nresults, our joint-training scheme does not provide\nbetter results compared to the usual retriever/reader\ntraining pipeline, resulting in the same 39.8 exact\nmatch score on NQ dev as in our regular reader\nmodel training.\n\n\n", "token_count": 382, "metadata": {"arxiv_id": "2004.04906", "title": "Dense Passage Retrieval for Open-Domain Question Answering", "authors": ["Vladimir Karpukhin", "Barlas O\u011fuz", "Sewon Min", "Patrick Lewis", "Ledell Wu", "Sergey Edunov", "Danqi Chen", "Wen-tau Yih"], "year": 2020, "url": "https://arxiv.org/pdf/2004.04906v3"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_0", "chunk_text": "Published as a conference paper at ICLR 2021\n\n## ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS\n### WITH MULTI-HOP DENSE RETRIEVAL\n\n\n**Wenhan Xiong** [1] _[\u2217]_ **Xiang Lorraine Li** [2] _[\u2217]_ **Srinivasan Iyer** _[\u2021]_ **Jingfei Du** _[\u2021]_\n\n\n**Patrick Lewis** _[\u2021\u2020]_ **William Wang** [1] **Yashar Mehdad** _[\u2021]_ **Wen-tau Yih** _[\u2021]_\n\n\n**Sebastian Riedel** _[\u2021\u2020]_ **Douwe Kiela** _[\u2021]_ **Barlas O\u02d8guz** _[\u2021]_\n\n\n_\u2021_ Facebook AI\n1University of California, Santa Barbara\n2University of Massachusetts Amherst\n\n_\u2020_ University College London\n_{_ xwhan, william _}_ @cs.ucsb.edu, xiangl@cs.umass.edu,\n_{_ sviyer, jingfeidu, plewis, mehdad, scottyih, sriedel, dkiela, barlaso _}_ @fb.com\n\n\nABSTRACT\n\n\nWe propose a simple and efficient multi-hop dense retrieval approach for answering\ncomplex open-domain questions, which achieves state-of-the-art performance on\ntwo multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previ\n     - us work, our method does not require access to any corpus-specific information,\nsuch as inter-document hyperlinks or human-annotated entity markers, and can\nbe applied to any unstructured text corpus. Our system also yields a much better\nefficiency-accuracy trade-off, matching the best published accuracy on HotpotQA\nwhile being 10 times faster at inference time. [1]\n\n\n1 INTRODUCTION\n\n\n_Open domain question answering_ is a challenging task where the answer to a given question needs to\nbe extracted from a large pool of documents. The prevailing approach (Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_450", "chunk_text": "Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set. Until recently, retrieval models were dependent\n\n- n traditional term-based information retrieval (IR) methods, which fail to capture the semantics of\nthe question beyond lexical matching and remain a major performance bottleneck for the task. Recent\nwork on dense retrieval methods instead uses pretrained encoders to cast the question and documents\ninto dense representations in a vector space and relies on fast maximum inner-product search (MIPS)\nto complete the retrieval. These approaches (Lee et al., 2019; Guu et al., 2020; Karpukhin et al.,\n2020) have demonstrated significant retrieval improvements over traditional IR baselines.\n\n\nHowever, such methods remain limited to _simple_ questions, where the answer to the question\nis explicit in a single piece of text evidence. In contrast, _complex_ questions typically involve\naggregating information from multiple documents, requiring logical reasoning or sequential (multihop) processing in order to infer the answer (see Figure 1 for an example). Since the process for\nanswering such questions might be sequential in nature, single-shot approaches to retrieval are\ninsufficient. Instead, iterative methods are needed to recursively retrieve new information at each\nstep, conditioned on the information already at hand. Beyond further expanding the scope of existing\ntextual open-domain QA systems, answering more complex questions usually involves _multi-hop_\n_reasoning_, which poses unique challenges for existing neural-based AI systems. With its practical\n\n\n_\u2217_ Equal Contribution\n1\n[https://github.com/facebookresearch/multihop_dense_retrieval.](https://github.com/facebookresearch/multihop_dense_retrieval)\n\n\n1\n\n\nPublished as a conference paper at ICLR 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: An overview of the multi-hop dense retrieval approach.\n\n\nand research values, multi-hop QA has been extensively studied recently (Talmor & Berant, 2018;\nYang et al., 2018; Welbl et al., 2018) and remains an active research area in NLP (Qi et al., 2019; Nie\net al., 2019; Min et al., 2019; Zhao et al., 2020; Asai", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_900", "chunk_text": "., 2018) and remains an active research area in NLP (Qi et al., 2019; Nie\net al., 2019; Min et al., 2019; Zhao et al., 2020; Asai et al., 2020; Perez et al., 2020).\n\n\nThe main problem in answering multi-hop open-domain questions is that the search space grows\nexponentially with each retrieval hop. Most recent work tackles this issue by constructing a document\ngraph utilizing either entity linking or existing hyperlink structure in the underlying Wikipedia\ncorpus (Nie et al., 2019; Asai et al., 2020). The problem then becomes finding the best path in this\ngraph, where the search space is bounded by the number of hyperlinks in each passage. However,\nsuch methods may not generalize to new domains, where entity linking might perform poorly, or\nwhere hyperlinks might not be as abundant as in Wikipedia. Moreover, efficiency remains a challenge\ndespite using these data-dependent pruning heuristics, with the best model (Asai et al., 2020) needing\nhundreds of calls to large pretrained models to produce a single answer.\n\n\nIn contrast, we propose to employ dense retrieval to the multi-hop setting with a simple recursive\nframework. Our method iteratively encodes the question and previously retrieved documents as a\nquery vector and retrieves the next relevant documents using efficient MIPS methods. With highquality, dense representations derived from strong pretrained encoders, our work first demonstrates\nthat the sequence of documents that provide sufficient information to answer the multi-hop question\ncan be accurately discovered from unstructured text, _without_ the help of corpus-specific hyperlinks.\nWhen evaluated on two multi-hop benchmarks, HotpotQA (Yang et al., 2018) and a multi-evidence\nsubset of FEVER (Thorne et al., 2018), our approach improves greatly over the traditional linkingbased retrieval methods. More importantly, the better retrieval results also lead to state-of-the-art\ndownstream results on both datasets. On HotpotQA, we demonstrate a vastly improved efficiencyaccuracy trade-off achieved by our system: by limiting the amount of retrieved contexts fed into\ndownstream models, our system can match the best published result while being 10x faster.\n\n\n2 METHOD\n\n\n2.1 PROBLEM DEFINITION\n\n\nThe retrieval task considered in this work can be described as follows (see also Figure 1). Given", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_1350", "chunk_text": "downstream models, our system can match the best published result while being 10x faster.\n\n\n2 METHOD\n\n\n2.1 PROBLEM DEFINITION\n\n\nThe retrieval task considered in this work can be described as follows (see also Figure 1). Given a\nmulti-hop question _q_ and a large text corpus _C_, the retrieval module needs to retrieve a sequence of\npassages _Pseq_ : _{p_ 1 _, p_ 2 _, ..., pn}_ that provide _sufficient_ information for answering _q_ . Practically, the\nretriever returns the _k_ best-scoring sequence candidates, _{Pseq_ [1] _[,][ P]_ _seq_ [2] _[, ...,][ P]_ _seq_ _[k]_ _[}]_ [ (] _[k][ \u226a|C|]_ [), with the]\nhope that at least one of them has the desired qualities. _k_ should be small enough for downstream\nmodules to process in a reasonable time while maintaining adequate recall. In general, retrieval also\nneeds to be efficient enough to handle real-world corpora containing millions of documents.\n\n\n2.2 MULTI-HOP DENSE RETRIEVAL\n\n\n**Model** Based on the sequential nature of the multi-hop retrieval problem, our system solves it in an\niterative fashion. We model the probability of selecting a certain passage sequence as follows:\n\n\n\n_P_ ( _Pseq|q_ ) =\n\n\n\n_n_\n\n- _P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) _,_\n\n\n_t_ =1\n\n\n2\n\n\nPublished as a conference paper at ICLR 2021\n\n\nwhere for _t_ = 1, we only condition on the original question for retrieval. At each retrieval step, we\nconstruct a new query representation based on previous results and the retrieval is implemented as\nmaximum inner product search over the dense representations of the whole corpus:\n\n\nexp ( _\u27e8_ _**p**_ _t,_ _**q**_ _t\u27e9_ )\n_P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) =\n\n~~\ufffd~~ _p\u2208C_ [exp (] _[\u27e8]_ _**[p]**_ _[,]_ _**[ q]**_ _[t][\u27e9]_ [)] _[,]_", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_1800", "chunk_text": " 1) =\n\n~~\ufffd~~ _p\u2208C_ [exp (] _[\u27e8]_ _**[p]**_ _[,]_ _**[ q]**_ _[t][\u27e9]_ [)] _[,]_ [ where] _**[ q]**_ _[t]_ [ =] _[ g]_ [(] _[q, p]_ [1] _[, ..., p][t][\u2212]_ [1][)][ and] _**[ p]**_ _[t]_ [ =] _[ h]_ [(] _[p][t]_ [)] _[.]_\n\n\nHere _\u27e8\u00b7, \u00b7\u27e9_ is the inner product between the query and passage vectors. _h_ ( _\u00b7_ ) and and _g_ ( _\u00b7_ ) are\npassage and query encoders that produce the dense representations. In order to reformulate the\nquery representation to account for previous retrieval results at time step _t_, we simply concatenate\nthe question and the retrieved passages as the inputs to _g_ ( _\u00b7_ ). Note that our formulation for each\nretrieval step is similar to existing single-hop dense retrieval methods (Lee et al., 2019; Guu et al.,\n2020; Karpukhin et al., 2020) except that we add the query reformulation process conditioned on\nprevious retrieval results. Additionally, instead of using a bi-encoder architecture with separately\nparameterized encoders for queries and passages, we use a shared RoBERTa-base (Liu et al., 2019)\nencoder for both _h_ ( _\u00b7_ ) and _g_ ( _\u00b7_ ). In \u00a73.1.3, we show this simple modification yields considerable\nimprovements. Specifically, we apply layer normalization over the start token\u2019s representations from\nRoBERTa to get the final dense query/passage vectors.\n\n\n**Training and Inference** The retriever model is trained as in Karpukhin et al. (2020), where each\ninput query (which at each step consists of a question and previously retrieved passages) is paired\nwith a positive passage and _m_ negative passages to approximate the softmax over all passages. The\npositive passage is the gold annotated evidence at step _t_ . Negative passages are a combination of\npassages in the current batch which correspond to other questions (", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_2250", "chunk_text": " positive passage and _m_ negative passages to approximate the softmax over all passages. The\npositive passage is the gold annotated evidence at step _t_ . Negative passages are a combination of\npassages in the current batch which correspond to other questions ( _in-batch_ ), and _hard_ negatives\nwhich are false adversarial passages. In our experiments, we obtain hard negatives from TF-IDF\nretrieved passages and their linked pages in Wikipedia. We note that using hyperlinked pages as\nadditional negatives is neither necessary nor critical for our approach. In fact we observe only a very\nsmall degradation in performance if we remove them from training (\u00a73.1.3). In addition to in-batch\nnegatives, we use a memory bank ( _M_ ) mechanism (Wu et al., 2018) to further increase the number\n\n- f negative examples for each question. The memory bank stores a large number of dense passage\nvectors. As we block the gradient back-propagation in the memory bank, its size ( _|M| \u226b_ batch size)\nis less restricted by the GPU memory size. Specifically, after training to convergence with the shared\nencoder, we freeze a copy of the encoder as the new passage encoder and collect a bank of passage\nrepresentations across multiple batches to serve as the set of negative passages. This simple extension\nresults in further improvement in retrieval. (\u00a73.1.3).\n\n\nFor inference, we first encode the whole corpus into an index of passage vectors. Given a question,\nwe use beam search to obtain top- _k_ passage sequence candidates, where the candidates to beam\nsearch at each step are generated by MIPS using the query encoder at step _t_, and the beams are scored\nby the sum of inner products as suggested by the probabilistic formulation discussed above. Such\ninference relies only on the dense passage index and the query representations, and does not need\nexplicit graph construction using hyperlinks or entity linking. The top- _k_ sequences will then be fed\ninto task-specific downstream modules to produce the desired outputs.\n\n\n3 EXPERIMENTS\n\n\n**Datasets** Our experiments focus on two datasets: _HotpotQA_ and _Multi-evidence FEVER_ . HotpotQA (Yang et al., 2018) includes 113k multi-hop questions. Unlike other multi-hop QA\ndatasets (Zhang et al., 2018; Talmor", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_2700", "chunk_text": "_ and _Multi-evidence FEVER_ . HotpotQA (Yang et al., 2018) includes 113k multi-hop questions. Unlike other multi-hop QA\ndatasets (Zhang et al., 2018; Talmor & Berant, 2018; Welbl et al., 2018), where the information\nsources of the answers are knowledge bases, HotpotQA uses documents in Wikipedia. Thus, its\nquestions are not restricted by the fixed KB schema and can cover more diverse topics. Each\nquestion in HotpotQA is also provided with ground truth support passages, which enables us to\nevaluate the intermediate retrieval performance. Multi-evidence FEVER includes 20k claims from\nthe FEVER (Thorne et al., 2018) fact verification dataset, where the claims can only be verified using\nmultiple documents. We use this dataset to validate the general applicability of our method.\n\n\n**Implementation Details** All the experiments are conducted on a machine with 8 32GB V100\nGPUs. Our code is based on Huggingface Transformers (Wolf et al., 2019). Our best retrieval results\nare predicted using the exact inner product search index (IndexFlatIP) in FAISS (Johnson et al., 2017).\n\n\n3\n\n\nPublished as a conference paper at ICLR 2021\n\n\nTable 1: Retrieval performance in recall at _k_ retrieved passages and precision/recall/F1.\n\n\nHotpotQA FEVER\nMethod\nR@2 R@10 R@20 Precision Recall F1\n\n\nTF-IDF 10.3 29.1 36.8 14.9 28.2 19.5\n\nTF-IDF + Linked 17.3 50.0 62.7 18.6 35.8 24.5\n\nDrKIT 38.3 67.2 71.0         -         -         Entity Linking          -          -          - 30.6 53.8 39.0\n\n\nMDR **65.9** **77.5** **80.2** **45.7** **69.1** **55.0**\n\n\nBoth datasets assume 2 hops, so we fix _n_ = 2 for all experiments. Since HotpotQA does not provide\nthe order of the passage sequences, as a heuristic, we consider the passage that includes the answer\n", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_3150", "chunk_text": "**\n\n\nBoth datasets assume 2 hops, so we fix _n_ = 2 for all experiments. Since HotpotQA does not provide\nthe order of the passage sequences, as a heuristic, we consider the passage that includes the answer\nspan as the final passage. [2] In \u00a73.1.3, we show that the order of the passages is important for effective\nretriever training. The hyperparameters can be found in Appendix B.1.\n\n\n3.1 EXPERIMENTS: RETRIEVAL\n\n\nWe evaluate our multi-hop dense retriever (MDR) in two different use cases: _direct_ and _reranking_,\nwhere the former outputs the top- _k_ results directly using the retriever scores and the latter applies a\ntask-specific reranking model to the initial results from MDR.\n\n\n3.1.1 DIRECT\n\n\nWe first compare MDR with several efficient retrieval methods that can directly find the top- _k_ passage\nsequences from a large corpus, including TF-IDF, TF-IDF + Linked, DrKIT and Entity Linking. **TF-**\n**IDF** is the standard term-matching baseline, while **TF-IDF + Linked** is a straightforward extension\nthat also extracts the hyperlinked passages from TF-IDF passages, and then reranks both TF-IDF\nand hyperlinked passages with BM25 [3] scores. **DrKIT** (Dhingra et al., 2020) is a recently proposed\ndense retrieval approach, which builds a entity-level (mentions of entities) dense index for retrieval.\nIt relies on hyperlinks to extract entity mentions and prunes the search space with a binary mask\nthat restricts the next hop to using hyperlinked entities. On FEVER, we additionally consider an\nentity linking baseline (Hanselowski et al., 2018) that is commonly used in existing fact verification\npipelines. This baseline first uses a constituency parser to extract potential entity mentions in the fact\nclaim and then uses the MediaWiki API to search documents with titles that match the mentions.\n\n\nTable 1 shows the performance of different retrieval methods. On HotpotQA the metric is recall at the\ntop _k_ paragraphs [4], while on FEVER the metrics are precision, recall and F1 in order to be consistent\nwith previous results. On both datasets, MDR substantially outperforms all baselines.\n\n\n3.1.", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_3600", "chunk_text": "\ntop _k_ paragraphs [4], while on FEVER the metrics are precision, recall and F1 in order to be consistent\nwith previous results. On both datasets, MDR substantially outperforms all baselines.\n\n\n3.1.2 RERANKING\n\n\n_Reranking_ documents returned by efficient retrieval methods with a more sophisticated model is a\ncommon strategy for improving retrieval quality. For instance, state-of-the-art multi-hop QA systems\nusually augment traditional IR techniques with large pretrained language models to select a more\ncompact but precise passage set. On HotpotQA, we test the effectiveness of MDR after a simple\ncross-attention reranking: each of the top _k_ passage sequences from MDR is first prepended with the\n\n- riginal question and then fed into a pretrained Transformer encoder, i.e., ELECTRA-large (Clark\net al., 2020), that predicts relevant scores. We train this reranking model with a binary crossentropy loss, with the target being whether the passage sequence cover both groundtruth passages.\nWe empirically compare our approach with two other existing reranking-based retrieval methods:\n**Semantic Retrieval** (Nie et al., 2019) uses BERT at both passage-level and sentence-level to select\ncontext from the initial TF-IDF and hyperlinked passages; **Graph Recurrent Retriever** (Asai et al.,\n2020) learns to recursively select the best passage sequence on top of a hyperlinked passage graph,\nwhere each passage node is encoded with BERT.\n\n\nTable 2 shows the reranking results. Following Asai et al. (2020), we use _Answer Recall_ and\n_Support Passage Exact Match (SP EM)_ [5] as the evaluation metrics. Even without reranking, MDR\nis already better than Semantic Retrieval, which requires around 50 BERT encoding (where each\n\n\n2If the answer span is in both, the one that has its title mentioned in the other passage is treated as the second.\n3https://pypi.org/project/rank-bm25\n4As the sequence length is 2 for HotpotQA, we pick the top _k_ /2 sequences predicted by MDR.\n5Whether the final predicted sequence covers both gold passages.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2021\n\n\nTable 2: HotpotQA reranked retrieval results\n(input passages for final answer prediction).\n\n\nMethod SP", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_4050", "chunk_text": " by MDR.\n5Whether the final predicted sequence covers both gold passages.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2021\n\n\nTable 2: HotpotQA reranked retrieval results\n(input passages for final answer prediction).\n\n\nMethod SP EM Ans Recall\n\n\nSemantic Retrieval 63.9 77.9\nGraph Rec Retriever 75.7 87.5\n\n\nMDR (direct) 65.9 75.4\nMDR (reranking) **81.2** **88.2**\n\n\n\nTable 3: Retriever Model Ablation on HotpotQA\nretrieval. _Single-hop_ here is equivalent to the DPR\nmethod (Karpukhin et al., 2020).\n\n\nRetriever variants R@2 R@10 R@20\n\n\nFull Retrieval Model 65.9 77.5 80.2\n\n  - w/o linked negatives 64.6 76.8 79.6\n\n  - w/o memory bank 63.7 74.2 77.2\n\n  - w/o shared encoder 59.9 70.6 73.1\n\n  - w/o order 17.6 55.6 62.3\nSingle-hop 25.2 45.4 52.1\n\n\n\nencoding involves cross-attention over a concatenated question-passage pair). After we rerank the\ntop-100 sequences from the dense retriever, our passage recall is better than the state-of-the-art Graph\nRecurrent Retriever, which uses BERT to process more than 500 passages. We do not compare the\nreranked results on FEVER, as most FEVER systems directly use BERT encoder to select the top\nevidence _sentences_ from the retrieved documents, instead of the reranking the documents.\n\n\n3.1.3 ANALYSIS\n\n\nTo understand the strengths and weaknesses of MDR, we conduct further analysis on HotpotQA dev.\n\n\n\n**Retrieval Error Analysis** HotpotQA contains two\nquestion categories: _bridge_ questions in which an\nintermediate entity is missing and needs to be retrieved before inferring the answer; and _comparison_\nquestions where two entities are mentioned simultaneously and compared in some way. In Figure 2,\nwe show the retrieval performance of both question\ntypes. The case of _comparison_ questions proves easier, since both entities needed for retrieval", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_4500", "chunk_text": "; and _comparison_\nquestions where two entities are mentioned simultaneously and compared in some way. In Figure 2,\nwe show the retrieval performance of both question\ntypes. The case of _comparison_ questions proves easier, since both entities needed for retrieval are present\nin the question.\n\n\nThis case appears almost solved, confirming recent\nwork demonstrating that dense retrieval is very effective at entity linking (Wu et al., 2019).\n\n\n\nFigure 2: The retrieval performance gap between comparison and bridge questions. Left:\nrecall of groundtruth passage sequences with\n- ut reranking. Right: Top-1 chain exact\nmatch after reranking.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the case of _bridge_ questions, we manually in\nrecall of groundtruth passage sequences with\nspect 50 randomly sampled erroneous examples after\n\n                          - ut reranking. Right: Top-1 chain exact\n\nreranking. We find that in half of these cases, our re\nmatch after reranking.\n\ntrieval model predicts an alternative passage sequence\nthat is also valid (see Appendix A.1 for examples).\nThis gives an estimated top-1 passage sequence accuracy of about 90%. Other remaining errors are\ndue to the dense method\u2019s inability to capture the exact n-gram match between the question and\npassages. This is a known issue (Lee et al., 2019; Karpukhin et al., 2020) of dense retrieval methods\nwhen dealing with questions that have high lexical overlap with the passages. To this end, a hybrid\nmulti-hop retrieval method with both term and dense index might be used to further improve the\nperformance on _bridge_ questions.\n\n\n\n**Retriever Ablation Study** In Table 3, we examine our model with different variations on HotpotQA\nto show the effectiveness of each proposed component. We see that further training with a memory\nbank results in modest gains, while using a shared encoder is crucial for the best performance.\nRespecting the ordering of passages in two hops is essential - training in an order-agnostic manner\nhardly works at all, and underperforms even the single-hop baseline. Finally, not using hyperlinked\nparagraphs from TF-IDF passages as additional negatives has only a minor impact on performance.\n\n\n**Question Decomposition for Retrieval** As multi-hop questions have more complex structures\nthan simple questions, recent studies (Min et al., 2019; Perez et al., 2020) propose to use", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_4950", "chunk_text": " only a minor impact on performance.\n\n\n**Question Decomposition for Retrieval** As multi-hop questions have more complex structures\nthan simple questions, recent studies (Min et al., 2019; Perez et al., 2020) propose to use explicit question decomposition to simplify the problem. Wolfson et al. (2020) shows that with\nTF-IDF, using decomposed questions improves the retrieval results. We investigate whether the\n\n\n5\n\n\nPublished as a conference paper at ICLR 2021\n\n\nconclusion still holds with stronger dense retrieval methods. We use the human-annotated question decomposition from the QDMR dataset (Wolfson et al., 2020) for analysis. For a question\nlike Q:Mick Carter is the landlord of a public house located at what\naddress?, QDMR provides two subquestions, SubQ1: What is the public house\nthat Mick Carter is the landlord of? and SubQ2: What is the address\nthat #1 is located at?. We sample 100 bridge questions and replace #1 in SubQ2 with\nthe correct answer (The Queen Victoria) to SubQ1. Note that this gives advantages to the decomposed\nmethod as we ignore any intermediate errors. We estimate the performance of potential decomposed\nmethods with the state-of-the-art single-hop dense retrieval model (Karpukhin et al., 2020).\n\n\nAs shown in Table 4, we did not observe\nany strong improvements from explicit ques\nTable 4: Comparison with decomposed dense retrieval\n\ntion decomposition, which is contrary to the\n\nwhich uses oracle question decomposition (test on\n\nfindings by Wolfson et al. (2020) when using\n\n100 bridge questions). See text for details about the\n\nterm-based IR methods. Moreover, as shown decomposed settings.\nin the third row of the table, when the 1st hop\n\nMethod R@2 R@10 R@20\n\n- f the decomposed retrieval (i.e., SubQ1)\nis replaced with the original question, no MDR 54.9 63.7 70.6\nperformance degradation is observed. This Decomp (SubQ1;SubQ2) 50.0 64.7 67.6\nsuggests that strong pretrained encoders can Decomp (Q;SubQ2) 51.0 64.7 68.6\neffectively learn to select necessary", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_5400", "chunk_text": "2) 50.0 64.7 67.6\nsuggests that strong pretrained encoders can Decomp (Q;SubQ2) 51.0 64.7 68.6\neffectively learn to select necessary information from the multi-hop question at each\nretrieval step. Regarding the performance drop when using explicit compositions, we hypothesize\nthat it is because some information in one decomposed subquestion could be useful for the other\nretrieval hop. Examples supporting this hypothesis can be found in Appendix A.2. While this could\npotentially be addressed by a different style of decomposition, our analysis suggests that decomposition approaches might be sub-optimal in the context of dense retrieval with strong pretrained\nencoders.\n\n\n\nTable 4: Comparison with decomposed dense retrieval\nwhich uses oracle question decomposition (test on\n100 bridge questions). See text for details about the\ndecomposed settings.\n\n\n\nMethod R@2 R@10 R@20\n\n\n\nMDR 54.9 63.7 70.6\nDecomp (SubQ1;SubQ2) 50.0 64.7 67.6\nDecomp (Q;SubQ2) 51.0 64.7 68.6\n\n\n\n3.2 EXPERIMENTS: HOTPOTQA\n\n\nWe evaluate how the better retrieval results of MDR improve multi-hop question answering in\nthis section. As our retriever system is agnostic to downstream models, we test two categories of\nanswer prediction architectures: the _extractive_ span prediction models based on pretrained masked\nlanguage models, such as BERT (Devlin et al., 2019) and ELECTRA (Clark et al., 2020), and the\nretrieval-augmented _generative_ reader models (Lewis et al., 2020b; Izacard & Grave, 2020), which\nare based on pretrained sequence-to-sequence (seq2seq) models such as BART (Lewis et al., 2020a)\nand T5 (Raffel et al., 2019). Note that compared to more complicated graph reasoning models (Fang\net al., 2019; Zhao et al., 2020), these two classes of models do not rely on hyperlinks and can be\napplied to any text.\n\n\n**Extractive** reader models learn to predict an answer span from the concatenation", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_5850", "chunk_text": "., 2019; Zhao et al., 2020), these two classes of models do not rely on hyperlinks and can be\napplied to any text.\n\n\n**Extractive** reader models learn to predict an answer span from the concatenation of the question\nand passage sequence ([ _q_, _p_ 1, ..., _pn_ ]). On top of the token representations produced by pretrained\nmodels, we add two prediction heads to predict the start and end position of the answer span. [6]\nTo predict the supporting sentences, we add another prediction head and predict a binary label at\neach sentence start. For simplicity, the same encoder is also responsible for reranking the top _k_\npassage sequences. The reranking detail has been discussed in \u00a73.1.2. Our best reader model is based\n\n- n ELECTRA (Clark et al., 2020), which has achieved the best single-model performance on the\nstandard SQuAD (Rajpurkar et al., 2018) benchmark. Additionally, we also report the performance\n\n- f BERT-large with whole word masking (BERT-wwm) to fairly compare with Asai et al. (2020).\n\n\n**Generative** models, such as RAG (Lewis et al., 2020b) and FiD (Izacard & Grave, 2020), are based\n\n- n pretrained seq2seq models. These methods finetune pretrained models with the concatenated\nquestions and retrieved documents as inputs, and answer tokens as outputs. This generative paradigm\nhas shown state-of-the-art performance on single-hop open-domain QA tasks. Specifically, FiD first\nuses the T5 encoder to process each retrieved passage sequence independently and then uses the\ndecoder to perform attention over the representations of all input tokens while generating answers.\n\n\n6To account for yes/no questions, we prepend _yes_ and _no_ tokens to the context.\n\n\n6\n\n\nPublished as a conference paper at ICLR 2021\n\n\nRAG is built on the smaller BART model. Instead of only tuning the seq2seq model, it also jointly\ntrain the question encoder of the dense retriever. We modified it to allow multi-hop retrieval.\n\n\nMore details about these two classes of reader models are described in Appendix B.2.\n\n\n3.2.1 RESULTS\n\nTable 5: HotpotQA-fullwiki test results.\n\n\nAnswer Support Joint\nMethods\nEM F1 EM", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_6300", "chunk_text": " multi-hop retrieval.\n\n\nMore details about these two classes of reader models are described in Appendix B.2.\n\n\n3.2.1 RESULTS\n\nTable 5: HotpotQA-fullwiki test results.\n\n\nAnswer Support Joint\nMethods\nEM F1 EM F1 EM F1\n\n\nGoldEn Retriever (Qi et al., 2019) 37.9 48.6 30.7 64,2 18.9 39.1\nSemantic Retrieval (Nie et al., 2019) 46.5 58.8 39.9 71.5 26.6 49.2\nTransformer-XH (Zhao et al., 2020) 51.6 64.1 40.9 71.4 26.1 51.3\nHGN (Fang et al., 2019) 56.7 69.2 50.0 76.4 35.6 59.9\nDrKIT (Dhingra et al., 2020) 42.1 51.7 37.1 59.8 24.7 42.9\nGraph Recurrent Retriever (Asai et al., 2020) 60.0 73.0 49.1 76.4 35.4 61.2\n\n\nMDR (ELECTRA Reader) **62.3** **75.3** **57.5** **80.9** **41.8** **66.6**\n\n\n**Comparison with Existing Systems** Table 5 compares the HotpotQA test performance of our best\nELECTRA reader with recently published systems, using the numbers from the official leaderboard,\nwhich measure answer and supporting sentence exact match (EM)/F1 and joint EM/F1. Among\nthese methods, only GoldEn Retriever (Qi et al., 2019) does not exploit hyperlinks. In particular,\nGraph Recurrent Retriever trains a graph traversal model for chain retrieval; TransformerXH (Zhao\net al., 2020) and HGN (Fang et al., 2019) explicitly encode the hyperlink graph structure within their\nanswer prediction models. In fact, this particular inductive bias provides a perhaps unreasonably\nstrong advantage in the specific context of HotpotQA, which by construction guarantees groundtruth passage sequences", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_6750", "chunk_text": "2019) explicitly encode the hyperlink graph structure within their\nanswer prediction models. In fact, this particular inductive bias provides a perhaps unreasonably\nstrong advantage in the specific context of HotpotQA, which by construction guarantees groundtruth passage sequences to follow hyperlinks. Despite not using such prior knowledge, our model\n\n- utperforms all previous systems by large margins, especially on supporting fact prediction, which\nbenefits more directly from better retrieval.\n\n\n\n**Reader** **Model** **Variants** Results for\nreader model variants are shown in Table 6. [7]\n\nTable 6: Reader comparison on HotpotQA dev set.\n\nFirst, we see that the BERT-wwm reader\nis 1-2% worse than the ELECTRA reader Model Top k EM F1\nwhen using enough passages. However, ELECTRA Top 50 61.7 74.3\nit still outperforms the results in (Asai Extractive ELECTRA Top 250 63.4 76.2\net al., 2020) which also uses BERT-wwm BERT-wwm Top 250 61.5 74.7\nfor answer prediction. While RAG and Multi-hop RAG Top 4*4 51.2 63.9\n\nGenerative\n\nFiD have shown strong improvements over FiD Top 50 61.7 73.1\nextractive models on single-hop datasets\nsuch as NaturalQuestions (Kwiatkowski\net al., 2019), they do not show an advantage in the multi-hop case. Despite having twice as many\nparameters as ELECTRA, FiD fails to outperform it using the same amount of context (top 50). In\ncontrast, on NaturalQuestions, FiD is 4 points better than a similar extractive reader when using the\ntop 100 passages in both. [8] We hypothesize that the improved performance on single-hop questions is\ndue to the ability of larger pretrained models to more effectively memorize single-hop knowledge\nabout real-world entities. [9] Compared to multi-hop questions that involve multiple relations and\nmissing entities, simple questions usually only ask about a certain property of an entity. It is likely\nthat such simple entity-centric information is explicitly mentioned by a single text piece in the\npretraining corpus, while the evidence for multihop questions is typically dispersed, making the\n\n\n7", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_7200", "chunk_text": " ask about a certain property of an entity. It is likely\nthat such simple entity-centric information is explicitly mentioned by a single text piece in the\npretraining corpus, while the evidence for multihop questions is typically dispersed, making the\n\n\n7For the compute-heavy generative models, we feed in as many passages as possible without running into\nmemory issues (Muli-hop RAG takes top 4 passages from hop1, and for each of those, takes another top 4 from\nhop2. They are not necessarily the same as the top 16 passages sequences.). As extractive models encode each\npassage sequence separately, we can use arbitrary number of input sequences. However, the performance mostly\nplateaus as we use over 200 input sequences.\n8We implemented NQ extractive readers with both RoBERTa-large and ELECTRA-large, and RoBERTa-large\nyielded a better answer EM of 47.3, which is much lower than the 51.4 answer EM achieved by FiD.\n9As shown by Roberts et al. (2020), a large pretrained seq2seq model can be finetuned to directly decode\nanswers with questions as the only inputs. However, we find that this retrieval-free approach performs poorly on\nmulti-hop questions. See Appendix C for the exact numbers.\n\n\n7\n\n\n\nTable 6: Reader comparison on HotpotQA dev set.\n\n\n\nModel Top k EM F1\n\n\n\nExtractive\n\n\n\nELECTRA Top 50 61.7 74.3\nELECTRA Top 250 63.4 76.2\nBERT-wwm Top 250 61.5 74.7\n\n\n\nMulti-hop RAG Top 4*4 51.2 63.9\nGenerative\nFiD Top 50 61.7 73.1\n\n\nPublished as a conference paper at ICLR 2021\n\n\nTable 7: Multi-Evidence FEVER Fact Verification Results. **Loose-Multi** represents the subset that\nrequires multiple evidence _sentences_ . **Strict-Multi** is a subset of **Loose-Multi** that require multiple\nevidence sentences from different _documents_ .\n\n\nLoose-Multi (1,960) Strict-Multi (1,059)\nMethod\nLA FEVER LA FEVER\n\n\nGEAR 66.4 38.0     -     \nGAT 66.1 38.2     -", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_7650", "chunk_text": "ose-Multi (1,960) Strict-Multi (1,059)\nMethod\nLA FEVER LA FEVER\n\n\nGEAR 66.4 38.0     -     \nGAT 66.1 38.2     -     \nKGAT with ESIM rerank 65.9 39.2 51.5 7.7\n\nKGAT with BERT rerank 65.9 40.1 51.0 6.2\n\n\nOurs + KGAT with BERT rerank **77.9** **42.0** **72.1** **16.2**\n\n\ncomplete reasoning chain nontrivial to memorize. More analysis on RAG can be found in Appendix\nA.3.\n\n\n\n**Inference Efficiency** To compare with existing multi-hop QA systems in terms of efficiency,\nwe follow Dhingra et al. (2020) and measure\nthe inference time with 16 CPU cores and batch\nsize 1. We implement our system with a fast\napproximate nearest neighbor search method,\n_i.e._, HNSW (Malkov & Yashunin, 2018), which\nachieves nearly the same performance as exact\nsearch. With an in-memory index, we observe\nthat the retrieval time is negligible compared\nto the forward pass of large pretrained models. Similarly, for systems that use term-based\nindices, the BERT calls for passage reranking\ncause the main efficiency bottleneck. Thus, for Figure 3: Efficiency-performance trade-off comsystems that do not release the end-to-end code, parison with published HotpotQA systems. The\nwe estimate the running time based on the num- curve is plotted with different number of top _k_\nber of BERT cross-attention forward passes (the ( _k_ =1,5,10,20,50,100,200) passage sequences we\nsame estimation strategy used by Dhingra et al. feed into the reader model. seq/Q denotes the time\n(2020)), and ignore the overhead caused by ad- required for each query.\nditional processing such as TF-IDF or linking\ngraph construction. As shown in Figure 3, our method is about 10 times faster than current state-ofthe-art systems while achieving a similar level of performance. Compared to two efficient systems\n(DrKIT and GoldEn), we achieve over 10 points improvement while only using", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_8100", "chunk_text": " Figure 3, our method is about 10 times faster than current state-ofthe-art systems while achieving a similar level of performance. Compared to two efficient systems\n(DrKIT and GoldEn), we achieve over 10 points improvement while only using the top-1 retrieval\nresult for answer and supporting sentence prediction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Efficiency-performance trade-off comparison with published HotpotQA systems. The\ncurve is plotted with different number of top _k_\n( _k_ =1,5,10,20,50,100,200) passage sequences we\nfeed into the reader model. seq/Q denotes the time\nrequired for each query.\n\n\n\n3.3 EXPERIMENTS: MULTI-EVIDENCE FEVER\n\n\nFor FEVER claim verification, we reuse the best open-sourced verification system, _i.e._, KGAT (Liu\net al., 2020), to show the benefit of our retrieval approach over existing retrieval methods. We report\nthe results in verification _label accuracy_ (LA) and the _FEVER score_ [10] in Table 7, where the numbers\n\n- f competitive baselines, GEAR (Zhou et al., 2019), graph attention network (GAT) (Velickovi\u02c7 c\u00b4\net al., 2017) and variants of KGAT are from the KGAT (Liu et al., 2020) paper. All these baselines\nuse entity linking for document retrieval, then rerank the sentences of the retrieved documents, and\nfinally use different graph attention mechanisms over the fully-connected sentence graph to predict\nverification labels. Since some instances in the multi-evidence subset used by previous studies only\nneeds multiple evidence _sentences_ from the same document, we additionally test on a strict multi-hop\nsubset with instances that need multiple _documents_ . As shown by the results, even without finetuning\nthe downstream modules, simply replacing the retrieval component with MDR leads to significant\nimprovements, especially on the strict multi-evidence subset.\n\n\n10FEVER scores takes into account both support sentence accuracy and label accuracy, similar as the joint\nmetrics in HotpotQA.\n\n\n8\n\n\nPublished as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_8550", "chunk_text": "Published as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto finetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved significant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\n\n- f questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model\u2019s hidden states. However, their method requires an initial round\n\n- f TF-IDF/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\n", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_9000", "chunk_text": " MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\nexpansion techniques (Rocchio, 1971; Voorhees, 1994; Ruthven & Lalmas, 2003) that are widely used\nin traditional IR systems. In particular, our system is similar in spirit to pseudo-relevance feedback\ntechniques (Croft & Harper, 1979; Cao et al., 2008; Lv & Zhai, 2010), where no additional user\ninteraction is required at the query reformulation stage. Existing studies mainly focus on alleviating\nthe uncertainty of the user query (Collins-Thompson & Callan, 2007) by adding relevant terms from\nthe first round of retrieval, where the retrieval target remains the same throughout the iterative process.\nIn contrast, the query reformulation in our approach aims to follow the multi-hop reasoning chain and\neffectively retrieves different targets at each step. Furthermore, instead of explicitly selecting terms\nto expand the query, we simply concatenate the whole passage and rely on the pretrained encoder to\nchoose useful information from the last retrieved passage.\n\n\n**Other Multi-hop QA Work** Apart from HotpotQA, other multi-hop QA datasets (Welbl et al.,\n2018; Talmor & Berant, 2018; Zhang et al., 2018) are mostly built from knowledge bases (KBs).\nCompared to questions in HotpotQA, questions in these datasets are rather synthetic and less diverse.\nAs multi-hop relations in KBs could be mentioned together in a single text piece, these datasets are\nnot designed for an open-domain setting which necessitates multi-hop retrieval. Existing methods on\nthese datasets either retrieve passages from a small passage pool pruned based on the the specific\ndataset (Sun et al., 2019; Dhingra et al., 2020), or focus on a non-retrieval setting where a compact\ndocuments set is already given (De Cao et al., 2018; Zhong et al., 2019; Tu et al., 2019; Beltagy et al.,\n2020). Compared to these research, our work aims at building an efficient multi-hop retrieval model\nthat easily scales to large real-world corpora that", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_9450", "chunk_text": " et al., 2019; Tu et al., 2019; Beltagy et al.,\n2020). Compared to these research, our work aims at building an efficient multi-hop retrieval model\nthat easily scales to large real-world corpora that include millions of open-domain documents.\n\n\n5 CONCLUSION\n\n\nIn this work, we generalized the recently proposed successful dense retrieval methods by extending\nthem to the multi-hop setting. This allowed us to handle complex multi-hop queries with much\nbetter accuracy and efficiency than the previous best methods. We demonstrated the versatility of our\napproach by applying it to two different tasks, using a variety of downstream modules. In addition,\nthe simplicity of the framework and the fact that it does not depend on a corpus-dependent graph\nstructure opens the possibility of applying such multi-hop retrieval methods more easily and broadly\ncross different domains and settings.\n\n\n9\n\n\nPublished as a conference paper at ICLR 2021\n\n\nREFERENCES\n\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning\nto retrieve reasoning paths over wikipedia graph for question answering. In _ICLR_ . OpenReview.net,\n2020.\n\n\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n_arXiv preprint arXiv:2004.05150_, 2020.\n\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answer pairs. In _Proceedings of the 2013 conference on empirical methods in natural_\n_language processing_, pp. 1533\u20131544, 2013.\n\n\nGuihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. Selecting good expansion terms\nfor pseudo-relevance feedback. In _Proceedings of the 31st annual international ACM SIGIR_\n_conference on Research and development in information retrieval_, pp. 243\u2013250, 2008.\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer\n\n - pen-domain questions. In _ACL (1)_, pp. 1870\u20131879. Association for Computational Linguistics,\n2017.\n\n\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: pre-training\ntext encoders as discrimin", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_12600", "chunk_text": "io, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. In _EMNLP_, pp. 2369\u20132380. Association for Computational Linguistics, 2018.\n\n\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J. Smola, and Le Song. Variational reasoning\nfor question answering with knowledge graph. In _AAAI_, pp. 6069\u20136076. AAAI Press, 2018.\n\n\nChen Zhao, Chenyan Xiong, Corby Rosset, Xia Song, Paul N. Bennett, and Saurabh Tiwary.\nTransformer-xh: Multi-evidence reasoning with extra hop attention. In _ICLR_ . OpenReview.net,\n2020.\n\n\nVictor Zhong, Caiming Xiong, Nitish Shirish Keskar, and Richard Socher. Coarse-grain fine-grain\ncoattention network for multi-evidence question answering. _arXiv preprint arXiv:1901.00603_,\n2019.\n\n\n12\n\n\nPublished as a conference paper at ICLR 2021\n\n\nJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun.\nGEAR: graph-based evidence aggregating and reasoning for fact verification. In _ACL (1)_, pp.\n892\u2013901. Association for Computational Linguistics, 2019.\n\n\n13\n\n\nPublished as a conference paper at ICLR 2021\n\n\nA QUALITATIVE ANALYSIS\n\n\nA.1 FALSE BRIDGE QUESTION ERROR CASES\n\n\nAs mentioned in \u00a73.1.3, half of the errors of bridge questions are not real errors. In Table 8, we\ncan see that the model predicts alternative passage sequences that could also be used to answer the\nquestions.\n\n\nTable 8: Error cases where our model predicts a passage sequence that is also correct. Important\nclues are marked in blue.\n\n\n**Q:** What languages did the son of Sacagawea speak?\n**Ground-truth SP Passage Titles:** Charbonneau, Oregon; Jean Baptiste Charbonneau\n**Predicted:**\n1. Museum of Human Beings: Museum of Human Beings, included in the National American\nIn ~~dian Heritage Month", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_13050", "chunk_text": "-truth SP Passage Titles:** Charbonneau, Oregon; Jean Baptiste Charbonneau\n**Predicted:**\n1. Museum of Human Beings: Museum of Human Beings, included in the National American\nIn ~~dian Heritage Month Bookli~~ st, November 2012 and 2013 is a novel written by Colin Sargent,\nwhich delves into the heart-rending life of Jean-Baptiste Charbonneau, the son of Sacagawea.\nSacagawea was the Native American guide, who at 16 led the Lewis and Clark expedition.\n2. Jean Baptiste Charbonneau: Jean Baptiste Charbonneau (February 11, 1805 \u2013 May 16, 1866)\nw ~~as an American Indian explo~~ rer, guide, fur trapper trader, military scout during the MexicanAmerican War, \u201dalcalde\u201d (mayor) of Mission San Luis Rey de Francia and a gold prospector\nand hotel operator in Northern California. He spoke French and English, and learned German\nand Spanish during his six years in Europe from 1823 to 1829. He spoke Shoshone, his mother\ntongue, and other western American Indian languages...\n\n\n**Q:** Altnahinch is located in a county that has a population density of how many per square mile?\n**Ground-truth SP Passage Titles:** Altnahinch Dam; County Antrim\n**Predicted:**\n1. Altnahinch: Altnahinch is a townland in County Antrim, Northern Ireland.\n2. County Antrim: County Antrim (named after the town of Antrim, from Irish: \u201dAontroim\u201d\n, ~~meaning \u201dlone ri~~ dge\u201d, )) is one of six counties that form Northern Ireland. Adjoined to the\nnorth-east shore of Lough Neagh, the county covers an area of 3046 km2 and has a population\n\n  - f about 618,000. County Antrim has a population density of 203 people per square kilometer /\n526 people per square mile...\n\n\n**Q:** What foundation do scholars give for the likelihood of collaboration on a William Shakespeare\nPlay written between 1588 and 1593?\n**Ground-truth SP Passage Titles:**\nAuthorship of Titus Andronicus, William Shakespeare\u2019s collaborations\n**Predicted:**\n1. Titus Andronicus: Tit", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_13500", "chunk_text": " Shakespeare\nPlay written between 1588 and 1593?\n**Ground-truth SP Passage Titles:**\nAuthorship of Titus Andronicus, William Shakespeare\u2019s collaborations\n**Predicted:**\n1. Titus Andronicus: Titus Andronicus is a tragedy by William Shakespeare, believed to have\nbeen written between 1588 and 1593, probably in collaboration with George Peele. It is thought\nto be Shakespeare\u2019s first tragedy, and is often seen as his attempt to emulate the violent and\nbloody revenge plays of his contemporaries, which were extremely popular with audiences\nthroughout the 16th century.\n2. William Shakespeare\u2019s collaborations: Like most playwrights of his period, William Shakespeare did not always write alone... Some of the following attributions, such as \u201dThe Two Noble\nKinsmen\u201d, have well-attested contemporary documentation; others, such as \u201dTitus Andronicus\u201d,\nare dependent on linguistic analysis by modern scholars...\n\n\n**Q:** Zach Parise\u2019s father played in which league?\n**Ground-truth SP Passage Titles:** Jordan Parise; Zach Parise\n**Predicted:**\n1. Zach Parise: Zachary Justin Parise (born July 28, 1984) is an American professional ice\nhockey left winger who is currently serving as an alternate captain for the Minnesota Wild in the\nNational Hockey League (NHL). He has also played for the New Jersey Devils, where he served\nas team captain and led the team to the 2012 Stanley Cup Finals. Parise\u2019s father, J. P. Paris\u00b4e...\n2. J. P. Parise\u00b4: Jean-Paul Joseph-Louis Parise (December 11, 1941 \u2013 January 7, 2015) was a\u00b4\nCanadian professional ice hockey coach and player. Parise played in the National Hockey\nLeague (NHL), most notably for the Minnesota North Stars and the New York Islanders.\n\n\n14\n\n\nPublished as a conference paper at ICLR 2021\n\n\nA.2 EXAMPLES FROM THE QUESTION DECOMPOSITION ANALYSIS\n\n\nTable 9: Sampled retrieval errors (marked in red) _only_ made by the decomposed system. These errors\ncould be potentially avoided if the model has access to the full information in the original question or\nprevious hop results. The important clue for correctly retrieving the documents or avoiding errors is\nmarked in blue. Once", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_13950", "chunk_text": " by the decomposed system. These errors\ncould be potentially avoided if the model has access to the full information in the original question or\nprevious hop results. The important clue for correctly retrieving the documents or avoiding errors is\nmarked in blue. Once decomposed, the marked information are not longer available in one of the\ndecomposed retrieval hop.\n\n\n**Multi-hop Question** : What is the birthday of the author of \u201dShe Walks These Hills\u201d?\n**Decomposed Questions** :\n1. Who is the author of She Walks These Hills?\n2. What is the birthday of Sharyn McCrumb?\n**Ground-truth SP Passages** :\nShe Walks These Hills: She Walks These Hills is a book written by Sharyn McCrumb and\npublished by Charles Scribner\u2019s Sons in 1994, which later went on to win the Anthony Award\nfor Best Novel in 1995.\nSharyn McCrumb: Sharyn McCrumb (born February 26, 1948) is an American writer whose\n~~books celebrate th~~ e history and folklore of Appalachia. McCrumb is the winner of numerous\nliterary awards...\n**Decomposed Error Case:**\n1. She Walks These Hills (\u2713)\n2. Tane McClure\u00b4 : Tan\u00b4e M. McClure (born June 8, 1958) is an American singer and actress.\n\n\n**Multi-hop Question:** When was the album with the song Unbelievable by American rapper\nThe Notorious B.I.G released?\n**Decomposed Questions:**\n1. What is the album with the song Unbelievable by American rapper The Notorious B.I.G?\n2. When was the album Ready to Die released?\n**Ground-truth SP Passages:**\nUnbelievable (The Notorious B.I.G. song): Unbelievable is a song by American rapper The\n~~Notorious B.I.G., recorded for his debut st~~ udio album Ready to Die...\nReady to Die: Ready to Die is the debut studio album by American rapper The Notorious B.I.G.;\n~~it was release~~ d on September 13, 1994, by Bad Boy Records and Arista Records...\n**Decomposed Error Case:**\n1. Unbelievable (The Notorious B.I.G. song) (\u2713)\n2. Ready to Die (The Stooges album", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_14400", "chunk_text": "1994, by Bad Boy Records and Arista Records...\n**Decomposed Error Case:**\n1. Unbelievable (The Notorious B.I.G. song) (\u2713)\n2. Ready to Die (The Stooges album): Ready to Die is the fifth and final studio album by Americ ~~an rock band Iggy and the Stooges.~~ The album was released on April 30, 2013...\n\n\n**Multi-hop Question:** Whose death dramatized in a stage play helped end the death penalty in\nAustralia?\n**Decomposed Questions:**\n1. What is the stage play that helped end the death penalty in Australia?\n2. Whose death was dramatized in Remember Ronald Ryan?\n**Ground-truth SP Passages** :\nBarry Dickins: Barry Dickins (born 1949) is a prolific Australian playwright, author, artist,\n~~actor, educato~~ r and journalist... His most well-known work is the award winning stage play\n\u201dRemember Ronald Ryan\u201d, a dramatization of the life and subsequent death of Ronald Ryan, the\nlast man executed in Australia...\nRonald Ryan: Ronald Joseph Ryan (21 February 1925 \u2013 3 February 1967) was the last person to\n~~be legally exe~~ cuted in Australia. Ryan was found guilty of shooting and killing warder George\nHodson during an escape from Pentridge Prison, Victoria, in 1965...\n**Decomposed Error Case:**\n1. Capital punishment in Australia: Capital punishment in Australia has been abolished in all\nju ~~risdictions. Queensland abolish~~ ed the death penalty in 1922. Tasmania did the same in\n1968, the federal government abolished the death penalty in 1973, with application also in the\nAustralian Capital Territory and the Northern Territory...\n2.Ronald Ryan(\u2713)\n\n\n15\n\n\nPublished as a conference paper at ICLR 2021\n\n\nA.3 EXTRACTIVE & GENERATIVE READER MODEL\n\n\n\nTable 6 demonstrates the answer prediction perfor- Table 10: Answer EM using top 50 remance for four different reader models. The extractive trieved passage chains\nmodels predict answers given the top 250 retrieved pas\nComp Bridge\n\nsage sequences (pairs of passage from hop1 and hop2). Model Overall (20%) (80%)\nSince generative models are generally heavier on the\n\nELECTRA 61.7", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_14850", "chunk_text": " the top 250 retrieved pas\nComp Bridge\n\nsage sequences (pairs of passage from hop1 and hop2). Model Overall (20%) (80%)\nSince generative models are generally heavier on the\n\nELECTRA 61.7 79.0 57.4\n\ncomputation side, we can only use fewer passages. Be\nFiD 61.7 75.3 58.3\n\nsides the observations alredy discussed in \u00a73.2.1, we\nhypothesize the worse performance of multi-hop RAG compared to FiD is partially due to the\nsmaller pretrained model used in RAG, i.e., BART is only half the size of T5-large. Also, as RAG\nback-propagate the gradients to the query encoder, it needs more memory footprint and can only\ntake in fewer retrieved contexts. Our RAG implementation largely follows the implementation of the\n\n- riginal paper and we did not use the PyTorch checkpoint (as used by FiD) to trade computation for\nmemory. We conjecture the multi-hop RAG performance will also improve if we augment the current\nimplementation with memory-saving tricks. However, given the same amount of context and read\nmodel size, the multi-hop RAG is still worse than the extractive ELECTRA reader, i.e., with only the\ntop 1 retrieved passage sequence, our ELECTRA reader gets 53.8 EM compared to the 51.2 answer\nEM achieved by multi-hop RAG when using more context.\n\n\nGiven the same number of retrieved passage sequences (top 50) as shown in table 10, FiD obtains\nsimilar performance to ELECTRA, despite that the generative model can generate arbitrary answers\nfor the given input. (We tried constrained decoding for the generative model. However, no significant\nperformance improvements were observed, indicating that the errors from the generative model are\nnot due to the free-form generation task.) Further question type analysis in HotpotQA showed that\nthe main difference comes from the comparison type of question, while for bridge question, FiD\nperforms slightly better than ELECTRA. This finding might indicate that for generation models,\nnumerical comparison is still a bigger issue compared to extractive models.\n\n\nB MODEL DETAILS\n\n\nB.1 BEST MODEL HYPERPARAMETERS\n\n\nTable 11: Hyperparameters of Retriever\n\nlearning rate 2e-5\n", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_15300", "chunk_text": " generation models,\nnumerical comparison is still a bigger issue compared to extractive models.\n\n\nB MODEL DETAILS\n\n\nB.1 BEST MODEL HYPERPARAMETERS\n\n\nTable 11: Hyperparameters of Retriever\n\nlearning rate 2e-5\nbatch size 150\nmaximum passage length 300\nmaximum query length at initial hop 70\nmaximum query length at 2nd hop 350\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 50\nweight decay 0\n\n\nTable 12: Hyperparameters of Extractive Reader (ELECTRA)\n\n\nlearning rate 5e-5\nbatch size 128\nmaximum sequence length 512\nmaximum answer length 30\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 7\nweight decay 0\n# of negative context per question 5\nweight of SP sentence prediction loss 0.025\n\n\n16\n\n\n\nTable 10: Answer EM using top 50 retrieved passage chains\n\n\n\nComp Bridge\nModel Overall\n(20%) (80%)\n\n\n\nELECTRA 61.7 79.0 57.4\n\nFiD 61.7 75.3 58.3\n\n\nPublished as a conference paper at ICLR 2021\n\n\nB.2 FURTHER DETAILS ABOUT READER MODELS\n\n\nB.2.1 EXTRACTIVE READER\n\n\nThe extractive reader is trained with four loss functions. With the [CLS] token, we predict a\nreranking score based on whether the passage sequence match the groundtruth supporting passages.\nOn top of the representation of each token, we predict a answer start score and answer end score.\nFinally, we prepend each sentence with the [unused0] special token and predict whether the\nsentence is one of the supporting sentences using the representations of the special token. At training\ntime, we pair each question with 1 groundtruth passage sequence and 5 negative passage sequence\nwhich do not contain the answer. At inference time, we feed in the top 250 passage sequences from\nMDR. We rank the predicted answer for each sequence with a linear combination of the reranking\nscore and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_15750", "chunk_text": "score and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is twice as large as the extractive\nmodels and has 770M parameters. We reuse the hyperparameters as described in Izacard & Grave\n(2020). The original FiD uses the top 100 passages for NaturalQuestions. In our case, we use the top\n50 retrieved passage sequences and concatenate the passages in each sequence before feeding into T5.\nIn order to fit this model into GPU, we make use of PyTorch checkpoint [11] for training.\n\n\nB.2.3 MULTI-HOP RAG\n\n\nThe RAG model aims to generate answer _y_ given question _x_ and the retrieved documents _z_ . Similarly,\nthe goal of multi-hop RAG can be expressed as: generate answer _y_ given question _x_ and retrieved\ndocuments in hop one _z_ 1 and hop two _z_ 2 (Limiting to two hops for HotpotQA). The model has three\ncomponents:\n\n\n   - Hop-one retriever _p\u03b7_ 1( _z_ 1 _|x_ ) with parameter _\u03b7_ 1 to represent the retrieved top-k passage\ndistribution (top-k truncated distribution) given the input question _x_ .\n\n\n    - Hop-two retriever _p\u03b7_ 2( _z_ 2 _|x, z_ 1) with parameter _\u03b7_ 2 to represent the hop-two retrieved top-k\npassage distribution given not only the question _x_ but also the retrieved document _z_ 1 from\nhop-one.\n\n\n   - A generator _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _,, y_ 1: _i\u2212_ 1) to represent the next token distribution given input\nquestion _x_, hop-one retrieved document _z_ 1, hop-two retrieved document _z_ 2 and previous\npredicted token _y_ 1: _i\u2212_ 1 parametrized by _\u03b8_\n\n\n**Multi-Hop RAG Sequence Model** As the RAG Sequence model, this model generates the answer\nsequence given the fixed set of documents from hop-one retriever and hop-two retriever. In order", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_16200", "chunk_text": " parametrized by _\u03b8_\n\n\n**Multi-Hop RAG Sequence Model** As the RAG Sequence model, this model generates the answer\nsequence given the fixed set of documents from hop-one retriever and hop-two retriever. In order to\nthe get the probability of the generated sequence, we marginalize through the two latent variables\ncorresponding to the two retrieval hops:\n\n\n_psequence_ ( _y|x_ ) =\n\n\n\n\n\n\n\n_p\u03b7_ 1( _z_ 1 _|x_ )  \n_z_ 1 _z_ 2\n\n\n\n_p\u03b7_ 2( _z_ 2 _|x, z_ 1)\n\n_z_ 2\n\n\n\n_N_\n\n- _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _, y_ 1: _i\u2212_ 1)\n\n\n_i_\n\n\n_N_\n\n- _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _, y_ 1: _i\u2212_ 1)\n\n\n_i_\n\n\n\n\n\n\n_z_ 1\n\n\n\n\n- _p\u03b7_ 1( _z_ 1 _|x_ ) _p\u03b7_ 2( _z_ 2 _|x, z_ 1)\n\n\n_z_ 2\n\n\n\n\n\n\n\n_N_\n\n\n\n\nwhere _z_ 1 and _z_ 2 are top k document from the respective retrieval modules.\n\n\n11https://pytorch.org/docs/stable/checkpoint.html\n\n\n17\n\n\nPublished as a conference paper at ICLR 2021\n\n\n**Multi-Hop RAG Token Model** Moreover, the model can make predictions based on different\npassage extracted at each token.\n\n\n_ptoken_ ( _y|x_ ) =\n\n\n\n_N_\n\n\n\n_i_\n\n\n\n\n\n\n_z_ 1\n\n\n\n\n- _p\u03b7_ 1( _z_ 1 _|x_ ) _p\u03b7_ 2( _z_ 2 _|x, z_ 1) _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _, y_ 1: _i\u2212_ 1)\n\n\n_z_ 2\n\n\n\nThe predicted probability for each token is the following\n\n\n\n_ptoken_ ( _yi|_ ( _x, yj_ )) =\n\n- - _p\u03b7_ 1( _z_ 1 _|x_ ) _p_\n\n\n\n\n\n\n\n_z_ 1\n\n\n\n_p\u03b7_ 1( _z_ 1", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_16650", "chunk_text": "yi|_ ( _x, yj_ )) =\n\n- - _p\u03b7_ 1( _z_ 1 _|x_ ) _p_\n\n\n\n\n\n\n\n_z_ 1\n\n\n\n_p\u03b7_ 1( _z_ 1 _|x_ ) _p\u03b7_ 2( _z_ 2 _|x, z_ 1) _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _, y_ 1: _i\u2212_ 1)\n\n_z_ 2\n\n\n\nC RETRIEVAL-FREE APPROACHES\n\n\nInspired by a recent work (Roberts et al., 2020) that trains the T5 seq2seq model to directly decode answers from questions ( _retrieval-free_ ), we conduct similar experiments on HotpotQA using\nBART (Lewis et al., 2020a). As shown in Figure 4, the performance gap between retrieval-based\nmethods and retrieval-free methods on multi-hop QA is much larger than the gap in the case of simple\nsingle-hop questions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Col2|\n|---|---|\n|||\n\n\n|Col1|Col2|\n|---|---|\n|||\n\n\nFigure 4: Performance gap between retrieval-free and retrieval-based methods on different QA\ndatasets.\n\n\nD A UNIFIED QA RETRIEVAL SYSTEM\n\n\nIn practice, when a fixed text corpus is given for open-domain systems, we do not know beforehand\nwhether the incoming questions require single or multiple text evidence. Thus, it is essential to build\na unified system that adaptively retrieves for multiple hops. Due to the simplicity of the approach, our\nmethod can easily be extended in the unified setup. To the best of our knowledge, only (Asai et al.,\n2020) test the same retrieval method on both single and multi-hop questions but with separate trained\nmodels. Here we take a further step and explore the possibility of using a single retrieval model for\nboth types of questions.\n\n\nTo enable adaptive retrieval, we add a binary prediction head on top of the question encoder. Once\nthe retriever finishes the 1-hop retrieval, it encodes concatenation of _q_ and _p_ 1 and predicts whether\nto stop retrieval using the final hidden state of the first token. We construct this unified setting\nwith NaturalQuestions-Open (Lee et al., 2019) (NQ) as single", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_17100", "chunk_text": "_ and _p_ 1 and predicts whether\nto stop retrieval using the final hidden state of the first token. We construct this unified setting\nwith NaturalQuestions-Open (Lee et al., 2019) (NQ) as single-hop and HotpotQA as multi-hop.\nAs the two datasets use different corpora, we merge the two [12] for easy comparison. As baselines,\n\n\n12The Wikipedia corpus of NQ is taken from DPR (Karpukhin et al., 2020).\n\n\n18\n\n\nPublished as a conference paper at ICLR 2021\n\n\nwe use the retrieval models trained only on the respective dataset. For HotpotQA, the baseline\nis the best multi-hop retrieval model discussed in the main text. For NQ, we follow the training\nmethod in DPR (Karpukhin et al., 2020), but with a shared question and passage encoder, which\nachieves stronger results. As the NQ corpus includes multiple passages of the same document and the\nHotpotQA corpus only uses the introduction passage, we are not able to compute the strict title-based\nsupport passage recall for HotpotQA as in \u00a73.2. Thus, we only evaluate answer recall. Results are in\nTable 13. In contrast to existing studies that train different models for each dataset, we show that\na unified dense retrieval model can maintain competitive performance on both, despite the vastly\ndifferent nature of both datasets. Note that the information-seeking questions in NQ is usually noisier\nand more ambiguous, while HotpotQA questions are more complicated and contains more lexical\n\n- verlaps with the evidence passages. Specifically, for NQ, the unified retrieval model achieves very\nsimilar performance as the single-dataset DPR model, while the performance on HotpotQA decreases\nmore. We conjecture that this is because the information-seeking questions in NQ cover more diverse\npatterns, and the added HotpotQA training questions do not cause a dramatic distribution shift from\nthe NQ test data. We leave the development of a more general retrieval system that handles different\nstyles of questions to future work.\n\n\nTable 13: Comparing the unified retrieval model with models specifically trained for each task. We\ntest the retrieval performance with a single merged corpus. For easy comparison, all three models are\nbased on BERT-base encoder which we find achieves stronger performance than RoBERTa-base on\nNQ. AR@K", "token_count": 500, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2009.12756_multihop_qi:chunk_17550", "chunk_text": " for each task. We\ntest the retrieval performance with a single merged corpus. For easy comparison, all three models are\nbased on BERT-base encoder which we find achieves stronger performance than RoBERTa-base on\nNQ. AR@K denotes answer recall at top-K retrieved passage sequences.\n\n\nNQ HotpotQA\nModel\nAR@20 AR@100 AR@20 AR@100\n\n\nsingle-hop only 80.7 87.3            -            multi-hop only            -            - 83.4 89.4\nunified 79.5 86.1 78.1 83.0\n\n\n19\n\n\n", "token_count": 132, "metadata": {"arxiv_id": "2009.12756", "title": "Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval", "authors": ["Wenhan Xiong", "Xiang Lorraine Li", "Srini Iyer", "Jingfei Du", "Patrick Lewis", "William Yang Wang", "Yashar Mehdad", "Wen-tau Yih", "Sebastian Riedel", "Douwe Kiela", "Barlas O\u011fuz"], "year": 2020, "url": "https://arxiv.org/pdf/2009.12756v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_0", "chunk_text": "## Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\nAlbert Gu [\u2217][1] and Tri Dao [\u2217][2]\n\n\n1\nMachine Learning Department, Carnegie Mellon University\n\n\n2\nDepartment of Computer Science, Princeton University\nagu@cs.cmu.edu, tri@tridao.me\n\n\n**Abstract**\n\n\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the\nTransformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention,\ngated convolution and recurrent models, and structured state space models (SSMs) have been developed to address\nTransformers\u2019 computational inefficiency on long sequences, but they have not performed as well as attention on important\nmodalities such as language. We identify that a key weakness of such models is their inability to perform content-based\nreasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses\ntheir weakness with discrete modalities, allowing the model to _selectively_ propagate or forget information along the\nsequence length dimension depending on the current token. Second, even though this change prevents the use of efficient\nconvolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a\nsimplified end-to-end neural network architecture without attention or even MLP blocks ( **Mamba** ). Mamba enjoys fast\ninference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves\n\n    - n real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art\nperformance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model\n\n    - utperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream\nevaluation.\n\n### **1 Introduction**\n\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged\nas an effective paradigm in modern machine learning. The backbone of these FMs are often _sequence models_, operating on\narbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and\ngenomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_450", "chunk_text": " a wide variety of domains such as language, images, speech, audio, time series, and\ngenomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever,\nVinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are\npredominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention\nlayer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely\nwithin a context window, allowing it to model complex data. However, this property brings fundamental drawbacks:\nan inability to model anything outside of a finite window, and quadratic scaling with respect to the window length.\nAn enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay,\nDehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these\nvariants have been shown to be empirically effective at scale across domains.\n\n\nRecently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have\nemerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of\nrecurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space\nmodels (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with\nlinear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range\ndependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the Long Range\n\n\n\u2217Alphabetical by first name.\n\n\n1\n\n\nArena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta,", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_900", "chunk_text": "phabetical by first name.\n\n\n1\n\n\nArena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu,\nand Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been\nsuccessful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022;\nSaon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such\n\nas text.\n\n\nWe propose a new class of **selective state space models**, that improves on prior work on several axes to achieve the\nmodeling power of Transformers while scaling linearly in sequence length.\n\n\n**Selection Mechanism.** First, we identify a key limitation of prior models: the ability to efficiently _select_ data in an\ninput-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic\ntasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM\nparameters based on the input. This allows the model to filter out irrelevant information and remember relevant information\nindefinitely.\n\n\n**Hardware-aware Algorithm.** This simple change poses a technical challenge for the computation of the model; in\nfact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this\nwith a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not\nmaterialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The\nresulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to\npseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3\u00d7 faster on A100 GPUs).\n\n\n**Architecture.** We simplify prior deep sequence model architectures by combining the design of prior SSM architectures\n(Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_1350", "chunk_text": " on A100 GPUs).\n\n\n**Architecture.** We simplify prior deep sequence model architectures by combining the design of prior SSM architectures\n(Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous\narchitecture design ( **Mamba** ) incorporating selective state spaces.\n\n\nSelective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them\nsuitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong\nperformance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory\nscales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only\nconstant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency\ntogether yield performance improvements on real data up to sequence length 1M.\n\n\nWe empirically validate Mamba\u2019s potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n\n**Synthetics.** On important synthetic tasks such as copying and induction heads that have been proposed as being key to\nlarge language models, Mamba not only solves them easily but can _extrapolate solutions indefinitely long_ ( _>_ 1M tokens).\n\n\n- **Audio and Genomics.** Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers\n\n - n modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing\nFID on a challenging speech generation dataset by more than half). In both settings, its _performance improves with longer_\n_context up to million-length sequences_ .\n\n\n- **Language Modeling.** Mamba is the first _linear-time sequence model that truly achieves Transformer-quality performance_,\nboth in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba\nexceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based\n\n - n LLaMa (Touvron et al. 2023). Our Mamba language model has 5\u00d7 generation throughput compared to Transformers\n\n - f similar size, and Mamba-", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_1800", "chunk_text": ", including very strong modern Transformer training recipes based\n\n - n LLaMa (Touvron et al. 2023). Our Mamba language model has 5\u00d7 generation throughput compared to Transformers\n\n - f similar size, and Mamba-3B\u2019s quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common\nsense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\n\n\n[Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.](https://github.com/state-spaces/mamba)\n\n### **2 State Space Models**\n\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related\nto RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n\n2\n\n\n**Selective State Space Model**\n\n_**with Hardware-aware State Expansion**_\n\n\n\ud835\udc34\n\n\n\n\ud835\udc65!\n\n\n\n\n\n\ud835\udc66!\n\n\n\n\n\n\n\n\n\n\n\n**Selection Mechanism**\n\n|\ud835\udc35 \ud835\udc36<br>! !<br>\u2206<br>!<br>Discretize<br>Project|Col2|\n|---|---|\n|||\n|||\n\n\n\nFigure 1: ( **Overview** .) Structured SSMs independently map each channel (e.g. _\ud835\udc37_ = 5) of an input _\ud835\udc65_ to output _\ud835\udc66_ through a higher\ndimensional latent state _\u210e_ (e.g. _\ud835\udc41_ = 4). Prior SSMs avoid materializing this large effective state ( _\ud835\udc37\ud835\udc41_, times batch size _\ud835\udc35_ and sequence\nlength _\ud835\udc3f_ ) through clever alternate computation paths requiring time-invariance: the (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ) parameters are constant across time. Our\nselection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize\nthe expanded states in more efficient levels of the GPU memory hierarchy.\n\n\n1-dimensional function or sequence _\ud835\udc65_ ( _\ud835\udc61_ ) \u2208 R \u21a6\u2192 _\ud835\udc66_ ( _\ud835\udc61_ ) \u2208 R through an implicit latent state _\u210e_ ( _\ud835\udc61_ )", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_2250", "chunk_text": " function or sequence _\ud835\udc65_ ( _\ud835\udc61_ ) \u2208 R \u21a6\u2192 _\ud835\udc66_ ( _\ud835\udc61_ ) \u2208 R through an implicit latent state _\u210e_ ( _\ud835\udc61_ ) \u2208 R _[\ud835\udc41]_ .\n\n\nConcretely, S4 models are defined with four parameters (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ), which define a sequence-to-sequence transformation\nin two stages.\n\n\n\n_\u210e_ [\u2032] ( _\ud835\udc61_ ) = _\ud835\udc68\u210e_ ( _\ud835\udc61_ ) + _\ud835\udc69\ud835\udc65_ ( _\ud835\udc61_ ) (1a)\n\n\n_\ud835\udc66_ ( _\ud835\udc61_ ) = _\ud835\udc6a\u210e_ ( _\ud835\udc61_ ) (1b)\n\n\n\n_\u210e\ud835\udc61_ = _\ud835\udc68\u210e\ud835\udc61_ - 1 + _\ud835\udc69\ud835\udc65\ud835\udc61_ (2a)\n\n\n_\ud835\udc66\ud835\udc61_ = _\ud835\udc6a\u210e\ud835\udc61_ (2b)\n\n\n\n~~_\ud835\udc58_~~\n_\ud835\udc72_ = ( _\ud835\udc6a\ud835\udc69, \ud835\udc6a\ud835\udc68\ud835\udc69, . . ., \ud835\udc6a\ud835\udc68_ _\ud835\udc69, . . ._ ) (3a)\n\n\n_\ud835\udc66_ = _\ud835\udc65_ \u2217 _\ud835\udc72_ (3b)\n\n\n\n**Discretization.** The first stage transforms the \u201ccontinuous parameters\u201d (\u0394 _, \ud835\udc68, \ud835\udc69_ ) to \u201cdiscrete parameters\u201d ( _\ud835\udc68, \ud835\udc69_ ) through\nfixed formulas _\ud835\udc68_ = _\ud835\udc53\ud835\udc34_ (\u0394 _, \ud835\udc68_ ) and _\ud835\udc69_ = _\ud835\udc53\ud835\udc35_ (\u0394 _, \ud835\udc68, \ud835\udc69_ ), where the pair ( _\ud835\udc53\ud835\udc34, \ud835\udc53\ud835\udc35_ ) is called a _discretization rule_ . Various rules can\nbe used such as the zero-order hold (ZOH) defined in equation (4).\n\n\n_\ud835\udc68_ = exp(\u0394 _\ud835\udc68_ ) _\ud835\udc69_ = (\u0394 _", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_2700", "chunk_text": "_ . Various rules can\nbe used such as the zero-order hold (ZOH) defined in equation (4).\n\n\n_\ud835\udc68_ = exp(\u0394 _\ud835\udc68_ ) _\ud835\udc69_ = (\u0394 _\ud835\udc68_ ) [\u2212][1] (exp(\u0394 _\ud835\udc68_ ) \u2212 _\ud835\udc70_ ) \u00b7 \u0394 _\ud835\udc69_ (4)\n\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such\nas resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly normalized\n(Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023). It also has connections to gating mechanisms of RNNs (Gu,\nGulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5. However, from a mechanical point\n\n- f view discretization can simply be viewed as the first step of the computation graph in the forward pass of an SSM.\nAlternate flavors of SSMs can bypass the discretization step and parameterize ( _\ud835\udc68, \ud835\udc69_ ) directly instead (Zhang et al. 2023),\nwhich may be easier to reason about.\n\n\n**Computation.** After the parameters have been transformed from (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ) \u21a6\u2192( _\ud835\udc68, \ud835\udc69, \ud835\udc6a_ ), the model can be computed\nin two ways, either as a **linear recurrence** (2) or a **global convolution** (3).\n\n\nCommonly, the model uses the convolutional mode (3) for efficient parallelizable training (where the whole input sequence\nis seen ahead of time), and switched into recurrent mode (2) for efficient autoregressive inference (where the inputs are\nseen one timestep at a time).\n\n\n**Linear Time Invariance (LTI).** An important property of equations (1) to (3) is that the model\u2019s dynamics are constant\nthrough time. In other words (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ), and consequently ( _\ud835\udc68, \ud835\udc69_ ) as well, are fixed", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_3150", "chunk_text": " is that the model\u2019s dynamics are constant\nthrough time. In other words (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ), and consequently ( _\ud835\udc68, \ud835\udc69_ ) as well, are fixed for all time-steps. This property is\n\n\n3\n\n\ncalled _linear time invariance (LTI)_, which is deeply connected to recurrence and convolutions. Informally, we think of\nLTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these\nclasses of models.\n\n\nThus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental efficiency constraints,\ndiscussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling\ncertain types of data, and our technical contributions involve removing the LTI constraint while overcoming the efficiency\nbottlenecks.\n\n\n**Structure and Dimensions.** Finally, we note that structured SSMs are so named because computing them efficiently\nalso requires imposing structure on the _\ud835\udc68_ matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022;\nGupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use.\n\n\nIn this case, the _\ud835\udc68_ \u2208 R _[\ud835\udc41]_ [\u00d7] _[\ud835\udc41]_ _, \ud835\udc69_ \u2208 R _[\ud835\udc41]_ [\u00d7][1] _, \ud835\udc6a_ \u2208 R [1][\u00d7] _[\ud835\udc41]_ matrices can all be represented by _\ud835\udc41_ numbers. To operate over an input\nsequence _\ud835\udc65_ - f batch size _\ud835\udc35_ and length _\ud835\udc3f_ with _\ud835\udc37_ channels, the SSM is applied independently to each channel. Note that in\nthis case, the total hidden state has dimension _\ud835\udc37\ud835\udc41_ per input, and computing it over the sequence length requires _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ )\ntime and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3.\n\n\n**General State Space", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_3600", "chunk_text": " the sequence length requires _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ )\ntime and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3.\n\n\n**General State Space Models.** We note that the term _state space model_ has a very broad meaning which simply represents\nthe notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in different\ndisciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal\nmodeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman filters (controls (Kalman 1960)),\nhidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes\nconvolutional) models at large (deep learning).\n\n\nThroughout this entire paper we use the term \u201cSSM\u201d to refer exclusively to the class of structured SSMs or S4 models (Gu,\nGoel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al. 2023; Smith, Warrington,\nand Linderman 2023) and use these terms interchangeably. For convenience we may also include derivatives of such\nmodels, such as those focusing on either the linear-recurrence or global-convolution viewpoints (Y. Li et al. 2023; Orvieto\net al. 2023; Poli et al. 2023), and clarify nuances when necessary.\n\n\n**SSM Architectures.** SSMs are standalone sequence transformations that can be incorporated into end-to-end neural\nnetwork architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear\nconvolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our\nprimary baselines.\n\n\n- Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be\nviewed as a degenerate linear SSM.\n\n\n- H3 (Dao, Fu, Saab, et al. ", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_4050", "chunk_text": "Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be\nviewed as a degenerate linear SSM.\n\n\n- H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM\nsandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a\nshift-SSM, before the main SSM layer.\n\n\n- Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global\nconvolution (Romero et al. 2021).\n\n\n- RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative\nparallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions.\n\n\n- RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \u201cWKV\u201d mechanism involves LTI recurrences and\ncan be viewed as the ratio of two SSMs.\n\n\nOther closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight\nin particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which\nwe view as the most closely related methods to our core selective SSM.\n\n\n4\n\n\n### **3 Selective State Space Models**\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate\nthis mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting\na technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits\nthe memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or\neven MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_4500", "chunk_text": "the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or\neven MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5).\n\n\n**3.1** **Motivation: Selection as a Means of Compression**\n\n\nWe argue that a fundamental problem of sequence modeling is _compressing context into a smaller state_ . In fact, we can\nview the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and\ninefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference\nrequires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and\nquadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state,\nimplying constant-time inference and linear-time training. However, their effectiveness is limited by how well this state\nhas compressed the context.\n\n\nTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\n\n\n- The **Selective Copying** task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the\nposition of the tokens to memorize. It requires _content-aware_ reasoning to be able to memorize the relevant tokens\n( _colored_ ) and filter out the irrelevant ones ( _white_ ).\n\n\n- The **Induction Heads** task is a well-known mechanism hypothesized to explain the majority of in-context learning\nabilities of LLMs (Olsson et al. 2022). It requires _context-aware_ reasoning to know when to produce the correct output in\nthe appropriate context ( _black_ ).\n\n\nThese tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the ( _\ud835\udc68, \ud835\udc69_ )\ntransitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed\nalong the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can\nsolve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty\nwith the Selective Copying task because of", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_4950", "chunk_text": " view, it is known that global convolutions can\nsolve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty\nwith the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between\ninputs-to-outputs is varying and cannot be modeled by static convolution kernels.\n\n\nIn summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress\ntheir state: efficient models must have a small state, while effective models must have a state that contains all necessary\ninformation from the context. In turn, we propose that a fundamental principle for building sequence models is **selectivity** :\n\n- r the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism\ncontrols how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion).\n\n\n**3.2** **Improving SSMs with Selection**\n\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along\nthe sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be input-dependent.\n\n\nAlgorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several\nparameters \u0394 _, \ud835\udc69, \ud835\udc6a_ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we\nhighlight that these parameters now have a length dimension _\ud835\udc3f_, meaning that the model has changed from time-invariant\nto time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3)\nwith implications for its efficiency, discussed next.\n\n\nWe specifically choose _\ud835\udc60\ud835\udc35_ ( _\ud835\udc65_ ) = Linear _\ud835\udc41_ ( _\ud835\udc65_ ), _\ud835\udc60\ud835\udc36_ ( _\ud835\udc65_ ) = Linear _\ud835\udc41_ ( _\ud835\udc65_ ), _\ud835\udc60_ \u0394 ( _\ud835\udc65_ ) = Broadcast _\ud835\udc37_ (Linear1( _\ud835\udc65_ )), and _\ud835\udf0f_ \u0394 = softplus,\nwhere Linear _\ud835\udc51_ is a parameterized projection to dimension _\ud835\udc51_ . The choice", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_5400", "chunk_text": " Broadcast _\ud835\udc37_ (Linear1( _\ud835\udc65_ )), and _\ud835\udf0f_ \u0394 = softplus,\nwhere Linear _\ud835\udc51_ is a parameterized projection to dimension _\ud835\udc51_ . The choice of _\ud835\udc60_ \u0394 and _\ud835\udf0f_ \u0394 is due to a connection to RNN gating\nmechanisms explained in Section 3.5.\n\n\n5\n\n\nOutput\n\n\nInput\n\n\n\n**Copying** **Selective Copying**\n\n\nOutput\n\n\nInput\n\n\n\nSolution **Induction Heads**\n\n\nPerfectly solved by LTI (e.g. convolutional) models that do not need to look at the actual inputs\n\n\n\n\n\nFigure 2: ( _Left_ ) The standard version of the Copying task involves constant spacing between input and output elements and is easily\nsolved by time-invariant models such as linear recurrences and global convolutions. ( _Right Top_ ) The Selective Copying task has random\nspacing in between inputs and requires time-varying models that can _selectively_ remember or ignore inputs depending on their content.\n( _Right Bottom_ ) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key\nability for LLMs.\n\n\n\n**Algorithm 1** SSM (S4)\n\n\n**Input:** _\ud835\udc65_ : (B _,_ L _,_ D)\n**Output:** _\ud835\udc66_ : (B _,_ L _,_ D)\n\n1: _\ud835\udc68_ : (D _,_ N) \u2190 Parameter\n_\u22b2_ Represents structured _\ud835\udc41_ \u00d7 _\ud835\udc41_ matrix\n2: _\ud835\udc69_ : (D _,_ N) \u2190 Parameter\n3: _\ud835\udc6a_ : (D _,_ N) \u2190 Parameter\n4: \u0394 : (D) \u2190 _\ud835\udf0f_ \u0394 (Parameter)\n5: _\ud835\udc68, \ud835\udc69_ : (D _,_ N) \u2190 discretize(\u0394 _, \ud835\udc68, \ud835\udc69_ )\n6: _\ud835\udc66_ \u2190 SSM( _\ud835\udc68, \ud835\udc69, \ud835\udc6a_ )( _\ud835\udc65_ )\n_\u22b2_ Time-invariant: recurrence or convolution\n\n7: **return** _\ud835\udc66_\n\n\n\n**Algorithm 2** SSM + Selection (S6)\n\n\n**Input:** _\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_5850", "chunk_text": "_ )( _\ud835\udc65_ )\n_\u22b2_ Time-invariant: recurrence or convolution\n\n7: **return** _\ud835\udc66_\n\n\n\n**Algorithm 2** SSM + Selection (S6)\n\n\n**Input:** _\ud835\udc65_ : (B _,_ L _,_ D)\n**Output:** _\ud835\udc66_ : (B _,_ L _,_ D)\n\n1: _\ud835\udc68_ : (D _,_ N) \u2190 Parameter\n_\u22b2_ Represents structured _\ud835\udc41_ \u00d7 _\ud835\udc41_ matrix\n2: _\ud835\udc69_ : (B _,_ L _,_ N) \u2190 _\ud835\udc60\ud835\udc35_ ( _\ud835\udc65_ )\n3: _\ud835\udc6a_ : (B _,_ L _,_ N) \u2190 _\ud835\udc60\ud835\udc36_ ( _\ud835\udc65_ )\n4: \u0394 : (B _,_ L _,_ D) \u2190 _\ud835\udf0f_ \u0394 (Parameter+ _\ud835\udc60_ \u0394 ( _\ud835\udc65_ ))\n5: _\ud835\udc68, \ud835\udc69_ : (B _,_ L _,_ D _,_ N) \u2190 discretize(\u0394 _, \ud835\udc68, \ud835\udc69_ )\n6: _\ud835\udc66_ \u2190 SSM( _\ud835\udc68, \ud835\udc69, \ud835\udc6a_ )( _\ud835\udc65_ )\n_\u22b2_ Time-varying: recurrence ( _scan_ ) only\n7: **return** _\ud835\udc66_\n\n\n\n**3.3** **Efficient Implementation of Selective SSMs**\n\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau,\nCho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on\nmodern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate\nspecial cases of selection, such as letting \u0394 vary over time in recurrent SSMs (Gu, Dao, et al. 2020). However, as previously\nmentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives\nused LTI (non-selective) models, most commonly in the", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_6300", "chunk_text": " al. 2020). However, as previously\nmentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives\nused LTI (non-selective) models, most commonly in the form of global convolutions.\n\n\n**3.3.1** **Motivation of Prior Models**\n\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods.\n\n\nAt a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in\nSection 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to _maximize_\n_hidden state dimension without paying speed and memory costs_ .\n\n\n- Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding\nthe former (2) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and\nmaterializing the latent state _\u210e_ with shape (B _,_ L _,_ D _,_ N), which is much larger (by a factor of _\ud835\udc41_, the SSM state dimension)\nthan the input _\ud835\udc65_ and output _\ud835\udc66_  - f shape (B _,_ L _,_ D). Thus the more efficient convolution mode was introduced which could\nbypass the state computation and materializes a convolution kernel (3a) of size only (B _,_ L _,_ D).\n\n\nPrior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by\na factor of _\ud835\udc41_ (\u2248 10 \u2212 100), much larger than traditional RNNs, without efficiency penalties.\n\n\n6\n\n\n**3.3.2** **Overview of Selective Scan: Hardware-Aware State Expansion**\n\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to\nrevisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and\nrecomputation. We make two main observations:\n\n\n- The naive recurrent computation uses _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ ) FLOPs while the convolutional computation uses _\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_6750", "chunk_text": "\nrecomputation. We make two main observations:\n\n\n- The naive recurrent computation uses _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ ) FLOPs while the convolutional computation uses _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37_ log( _\ud835\udc3f_ )) FLOPs,\nand the former has a lower constant factor. Thus for long sequences and not-too-large state dimension _\ud835\udc41_, the recurrent\nmode can actually use fewer FLOPs.\n\n\nThe two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like\nthe convolutional mode, we can attempt to not actually materialize the full state _\u210e_ .\n\n\nThe main idea is to leverage properties of modern accelerators (GPUs) to materialize the state _\u210e_ - nly in more efficient\nlevels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory\nbandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our\nscan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared\nto a standard implementation.\n\n\nConcretely, instead of preparing the scan input ( _\ud835\udc68, \ud835\udc69_ ) of size (B _,_ L _,_ D _,_ N) in GPU HBM (high-bandwidth memory), we load\nthe SSM parameters (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM,\nand then write the final outputs of size (B _,_ L _,_ D) back to HBM.\n\n\nTo avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient\nparallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023).\n\n\nFinally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply\nthe classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but\nrecomputed in the backward pass when", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_7200", "chunk_text": "Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply\nthe classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but\nrecomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan\nlayer has the same memory requirements as an optimized transformer implementation with FlashAttention.\n\n\nDetails of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated\nin Figure 1.\n\n\n**3.4** **A Simplified SSM Architecture**\n\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into\nneural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are\ngenerally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We\nsimplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is\ninspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention.\n\n\nThis architecture involves expanding the model dimension _\ud835\udc37_ by a controllable expansion factor _\ud835\udc38_ . For each block, most\n\n- f the parameters (3 _\ud835\udc38\ud835\udc37_ [2] ) are in the linear projections (2 _\ud835\udc38\ud835\udc37_ [2] for input projections, _\ud835\udc38\ud835\udc37_ [2] for output projection) while\nthe inner SSM contributes less. The number of SSM parameters (projections for \u0394 _, \ud835\udc69, \ud835\udc6a_, and the matrix _\ud835\udc68_ ) are much\nsmaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form\nthe Mamba architecture. We always fix to _\ud835\udc38_ = 2 in our experiments and use two stacks of the block to match the 12 _\ud835\udc37_ [2]\n\nparameters of a Transformer\u2019s interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation\nfunction (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_7650", "chunk_text": "\u2019s interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation\nfunction (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP\nbecomes the popular \u201cSwiGLU\u201d variant (Chowdhery et al. 2023; Dauphin et al. 2017; Shazeer 2020; Touvron et al. 2023).\nFinally, we additionally use an optional normalization layer (we choose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)),\nmotivated by RetNet\u2019s usage of a normalization layer in a similar location (Y. Sun et al. 2023).\n\n\n**3.5** **Properties of Selection Mechanisms**\n\n\nThe selection mechanism is a broader concept that can be applied in different ways, such as to more traditional RNNs or\nCNNs, to different parameters (e.g. _\ud835\udc68_ in Algorithm 2), or using different transformations _\ud835\udc60_ ( _\ud835\udc65_ ).\n\n\n7\n\n\nX ~~X~~ ~~X~~\n\n\n\n\n\n\n\nX ! !\n\n\n\n\n\n\n\n\n\nX\n\n\n\nLinear\n\nprojection\n\n\nSequence\ntransformation\n\n\nNonlinearity\n(activation or\nmultiplication)\n\n\n|X<br>SSM<br>X<br>Conv|Col2|\n|---|---|\n|||\n\n\n|X<br>!|Col2|\n|---|---|\n|||\n|||\n\n\n|X<br>SSM<br>! !<br>Conv|Col2|\n|---|---|\n|||\n|||\n\n\n\n**H3** \u2a02 **Gated MLP** **Mamba**\n\n\nFigure 3: ( **Architecture** .) Our simplified block design combines the H3 block, which is the basis of most SSM architectures, with\nthe ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block\nhomogenously. Compared to the H3 block, Mamba replaces the first multiplicative gate with an activation function. Compared to\nthe MLP block, Mamba adds an SSM to the main branch. For _\ud835\udf0e_ we use the SiLU / Swish activation (Hendrycks and Gimpel 2016;\nRamachandran, Z", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_8100", "chunk_text": "\nthe MLP block, Mamba adds an SSM to the main branch. For _\ud835\udf0e_ we use the SiLU / Swish activation (Hendrycks and Gimpel 2016;\nRamachandran, Zoph, and Quoc V Le 2017).\n\n\n**3.5.1** **Connection to Gating Mechanisms**\n\n\nWe highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection\nmechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time systems\nis well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1 is an improvement of\nGu, Johnson, Goel, et al. (2021, Lemma 3.1) generalizing to the ZOH discretization and input-dependent gates (proof in\nAppendix C). More broadly, \u0394 in SSMs can be seen to play a generalized role of the RNN gating mechanism. In line with\nprior work, we adopt the view that _discretization of SSMs is the principled foundation of heuristic gating mechanisms_ .\n\n\n**Theorem 1.** _When \ud835\udc41_ = 1 _, \ud835\udc68_ = \u22121 _, \ud835\udc69_ = 1 _,\ud835\udc60_ \u0394 = Linear( _\ud835\udc65_ ) _, and \ud835\udf0f_ \u0394 = softplus _, then the selective SSM recurrence (Algorithm 2)_\n_takes the form_\n_\ud835\udc54\ud835\udc61_ = _\ud835\udf0e_ (Linear( _\ud835\udc65\ud835\udc61_ ))\n\n(5)\n_\u210e\ud835\udc61_ = (1 \u2212 _\ud835\udc54\ud835\udc61_ ) _\u210e\ud835\udc61_                          - 1 + _\ud835\udc54\ud835\udc61\ud835\udc65\ud835\udc61_ _._\n\n\nAs mentioned in Section 3.2, our specific choices of _\ud835\udc60_ \u0394 _,\ud835\udf0f_ \u0394 is from this connection. In particular, note that if a given input _\ud835\udc65\ud835\udc61_\nshould be completely ignored (as necessary in the synthetic tasks), all _\ud835\udc37_ channels should ignore it, and so we project the\ninput down to 1 dimension before repeating/broadcasting with \u0394", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_8550", "chunk_text": " _\ud835\udc65\ud835\udc61_\nshould be completely ignored (as necessary in the synthetic tasks), all _\ud835\udc37_ channels should ignore it, and so we project the\ninput down to 1 dimension before repeating/broadcasting with \u0394.\n\n\n**3.5.2** **Interpretation of Selection Mechanisms**\n\n\nWe elaborate on three particular mechanistic effects of selection.\n\n\n**Variable Spacing.** Selectivity allows filtering out irrelevant noise tokens that may occur between inputs of interest.\nThis is exemplified by the Selective Copying task, but occurs ubiquitously in common data modalities, particularly for\ndiscrete data \u2013 for example the presence of language fillers such as \u201cum\u201d. This property arises because the model can\nmechanistically filter out any particular input _\ud835\udc65\ud835\udc61_, for example in the gated RNN case (Theorem 1) when _\ud835\udc54\ud835\udc61_ \u2192 0.\n\n\n**Filtering Context.** It has been empirically observed that many sequence models do not improve with longer context (F.\nShi et al. 2023), despite the principle that more context should lead to strictly better performance. An explanation is\nthat many sequence models cannot effectively ignore irrelevant context when necessary; an intuitive example are global\nconvolutions (and general LTI models). On the other hand, selective models can simply reset their state at any time\nto remove extraneous history, and thus their performance in principle improves monotonicly with context length (e.g.\nSection 4.3.2).\n\n\n8\n\n\n**Boundary Resetting.** In settings where multiple independent sequences are stitched together, Transformers can keep\nthem separate by instantiating a particular attention mask, while LTI models will bleed information between the sequences.\nSelective SSMs can also reset their state at boundaries (e.g. \u0394 _\ud835\udc61_ \u2192\u221e, or Theorem 1 when _\ud835\udc54\ud835\udc61_ \u2192 1). These settings may\n\n- ccur artificially (e.g. packing documents together to improve hardware utilization) or naturally (e.g. episode boundaries in\nreinforcement learning (Lu et al. 2023)).\n\n\nAdditionally, we elaborate on effects of each selective parameter.\n\n\n**Interpretation of** \u0394 **.** In general, \u0394 controls the balance between how much to focus or ignore the current input _\ud835\udc65\ud835\udc61_ . It\ngeneral", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_9000", "chunk_text": "Additionally, we elaborate on effects of each selective parameter.\n\n\n**Interpretation of** \u0394 **.** In general, \u0394 controls the balance between how much to focus or ignore the current input _\ud835\udc65\ud835\udc61_ . It\ngeneralizes RNN gates (e.g. _\ud835\udc54\ud835\udc61_ in Theorem 1): mechanically, a large \u0394 resets the state _\u210e_ and focuses on the current input _\ud835\udc65_,\nwhile a small \u0394 persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as a continuous system\ndiscretized by a timestep \u0394, and in this context the intuition is that large \u0394 \u2192\u221e represents the system focusing on the\ncurrent input for longer (thus \u201cselecting\u201d it and forgetting its current state) while a small \u0394 \u2192 0 represents a transient\ninput that is ignored.\n\n\n**Interpretation of** _\ud835\udc68_ **.** We remark that while the _\ud835\udc68_ parameter could also be selective, it ultimately affects the model\n\n- nly through its interaction with \u0394 via _\ud835\udc68_ = exp(\u0394 _\ud835\udc68_ ) (the discretization (4)). Thus selectivity in \u0394 is enough to ensure\nselectivity in ( _\ud835\udc68, \ud835\udc69_ ), and is the main source of improvement. We hypothesize that making _\ud835\udc68_ selective in addition to (or\ninstead of) \u0394 would have similar performance, and leave it out for simplicity.\n\n\n**Interpretation of** _\ud835\udc69_ **and** _\ud835\udc6a_ **.** As discussed in Section 3.1, the most important property of selectivity is filtering out\nirrelevant information so that a sequence model\u2019s context can be compressed into an efficient state. In an SSM, modifying\n_\ud835\udc69_ and _\ud835\udc6a_ to be selective allows finer-grained control over whether to let an input _\ud835\udc65\ud835\udc61_ into the state _\u210e\ud835\udc61_, or the state into the\n\n- utput _\ud835\udc66\ud835\udc61_ . These can be interpreted as allowing the model to modulate the recurrent dynamics based on content (input)\nand context (hidden states) respectively.\n\n\n**3.6** **Additional Model Details**\n\n\n**Real vs. Complex.** Most prior SSMs use complex numbers", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_9450", "chunk_text": " as allowing the model to modulate the recurrent dynamics based on content (input)\nand context (hidden states) respectively.\n\n\n**3.6** **Additional Model Details**\n\n\n**Real vs. Complex.** Most prior SSMs use complex numbers in their state _\u210e_, which is necessary for strong performance\n\n- n many tasks in perceptual modalities (Gu, Goel, and R\u00e9 2022). However, it has been empirically observed that completely\nreal-valued SSMs seem to work fine, and possibly even better, in some settings (Ma et al. 2023). We use real values as\nthe default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeoff is related to the\ncontinuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous modalities (e.g. audio,\nvideo) but not discrete (e.g. text, DNA).\n\n\n**Initialization.** Most prior SSMs also suggest special initializations, particularly in the complex-valued case, which can\nhelp in several settings such as low-data regimes. Our default initialization for the complex case is S4D-Lin and for the real\ncase is S4D-Real (Gu, Gupta, et al. 2022), which is based on the HIPPO theory (Gu, Dao, et al. 2020). These define the _\ud835\udc5b_ - th\nelement of _\ud835\udc68_ as \u22121/2 + _\ud835\udc5b\ud835\udc56_ and \u2212( _\ud835\udc5b_ + 1) respectively. However, we expect many initializations to work fine, particularly in\nthe large-data and real-valued SSM regimes; some ablations are considered in Section 4.6.\n\n\n**Parameterization of** \u0394 **.** We defined the selective adjustment to \u0394 as _\ud835\udc60_ \u0394 ( _\ud835\udc65_ ) = Broadcast _\ud835\udc37_ (Linear1( _\ud835\udc65_ )), which was\nmotivated by the mechanics of \u0394 (Section 3.5). We observe that it can be generalized from dimension 1 to a larger\ndimension R. We set this to be a small fraction of D, which uses a negligible number of parameters compared to the main\nLinear projections in the block. We additionally note that the broadcasting operation can instead be viewed as another\nLinear projection,", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_9900", "chunk_text": "\ndimension R. We set this to be a small fraction of D, which uses a negligible number of parameters compared to the main\nLinear projections in the block. We additionally note that the broadcasting operation can instead be viewed as another\nLinear projection, initialized to a specific pattern of 1\u2019s and 0\u2019s; if this projection is trainable, this leads to the alternative\n_\ud835\udc60_ \u0394 ( _\ud835\udc65_ ) = Linear _\ud835\udc37_ (Linear _\ud835\udc45_ ( _\ud835\udc65_ )), which can be viewed as a low-rank projection.\n\n\nIn our experiments, the \u0394 parameter (which can be viewed as a bias term) is initialized to _\ud835\udf0f_ \u0394 [\u2212][1][(][Uniform][([][0] _[.]_ [001] _[,]_ [ 0] _[.]_ [1][]))][,]\nfollowing prior work on SSMs (Gu, Johnson, Timalsina, et al. 2023).\n\n\n**Remark 3.1.** _For brevity in our experimental results, we sometimes abbreviate selective SSMs as_ S6 models _, because they are_\n_S4 models with a_ selection _mechanism and computed with a_ scan _._\n\n\n9\n\n\n### **4 Empirical Evaluation**\n\nIn Section 4.1 we test Mamba\u2019s ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate on three\ndomains, each evaluated on autoregressive pretraining as well as downstream tasks.\n\n\n- Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation.\n\n\n- Section 4.3: DNA sequence pretraining, and fine-tuning on a long-sequence classification task.\n\n\n- Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.\n\n\nFinally, Section 4.5 shows Mamba\u2019s computational efficiency at both training and inference time, and Section 4.6 ablates\nvarious components of the architecture and selective SSMs.\n\n\n**4.1** **Synthetic Tasks**\n\n\nFull experiment details for these tasks including task details and training protocol are in Appendix E.1.\n\n\n**4.1.1** **Selective Copying**\n\n\nThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test\nthe memor", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_10350", "chunk_text": " details and training protocol are in Appendix E.1.\n\n\n**4.1.1** **Selective Copying**\n\n\nThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test\nthe memorization abilities of recurrent models. As discussed in Section 3.1, LTI SSMs (linear recurrences and global\nconvolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for example, by\nconstructing a convolution kernel of exactly the right length (Figure 2). This was explicitly validated in earlier work on\nglobal convolutions (Romero et al. 2021). The Selective Copying task prevents this shortcut by randomizing the spacing\nbetween tokens. Note that this task has been introduced before as the Denoising task (Jing et al. 2019).\n\n\nNote that many previous works argue that adding architecture gating (multiplicative interactions) can endow models with\n\u201cdata-dependence\u201d and solve related tasks (Dao, Fu, Saab, et al. 2023; Poli et al. 2023). However, we find this explanation\ninsufficient intuitively because such gating does not interact along the sequence axis, and cannot affect the spacing between\ntokens. In particular architecture gating is not an instance of a selection mechanism (Appendix A).\n\n\nTable 1 confirms that gated architectures such as H3 and Mamba only partially improve performance, while the selection mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more powerful\narchitectures.\n\n\n**4.1.2** **Induction Heads**\n\n\nInduction heads (Olsson et al. 2022) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021) that is\nsurprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative recall and copy:\nfor example, if the model has seen a bigram such as \u201cHarry Potter\u201d in the sequence, then the next time \u201cHarry\u201d appears in\nthe same sequence, the model should be able to predict \u201cPotter\u201d by copying from history.\n\n\n**Dataset.** We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is\ncomparable to prior work on this task", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_10800", "chunk_text": " \u201cPotter\u201d by copying from history.\n\n\n**Dataset.** We train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of 16, which is\ncomparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We additionally investigate\ngeneralization and extrapolation abilities by evaluating on a range of sequence lengths from 2 [6] = 64 up to 2 [20] = 1048576 at\ntest time.\n\n\n**Models.** Following established work on induction heads, we use 2 layer models, which allows attention to mechanistically\nsolve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads, with various positional\nencodings) and SSM variants. We use a model dimension _\ud835\udc37_ - f 64 for Mamba and 128 for the other models.\n\n\n**Results.** Table 2 shows that Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the task perfectly\nbecause of its ability to selectively remember the relevant token while ignoring everything else in between. **It generalizes**\n**perfectly to million-length sequences, or** 4000\u00d7 **longer than it saw during training**, while no other method goes\nbeyond 2\u00d7.\n\n\n10\n\n\nModel Arch. Layer Acc.\n\n\nS4 No gate S4 18.3\n\n  - No gate S6 **97.0**\n\n\nH3 H3 S4 57.0\n\nHyena H3 Hyena 30.1\n\n  - H3 S6 **99.7**\n\n\n  - Mamba S4 56.4\n\n  - Mamba Hyena 28.4\nMamba Mamba S6 **99.8**\n\n\nTable 1: ( **Selective Copying** .)\nAccuracy for combinations of architectures\nand inner sequence layers.\n\n\n\n\n\n\n\n\n\n|Induction Heads Extrapolation|Col2|Col3|nduction He|eads Extrapolation|n|\n|---|---|---|---|---|---|\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_11250", "chunk_text": "2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|nduction He|ads Extrapolation||\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|\n|2<br>3<br>4<br>5<br>0", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_11700", "chunk_text": "HA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|\n|2<br>3<br>4<br>5<br>0<br>2<br>4<br>6<br>8<br>0<br>Induction Heads Extrapolation<br>MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|MHA~~-~~<br>MHA~~-~~<br>MHA~~-~~<br>H3<br>Hyena<br>Mamb<br>Rando<br>Train|2<br>|3|4|5|\n\n\nTable 2: ( **Induction Heads** .) Models are trained on sequence length 2 [8] =\n256, and tested on increasing sequence lengths of 2 [6] = 64 up to 2 [20] =\n1048576. Full numbers in Table 11.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: ( **Scaling Laws** .) Models of size \u2248 125 _\ud835\udc40_ to \u2248 1 _._ 3 _\ud835\udc35_ parameters, trained on the Pile. Mamba scales better than all other\nattention-free models and is the first to match the performance of a very strong \u201cTransformer++\u201d recipe that has now become standard,\nparticularly as the sequence length grows.\n\n\nOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation) is slightly\nbetter than the others; also note that all attention models were only tested up to sequence length 2 [14] = 16384 due to\nmemory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the findings in Poli et al. (2023).\n\n\n**4.2** **Language Modeling**\n\n\nWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on both\npretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to mirror GPT3\nspecifications. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown\net al. (2020). All training details are in Appendix E.2.\n\n\n**", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_12150", "chunk_text": " We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown\net al. (2020). All training details are in Appendix E.2.\n\n\n**4.2.1** **Scaling Laws**\n\n\nFor baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the strongest\nTransformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa architectures (e.g.\nrotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher learning rates). We also\ncompare against other recent subquadratic architectures (Figure 4). All model details are in Appendix E.2.\n\n\nFigure 4 shows scaling laws under the standard Chinchilla (Hoffmann et al. 2022) protocol, on models from \u2248 125 _\ud835\udc40_ to \u2248 1 _._ 3 _\ud835\udc35_\nparameters. **Mamba is the first attention-free model to match the performance of a very strong Transformer**\n**recipe (Transformer++) that has now become standard, particularly as the sequence length grows.** (We note\nthat full results on context length 8k are missing for the RWKV and RetNet baselines, prior strong recurrent models that\ncan also be interpreted as SSMs, because of a lack of efficient implementations leading to out-of-memory or unrealistic\ncomputation requirements.)\n\n\n11\n\n\n**4.2.2** **Downstream Evaluations**\n\n\nTable 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We compare\nagainst the most well-known open source models at these sizes, most importantly Pythia (Biderman et al. 2023) and\nRWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length (300B tokens) as our\nmodels. (Note that Mamba and Pythia are trained with context length 2048, while RWKV was trained with context length\n1024.)\n\n\nTable 3: ( **Zero-shot Evaluations** .) Best results for each size in bold. We compare against open source LMs with various tokenizers,\ntrained for up to 300B tokens. Pile refers to the validation split,", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_12600", "chunk_text": "Table 3: ( **Zero-shot Evaluations** .) Best results for each size in bold. We compare against open source LMs with various tokenizers,\ntrained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and tokenizer\n(GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches baselines at twice\nthe model size.\n\n\nModel Token. Pile LAMBADA LAMBADA HellaSwag PIQA Arc-E Arc-C WinoGrande Average\nppl \u2193 ppl \u2193 acc \u2191 acc \u2191 acc \u2191 acc \u2191 acc \u2191 acc \u2191 acc \u2191\n\n\nHybrid H3-130M GPT2 - 89.48 25.77 31.7 64.2 44.4 24.2 50.6 40.1\nPythia-160M NeoX 29.64 38.10 33.0 30.2 61.4 43.2 24.1 **51.9** 40.6\n**Mamba-130M** NeoX **10.56** **16.07** **44.3** **35.3** **64.5** **48.0** **24.3** **51.9** **44.7**\n\n\nHybrid H3-360M GPT2 - 12.58 48.0 41.5 68.1 51.4 24.7 54.1 48.0\nPythia-410M NeoX 9.95 10.84 51.4 40.6 66.9 52.1 24.6 53.8 48.2\n**Mamba-370M** NeoX **8.28** **8.14** **55.6** **46.5** **69.5** **55.1** **28.0** **55.3** **50.0**\n\n\nPythia-1B NeoX 7.82 7.92 56.1 47.2 70.7 57.0 27.1 53.5 51.9\n**Mamba-790M** NeoX **7.33** **6.02**", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_13050", "chunk_text": ".92 56.1 47.2 70.7 57.0 27.1 53.5 51.9\n**Mamba-790M** NeoX **7.33** **6.02** **62.7** **55.1** **72.1** **61.2** **29.5** **56.1** **57.1**\n\n\nGPT-Neo 1.3B GPT2 - 7.50 57.2 48.9 71.1 56.2 25.9 54.9 52.4\n\nHybrid H3-1.3B GPT2 - 11.25 49.6 52.6 71.3 59.2 28.1 56.9 53.0\nOPT-1.3B OPT - 6.64 58.0 53.7 72.4 56.7 29.6 59.5 55.0\n\nPythia-1.4B NeoX 7.51 6.08 61.7 52.1 71.0 60.5 28.5 57.2 55.2\n\nRWKV-1.5B NeoX 7.70 7.04 56.4 52.5 72.4 60.5 29.4 54.6 54.3\n\n**Mamba-1.4B** NeoX **6.80** **5.04** **64.9** **59.1** **74.2** **65.5** **32.8** **61.5** **59.7**\n\n\nGPT-Neo 2.7B GPT2 - 5.63 62.2 55.8 72.1 61.1 30.2 57.6 56.5\n\nHybrid H3-2.7B GPT2 - 7.92 55.7 59.7 73.3 65.6 32.3 61.4 58.0\nOPT-2.7B OPT - 5.12 63.6 60.6 74.8 60.8 31.3 61.0 58.", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_13500", "chunk_text": ".3 61.4 58.0\nOPT-2.7B OPT - 5.12 63.6 60.6 74.8 60.8 31.3 61.0 58.7\n\nPythia-2.8B NeoX 6.73 5.04 64.7 59.3 74.0 64.1 32.9 59.7 59.1\n\nRWKV-3B NeoX 7.00 5.24 63.9 59.6 73.7 67.8 33.1 59.6 59.6\n\n**Mamba-2.8B** NeoX **6.22** **4.23** **69.2** **66.1** **75.2** **69.7** **36.3** **63.5** **63.3**\n\n\nGPT-J-6B GPT2 - 4.10 68.3 66.3 75.4 67.0 36.6 64.1 63.0\n\nOPT-6.7B OPT - 4.25 67.7 67.2 76.3 65.6 34.9 65.5 62.9\n\nPythia-6.9B NeoX 6.51 4.45 67.1 64.0 75.2 67.3 35.5 61.3 61.7\n\nRWKV-7.4B NeoX 6.31 4.38 67.2 65.5 76.1 67.8 37.5 61.0 62.5\n\n\n**4.3** **DNA Modeling**\n\n\nMotivated by the success of large language models, there has been recent exploration into using the foundation model\nparadigm for genomics. DNA has been likened to language in that it consists of sequences of discrete tokens with a finite\nvocabulary. It is also known for requiring long-range dependencies to model (Avsec et al. 2021). We investigate Mamba as\na FM backbone for pretraining and fine-tuning in the same setting as recent works on long-sequence models for DNA\n(Nguyen, Poli, et al.", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_13950", "chunk_text": "Avsec et al. 2021). We investigate Mamba as\na FM backbone for pretraining and fine-tuning in the same setting as recent works on long-sequence models for DNA\n(Nguyen, Poli, et al. 2023). In particular, we focus on two explorations of scaling laws across model size and sequence\nlength (Figure 5), and a difficult downstream synthetic classification task requiring long context (Figure 6).\n\n\nFor pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training and\nmodel details (see also Appendix E.2). For the dataset, we largely follow the setup of HyenaDNA (Nguyen, Poli, et al. 2023),\nwhich uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5 billion tokens (DNA base\npairs) in the training split.\n\n\n12\n\n\nFigure 5: ( **DNA Scaling Laws** .) Pretraining on the HG38 (human genome) dataset. ( _Left_ ) Fixing short context length 2 [10] = 1024 and\nincreasing size from \u2248 200 _\ud835\udc3e_ to \u2248 40 _\ud835\udc40_ parameters, Mamba scales better than baselines. ( _Right_ ) Fixing model size and increasing sequence\nlengths while keeping tokens/batch and total training tokens fixed. Unlike baselines, the selection mechanism of Mamba facilitates\nbetter performance with increasing context length.\n\n\n**4.3.1** **Scaling: Model Size**\n\n\nIn this experiment, we investigate the scaling properties of genomics foundation models with various model backbones\n(Figure 5 _Left_ ).\n\n\n**Training.** To advantage the baselines, we train on a short sequence length of 1024; as shown in Section 4.3.2, we expect\nresults to favor Mamba even more at longer sequence lengths. We fix a global batch size of 1024, for a total of 2 [20] \u2248 1 _\ud835\udc40_\ntokens per batch. Models were trained for 10 _\ud835\udc3e_ gradient steps for a total of 10 _\ud835\udc35_ tokens.\n\n\n**Results.** Figure 5 ( _Left_ ) shows that Mamba\u2019s pretraining perplexity improves smoothly with model size, and that Mamba\nscales better than both Hy", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_14400", "chunk_text": " a total of 10 _\ud835\udc35_ tokens.\n\n\n**Results.** Figure 5 ( _Left_ ) shows that Mamba\u2019s pretraining perplexity improves smoothly with model size, and that Mamba\nscales better than both HyenaDNA and Transformer++. For example, at the largest model size of \u2248 40 _\ud835\udc40_ parameters, the\ncurve shows that **Mamba can match the Transformer++ and HyenaDNA models with roughly** 3\u00d7 **to** 4\u00d7 **fewer**\n**parameters** .\n\n\n**4.3.2** **Scaling: Context Length**\n\n\nIn the next DNA experiment, we investigate the scaling properties of models with respect to sequence length. We only\ncompare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at longer sequence\nlengths. We pretrain models on sequence lengths 2 [10] = 1024, 2 [12] = 4096, 2 [14] = 16384, 2 [16] = 65536, 2 [18] = 262144,\n2 [20] = 1048576. We fix a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models were trained for 20 _\ud835\udc3e_\ngradient steps for a total of \u2248 330 _\ud835\udc35_ tokens. The longer sequence lengths used sequence length warmup similar to (Nguyen,\nPoli, et al. 2023).\n\n\n**Results.** Figure 5 ( _Right_ ) shows that **Mamba is able to make use of longer context even up to extremely long**\n**sequences of length 1M**, and its pretraining perplexity improves as the context increases. On the other hand, the\nHyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on properties of the\nselection mechanism. In particular, LTI models cannot selectively ignore information; from a convolutional perspective, a\nvery long convolution kernel is aggregating all information across a long sequence which may be very noisy. Note that\nwhile HyenaDNA claims to improve with longer context, their results do not control for computation time.\n\n\n**4.3.3** **Synthetic Species Classification**\n\n\nWe evaluate models on a downstream task of classifying between 5 different species by", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_14850", "chunk_text": "while HyenaDNA claims to improve with longer context, their results do not control for computation time.\n\n\n**4.3.3** **Synthetic Species Classification**\n\n\nWe evaluate models on a downstream task of classifying between 5 different species by randomly sampling a contiguous\nsegment of their DNA. This task is adapted from HyenaDNA, which used the species {human _,_ lemur _,_ mouse _,_ pig _,_ hippo}.\nWe modify the task to be significantly more challenging by classifying between the five _great apes_ species\n{human _,_ chimpanzee _,_ gorilla _,_ - rangutan _,_ bonobo}, which are known to share 99% of their DNA.\n\n\n13\n\n\nFigure 6: ( **Great Apes DNA Classification** .) Accuracy after finetuning on sequences of length 2 [10] = 1024 up to 2 [20] = 1048576 using\npretrained models of the same context length. Numerical results in\nTable 13.\n\n\n**4.4** **Audio Modeling and Generation**\n\n\n\nFigure 7: ( **Audio Pretraining** .) Mamba improves performance\n\n- ver prior state-of-the-art (Sashimi) in autoregressive audio modeling, while improving up to minute-long context or million-length\nsequences (controlling for computation).\n\n\n\nFor the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel et al.\n2022). This model comprises:\n\n\n1. a U-Net backbone with two stages of pooling by a factor _\ud835\udc5d_ that doubles the model dimension _\ud835\udc37_ per stage,\n\n\n2. alternating S4 and MLP blocks in each stage.\n\n\nWe consider replacing the S4+MLP blocks with Mamba blocks. Experiment details are in Appendix E.4.\n\n\n**4.4.1** **Long-Context Autoregressive Pretraining**\n\n\nWe evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a standard\npiano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of 16000 Hz. Pretraining\ndetails largely follow the standard language modeling setup (Section 4.2). Figure 7 evaluates the effect of increasing training\nsequence lengths from 2 [13] = 8192 to 2 [20] \u2248 10", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_15300", "chunk_text": " Pretraining\ndetails largely follow the standard language modeling setup (Section 4.2). Figure 7 evaluates the effect of increasing training\nsequence lengths from 2 [13] = 8192 to 2 [20] \u2248 10 [6], while keeping computation fixed. (There are some slight edge cases to the\nway the data is curated, which may lead to kinks in the scaling curves. For example, only minute-long clips were available\nso the maximum sequence length is actually bounded by 60 _\ud835\udc60_ - 16000 _\ud835\udc3b\ud835\udc67_ = 960000.)\n\n\n**Both Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is**\n**better throughout, and the gap widens at longer lengths.** The main metric is bits per byte (BPB), which is a constant\nfactor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.\n\n\nWe note one important detail: this is the only experiment in this paper in which we switched from the real parameterization\nto complex (Section 3.6). We show additional ablations in Appendix E.4.\n\n\n**4.4.2** **Autoregressive Speech Generation**\n\n\nSC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting of\n1-second clips sampled at 16000 Hz of the digits \u201czero\u201d through \u201cnine\u201d with highly variable characteristics. We largely\nfollow the autoregressive training setup and generation protocol of Goel et al. (2022).\n\n\nTable 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al. (2022):\nWaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette 2019), DiffWave (Z.\nKong et al. 2021), and SaShiMi. **A small Mamba model outperforms the state-of-the-art (and much larger) GAN-**\n**and diffusion- based models.** A larger model parameter-matched to the baselines further improves on fidelity metrics\ndramatically.\n\n\nTable", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_15750", "chunk_text": " small Mamba model outperforms the state-of-the-art (and much larger) GAN-**\n**and diffusion- based models.** A larger model parameter-matched to the baselines further improves on fidelity metrics\ndramatically.\n\n\nTable 5 takes the small Mamba model and investigates combinations of different architectures for the outer stages and\ncenter stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba _>_ S4+MLP _>_\nMHA+MLP in the center blocks.\n\n\n14\n\n\nTable 4: ( **SC09** ) Automated metrics for unconditional generation on\na challenging dataset of fixed-length speech clips. ( _Top to Bottom_ )\nAutoregressive baselines, non-autoregressive baselines, Mamba, and\ndataset metrics.\n\n\nModel Params NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nSampleRNN 35.0M 2.042 8.96 1.71 3.02 1.76\nWaveNet 4.2M 1.925 5.08 2.27 5.80 1.47\n\nSaShiMi 5.8M 1.873 1.99 5.13 42.57 0.74\n\n\nWaveGAN 19.1M   - 2.03 4.90 36.10 0.80\n\nDiffWave 24.1M   - 1.92 5.26 51.21 0.68\n\n+ SaShiMi 23.0M     - 1.42 5.94 69.17 0.59\n\n\n**Mamba** 6.1M **1.852** 0.94 6.26 88.54 0.52\n\n**Mamba** 24.3M 1.860 **0.67** **7.33** **144.9** **0.36**\n\n\nTrain    -    - 0 _._ 00 8 _._ 56 292 _._ 5 0 _._ 16\n\nTest    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_16200", "chunk_text": "Test    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC09 Model Ablations** ) Models with 6M parameters. In\nSaShiMi\u2019s U-Net backbone, there are 8 center blocks operating on\nsequence length 1000, sandwiched on each side by 8 outer blocks on\nsequence length 4000, sandwiched by 8 outer blocks on sequence\nlength 16000 (40 blocks total). The architecture of the 8 center\nblocks are ablated independently of the rest. Note that Transformers\n(MHA+MLP) were not tested in the more important outer blocks\nbecause of efficiency constraints.\n\n\nOuter Center NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\n\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\n\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\n\nMamba MHA+MLP **1.850** 1.37 5.63 58.23 0.62\n\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\n\nMamba Mamba 1.852 **0.94** **6.26** **88.54** **0.52**\n\n\n\nWe benchmark the speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of\n(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6.", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_16650", "chunk_text": " standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained) would have higher inference\nthroughput than a 5\u00d7 smaller Transformer-1.3B. Details in Appendix E.5, which additionally includes a benchmark of\nmemory consumption.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: ( **Efficiency Benchmarks** .) ( _Left_ ) Training: our efficient scan is 40\u00d7 faster than a standard implementation. ( _Right_ ) Inference:\nas a recurrent model, Mamba can achieve 5\u00d7 higher throughput than Transformers.\n\n\n**4.6** **Model Ablations**\n\n\nWe perform a series of detailed ablations on components of our model, focusing on the setting of language modeling with\nsize \u2248 350M models at Chinchilla token counts (same setting as Figure 4).\n\n\n**4.6.1** **Architecture**\n\n\nTable 6 investigates the effects of the architecture (block) and its inner SSM layer (Figure 3). We find that\n\n\n- Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very similar.\n\n\nReplacing the complex-valued S4 variant from previous work with a real-valued one does not affect performance much,\nsuggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware efficiency.\n\n\nReplacing any of these with a selective SSM (S6) significantly improves performance, validating the motivation of\nSection 3.\n\n\n15\n\n\nTable 6: ( **Ablations: Architecture and SSM layer** .) The Mamba block performs similarly to H3 while being simpler. In the inner layer,\nthere is little difference among different parameterizations of LTI models, while selective SSMs (S6) provide a large improvement. More\nspecifically, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.\n\n\n\nModel Arch. SSM Layer Perplexity\n\n\nHyena H3 Hyena 10 _._ 24\nH3 H3 S4 (complex) 10 _._ 30\n\n      - H3 S4 (real) 10 _._", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_17100", "chunk_text": ". SSM Layer Perplexity\n\n\nHyena H3 Hyena 10 _._ 24\nH3 H3 S4 (complex) 10 _._ 30\n\n      - H3 S4 (real) 10 _._ 34\n\n      - H3 S6 8 _._ 95\n\n\nTable 7: ( **Ablations: Selective parameters** .) \u0394 is the most important parameter (Theorem 1), but using multiple selective parameters\ntogether synergizes.\n\n\nSelective \u0394 Selective _\ud835\udc69_ Selective _\ud835\udc6a_ Perplexity\n\n\n\u2717 \u2717 \u2717 10.93\n\n\u2717 \u2713 \u2717 10.15\n\n\u2717 \u2717 \u2713 9.98\n\n\u2713 \u2717 \u2717 9.81\n\n\u2713 \u2713 \u2713 8.71\n\n\n\nModel Arch. SSM Layer Perplexity\n\n\n- Mamba Hyena 10 _._ 75\n\n- Mamba S4 (complex) 10 _._ 54\n\n- Mamba S4 (real) 10 _._ 56\nMamba Mamba S6 8 _._ 69\n\n\nTable 8: ( **Ablations: Parameterization of** _\ud835\udc68_ .) The more\nstandard initializations based on S4D-Lin (Gu, Gupta, et al.\n2022) perform worse than S4D-Real or a random initialization,\nwhen the SSM is selective.\n\n\n_\ud835\udc68\ud835\udc5b_ Initialization Field Perplexity\n\n\n_\ud835\udc68\ud835\udc5b_ = \u2212 [1] 2 [+] _[ \ud835\udc5b\ud835\udc56]_ Complex 9.16\n\n_\ud835\udc68\ud835\udc5b_ = \u22121/2 Real 8.85\n_\ud835\udc68\ud835\udc5b_ = \u2212( _\ud835\udc5b_ + 1) Real 8.71\n_\ud835\udc68\ud835\udc5b_ \u223c exp(N (0 _,_ 1)) Real 8.71\n\n\n\n\nThe Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a selective\nlayer).\n\n\nWe also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybrid\nattention architecture) in Appendix E.2.2.\n\n\n**4.6.2** **Selective SSM**\n\n\nTable 7 ab", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_17550", "chunk_text": "aving the Mamba block with other blocks such as MLP (a traditional architecture) MHA (a hybrid\nattention architecture) in Appendix E.2.2.\n\n\n**4.6.2** **Selective SSM**\n\n\nTable 7 ablates the selective SSM layer by considering different combinations of selective \u0394, _\ud835\udc69_, and _\ud835\udc6a_ parameters (Algorithm 2), showing that \u0394 is the most important parameter due to its connection to RNN gating (Theorem 1).\n\n\nTable 8 considers different initializations of the SSM, which have been shown to make a large difference in some data\nmodalities and settings (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022). On language modeling, we find that simpler\nreal-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued parameterizations (S4D-Lin,\nrow 1) perform better. Random initializations also work well, consistent with findings from prior work (Mehta et al.\n2023).\n\n\nTable 9 and Table 10 consider varying the dimension of the \u0394 and ( _\ud835\udc69, \ud835\udc6a_ ) projections respectively. Changing them from\nstatic to selective provides the most benefit, while increasing the dimensions further generally improves performance\nmodestly with a small increase in parameter count.\n\n\nOf particular note is the dramatic improvement of the selective SSM when the state size _\ud835\udc41_ is increased, with over a 1.0\nperplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in Sections 3.1\nand 3.3.\n\n### **5 Discussion**\n\n\nWe discuss related work, limitations, and some future directions.\n\n\n**Related Work.** Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an\nextended related work of SSMs and other related models.\n\n\n16\n\n\nTable 9: ( **Ablations: Expressivity of** \u0394.)\nThe selection mechanism of \u0394 constructs it\n\nwith a projection of the input. Projecting it\neven to dim. 1 provides a large increase in\nperformance; increasing it further provides\nfurther improvements at the cost of a modest increase in parameters. State size fixed\n\nto _\ud835\udc41_ = 16.\n\n\n\nTable 10: ( **Ablations: SSM state", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_18000", "chunk_text": " increase in\nperformance; increasing it further provides\nfurther improvements at the cost of a modest increase in parameters. State size fixed\n\nto _\ud835\udc41_ = 16.\n\n\n\nTable 10: ( **Ablations: SSM state dimension** .) ( _Top_ ) Constant _\ud835\udc69_ and _\ud835\udc6a_ ( _Bottom_ ) Selective\n_\ud835\udc69_ and _\ud835\udc6a_ . Increasing the SSM state dimension _\ud835\udc41_, which can be viewed as an expansion\nfactor on the dimension of the recurrent state, can significantly improve performance for\na negligible cost in parameters/FLOPs, but only when _\ud835\udc69_ and _\ud835\udc6a_ are also selective. Size of\n\u0394 projection fixed to 64.\n\n\nState dimension _\ud835\udc41_ Params (M) Perplexity\n\n\n\nSize of \u0394 proj. Params (M) Perplexity\n\n\n- 358.9 9.12\n\n1 359.1 8.97\n\n2 359.3 8.97\n\n4 359.7 8.91\n\n8 360.5 8.83\n\n16 362.1 8.84\n\n32 365.2 8.80\n\n64 371.5 8.71\n\n\n\n1 367.1 9.88\n\n2 367.4 9.86\n\n4 368.0 9.82\n\n8 369.1 9.82\n\n16 371.5 9.81\n\n\n1 367.1 9.73\n\n2 367.4 9.40\n\n4 368.0 9.09\n\n8 369.1 8.84\n\n16 371.5 8.71\n\n\n\n**No Free Lunch: Continuous-Discrete Spectrum.** Structured SSMs were originally defined as discretizations of\ncontinuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptual\nsignals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on\ndiscrete modalities such as text and DNA; but this conversely can impede their performance on data that LTI SSMs excel\n\n- n. Our ablations on audio waveforms examine this tradeoff in more detail.\n\n\n**", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_18450", "chunk_text": " on\ndiscrete modalities such as text and DNA; but this conversely can impede their performance on data that LTI SSMs excel\n\n- n. Our ablations on audio waveforms examine this tradeoff in more detail.\n\n\n**Downstream Affordances.** Transformer-based foundation models (particularly LLMs) have a rich ecosystem of properties and modes of interaction with pretrained models, such as fine-tuning, adaptation, prompting, in-context learning,\ninstruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer alternatives such\nas SSMs have similar properties and affordances.\n\n\n**Scaling.** Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs\n(e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sun\net al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba still\ncompares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and\nadjustments to the model that are not discussed in this paper.\n\n### **6 Conclusion**\n\n\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent\nreasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba\nachieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong\nTransformer models. We are excited about the broad applications of selective state space models to build foundation models\nfor different domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Our\nresults suggest that Mamba is a strong candidate to be a general sequence model backbone.\n\n\n**Acknowledgments**\n\n\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\n\n### **References**\n\n\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. \u201cUnitary Evolution Recurrent Neural Networks\u201d. In: _The Interna-_\n_tional Conference on Machine Learning (ICML)_ . 2016, pp. 1120\u20131128.\n\n\n17\n\n\n[2] \u017diga Avsec", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_18900", "chunk_text": "Unitary Evolution Recurrent Neural Networks\u201d. In: _The Interna-_\n_tional Conference on Machine Learning (ICML)_ . 2016, pp. 1120\u20131128.\n\n\n17\n\n\n[2] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,\nYannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. \u201cEffective Gene Expression Prediction from\nSequence by Integrating Long-range Interactions\u201d. In: _Nature Methods_ 18.10 (2021), pp. 1196\u20131203.\n\n[3] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. \u201cUsing Fast Weights to Attend\nto the Recent Past\u201d. In: _Advances in Neural Information Processing Systems (NeurIPS)_ 29 (2016).\n\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \u201cLayer Normalization\u201d. In: _arXiv preprint arXiv:1607.06450_\n(2016).\n\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \u201cNeural Machine Translation by Jointly Learning to\nAlign and Translate\u201d. In: _The International Conference on Learning Representations (ICLR)_ . 2015.\n\n[6] David Balduzzi and Muhammad Ghifary. \u201cStrongly-typed Recurrent Neural Networks\u201d. In: _International Conference_\n\n_on Machine Learning_ . PMLR. 2016, pp. 1292\u20131300.\n\n[7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. \u201cPythia: A Suite for Analyzing\nLarge Language Models across Training and Scaling\u201d. In: _The International Conference on Machine Learning (ICML)_ .\nPMLR. 2023, pp. 2397\u20132430.\n\n[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. \u201cPIQA: Reason", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_19800", "chunk_text": ".10509_ (2019).\n\n[15] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. \u201cRethinking Attention with Performers\u201d. In: _The_\n_International Conference on Learning Representations (ICLR)_ . 2021.\n\n[16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. \u201cPaLM: Scaling Language Modeling with Pathways\u201d.\nIn: _Journal of Machine Learning Research_ [24.240 (2023), pp. 1\u2013113. url: http://jmlr.org/papers/v24/22-](http://jmlr.org/papers/v24/22-1144.html)\n[1144.html.](http://jmlr.org/papers/v24/22-1144.html)\n\n[17] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. \u201cEmpirical Evaluation of Gated Recurrent\nNeural Networks on Sequence Modeling\u201d. In: _arXiv preprint arXiv:1412.3555_ (2014).\n\n[18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\n\u201cThink you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\u201d. In: _arXiv preprint arXiv:1803.05457_\n(2018).\n\n[19] Tri Dao. \u201cFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\u201d. In: _The International_\n_Conference on Learning Representations (ICLR)_ . 2024.\n\n[20] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. \u201cFlashAttention: Fast and Memory-Efficient\nExact Attention with IO-Awareness\u201d. In: _Advances in Neural Information Processing Systems (", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_20700", "chunk_text": " on Learning Representations (ICLR)_ .\n\n2020.\n\n[27] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao\nBai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,\nAndy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. \u201cA Mathematical Framework for Transformer Circuits\u201d. In: _Transformer Circuits_\n_Thread_ (2021). https://transformer-circuits.pub/2021/framework/index.html.\n\n[28] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. \u201cBlock-State\nTransformer\u201d. In: _arXiv preprint arXiv:2306.09539_ (2023).\n\n[29] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu,\nYangyang Shi, Ozlem Kalinli, Mike Seltzer, and Mark J. F. Gales. \u201cMulti-Head State Space Model for Speech\nRecognition\u201d. In: _Proc. INTERSPEECH 2023_ [. 2023, pp. 241\u2013245. doi: 10.21437/Interspeech.2023-1036.](https://doi.org/10.21437/Interspeech.2023-1036)\n\n[30] Karl J Friston, Lee Harrison, and Will Penny. \u201cDynamic Causal Modelling\u201d. In: _Neuroimage_ 19.4 (2003), pp. 1273\u2013\n\n1302.\n\n[31] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher\nR\u00e9. \u201cSimple Hardware-efficient Long Convolutions for Sequence Modeling\u201d. In: _The International Conference on_\n_Machine Learning (ICML)_ (2023).\n\n[32] Ken-ichi Funahashi and Yuichi Nakamura. \u201cApproximation of Dynamical Systems by Continuous Time Rec", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_21150", "chunk_text": "utions for Sequence Modeling\u201d. In: _The International Conference on_\n_Machine Learning (ICML)_ (2023).\n\n[32] Ken-ichi Funahashi and Yuichi Nakamura. \u201cApproximation of Dynamical Systems by Continuous Time Recurrent\nNeural Networks\u201d. In: _Neural Networks_ 6.6 (1993), pp. 801\u2013806.\n\n[33] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,\nAnish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. \u201cThe Pile: An 800GB Dataset of Diverse Text for\nLanguage Modeling\u201d. In: _arXiv preprint arXiv:2101.00027_ (2020).\n\n[34] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey\nHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin\nWang, and Andy Zou. _A Framework for Few-shot Language Model Evaluation_ . Version v0.0.1. Sept. 2021. doi:\n[10.5281/zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628.](https://doi.org/10.5281/zenodo.5371628)\n\n[35] Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. \u201cIt\u2019s Raw! Audio Generation with State-Space Models\u201d.\nIn: _The International Conference on Machine Learning (ICML)_ . 2022.\n\n[36] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. \u201cHIPPO: Recurrent Memory with Optimal\nPolynomial Projections\u201d. In: _Advances in Neural Information Processing Systems (NeurIPS)_ . 2020.\n\n[37] Albert Gu, Karan Goel, and Christopher R\u00e9. \u201cEfficiently Modeling Long Sequences with Structured State Spaces\u201d.\nIn: _The International Conference on Learning Representations (ICLR)_ . 2022.\n\n[38] Albert Gu, Caglar Gulcehre", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_21600", "chunk_text": " Christopher R\u00e9. \u201cEfficiently Modeling Long Sequences with Structured State Spaces\u201d.\nIn: _The International Conference on Learning Representations (ICLR)_ . 2022.\n\n[38] Albert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hoffman, and Razvan Pascanu. \u201cImproving the Gating Mechanism\n\n   - f Recurrent Neural Networks\u201d. In: _The International Conference on Machine Learning (ICML)_ . 2020.\n\n[39] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. \u201cOn the Parameterization and Initialization of Diagonal\nState Space Models\u201d. In: _Advances in Neural Information Processing Systems (NeurIPS)_ . 2022.\n\n[40] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. \u201cCombining Recurrent,\nConvolutional, and Continuous-time Models with the Linear State Space Layer\u201d. In: _Advances in Neural Information_\n_Processing Systems (NeurIPS)_ . 2021.\n\n[41] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. \u201cHow to Train Your HIPPO: State Space\nModels with Generalized Basis Projections\u201d. In: _The International Conference on Learning Representations (ICLR)_ .\n\n2023.\n\n[42] Ankit Gupta, Albert Gu, and Jonathan Berant. \u201cDiagonal State Spaces are as Effective as Structured State Spaces\u201d.\nIn: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 22982\u201322994.\n\n[43] Ankit Gupta, Harsh Mehta, and Jonathan Berant. \u201cSimplifying and Understanding State Space Models with Diagonal\nLinear RNNs\u201d. In: _arXiv preprint arXiv:2212.00768_ (2022).\n\n[44] David Ha, Andrew Dai, and Quoc V. Le. \u201cHyperNetworks\u201d. In: _The International Conference on Learning Representa-_\n_tions (ICLR)_ . 2017.\n\n[45] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. \u201cDream to Control: Learning Behaviors by\nLatent Imagination\u201d. In: _The International Conference on Learning Representations (IC", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_22050", "chunk_text": "45] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. \u201cDream to Control: Learning Behaviors by\nLatent Imagination\u201d. In: _The International Conference on Learning Representations (ICLR)_ . 2020.\n\n\n19\n\n\n[46] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. \u201cLiquid\nStructural State-Space Models\u201d. In: _The International Conference on Learning Representations (ICLR)_ . 2023.\n\n[47] Mikael Henaff, Arthur Szlam, and Yann LeCun. \u201cRecurrent Orthogonal Networks and Long-Memory Tasks\u201d. In:\n_The International Conference on Machine Learning (ICML)_ . 2016.\n\n[48] Dan Hendrycks and Kevin Gimpel. \u201cGaussian Error Linear Units (GELUs)\u201d. In: _arXiv preprint arXiv:1606.08415_\n(2016).\n\n[49] Sepp Hochreiter. \u201cUntersuchungen zu dynamischen neuronalen Netzen\u201d. In: _Diploma, Technische Universit\u00e4t_\n_M\u00fcnchen_ 91.1 (1991), p. 31.\n\n[50] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J\u00fcrgen Schmidhuber, et al. _Gradient Flow in Recurrent Nets: The_\n_Difficulty of Learning Long-term Dependencies_ . 2001.\n\n[51] Sepp Hochreiter and J\u00fcrgen Schmidhuber. \u201cLong Short-Term Memory\u201d. In: _Neural Computation_ 9.8 (1997), pp. 1735\u2013\n\n1780.\n\n[52] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. \u201cAn Empirical Analysis of ComputeOptimal Large Language Model Training\u201d. In: _Advances in Neural Information Processing Systems (NeurIPS)_ 35 (2022),\npp. 30016\u201330030.\n\n[53] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. \u201cTransformer Quality in", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_22500", "chunk_text": " Processing Systems (NeurIPS)_ 35 (2022),\npp. 30016\u201330030.\n\n[53] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. \u201cTransformer Quality in Linear Time\u201d. In: _The International_\n_Conference on Machine Learning (ICML)_ . PMLR. 2022, pp. 9099\u20139117.\n\n[54] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. \u201cDeep\nLearning for Time Series Classification: A Review\u201d. In: _Data Mining and Knowledge Discovery_ 33.4 (2019), pp. 917\u2013\n\n963.\n\n[55] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. \u201cData Movement is All You Need: A\nCase Study on Optimizing Transformers\u201d. In: _Proceedings of Machine Learning and Systems_ 3 (2021), pp. 711\u2013732.\n\n[56] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. \u201cGated\nOrthogonal Recurrent Units: On Learning to Forget\u201d. In: _Neural Computation_ 31.4 (2019), pp. 765\u2013783.\n\n[57] Rudolph Emil Kalman. \u201cA New Approach to Linear Filtering and Prediction Problems\u201d. In: (1960).\n\n[58] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. \u201cTransformers are RNNs: Fast\nAutoregressive Transformers with Linear Attention\u201d. In: _International Conference on Machine Learning_ . PMLR. 2020,\npp. 5156\u20135165.\n\n[59] Shiva Kaul. \u201cLinear Dynamical Systems as a Core Computational Primitive\u201d. In: _Advances in Neural Information_\n_Processing Systems_ 33 (2020), pp. 16808\u201316820.\n\n[60] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. \u201cDiffWave: A Versatile Diffusion Model\nfor Audio Synthesis\u201d. In: _International Conference on", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_22950", "chunk_text": ".\n\n[60] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. \u201cDiffWave: A Versatile Diffusion Model\nfor Audio Synthesis\u201d. In: _International Conference on Learning Representations_ . 2021.\n\n[61] Chrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. \u201cTime-Parameterized Convolutional Neural\nNetworks for Irregularly Sampled Time Series\u201d. In: _arXiv preprint arXiv:2308.03210_ (2023).\n\n[62] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. \u201cImageNet Classification with Deep Convolutional Neural\nNetworks\u201d. In: _Advances in Neural Information Processing Systems (NeurIPS)_ 25 (2012).\n\n[63] Tao Lei. \u201cWhen Attention Meets Fast Recurrence: Training Language Models with Reduced Compute\u201d. In: _Proceedings_\n\n_of the 2021 Conference on Empirical Methods in Natural Language Processing_ . 2021, pp. 7633\u20137648.\n\n[64] Tao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. \u201cSimple Recurrent Units for Highly Parallelizable\nRecurrence\u201d. In: _arXiv preprint arXiv:1709.02755_ (2017).\n\n[65] Mario Lezcano-Casado and David Mart\u00ednez-Rubio. \u201cCheap Orthogonal Constraints in Neural Networks: A Simple\nParametrization of the Orthogonal and Unitary Group\u201d. In: _The International Conference on Machine Learning_\n_(ICML)_ . 2019.\n\n[66] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. \u201cWhat Makes Convolutional Models Great\n\n   - n Long Sequence Modeling?\u201d In: _The International Conference on Learning Representations (ICLR)_ . 2023.\n\n[67] Vasileios Lioutas and Yuhong Guo. \u201cTime-aware Large Kernel Convolutions\u201d. In: _The International Conference on_\n_Machine Learning (ICML)_ . PMLR. 2020, pp. 6172\u20136183.\n\n[68] Chris Lu, Yannick Schroecker, Albert Gu, Em", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_24300", "chunk_text": " preprint_\n_arXiv:1609.03499_ (2016).\n\n[79] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.\n\u201cResurrecting Recurrent Neural Networks for Long Sequences\u201d. In: _The International Conference on Machine Learning_\n_(ICML)_ . 2023.\n\n[80] Denis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle,\nMarco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. \u201cThe LAMBADA Dataset: Word Prediction Requiring a Broad\nDiscourse Context\u201d. In: _Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics_ . 2016,\npp. 1525\u20131534.\n\n[81] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. \u201cOn the Difficulty of Training Recurrent Neural Networks\u201d.\nIn: _International Conference on Machine Learning_ . 2013, pp. 1310\u20131318.\n\n[82] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung,\nMatteo Grella, Kranthi Kiran GV, et al. \u201cRWKV: Reinventing RNNs for the Transformer Era\u201d. In: _arXiv preprint_\n_arXiv:2305.13048_ (2023).\n\n[83] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. \u201cRandom Feature\nAttention\u201d. In: _The International Conference on Learning Representations (ICLR)_ . 2021.\n\n[84] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon,\nand Christopher R\u00e9. \u201cHyena Hierarchy: Towards Larger Convolutional Language Models\u201d. In: _The International_\n_Conference on Machine Learning (ICML)_ . 2023.\n\n[85] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_24750", "chunk_text": "al Language Models\u201d. In: _The International_\n_Conference on Machine Learning (ICML)_ . 2023.\n\n[85] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and\nYiran Zhong. \u201cToeplitz Neural Network for Sequence Modeling\u201d. In: _The International Conference on Learning_\n_Representations (ICLR)_ . 2023.\n\n[86] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. \u201cThe devil in\nlinear transformer\u201d. In: _arXiv preprint arXiv:2210.10340_ (2022).\n\n[87] Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran\nZhong. \u201cCosFormer: Rethinking Softmax in Attention\u201d. In: _The International Conference on Learning Representations_\n_(ICLR)_ . 2022.\n\n[88] Ali Rahimi and Benjamin Recht. \u201cRandom Features for Large-Scale Kernel Machines\u201d. In: _Advances in Neural_\n_Information Processing Systems (NeurIPS)_ 20 (2007).\n\n\n21\n\n\n[89] Prajit Ramachandran, Barret Zoph, and Quoc V Le. \u201cSwish: A Self-gated Activation Function\u201d. In: _arXiv preprint_\n_arXiv:1710.05941_ 7.1 (2017), p. 5.\n\n[90] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. \u201cCKConv: Continuous\nKernel Convolution For Sequential Data\u201d. In: _arXiv preprint arXiv:2102.02611_ (2021).\n\n[91] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. \u201cWinogrande: An Adversarial Winograd\nSchema Challenge at Scale\u201d. In: _Communications of the ACM_ 64.9 (2021), pp. 99\u2013106.\n\n[92] George Saon, Ankit Gupta, and", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_25200", "chunk_text": "e: An Adversarial Winograd\nSchema Challenge at Scale\u201d. In: _Communications of the ACM_ 64.9 (2021), pp. 99\u2013106.\n\n[92] George Saon, Ankit Gupta, and Xiaodong Cui. \u201cDiagonal State Space Augmented Transformers for Speech Recognition\u201d. In: _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_ . IEEE.\n2023, pp. 1\u20135.\n\n[93] Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. \u201cLinear Transformers are Secretly Fast Weight Programmers\u201d.\nIn: _The International Conference on Machine Learning (ICML)_ . PMLR. 2021, pp. 9355\u20139366.\n\n[94] J\u00fcrgen Schmidhuber. \u201cLearning to control fast-weight memories: An alternative to dynamic recurrent networks\u201d.\nIn: _Neural Computation_ 4.1 (1992), pp. 131\u2013139.\n\n[95] Noam Shazeer. \u201cGLU Variants Improve Transformer\u201d. In: _arXiv preprint arXiv:2002.05202_ (2020).\n\n[96] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny\nZhou. \u201cLarge Language Models can be Easily Distracted by Irrelevant Context\u201d. In: _The International Conference on_\n_Machine Learning (ICML)_ . PMLR. 2023, pp. 31210\u201331227.\n\n[97] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. \u201cSequence Modeling with Multiresolution Convolutional Memory\u201d.\nIn: _The International Conference on Machine Learning (ICML)_ . PMLR. 2023, pp. 31312\u201331327.\n\n[98] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. \u201cSimplified State Space Layers for Sequence\nModeling\u201d. In: _The International Conference on Learning Representations (ICLR)_ . 2023.\n\n[99] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. \u201c", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_25650", "chunk_text": ": _The International Conference on Learning Representations (ICLR)_ . 2023.\n\n[99] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. \u201cRoformer: Enhanced Transformer\nwith Rotary Position Embedding\u201d. In: _arXiv preprint arXiv:2104.09864_ (2021).\n\n[100] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. \u201cRetentive\nnetwork: A successor to transformer for large language models\u201d. In: _arXiv preprint arXiv:2307.08621_ (2023).\n\n[101] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. \u201cSequence to Sequence Learning with Neural Networks\u201d. In: _Advances_\n_in Neural Information Processing Systems (NeurIPS)_ 27 (2014).\n\n[102] Corentin Tallec and Yann Ollivier. \u201cCan Recurrent Neural Networks Warp Time?\u201d In: _The International Conference_\n\n_on Learning Representations (ICLR)_ . 2018.\n\n[103] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian\nRuder, and Donald Metzler. \u201cLong Range Arena: A Benchmark for Efficient Transformers\u201d. In: _International_\n_Conference on Learning Representations (ICLR)_ . 2021.\n\n[104] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. \u201cEfficient Transformers: A Survey\u201d. In: _ACM Computing_\n_Surveys_ 55.6 (2022), pp. 1\u201328.\n\n[105] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste\nRozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. \u201cLlama: Open and Efficient Foundation Language Models\u201d.\nIn: _arXiv preprint arXiv:2302.13971_ (2023).\n\n[106] Ash", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_26550", "chunk_text": " Yejin Choi. \u201cHellaSwag: Can a Machine Really Finish\nYour Sentence?\u201d In: _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_ . 2019.\n\n\n22\n\n\n[113] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind.\n\u201cAn Attention Free Transformer\u201d. In: _arXiv preprint arXiv:2105.14103_ (2021).\n\n[114] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. \u201cEffectively Modeling Time\nSeries with Simple Discrete State Spaces\u201d. In: _The International Conference on Learning Representations (ICLR)_ . 2023.\n\n[115] Lin Zheng, Chong Wang, and Lingpeng Kong. \u201cLinear complexity randomized self-attention mechanism\u201d. In:\n_International Conference on Machine Learning_ . PMLR. 2022, pp. 27011\u201327041.\n\n[116] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. \u201cEfficient Long\nSequence Modeling via State Space Augmented Transformer\u201d. In: _arXiv preprint arXiv:2212.08136_ (2022).\n\n\n23\n\n\n### **A Discussion: Selection Mechanism**\n\nOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. It\ncan also be viewed as related to \u201cfast weights\u201d (J. Ba et al. 2016; Schmidhuber 1992), which connects classical RNNs with\nthe mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct concept\nthat is worth clarifying.\n\n\n**Gating.** Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber\n1997) and GRU (J. Chung et al. 2014), or the gated equation (5) in Theorem 1. This was interpreted as a particular mechanism\nfor controlling whether to let an input into the hidden state of an RNN. In", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_27000", "chunk_text": "U (J. Chung et al. 2014), or the gated equation (5) in Theorem 1. This was interpreted as a particular mechanism\nfor controlling whether to let an input into the hidden state of an RNN. In particular, this affects the propagation of signal\nthrough time and causes inputs to interact along the sequence length dimension.\n\n\nHowever, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction\n(often with an activation function). For example, _elementwise_ multiplicative components of neural network architectures\n(that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehta\net al. 2023), despite a very different meaning than the original RNN sense. Thus we believe the original concept of _RNN_\n_gating_ versus the popular usage of _multiplicative gating_ actually have a very different semantic meaning.\n\n\n**Hypernetworks.** Hypernetworks refer to neural networks whose parameters are themselves generated by smaller\nneural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to define a large RNN whose\nrecurrent parameters are generated by a smaller RNN, and other variants have been around for a long time (Schmidhuber\n1992).\n\n\n**Data-dependence.** Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the\nmodel depend on the data (Poli et al. 2023).\n\n\n**Example: GLU Activation.** To illustrate the issues with these concepts, consider a simple diagonal linear layer _\ud835\udc66_ = _\ud835\udc6b\ud835\udc65_,\nwhere _\ud835\udc6b_ is a diagonal weight parameter. Now suppose that _\ud835\udc6b_ is itself generated from a linear transformation of _\ud835\udc65_,\nwith an optional nonlinearity: _\ud835\udc6b_ = _\ud835\udf0e_ ( _\ud835\udc7e\ud835\udc65_ ). Since it is diagonal, the multiplication becomes an elementwise product:\n_\ud835\udc66_ = _\ud835\udf0e_ ( _\ud835\udc7e\ud835\udc65_ ) \u25e6 _\ud835\udc65_ .\n\n\nThis is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative\n\u201cbranch\u201d), hypernetworks (since the parameter _\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_27450", "chunk_text": "\ufffd\ufffd_ ) \u25e6 _\ud835\udc65_ .\n\n\nThis is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative\n\u201cbranch\u201d), hypernetworks (since the parameter _\ud835\udc6b_ is generated by another layer), and data-dependent (since _\ud835\udc6b_ depends\n\n- n the data _\ud835\udc65_ ). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an\nactivation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer.\n\n\n**Selection.** Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating,\nhypernetworks, or data-dependence, so can an enormous range of other constructions\u2014essentially anything with a\nmultiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as\nwell\u2014and we find it uninformative to think of them as such.\n\n\nInstead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1)\nand also has a deeper history of connections to SSMs through variable (input-dependent) discretization of \u0394 (Funahashi\nand Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term \u201cgating\u201d in favor of _selection_ to\nclarify the overloaded use of former. More narrowly, we use selection to refer to the _mechanistic_ action of a model to select\n\n- r ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated\nRNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and\nGuo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention.\n\n### **B Related Work**\n\n\nWe overview several prior works related to our methods. We mention that some of the most closely related models include\nrecurrent layers such as S4, S5, and quasi-RNNs", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_27900", "chunk_text": ") and even attention.\n\n### **B Related Work**\n\n\nWe overview several prior works related to our methods. We mention that some of the most closely related models include\nrecurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV.\n\n\n24\n\n\n**B.1** **S4 Variants and Derivatives**\n\n\nWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our\nmethod.\n\n\n- S4 (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) introduced the first structured SSM, describing diagonal\nstructure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to a\nconnection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020).\n\n\n- DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022).\n\n\n- S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the\nfirst S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective state\ndimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to\nMIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISO\ndimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome the\ncomputation issue, (iii) adding the selection mechanism.\n\n\nLu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Their\nmechanism can be viewed as a particular hard-coded instance of a selection mechanism, where _\ud835\udc68_ is manually set to 0,\ninstead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically\nto this setting and probe if the model has learned to automatically reset its", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_28350", "chunk_text": "\ud835\udc68_ is manually set to 0,\ninstead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically\nto this setting and probe if the model has learned to automatically reset its state on episode boundaries.\n\n\n- Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of\nbeing an exponential moving average (EMA). They additionally make an interesting connection of the discretization step\n\n - f SSMs to an EMA _damping_ term. Contrary to findings in the original S4 papers, this was the first model to show that\nreal-valued SSMs are empirically effective in certain settings or when combined with different architectural components.\n\n\n- Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this\nperspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally\nand close to LTI.\n\n\n- SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox\n\n2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of\nS4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do\nfast autoregressive inference directly.\n\n\nNotably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually\nstrictly LTI (linear time invariant).\n\n\n**B.2** **SSM Architectures**\n\n\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating\n\n- ne of the previous SSMs as a black box layer.\n\n\n- GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated\nattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most\nimportantly, its projection", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_28800", "chunk_text": " network architecture incorporating SSMs. It is motivated by the gated\nattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most\nimportantly, its projection _contracts_ the model dimension to reduce the state size of the SSM, while ours _expands_ the\nmodel dimension in order to increase the state size, based on the motivation in Section 3.1.\n\n\n- Mega (Ma et al. 2023) combined the EMA simplification of S4 described above into a hybrid architecture using an efficient\nattention approximation.\n\n\n- H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is\nthe first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later\narchitectures.\n\n\n- Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the\ninput. While sharing the \u201cselection\u201d name, we consider this an architectural modification that is closer to architectural\ngating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective\n\n\n25\n\n\nCopying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones\n(indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0).\n\n\n- RetNet (Y. Sun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4 layer to a\nspecial case where the state dimension is _\ud835\udc41_ = 1. Although not framed as such, its recurrence can be viewed as a special\ncase of a linear SSM.\n\n\nIts primary source of improvement is using a linear attention with large _head dimension_, which can be viewed as another\nmethod to perform input-dependent state expansion. Using a larger head dimension in the context of linear attention\nvariants was first done by H3, but not extensively used since this requires a proportional amount of extra computation.\nRetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention\ninstead of convolutions, made feasible by their", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_29250", "chunk_text": " done by H3, but not extensively used since this requires a proportional amount of extra computation.\nRetNet avoids this with an alternate way to parallelize the computation with a variant of standard multi-head attention\ninstead of convolutions, made feasible by their particular special case of SSMs which acts as a simple EMA.\n\n\n- RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-free\nTransformer (S. Zhai et al. 2021)), another variant of linear attention. Its main \u201cWKV\u201d mechanism involves LTI recurrences\nand can be seen as the ratio of two SSMs.\n\n\nWe also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformer\u2019s\nMHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLP\nblocks.\n\n\n**B.3** **Relationship to RNNs**\n\n\nRNNs and SSMs are broadly related, as they both involve the concepts of _recurrence_ - n a latent _state_ .\n\n\nSeveral older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016),\nand simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities.\nBecause of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs,\nand are thus more powerful in a sense than the family of LTI structured SSMs above. The main differences are:\n\n\n  - They do not use state expansion ( _\ud835\udc41_ = 1) or selective _\ud835\udc69, \ud835\udc6a_ parameters, both of which are important for performance\n(Section 4.6).\n\n\n   They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +\ndiscretization (Theorem 1). The connections to principled SSM theory provides better parameterizations and\ninitializations (Section 3.6).\n\n\nAdditionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991;\nHochreiter", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_29700", "chunk_text": " principled SSM theory provides better parameterizations and\ninitializations (Section 3.6).\n\n\nAdditionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991;\nHochreiter, Bengio, et al. 2001; Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The former\ncould be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the latter\nwas difficult without theory later developed for SSMs. For example, modern structured SSMs differ in more careful\nparameterization of the recurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson,\nGoel, et al. 2021; Gu, Johnson, Timalsina, et al. 2023)), or direct analysis (Gupta, Mehta, and Berant 2022; Kaul 2020; Orvieto\net al. 2023)).\n\n\nWe also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016; Henaff, Szlam,\nand LeCun 2016; Lezcano-Casado and Mart\u00ednez-Rubio 2019; Mhammedi et al. 2017; Vorontsov et al. 2017) which are\nmotivated by constraining the _\ud835\udc68_ transition matrix to be orthogonal or unitary, in order to control its eigenvalues and\nprevent the vanishing gradient problem. However, these had other limitations; we believe that these stem from the fact\nthat orthogonal/unitary RNNs are also LTI. For example, they are almost always evaluated on the Copying task which they\ncan solve perfectly, but observed to struggle on the Selective Copying task (Jing et al. 2019).\n\n\n**B.4** **Linear Attention**\n\n\nThe Linear Attention (LA) (Katharopoulos et al. 2020) framework is an important result popularizing kernel attention and\nshowing how it relates to recurrent autoregressive models. Many variants have proposed alternative kernels and other\nmodifications. Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_30150", "chunk_text": " attention and\nshowing how it relates to recurrent autoregressive models. Many variants have proposed alternative kernels and other\nmodifications. Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature map to approximate softmax\nattention (i.e. the exp feature map) using the random Fourier feature approximation of Gaussian kernels (Rahimi and\nRecht 2007). Performer (Choromanski et al. 2021) finds an approximation to the exponential kernel involving only positive\n\n\n26\n\n\nfeatures, which also allows the softmax normalization term. TransNormer (Qin, Han, W. Sun, D. Li, et al. 2022) showed\nthat the LA denominator term can be unstable and proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al.\n2022) augments RFA with a cosine reweighting mechanism that incorporates positional information to emphasize locality.\nLinear Randomized Attention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importance\nsampling, and generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed\nnumerator).\n\n\nAside from kernel attention, many other variants of efficient attention exist; the survey Tay, Dehghani, Bahri, et al. (2022)\n\n- ffers an extensive categorization of many of these.\n\n\n**B.5** **Long Context Models**\n\n\nLong context has become a popular subject, and several recent models have claimed to scale to longer and longer sequences.\nHowever, these are often from a computational standpoint and have not been extensively validated. These include:\n\n\n- Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a Transformer\nbackbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization tasks; their main result\nis similar to our Induction Heads extrapolation experiment (Table 2).\n\n\n- LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length _<_ 100 _\ud835\udc3e_ for actual tasks.\n\n\n- Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al. 2023), which claimed to leverage up to", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_30600", "chunk_text": " length _<_ 100 _\ud835\udc3e_ for actual tasks.\n\n\n- Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al. 2023), which claimed to leverage up to 1M context. However, their experiments trained on proportionally more data at longer contexts, making it hard to conclude if quality\nimprovements at 1M context are due to context length or due to more data and computation.\n\n\n- Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer to\n\n=\nmodel audio waveforms of length 2 [20] 1048576, although did not discuss performance tradeoffs when controlling for\ncomputation and model size.\n\n\nIn contrast, we believe this work presents one of the first approaches to meaningfully demonstrate increasing performance\nwith longer context.\n\n### **C Mechanics of Selective SSMs**\n\n\n_Proof of Theorem 1._ Consider a selective SSM (Algorithm 2) with _\ud835\udc41_ = 1 _, \ud835\udc68_ = \u22121 _, \ud835\udc69_ = 1 _,\ud835\udc60_ \u0394 = Linear( _\ud835\udc65_ ) _,\ud835\udf0f_ \u0394 = softplus. The\ncorresponding continuous-time SSM (1) is\n\n\n_\u210e_ ( _\ud835\udc61_ ) = \u2212 _\u210e_ ( _\ud835\udc61_ ) + _\ud835\udc65_ ( _\ud835\udc61_ )\n\n\nwhich is also called a _leaky integrator_ .\n\n\nThe discretization step size is\n\n\n\u0394 _\ud835\udc61_ = _\ud835\udf0f_ \u0394 (Parameter + _\ud835\udc60_ \u0394 ( _\ud835\udc65\ud835\udc61_ ))\n\n\n= softplus(Parameter + Linear( _\ud835\udc65\ud835\udc61_ ))\n\n\n= softplus(Linear( _\ud835\udc65\ud835\udc61_ ))\n\n\nwhere we observe that the parameter can be viewed as a learnable bias and folded into the linear projection.\n\n\nNow applying the zero-order hold (ZOH) discretization formulas:\n\n\n1\n_\ud835\udc68\ud835\udc61_ = exp(\u0394 _\ud835\udc68_ ) =\n1 + exp(Linear( _\ud835\udc65\ud835\udc61_ )) [=] _[ \ud835\udf0e]_ [(\u2212][Linear][(] _[\ud835\udc65][\ud835\udc61]_", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_31050", "chunk_text": "\u0394 _\ud835\udc68_ ) =\n1 + exp(Linear( _\ud835\udc65\ud835\udc61_ )) [=] _[ \ud835\udf0e]_ [(\u2212][Linear][(] _[\ud835\udc65][\ud835\udc61]_ [))]\n\n\n= 1 \u2212 _\ud835\udf0e_ (Linear( _\ud835\udc65\ud835\udc61_ ))\n\n\n_\ud835\udc69\ud835\udc61_ = (\u0394 _\ud835\udc68_ ) [\u2212][1] (exp(\u0394 _\ud835\udc68_ ) \u2212 _\ud835\udc70_ ) \u00b7 \u0394 _\ud835\udc69_ = \u2212(exp(\u0394 _\ud835\udc68_ ) \u2212 _\ud835\udc70_ ) = 1 \u2212 _\ud835\udc68_\n\n\n= _\ud835\udf0e_ (Linear( _\ud835\udc65\ud835\udc61_ )) _._\n\n\n27\n\n\nThus the final discrete recurrence (2a) is\n\n\n_\ud835\udc54\ud835\udc61_ = _\ud835\udf0e_ (Linear( _\ud835\udc65\ud835\udc61_ ))\n\n\n_\u210e\ud835\udc61_ = (1 \u2212 _\ud835\udc54\ud835\udc61_ ) _\u210e\ud835\udc61_                          - 1 + _\ud835\udc54\ud835\udc61\ud835\udc65\ud835\udc61_\n\n\nas desired. \n### **D Hardware-aware Algorithm For Selective SSMs**\n\n\nWithout input-dependent selectivity, SSMs can be efficiently implemented as a convolution (Dao, Fu, Saab, et al. 2023; Gu,\nGoel, and R\u00e9 2022), which leverages the fast Fourier transform (FFT) as primitive. With selectivity, SSMs are no-longer\nequivalent to convolution, but we leverage the parallel associative scan. While SSM scans are theoretically efficient\n( _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ ) FLOPs, scaling linear in _\ud835\udc3f_ ), training foundation models with selective SSMs requires them to be efficient on\nmodern hardware (GPUs) as well. We describe how we use _kernel fusion_ and _recomputation_ to make SSM scan fast and\nmemory-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5,\nshowing that it is up to 7\u00d7 times faster than attention at sequence length 32K, and is as memory-efficient as the", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_31500", "chunk_text": "-efficient. We evaluate the speed of our scan implementation compared to convolution and attention in Section 4.5,\nshowing that it is up to 7\u00d7 times faster than attention at sequence length 32K, and is as memory-efficient as the best\nattention implementation (FlashAttention).\n\n\n**Speed.** On modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by memorybandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This the case with our\nscan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to significant speedup compared to\na standard implementation.\n\n\nThe standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input _\ud835\udc68, \ud835\udc69_ - f size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in GPU\nHBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel associative scan implementation to\nwrite the scan output of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) to GPU HBM, then multiply that scan output with _\ud835\udc6a_ to produce an output of size\n( _\ud835\udc35, \ud835\udc3f, \ud835\udc37_ ). However, this requires the number of memory reads/writes on the order of _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37\ud835\udc41_ ). We can instead fuse the\ndiscretization step, the scan, and the multiplication with _\ud835\udc6a_ into one kernel:\n\n\n1. We read in _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc37_ + _\ud835\udc37\ud835\udc41_ ) bytes of memory (\u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ ) from slow HBM to fast SRAM.\n\n\n2. We discretize to produce _\ud835\udc68, \ud835\udc69_  - f size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in SRAM.\n\n\n3. We perform a parallel associative scan, yielding intermediate states of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in SRAM.\n\n\n4.", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_31950", "chunk_text": "\ufffd, \ud835\udc41_ ) in SRAM.\n\n\n3. We perform a parallel associative scan, yielding intermediate states of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) in SRAM.\n\n\n4. We multiply and sum with _\ud835\udc6a_, producing outputs of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37_ ) and write it to HBM.\n\n\nThis way, we reduce IOs by a factor of _\ud835\udc42_ ( _\ud835\udc41_ ) (the state dimension), which in practice speeds up the operation by 20-40\ntimes (Section 4.5).\n\n\nFor sequence length _\ud835\udc3f_ too long where we cannot fit the sequence in SRAM (which is much smaller than HBM), we split the\nsequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate scan states, we can\ncontinue the scan with the next chunk.\n\n\n**Memory.** We describe how we use the classical technique of _recomputation_ to reduce the total amount of memory\nrequired to train selective SSM layers.\n\n\nFrom the way we fuse the forward pass, we do not save the intermediate states of size ( _\ud835\udc35, \ud835\udc3f, \ud835\udc37, \ud835\udc41_ ) to avoid memory blowup.\nHowever, these intermediate states are necessary for the backward pass to compute gradients. We instead recompute those\nintermediate states in the backward pass. Since the inputs \u0394 _, \ud835\udc68, \ud835\udc69, \ud835\udc6a_ and output gradient read from HBM to SRAM are\n\n- f size _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc41_ + _\ud835\udc37\ud835\udc41_ ), and the input gradients are also of size _\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc41_ + _\ud835\udc37\ud835\udc41_ ), recomputation avoids the cost of reading\n_\ud835\udc42_ ( _\ud835\udc35\ud835\udc3f\ud835\udc41\ud835\udc37_ ) elements from HBM. This means that recomputation of the SSM states in the backward pass speeds up the\ncomputation compared to storing them and reading them from HBM.\n\n\nBeyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize the\nmemory requirement of the entire selective SSM block (input projection, convolution", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_32400", "chunk_text": " the\ncomputation compared to storing them and reading them from HBM.\n\n\nBeyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize the\nmemory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output projection).\nIn particular, we do not save intermediate activations that take a lot of memory but are fast to recompute (e.g. output of\nactivation function or short convolution). As a result, the selective SSM layer has the same memory requirement as an\n\n\n28\n\n\nTable 11: ( **Induction heads** .) Models are trained on sequence length 2 [8] = 256, and tested on various sequence lengths of 2 [6] = 64 up to\n2 [20] = 1048576. \u2713 denotes perfect generalization accuracy, while \u2717 denotes out of memory.\n\n\nModel Params Test Accuracy (%) at Seqence Length\n\n\n2 [6] 2 [7] 2 [8] 2 [9] 2 [10] 2 [11] 2 [12] 2 [13] 2 [14] 2 [15] 2 [16] 2 [17] 2 [18] 2 [19] 2 [20]\n\n\nMHA-Abs 137K \u2713 99.6 100.0 58.6 26.6 18.8 9.8 10.9 7.8 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717\n\nMHA-RoPE 137K \u2713 \u2713 100.0 83.6 31.3 18.4 8.6 9.0 5.5 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717\n\nMHA-xPos 137K \u2713 \u2713 100.0 99.6 67.6 25.4 7.0 9.0 7.8 \u2717 \u2717 \u2717 \u2717 \u2717 \u2717\n\nH3 153K \u2713 \u2713 100.0 80.9 39.5 23.8 14.8 8.2 5.9 6.6 8.2 4.7 8.2 6.3 7.4\nHyena 69M [\u2217] 97.7 \u2713 100.0 \u2713 44", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_32850", "chunk_text": " 5.9 6.6 8.2 4.7 8.2 6.3 7.4\nHyena 69M [\u2217] 97.7 \u2713 100.0 \u2713 44.1 12.5 6.6 5.1 7.0 5.9 6.6 6.6 5.9 6.3 9.8\nMamba 74K \u2713 \u2713 100.0 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\n\n~~\u2217~~ Most of the parameters are in learnable positional encodings.\n\n\n- ptimized Transformer implementation with FlashAttention. In particular, each attention layer (FlashAttention) stores\naround 12 bytes of activations per token, an each MLP layer stores around 20 bytes of activations per token, for a total of\n32 bytes ((assuming mixed-precision training in FP16 or BF16)). Each selective SSM stores around 16 bytes of activations\nper token. Hence two layers of selective SSMs have around the same activation memory as an attention layer and an MLP\nlayer.\n\n### **E Experimental Details and Additional Results**\n\n\n**E.1** **Synthetic Tasks**\n\n\n**Selective Copying.** Our setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including the\nwhite \u201cnoise\u201d token from Figure 2) and requiring models to memorize 16 \u201cdata\u201d tokens. We use 2 layer models with a model\ndimension of _\ud835\udc37_ = 64.\n\n\nModels are trained for 400K steps at a constant learning rate of 0 _._ 0001 with a batch size of 64.\n\n\n**Induction Heads.** Training consists of randomly generating data every step, with a batch size of 8. We choose an\n\u201cepoch\u201d size of 8192 steps, and track the accuracy on fixed validation sets (also randomly generated) of each target sequence\nlength. For the MHA-Abs and Mamba models, results are reported after the 25th epoch (8192 \u00d7 25 = 204800 steps). For the\nMHA-RoPE and MHA-xPos models, results are reported after the 50th epoch (8192 \u00d7 50 = 409600 steps). For the LTI H3\nand Hyena models, results are reported", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_33300", "chunk_text": " the\nMHA-RoPE and MHA-xPos models, results are reported after the 50th epoch (8192 \u00d7 50 = 409600 steps). For the LTI H3\nand Hyena models, results are reported after the 10th epoch (81920 steps) because they had converged by then and failed\nto improve further.\n\n\nWe use the Adam optimizer with no weight decay. All models are trained at constant learning rates 2 _\ud835\udc52_ - 4 and 1 _\ud835\udc52_ - 3, and\nthe better results are reported for each model (2 _\ud835\udc52_ - 4 for all models except Mamba). The attention and Hyena models did\nnot learn at LR 1 _\ud835\udc52_ - 3. H3 learned at both LRs, but interestingly generalized better to shorter sequences at the smaller LR of\n2 _\ud835\udc52_ - 4. Mamba learned at both LRs, but extrapolated better at the larger LR of 1 _\ud835\udc52_ - 3.\n\n\n**E.2** **Language Modeling**\n\n\n**E.2.1** **Scaling Law Details**\n\n\nScaling law experiments generally followed the GPT3 recipe. All models were trained on the Pile with the GPT2 tokenizer.\n\n\n**Model Sizes.** Table 12 specifies the model sizes we use for scaling laws. This is taken directly from the GPT3 specifications (Brown et al. 2020), with very minor modifications. First, we changed the batch size of the 1.3B model from 1M\ntokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch size. Second, we changed the\nnumber of training steps and total tokens to roughly match Chinchilla scaling laws (Hoffmann et al. 2022), which specify\nthat training tokens should increase proportionally to model size.\n\n\n**Training Recipes.** All models used the AdamW optimizer with\n\n\n29\n\n\nTable 12: ( **Scaling Law Model Sizes** .) Our model sizes and hyperparameters for scaling experiments. (Model dimension and number of\nheads applies only to Transformer models.)\n\n\nParams n_layers d_model n_heads / d_head Training steps Learning Rate Batch Size Tokens\n\n\n125M 12 768 12 / 64 4800 6e-4 0.5", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_33750", "chunk_text": "\nheads applies only to Transformer models.)\n\n\nParams n_layers d_model n_heads / d_head Training steps Learning Rate Batch Size Tokens\n\n\n125M 12 768 12 / 64 4800 6e-4 0.5M tokens 2.5B\n\n350M 24 1024 16 / 64 13500 3e-4 0.5M tokens 7B\n\n760M 24 1536 16 / 96 29000 2.5e-4 0.5M tokens 15B\n\n1.3B 24 2048 32 / 64 50000 2e-4 0.5M tokens 26B\n\n\n  - gradient clip value 1 _._ 0\n\n\n  - weight decay 0 _._ 1\n\n\n  - no dropout\n\n\n  - linear learning rate warmup with cosine decay\n\n\nBy default, the peak learning rate is the GPT3 specification.\n\n\nWe give several models an \u201cimproved recipe\u201d, inspired by changes adopted by popular large language models such as\nPaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al. 2023). These include:\n\n\n  - linear learning rate warmup with cosine decay to 1 _\ud835\udc52_   - 5, with a peak value of 5\u00d7 the GPT3 value\n\n\n  - no linear bias terms\n\n\n  - RMSNorm instead of LayerNorm\n\n\n  - AdamW hyperparameter _\ud835\udefd_ = ( _._ 9 _, ._ 95) (the GPT3 value) instead of the PyTorch default of _\ud835\udefd_ = ( _._ 9 _, ._ 999)\n\n\n**Architecture and Training Details.** Our models are:\n\n\n- **Transformer** : The standard Transformer based on GPT3 (Table 12).\n\n\n- **Transformer++** : A Transformer with an improved architecture, namely rotary positional encodings (Su et al. 2021) and\nSwiGLU MLP (Shazeer 2020), and the improved training recipe above.\n\n\n**Hyena** : Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an MLP) with\nstandard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondingly", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_34200", "chunk_text": "aving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an MLP) with\nstandard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of layers is correspondingly\nincreased by 1 _._ 5\u00d7 to preserve parameter count.\n\n\n- **H3++** : The H3 architecture with a few modifications, including (i) using the same \u201cthin\u201d Hyena dimensions above (ii) the\nimproved training recipe above (iii) a linear attention _head dimension_  - f 8.\n\n\n- **RWKV** : The default RWKV model from B. Peng et al. (2023), including its modified MLP block. We also used as much of\nits specified training recipe as possible, such as increasing the learning rates by 2\u00d7 or 3\u00d7 on certain parameters.\n\n\n- **RetNet** : The default RetNet model from Y. Sun et al. (2023). We also gave it the improved training recipe above.\n\n\n- **Mamba** : The standard Mamba architecture, with the improved training recipe.\n\n\n**E.2.2** **Additional Scaling Law Ablations**\n\n\nWe perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws in\nFigure 4 ( _Left_ ).\n\n\n**Mamba Architecture: Interleaving Blocks.** We test the effect of different architectural blocks combined with the\nMamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with an extra\nconv \u2192 SSM path added. This leads to two natural ablations:\n\n\nWhat if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This can also be\ninterpreted as taking Mamba and removing half of the SSMs.\n\n\n30\n\n\nFigure 9: ( **Scaling laws: extra ablations** .) ( _Left_ ) Instead of ( _Right_ ) Instead of\n\n\nWhat if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted as taking\na Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks.\n\n\nFigure 9 ( _Right_ ) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neither", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_34650", "chunk_text": "GLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the MLP blocks.\n\n\nFigure 9 ( _Right_ ) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly, neither\nchange matters too much. The Mamba-MLP architecture is only slightly worse, and still better than all models except\nTransformer++. The Mamba-MHA architecture is only slightly better, which is somewhat surprising in light of the fact\nthat many recent works have found that combining (LTI) SSMs with Attention can lead to substantial improvements (Dao,\nFu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta, and Cui 2023; Zuo et al. 2022).\n\n\n**H3 Architecture: Training Recipes.** Next we ablate differences between the Hyena and H3++ models, our weakest\nand strongest models outside of Transformer++ and Mamba, particularly to isolate the effect of training recipes.\n\n\n- **Hyena** : The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).\n\n\n**Hyena+** : The same architecture but with the improved training recipe described above.\n\n\n- **H3+** : The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution kernel.\n\n\n- **H3++** : The same as H3+, but with a linear attention _head dimension_ - f 8. This increases computation inside the SSM\nrecurrence but does not increase parameters.\n\n\nOur general convention is that \u201cModel+\u201d represents the base model with the improved training recipe, and \u201cModel++\u201d also\nallows for architectural changes.\n\n\nFigure 9 ( _Right_ ) shows that\n\n\nA large improvement is achieved by the improved training recipe, which was used for many of the models in the main\nFigure 4 (RetNet, H3++, Transformer++, Mamba).\n\n\n- The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with findings throughout this paper.\n\n\nThe head dimension expansion improves performance, consistent with one of our main themes that expanded state\ndimension improves performance for SSMs (Section 3).\n\n\n**E.2.3** **Downstream Evaluation Details**\n\n\nThis pretraining procedure", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_35100", "chunk_text": " paper.\n\n\nThe head dimension expansion improves performance, consistent with one of our main themes that expanded state\ndimension improves performance for SSMs (Section 3).\n\n\n**E.2.3** **Downstream Evaluation Details**\n\n\nThis pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens and with the GPT-NeoX\ntokenizer (Black et al. 2022) instead of GPT2 tokenizer. For the 1.3B model, we use a batch size of 1M tokens to be consistent\nwith the GPT3 specifications. We report the perplexity on the Pile validation set, and for this metric only compare to\nmodels trained on the same dataset and with the same tokenizer, in particular Pythia and RWKV.\n\n\nFor downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021), as done by most\nwork in this area. We evaluate on the following tasks/datasets that measure common sense reasoning:\n\n\n  - LAMBADA (Paperno et al. 2016)\n\n\n  - HellaSwag (Zellers et al. 2019)\n\n\n31\n\n\n  - PIQA (Bisk et al. 2020)\n\n\n  - ARC-challenge (P. Clark et al. 2018)\n\n\n  - ARC-easy: an easy subset of ARC-challenge\n\n\n  - WinoGrande (Sakaguchi et al. 2021)\n\n\nWe report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence length for\nHellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these task).\n\n\n**E.3** **DNA Modeling**\n\n\n**E.3.1** **Pretraining Details**\n\n\nWe describe the dataset and training procedure of the HG38 pretraining task in more detail.\n\n\nThe dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split contains a\ntotal of _\ud835\udc46_ = 34021 segments of length 2 [17] = 131072 that cover the genome, for a total of approximately 4.5 billion tokens\n(DNA base pairs). These segments are pairs of (chromosome number, starting index, ending index), and can be extended if", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_35550", "chunk_text": " [17] = 131072 that cover the genome, for a total of approximately 4.5 billion tokens\n(DNA base pairs). These segments are pairs of (chromosome number, starting index, ending index), and can be extended if\nnecessary (e.g. to get longer segments).\n\n\nWe deviate from HyenaDNA when the training sequence length is not 2 [17] . HyenaDNA always takes a fixed sub-segment\n(e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length each epoch is fixed\nto 34021 samples and doesn\u2019t necessarily go through the whole genome. On the other hand, we use the entire training\ndata:\n\n\n- When the context length _\ud835\udc3f_ is less than (or equal to) 2 [17], we divide up each segment into non-overlapping sub-segments\n\n - f length _\ud835\udc3f_, so that there are _\ud835\udc46_ \u00d7 [2] _\ud835\udc3f_ [17] [total samples and] _[ \ud835\udc46]_ [\u00d7][ 2][17][ \u2248] [4] _[.]_ [5] _[\ud835\udc35]_ [tokens per epoch.]\n\n\n- When the context length _\ud835\udc3f_ is greater than 2 [17], we turn each segment into two samples, one that begins with the prescribed\nsegment and one that ends with the prescribed segment. Thus each epoch has 2 _\ud835\udc46_ items and 2 _\ud835\udc46\ud835\udc3f_ tokens per epoch. For\nexample, at sequence length 2 [18] = 262144 there are 4\u00d7 as many tokens as the default, and at sequence length 2 [20] there\nare 16\u00d7 as many tokens.\n\n\nOther training details generally follow the same protocol as our language modeling experiments (Appendix E.2). For\nexample, we use the AdamW with ( _\ud835\udefd_ 1 _, \ud835\udefd_ 2) = (0 _._ 9 _,_ 0 _._ 95), no dropout, weight decay 0 _._ 1. We use a cosine learning rate\nscheduler with linear warmup for 10% of total steps.\n\n\n**E.3.2** **Scaling: Model Size Details**\n\n\n**Models.** The models we consider are:\n\n\nTransformer++: a Transformer with improved architecture,", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_36000", "chunk_text": " learning rate\nscheduler with linear warmup for 10% of total steps.\n\n\n**E.3.2** **Scaling: Model Size Details**\n\n\n**Models.** The models we consider are:\n\n\nTransformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su et al.\n\n2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani et al. 2017).\n\n\n- HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a Transformer with\nthe MHA block replaced by an H3 block using a global convolution parameterized by an MLP.\n\n\n- Mamba: the standard Mamba architecture.\n\n\n**Model Sizes.** We use the following model sizes.\n\n\nBlocks 4 5 6 7 8 10 12\n\nModel Dimension 64 96 128 192 256 384 512\n\nParams (Approx.) 250K 700K 1.4M 3.5M 7.0M 19.3M 40.7M\n\n\nNote that the number of blocks for Mamba is doubled, because one Transformer \u201clayer\u201d includes both the MHA and MLP\nblocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).\n\n\n32\n\n\n**Training.** For each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1 _\ud835\udc52_ - 3 _,_ 2 _\ud835\udc52_ - 3 _,_ 4 _\ud835\udc52_ 3 _,_ 8 _\ud835\udc52_ - 3}. The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal Mamba learning\nrate was 8e-3; note that Mamba performed better than baselines with matched learning rates (2e-3), but was more stable\nand improved even more at higher learning rates. (Furthermore, as this LR is on the upper range of the sweep, it is possible\nthat our results are still suboptimal.)\n\n\nNote that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for simplicity. The\n\n- ptimal LR should go down for larger models, but", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_36450", "chunk_text": "\nthat our results are still suboptimal.)\n\n\nNote that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for simplicity. The\n\n- ptimal LR should go down for larger models, but we didn\u2019t find a noticeable effect at the small model sizes (at most a few\nmillion parameters) we considered.\n\n\n**E.3.3** **Scaling: Context Length Details**\n\n\nWe use a total batch size of 2 [24] \u2248 16 _\ud835\udc40_ tokens per training step, for every sequence length (e.g. at length 2 [20] there are\n16 segments per batch and at length 2 [10] there are 16384 segments per batch). This is a large batch size relative to the\nmodel size by usual LM standards, but note that a batch size of 2 [23] is the minimum possible on a machine with 8 GPUs and\nsequence length of 2 [2] 0, and that HyenaDNA used much larger batches of 2 [28] .\n\n\nThe learning rate used was 0 _._ 008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same learning rate\n\n- f 0 _._ 002 from the previous section for HyenaDNA, but found that it was unstable at the longest context length.\n\n\n**Sequence Length Warmup.** Following (Nguyen, Poli, et al. 2023), we use sequence length warmup (SLW) during\npretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from 2 [10] = 1024.\n(Note that because of how data is curated, at the longest sequence lengths more steps and tokens are spent proportionally.\nIn particular, each stage up to length 2 [17] processes the same number of tokens, but 4\u00d7 as many tokens are processed at\nlength 2 [18], 8\u00d7 as many at length 2 [19], and 16\u00d7 as many at length 2 [20] .)\n\n\nUnlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively\nhalved as the sequence lengths are doubled in each stage.\n\n\n**Remark E.1.** _We also note that the schedule was not tuned, and we never experimented with turning", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_36900", "chunk_text": " of tokens per gradient update, so the batch size is successively\nhalved as the sequence lengths are doubled in each stage.\n\n\n**Remark E.1.** _We also note that the schedule was not tuned, and we never experimented with turning off sequence length_\n_warmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at similar_\n_lengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either._\n\n\n**E.3.4** **Species (Great Apes) Classification**\n\n\nModels are causal and therefore only the last element (across the sequence length) of the model\u2019s output is used for the\nclassification head. Note that we control for the total number of elements in the loss function per gradient step. The\npretraining objective includes all positions across the sequence length, so that batch_size \u00d7 sequence_length is held\nconstant; in other words, the batch size decreases as the sequence length increases. However, for a classification task, since\n\n- nly the last position enters the loss, the batch size itself is held constant. Note that this also means that fine-tuning models\nwith longer sequence lengths is more computationally expensive.\n\n\nTraining consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which are all\nindependently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then uniformly\npicking a contiguous segment of DNA.\n\n\nFollowing (Nguyen, Poli, et al. 2023), models with a maximum context length greater than 2 [14] = 16384 use sequence length\n\n= = =\nwarmup with 1 epoch at length 2 [14] 16384, 1 epoch at length 2 [15] 32768, 1 epoch at length 2 [16] 65536, and so on up to\n\n=\nthe maximum sequence length. For example, the model with 2 [20] 1048576 context undergoes 6 epochs of sequence length\nwarmup before 4 more epochs at its maximum sequence length.\n\n\nThe learning rate for all Hyena models is 4e \u2212 5, while the learning rate for all Mamba models is 1e \u2212 4. These were found\nby performing learning rate sweeps for each model among {1 _\ud835\udc52_", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_37350", "chunk_text": " for all Hyena models is 4e \u2212 5, while the learning rate for all Mamba models is 1e \u2212 4. These were found\nby performing learning rate sweeps for each model among {1 _\ud835\udc52_ - 5 _,_ 2 _\ud835\udc52_ - 5 _,_ 4 _\ud835\udc52_ - 5 _,_ 1 _\ud835\udc52_ - 4 _,_ 2 _\ud835\udc52_ - 4} for the smaller sequence\nlengths (2 [10] _,_ 2 [12] _,_ 2 [14] _,_ 2 [16] ), and these values were consistently found to be the best for each model. An abridged learning rate\nsweep was done at length 2 [18], which agreed with these values, and a single run at length 2 [20] was performed (as described\nabove, the computational cost of these experiments is proportional to the sequence length). The learning rate followed\na cosine decay schedule with warmup with 5 epochs of linear warmup to the maximum learning rate, and 5 epochs of\ncosine decay down to 1 _\ud835\udc52_ - 6. The unusually long learning rate warmup schedule was chosen because the sequence length\n\n\n33\n\n\nTable 13: ( **Great Apes DNA Classification** .) Accuracy after fine-tuning on sequences of length 2 [10] = 1024 up to 2 [20] = 1048576 using\npretrained models of the same context length. Random guessing is 20%.\n\n\nModel Params Accuracy (%) at Seqence Length\n\n\n2 [10] 2 [12] 2 [14] 2 [16] 2 [18] 2 [20]\n\n\nHyenaDNA 1.4M 28.04 28.43 41.17 42.22 31.10 54.87\nMamba 1.4M 31.47 27.50 27.66 40.72 42.41 **71.67**\n\n\nMamba 7M 30.00 29.01 31.48 43.73 56.60 **81.31**\n\n\nTable 14: YouTubeMix length scaling sequence lengths and batch sizes.\n\n\nSeqence length Batch size Tokens / batch\n\n\n468 \u00d7 2048 = 958464 ", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_37800", "chunk_text": "31.48 43.73 56.60 **81.31**\n\n\nTable 14: YouTubeMix length scaling sequence lengths and batch sizes.\n\n\nSeqence length Batch size Tokens / batch\n\n\n468 \u00d7 2048 = 958464 1 958464\n\n234 \u00d7 2048 = 479232 2 958464\n\n117 \u00d7 2048 = 239616 4 958464\n\n59 \u00d7 2048 = 120832 8 966656\n\n30 \u00d7 2048 = 61440 16 983040\n\n15 \u00d7 2048 = 30720 32 983040\n\n8 \u00d7 2048 = 16384 64 1048576\n\n4 \u00d7 2048 = 8192 128 1048576\n\n\nwarmup was also long (e.g. comprising 6 out of 10 epochs for the model with context length 2 [20] ); we did not experiment\nwith this choice.\n\n\nResults for the Species classification task are in Table 13.\n\n\n**E.4** **Audio Details**\n\n\n**E.4.1** **YouTubeMix Audio Pretraining**\n\n\n**Model.** We use a model with 3 blocks per stage (3 \u00d7 5 = 15 total Mamba blocks), pooling factor _\ud835\udc5d_ = 16, and outer\ndimension _\ud835\udc37_ = 64, for about 3.5M parameters.\n\n\n**Dataset.** The data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of 256.\n\n\nThe dataset consists of clips of up to 1 minute long, or length 960000, which is subsampled and divided into segments of any\ndesired sequence length. Since the architecture involves two stages of pooling by a factor of 16, and we want the resulting\nsequence length to be a a multiple of 8 for hardware efficiency, the longest possible sequence is 468 \u00d7 2048 = 958464. The\nrest of our sequence lengths are defined by successively halving this and rounding up to the nearest multiple of 2048.\n\n\nTable 14 lists the specifications used in Figure 7. Beyond the varying batch sizes, the number of valid segments in the\ntraining set varied between different sequence lengths (e.g. the number of training steps per epoch was not constant for\ndifferent points in the graph), which may have contributed to k", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_38250", "chunk_text": ". Beyond the varying batch sizes, the number of valid segments in the\ntraining set varied between different sequence lengths (e.g. the number of training steps per epoch was not constant for\ndifferent points in the graph), which may have contributed to kinks in the scaling curves.\n\n\n**Training.** Models were trained for 200 _\ud835\udc3e_ training steps with a maximum learning rate of 0 _._ 002, 20 _\ud835\udc3e_ (10%) warmup steps,\nand weight decay 0 _._ 1 (similar to our general pretraining recipe across domains).\n\n\n**Additional Ablations: SSM Parameterizations.** We investigate SSM parameterizations on long-form audio waveform\npretraining in the setting of Figure 7. The setting is modified slightly to use larger models (8 layers and _\ud835\udc37_ = 64 for 6M\nparams, the SaShiMi default), shorter sequences (2 [11] = 2048 to 2 [18] = 262144 instead of 2 [13] to 2 [20] ), lower LR (0 _._ 001 from\n0 _._ 002), and shorter training cycles (100K instead of 200K steps).\n\n\nFigure 10 shows that the change from S4 \u2192 S6 (i.e. the selection mechanism) is not always beneficial. On long-form\naudio waveforms, it in fact significantly hampers performance, which may be intuitive from the point of view that audio\n\n\n34\n\n\n|Col1|Audio Waveforms - SSM Parameterization|on|\n|---|---|---|\n|50|S4+ML<br>Mamba<br>+comp<br>~~-~~select<br>~~-~~select<br>   (Mam|S4+ML<br>Mamba<br>+comp<br>~~-~~select<br>~~-~~select|\n|40<br>45|40<br>45|40<br>45|\n|30<br>35|30<br>35|30<br>35|\n\n\n\nFigure 10: ( **Audio Pretraining (YouTubeMix) Ablations** .) As a uniformly-sampled \u201ccontinuous\u201d signal modality, audio waveforms actually benefit from LTI models which have matching inductive bias. ( _Left_ ) Homogenous models (all blocks have the same parameterization)\n( _Right_ ) Only the center U-Net blocks are ablated", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_38700", "chunk_text": "ality, audio waveforms actually benefit from LTI models which have matching inductive bias. ( _Left_ ) Homogenous models (all blocks have the same parameterization)\n( _Right_ ) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as figure on left.\n\n\nis uniformly sampled and very smooth, and therefore benefits from continuous linear time-invariant (LTI) methods.\nAfter ablating away the selection mechanism, note that the resulting model is the S4 layer inside the Mamba block. To\ndisambiguate, we call this Mamba-S4 as opposed the default Mamba architecture Mamba-S6.\n\n\nHowever, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers. The\nperformance differences shrink dramatically; this reinforces the hypothesis that layers closer to the _raw_ audio signal should\nbe LTI, but once they are \u201ctokenized\u201d and compressed by the outer layers, the inner layers no longer need to be LTI. In this\nsetting however, the real-valued SSM still underperforms the complex-valued one.\n\n\n**E.4.2** **SC09 Speech Generation**\n\n\nAutoregressive training largely followed the autoregressive language modeling protocol, such as\n\n\n  - Weight decay 0 _._ 1\n\n\n  - Learning rate warmup for 10% of total steps\n\n\n  - AdamW optimizer with _\ud835\udefd_ = (0 _._ 9 _,_ 0 _._ 95)\n\n\n  - Gradient clip value 0 _._ 1\n\n\nWe used a learning rate of 0 _._ 002 and 200000 training steps at a batch size of 16.\n\n\nThe large Mamba model in Table 4 has 15 layers per stage with an outer dimension of _\ud835\udc37_ = 96 and pooling factor 4. We\nnote that this dataset is small (training went through 100 epochs) and for this large model, there was significant overfitting\n\n- f the BPB or NLL. However, automated metrics of generated samples continually improving throughout training.\n\n\nThe models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of D = 64 and\npooling factor 4. The S4+MLP block has roughly 2 _\ud835\udc37", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_39150", "chunk_text": "The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of D = 64 and\npooling factor 4. The S4+MLP block has roughly 2 _\ud835\udc37_ [2] + 4 _\ud835\udc37_ [2] parameters (expansion factor 2 in the MLP). The Transformer\nblock has 4 _\ud835\udc37_ [2] + 2 _\ud835\udc37_ [2] parameters (expansion factor 1 in the MLP). The Mamba block has the usual \u2248 6 _\ud835\udc37_ [2] parameters. All\nmodels have roughly 6M total parameters.\n\n\n**E.5** **Efficiency Benchmark**\n\n\n**Scan Operation.** We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), against\nconvolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operations\n\n- utside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing the\nQKV projections in attention.\n\n\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing the\nparameters _\ud835\udc68, \ud835\udc69, \ud835\udc6a_ in HBM.\n\n\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the large\nparameters in HBM.\n\n\n35\n\n\nTable 15: ( **Memory benchmark** .) Mamba\u2019s memory footprint is comparable to the most optimized Transformer. Results for 125M\nmodels.\n\n\nBatch size Transformer (w/ FlashAttention-2) Mamba\n\n\n1 4.6GB 4.8GB\n\n2 5.2GB 5.8GB\n\n4 6.9GB 7.3GB\n\n8 11.5GB 12.3GB\n\n16 20.7GB 23.1GB\n\n32 34.5GB 38.2GB\n\n\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and the\nfilters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexity\nis _\ud835\udc42_ ( _\ud835\udc3f_ log( _\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_39600", "chunk_text": ", which separately performs FFTs on the inputs and the\nfilters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexity\nis _\ud835\udc42_ ( _\ud835\udc3f_ log( _\ud835\udc3f_ )) for sequence length _\ud835\udc3f_ .\n\n\nFor attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2024)), with causal\nmask. Note that FlashAttention-2 with causal mask is about 1.7\u00d7 faster than without causal mask, since approximately\n\n- nly half of the attention entries are computed.\n\n\nWe use batch size of 1 and increase the sequence length from 2 [9] = 512, 2 [10] \u2248 1 _\ud835\udc3e_, 2 [11] \u2248 2 _\ud835\udc3e_, up to 2 [19] \u2248 500 _\ud835\udc3e_ (some of the\nbaselines run out of memory before reaching 500K). We use a model dimension of _\ud835\udc37_ = 1024 and state dimension _\ud835\udc41_ = 16.\nWe measure with BF16 inputs, which is the data type most commonly used for large scale training.\n\n\n**End-to-end Inference.** We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba 6.9B\nmodel, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard Transformer\nimplementation in the Huggingface transformers library.\n\n\nWe set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16,\n32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s) as\nbatch size \u00d7 128/time taken. We repeat the measurements 3 times and take the average. Measurements are done on an\nA100 80GB PCIe GPU.\n\n\n**Memory Benchmark.** The memory usage simply scales proportionally to the size of the activation tensors, as with\nmost deep sequence models. We report measurements of the training memory requirements of 125M models on 1 A100\n80GB GPU. Each batch consists of sequences of length ", "token_count": 500, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2312.00752_rag_roadmap_asai:chunk_40050", "chunk_text": " scales proportionally to the size of the activation tensors, as with\nmost deep sequence models. We report measurements of the training memory requirements of 125M models on 1 A100\n80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-efficient Transformer\nimplementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2). Table 15 shows that\nMamba\u2019s memory requirement is comparable to a similar-sized Transformer with an extremely optimized implementation,\nand we expect further improvement in Mamba\u2019s memory footprint in the future.\n\n\n36\n\n\n", "token_count": 122, "metadata": {"arxiv_id": "2312.00752", "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "authors": ["Albert Gu", "Tri Dao"], "year": 2023, "url": "https://arxiv.org/pdf/2312.00752v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_0", "chunk_text": "Preprint, Under Review\n\n## **RAFT: Adapting Language Model to Domain Specific RAG**\n\n\n\n**Tianjun Zhang** _[\u2217]_\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n{tianjunz}@berkeley.edu\n\n```\n\n\n**Shishir G. Patil, Naman Jain, Sheng Shen**\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n{shishirpatil,naman_jain,sheng.s}@berkeley.edu\n\n```\n\n\n**Matei Zaharia, Ion Stoica, Joseph E. Gonzalez**\nDepartment of Computer Science\nUC Berkeley\nBerkeley, CA 94720, USA\n```\n {matei,istoica,jegonzal}@berkeley.edu\n\n```\n\n**Abstract**\n\n\nPretraining Large Language Models (LLMs) on large corpora of textual\ndata is now a standard paradigm. When using these LLMs for many\ndownstream applications, it is common to additionally incorporate new information into the pretrained model either through RAG-based-prompting,\n\n    - r finetuning. However, the best methodology to incorporate information\nremains an open question. In this paper, we present Retrieval Augmented\nFine Tuning (RAFT), a training recipe which improves the model\u2019s ability\nto answer questions in \"open-book\" in-domain settings. In training RAFT,\ngiven a question, and a set of retrieved documents, we train the model to\nignore those documents that don\u2019t help in answering the question, which\nwe call, distractor documents. RAFT accomplishes this by citing verbatim\nthe right sequence from the relevant document to help answer the question.\nThis coupled with RAFT\u2019s chain-of-thought-style response helps improve\nthe model\u2019s ability to reason. In domain specific RAG, RAFT consistently\nimproves the model\u2019s performance across PubMed, HotpotQA, and Gorilla\ndatasets, presenting a post-training recipe to improve pre-trained LLMs to\nin-domain RAG.\n\n\n**1** **Introduction**\n\n\nTrained on vast quantities of public data, Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized\ndomains", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_450", "chunk_text": ", Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized\ndomains to support tasks ranging from code completion for specific software frameworks\nto question answering on specific document collections (e.g., legal or medical documents).\nIn these settings, general knowledge reasoning is less critical and instead the primary goal\nis to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the\nspecialized domains (e.g., recent news, enterprise private documents, or program resources\nconstructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\n\n\nThis paper studies the following question \u2013 _How do we adapt pre-trained LLMs for Retrieval_\n_Augmented Generation (RAG) in specialized domains?_\n\n\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and supervised fine-tuning. RAG based methods allow the LLM to reference the documents when\n\n\n_\u2217_ Corresponding author, personal website: `tianjunz.github.io`\n\n\n1\n\n\nPreprint, Under Review\n\n\n_Bake in Knowledge_\n\n\n_at Train Time_\n\n\nquery answer\n\n\n\u201cClosed book\u201d\n\n\n\n_Model can use_\n\n_External Docs at Test_\n\n\nquery answer\n\n\n\u201cOpen book\u201d\n\n\n\n_**Teach**_ _Model to_\n\n_use External Docs at Test_\n\n\nquery answer\n\n\nRAFT (Proposed)\n\n\n\nFigure 1: **How best to prepare for an Exam?** (a) Fine-tuning based approaches implement\n\"studying\" by either directly \"memorizing\" the input documents or answering practice\nQA without referencing the documents. (b) Alternatively, in-context retrieval methods fail\nto leverage the learning opportunity afforded by the fixed domain and are equivalent to\ntaking an open-book exam without studying. In contrast, our approach (c) RAFT leverages\nfine-tuning with question-answer pairs while referencing the documents in a simulated\nimperfect retrieval setting \u2014 thereby effectively preparing for the open-book exam setting.\n\n\nanswering questions. However, RAG based in-context learning methods fail to leverage\nthe learning opportunity afforded by the", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_900", "chunk_text": " question-answer pairs while referencing the documents in a simulated\nimperfect retrieval setting \u2014 thereby effectively preparing for the open-book exam setting.\n\n\nanswering questions. However, RAG based in-context learning methods fail to leverage\nthe learning opportunity afforded by the fixed domain setting and early access to the test\ndocuments. Alternatively, supervised fine-tuning offers the opportunity to learn more\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\ndocuments at test time (don\u2019t incorporate RAG) or fail to account for the imperfections in\nretrieval process during training.\n\n\nWe can draw an analogy to an open-book exam. Existing in-context retrieval methods are\nequivalent to taking an open-book exam without studying. Alternatively, existing finetuning based approaches implement \u201cstudying\" by either directly \u201cmemorizing\" Xiong\net al. (2023) the input documents or answering practice questions Wang et al. (2022) without\nreferencing the documents. While these approaches leverage in-domain learning they fail to\nprepare for the open-book nature of the test setting.\n\n\nIn this paper, we study how to combine instruction fine-tuning (IFT) with retrieval augmented generation (RAG). We propose a novel adaptation strategy \u2013 Retrieval-Augmented\nFine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both\nincorporate domain knowledge while also improving in-domain RAG performance. RAFT\naims to not only enable models to learn domain-specific knowledge through fine-tuning,\nbut also to ensure robustness against distracting retrieved information. This is achieved\nby training the models to understand the dynamics between the question (prompt), the\ndomain-specific documents retrieved, and the right answer. Going back to our analogy to\nthe open book exam, our approach is analogous to studying for an open-book exam by\nrecognizing relevant, and irrelevant retrieved documents.\n\n\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate\nanswer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic\n(2023), and in the presence of distractor documents ( _Dk_ ). We explain the methodology in\nSection 3 and analyze the sensitivity to the number of distractor documents (", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_1350", "chunk_text": " Wei et al. (2022); Anthropic\n(2023), and in the presence of distractor documents ( _Dk_ ). We explain the methodology in\nSection 3 and analyze the sensitivity to the number of distractor documents ( _k_ ) at train- and\ntest- time in Section 5. RAFT consistently outperforms Supervised-finetuning both withand without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),\nand HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),\npresenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.\nOur code is available at `[https://github.com/ShishirPatil/gorilla](https://github.com/ShishirPatil/gorilla)` .\n\n\n**2** **LLMs for Open-Book Exam**\n\n\nTo understand our goal better, we expand on our analogy between training an LLM with\nthe real-world setting of prepararing for an exam.\n\n\n**Closed-Book Exam** A closed book exam often refers to the scenario where the LLMs do\nnot have access to any additional documents or references to answer the questions during\n\n\n2\n\n\nPreprint, Under Review\n\n\nFigure 2: **Overview of our RAFT method.** The top-left figure depicts our approach of\nadapting LLMs to _reading_ solution from a set of positive and distractor documents in\ncontrast to standard RAG setup where models are trained based on the retriever outputs,\nwhich is a mixture of both memorization and reading. At test time, all methods follow the\nstandard RAG setting, provided with a top-k retrieved documents in the context.\n\n\nthe exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is\nused as a chatbot. In this scenario the LLM draws from the knowledge baked in during\npre-training and supervised-finetuning to respond to the users\u2019 prompt.\n\n\n**Open Book Exam** In contrast, we liken the open-book exam setting to the scenario in\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\nIn such scenarios, typically, the LLM is paired with retriever which retrieves \u2018k\u2019 documents\n(or specific segments of the", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_1800", "chunk_text": " in\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\nIn such scenarios, typically, the LLM is paired with retriever which retrieves \u2018k\u2019 documents\n(or specific segments of the document) which are appended to the users\u2019 prompt. It is\n\n- nly through these documents retrieved that the LLM gains access to \u201cdomain-specific\ninformation\u201d. As a result, we argue that the LLM\u2019s performance in these settings, where it\nis trained as a general-purpose LLM is largely dependent on the quality of the retriever and\nhow accurately the retriever can identify the most relevant piece of information.\n\n\n**Domain-Specific Open-Book Exam** In this paper, we focus on the narrower but increasingly popular domain than the general open book exam, which we call the domain-specific\n\n- pen-book exam. Here, we know apriori the domain in which the LLM will be tested. The\nLLM can respond to the users\u2019 prompt using use any and all information from this specific\ndomain, which it has been fine-tuned on. Examples of domain specific examples include\nenterprise documents, code repositories belonging to an organization, etc. In all these\nscenarios, the LLM will be used to respond to the questions, whose answers can be found\nwithin a collection of documents. The retrieval technique itself has little to no-impact on the\nmechanism (though it may impact the accuracy). This paper studies the domain-specific\n\n- pen-book setting and how to adapt a pretrained LLM to this specific domain, including\nhow to make it more robust to a varying number of retrieved documents and distractors.\n\n\n**3** **RAFT**\n\n\nIn this section, we present RAFT, a novel way of training LLMs for domain-specific openbook exams. We first introduce the classical technique of supervised fine-tuning, followed\nwith the key takeaways from our experiments. Then, we introduce RAFT, a modified\nversion of general instruction tuning. Lastly, we provide an overview of the experiments to\nexpect in the later sections.\n\n\n**Supervised Finetuning**\n\n\nConsider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The\nformulation consists of the Dataset ( _D_ ) from which a set of Question ( _Q_ ) and corresponding\nanswer ( _A_ ) pairs are derived or already available. In the", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_2250", "chunk_text": " setting for a Question-Answer dataset. The\nformulation consists of the Dataset ( _D_ ) from which a set of Question ( _Q_ ) and corresponding\nanswer ( _A_ ) pairs are derived or already available. In the classical SFT setting, the model is\ntrained to improve it\u2019s ability to answer the questions based on it\u2019s knowledge - obtained\neither during pre-training, or during the SFT training phase. The model so trained can also\n\n\n3\n\n\nPreprint, Under Review\n\n\nbe used at test-time with Retrieval Augmented Generation (RAG) setting, where additional\ndocuments can be introduced in the prompt to help the model answer the question. This\ncan be represented as follows:\n\n\n{Train: **Q** _\u2192_ **A** }, {0-shot Inference: **Q** _\u2192_ **A** }, {RAG Inference: **Q** + **D** _\u2192_ **A** }\n\n\n**RAFT:** Retrieval Augmented Fine-Tuning (RAFT), presents a novel recipe to prepare finetuning data to tailor the models for domain-specific open-book setting, equivalent to indomain RAG In RAFT, we prepare the training data such that each data point contains a\nquestion ( _Q_ ), a set of documents ( _Dk_ ), and a corresponding Chain-of-though style answer\n( _A_ _[\u2217]_ ) generated from one of the document ( _D_ _[\u2217]_ ). We differentiate between two types of\ndocuments: \u2018golden\u2019 documents ( _D\u2217_ ) i.e. the documents from which the answer to the\nquestion can be deduced, and \u2018distractor\u2019 documents ( _Di_ ) that do not contain answerrelevant information. As an implementation detail, the \u2018golden\u2019 document doesn\u2019t need to\nbe a single document, but can be more than one document, as is the case in HotpotQA Yang\net al. (2018). Then, for _P_ fraction of the questions ( _qi_ ) in the dataset, we retain the golden\ndocument ( _di_ _[\u2217]_ [) along with distractor documents (] _[d][k][\u2212]_ [1][). For][ (][1] _[ \u2212]_ _[P]_ [)][ fraction of the questions]\n( _qi_ ) in the dataset,", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_2700", "chunk_text": " distractor documents (] _[d][k][\u2212]_ [1][). For][ (][1] _[ \u2212]_ _[P]_ [)][ fraction of the questions]\n( _qi_ ) in the dataset, we include no golden document and only include distractor documents\n( _dk_ ). We then fine-tune the language model using standard supervised training (SFT)\ntechnique, training it to generate answers from the provided documents and question. Fig. 2\nillustrates the high-level design principal for RAFT .\n\n\nWe demonstrate that our RAG approach trains the model to perform better RAG on the set\n\n- f documents it is trained on _i.e., in-domain_ . By removing the golden documents in some\ninstances, we are compelling the model to memorize answers instead of deriving them from\nthe context. The training data for RAFT is as follows, and an example training data can be\nseen in Fig. 3:\n\n\n**P** % of data: **Q** + **D** _[\u2217]_ + **D** 1 + **D** 2 + . . . + **D** _k \u2192_ **A** _\u2217_\n\n\n(1 _\u2212_ **P** ) % of data: **Q** + **D** 1 + **D** 2 + . . . + **D** _k \u2192_ **A** _\u2217_\n\n\nSubsequently, for the test scenario, the model is provided with the Q and top-k documents\nretrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.\n\n\nA key factor in enhancing training quality is the generation of a reasoning process, such\nas Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we\ndemonstrate that creating a full reasoning chain and in-addition, clearly citing sources\nenhances the model\u2019s accuracy in answering questions. In Fig. 3, we illustrate this setup. Generating the training data in this fashion, involves presenting the model with a\nquestion, context, and verified answers, and then requesting it to form a reasoning chain\nthat appropriately references the original context.\n\n\nFor all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_3150", "chunk_text": " chain\nthat appropriately references the original context.\n\n\nFor all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an example of the generation step in Fig. 3, the detailed\nreasoning answer includes a citation from the original context inside `##begin_quote##` and\n`##end_quote##` as well as the detailed explanation on how to reach the conclusion based on\nthe citations. We demonstrate that adding detailed reasoning paragraphs can help boost the\nmodel\u2019s performance in our experiment section.\n\n\n**4** **Evaluation**\n\n\nWe design our experiments to study how well RAFT performs compared to various baselines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading\nand extracting information from in-domain documents, than domain-specific finetuned\nmodel, and general-purpose model with RAG. As an ablation, we also demonstrate how\nimportant it is for the model to learn with Chain-of-Thought responses. In this section,\nwe will first introduce all the datasets we used in the experiments, then all the baseline\nmodel/fine-tuning techniques that we benchmark against.\n\n\n4\n\n\nPreprint, Under Review\n\n```\n Question: The Oberoi family is part of a hotel company that has a head office\n in what city?\n\n context: [The Oberoi family is an Indian family that is famous for its\n involvement in hotels, namely through The Oberoi Group]...[It is located in\n city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\n Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\n towers that comprises a hotel and the Airlangga Apartment respectively]...[The\n Oberoi Group is a hotel company with its head office in Delhi.]\n\n Instruction: Given the question, context and answer above, provide a logical\n reasoning for that answer. Please use the format of: ##Reason: {reason}\n ##Answer: {answer}.\n\n -------------------------------------------------------------------------------------\n CoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\n Indian family that is famous for its involvement in hotels, namely through The\n Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\n the Oberoi group, and the", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_3600", "chunk_text": "begin_quote## The Oberoi family is an\n Indian family that is famous for its involvement in hotels, namely through The\n Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\n the Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel\n company with its head office in Delhi. ##end_quote## establishes the head\n office of The Oberoi Group. Therefore, the Oberoi family is part of a hotel\n company whose head office is in Delhi. ##Answer: Delhi\n\n```\n\nFigure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,\ncontrasting them with the correct reasoning and answers. The LLM is prompted to identify\nerrors in its reasoning and extract key insights for improvement. This figure specifically\nrepresents the \u2018GenerateExplanation\u2018 step in the RAFT algorithm (Section 3).\n\n\nTable 1: **RAFT improves RAG performance for all specialized domains** : Across PubMed,\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Finetuning improves significantly of the performance of the base model, RAFT consistently\n\n- utperforms the existing domain-specific finetuning method with or without RAG. This\nsuggests the need to train the model with context. We compare our model with LLaMA\nfinetuning receipes, and provide GPT-3.5 for reference.\n\n\nPubMed HotPot HuggingFace Torch Hub TensorFlow\n\n\nGPT-3.5 + RAG 71.60 **41.5** 29.08 60.21 65.59\n\n\nLLaMA2-7B 56.5 0.54 0.22 0 0\n\nLLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06\n\nDSF 59.7 6.38 61.06 84.94 86.56\n\nDSF + RAG 71.6 4.41 42.59 82.80 60.29\n\n\nRAFT (LLaMA2-7B) **73.30** 35.28 **74.00** **84.95** **86.86**\n\n\n**Datasets** In our experiments, we use the following datasets to evaluate our model and\nall baselines. We", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_4050", "chunk_text": ") **73.30** 35.28 **74.00** **84.95** **86.86**\n\n\n**Datasets** In our experiments, we use the following datasets to evaluate our model and\nall baselines. We selected these datasets to represent both popular and diverse domains\nincluding Wikipedia, Coding/API documents, and question-answering on medical documents. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and\nHotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,\nmainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,\nand TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.\nThese benchmarks measure how to generate the correct, functional, and executable API\ncalls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering\ndataset tailored only for biomedical-research question-answering. It mainly focuses on\nanswering medical and biology questions based on a given set of documents. We would\n\n\n5\n\n\nPreprint, Under Review\n\n\nlike to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas\nthe latter two domains are on domain-specific documents.\n\n\n**Baselines** We consider the following baselines for our experiments:\n\n\n   - LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used\ninstruction-finetuned model for QA tasks, where we provide clearly written instructions, but no reference documentation.\n\n   - LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,\nexcept here we include reference documents. This is a popular technique when\ndealing with domain-specific QA tasks.\n\n   - Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervisedfinetuning, without documents in context. We find that its mostly useful to align\nthe answering style of the model as well as get familiar with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_4500", "chunk_text": " with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does not know, it can still refer to the context.\n\n\n4.1 Results\n\n\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\n\n- utperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes/no question, we don\u2019t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.\n\n\nOverall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its\nanswering style not aligning with the ground truth. By applying domain-specific tuning,\nwe significantly enhance its performance. This process enables the model to learn and adopt\nthe appropriate style of answering. However, introducing RAG to a domain-specifically\nfine-tuned (DSF) model doesn\u2019t invariably lead to better outcomes. This might indicate that\nthe model lacks training in context processing and extracting useful information from it. By\nincorporating our method, RAFT, we train the model not only to match its answering style\nwith that required but also to improve its document processing capabilities. Consequently,\n\n- ur approach outperforms all others.\n\n\n4.2 Effect of CoT\n\n\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\nin enhancing the model\u2019s performance. As indicated in Table 2, simply providing the answer\nto a question", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_4950", "chunk_text": " all others.\n\n\n4.2 Effect of CoT\n\n\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\nin enhancing the model\u2019s performance. As indicated in Table 2, simply providing the answer\nto a question may not always be adequate. This approach can lead to a rapid decrease\nin loss, resulting in the model beginning to overfit. Incorporating a reasoning chain that\nnot only guides the model to the answer but also enriches the model\u2019s understanding can\nimprove the overall accuracy and prevent overfitting to concise answers. In our experiments,\nintegrating the Chain-of-Thought significantly enhances training robustness. We employ\nGPT-4-1106 to generate our Chain-of-Thought prompts and include an example of the\nprompt we used in Figure 3.\n\n\n4.3 Qualitative Analysis\n\n\nTo illustrate the potential advantages of RAFT over the domain-specifically fine-tuned\n(DSF) approach, we present a comparative example in Figure 4. This example qualitatively\n\n\n6\n\n\nPreprint, Under Review\n\n\nTable 2: **Ablation on Chain-of-Thought** : The numbers of RAFT and RAFT without\nCoT. Results on various datasets show that adding CoT can significantly improve the\nperformance of the finetuned model. With a gains of 9.66% and 14.93% in the Hotpot QA\nand HuggingFace datasets respectively.\n\n\nPubMed HotpotQA HuggingFace Torch Hub TensorFlow\n\n\nRAFT w.o CoT 68.30 25.62 59.07 **86.56** 83.21\n\nRAFT **73.30** **35.28** **74.00** 84.95 **86.86**\n\n\ndemonstrates a scenario where the DSF model becomes confused by a question asking for\nthe identity of a screenwriter. Instead of providing the correct name, it mistakenly cites one\n\n- f the films written by the screenwriter. In contrast, the RAFT model accurately answers the\nquestion. This discrepancy suggests that training a model solely with question-answer pairs\nmay impair its ability to derive relevant context from provided documents. The comparison\nunderscores the importance of incorporating both standard instructional tuning and context\ncomprehension into the training dataset to preserve and enhance the model\u2019s ability to\nprocess text effectively.\n\n\nFigure 4: **Comparison of RAFT and DSF** : On the", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_5400", "chunk_text": "underscores the importance of incorporating both standard instructional tuning and context\ncomprehension into the training dataset to preserve and enhance the model\u2019s ability to\nprocess text effectively.\n\n\nFigure 4: **Comparison of RAFT and DSF** : On the HotPot QA dataset, we can see that DSF\nmodel extracts the wrong information from the context when the question is asking who is\nthe screen writer and it answers a film name. RAFT manages to get the accurate results .\n\n\n4.4 Should we train the LLM always with the golden context for RAG?\n\n\nIn our exploration of whether large language models (LLMs) should always be trained with\nthe golden context for Retrieval-Augmented Generation (RAG), we address a key question:\nwhat proportion (p%) of the training data should include golden documents? Intuitively,\n\n- ne might assume that for effective training in reading and extracting information from\ncontext (e.g., RAG tasks), the golden document should always be included during training\n(P = 100%). However, our findings challenge this assumption: incorporating a portion of\nthe training data without the golden document in the context (P = 80%) appears to enhance\nthe model\u2019s performance on RAG tasks.\n\n\n7\n\n\nPreprint, Under Review\n\n\n\n\n\n\n\n\n\n\n|Test Domain: NQ|Col2|Col3|ain: NQ|Col5|Col6|\n|---|---|---|---|---|---|\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ|ain: NQ|ain: NQ|ain: NQ|\n|0<br>20<br>40<br>60<br>80<br>100<br>", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_5850", "chunk_text": ">0.45<br>Final Accuracy<br>Test Domain: NQ|ain: NQ|ain: NQ|ain: NQ|\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ||||||\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ||||||\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ||||||\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ||||||\n\n\n|Test Domain: TQA<br>0.65<br>Accuracy<br>0.60<br>Final<br>0.55<br>0.50<br>0 20 40 60 80 100|Col2|Col3|Domain|: TQA|Col6|Col7|\n|---|---|---|---|---|---|---|\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.50<br>0.55<br>0.60<br>0.65<br>Final Accuracy<br>Test Domain: TQA|||||||\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.50<br>0.55<br>0.60<br>0.65<br>Final Accuracy<br>Test Domain: TQA|||||||\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.50<br>0.55<br>0.60<br>0.65<br>Final Accuracy<br>Test Domain:", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_6300", "chunk_text": "|\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.50<br>0.55<br>0.60<br>0.65<br>Final Accuracy<br>Test Domain: TQA|||||||\n\n\n|Test Domain: HoPo<br>0.60<br>Accuracy<br>0.55<br>0.50 Final<br>0.45<br>0.40<br>0 20 40 60 80 100|Col2|Col3|Domain:|HoPo|Col6|Col7|\n|---|---|---|---|---|---|---|\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Final Accuracy<br>Test Domain: HoPo|||||||\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Final Accuracy<br>Test Domain: HoPo|||||||\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.40<br>0.45<br>0.50<br>0.55<br>0.60<br>Final Accuracy<br>Test Domain: HoPo|||||||\n\n\n\nFigure 5: **How many golden documents to involve?** We study the hyperparameter P%\nwhere it indicates how much portion of training data is with golden document. Results\n\n- n NQ, TQA and HotpotQA suggest that mixing some amount of data that the golden\ndocument is not put in the context is helpful for in-domain RAG.\n\n\nFigure 5 presents our investigation into the hyperparameter P%, which represents the\npercentage of training instances that should include golden documents. We find that the\n\n- ptimal proportion varies across datasets, with P% ranging from 40%, 60%, and 100%. This\nindicates that training your LLM without the correct corresponding context at times can be\nbeneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_6750", "chunk_text": " LLM without the correct corresponding context at times can be\nbeneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we maintain this format by providing the golden document with four distractors.\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of\ntraining data without the golden documents in the context proves to be advantageous.\n\n\n**5** **RAFT Generalizes to Top-K RAG**\n\n\nWe now study another important problem: How does the number of distractor documents\nin RAFT affect the model\u2019s performance when augmented with top-k RAG results during\nevaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text\n(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is\nparticularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to\nensure high recall. Such a scenario necessitates the model to have the ability to discern and\ndisregard irrelevant content, focusing solely on pertinent information.\n\n\n5.1 Making Model Robust to top-K RAG\n\n\nTo tackle the challenge of enhancing large language models\u2019 (LLMs) ability to sift through\nirrelevant text within the retrieval pipeline, our analysis revealed that training solely with\ngolden (highly relevant) documents can inadvertently diminish the model\u2019s ability to discern and disregard irrelevant information. To address this, our algorithm, RAFT, adopts\na strategy that integrates golden documents with a mix of irrelevant ones. This method\n- logy prompts us to investigate the ideal fraction of distractor (irrelevant) documents to\nincorporate throughout the training process and to assess how well this training approach\nadapts to different volumes of documents encountered by the Retrieval-Augmented Generation (RAG) during the test phase. Our aim is to refine the balance between relevant\nand irrelevant information to strenghten the model\u2019s efficiency in identifying and utilizing\npertinent content. Notice that Sec 4.4 looked what what P% of training data should include\ndistractors, while in this section, we study test-time scenarios.\n\n\n**Training with Distractor Documents** To enhance the robustness of LLMs against irrelevant\ntext in retrieved documents, we adopted a fin", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_7200", "chunk_text": " of training data should include\ndistractors, while in this section, we study test-time scenarios.\n\n\n**Training with Distractor Documents** To enhance the robustness of LLMs against irrelevant\ntext in retrieved documents, we adopted a finetuning approach that incorporates both\ngolden (highly relevant) documents and distractor (irrelevant) documents. The model was\ntrained with varying numbers of distractor documents, but consistently evaluated using\nthe top-3 documents obtained from the retriever - not to be confused with _p_ . Our findings,\ndetailed in Fig. 6, reveal that finetuning with only the golden document frequently results in\ninferior performance compared to configurations that include a greater number of distractor\ndocuments. As we can see in the figure, the better performance for Natural Questions is\n\n\n8\n\n\nPreprint, Under Review\n\n\n\n\n\n\n\n\n\n\n|0.32 Natural Questions Train D*|Col2|Natu|ural Questi|ions Trai|in D*|\n|---|---|---|---|---|---|\n|2<br>4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.22<br>0.24<br>0.26<br>0.28<br>0.30<br>Final Accuracy<br>Train D*<br>~~Train D* + 1D~~<br>Train D* + 2D<br>~~Train D* + 3D~~||||Trai<br>~~Trai~~|n D*<br>~~n D* + 1D~~|\n|2<br>4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.22<br>0.24<br>0.26<br>0.28<br>0.30<br>Final Accuracy<br>Train D*<br>~~Train D* + 1D~~<br>Train D* + 2D<br>~~Train D* + 3D~~||||Trai<br>~~Trai~~|n D* + 2D<br>~~n D* + 3D~~|\n|2<br>4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.22<br>0.24<br>0.26<br>0.28<br>0.30<br>Final Accuracy<br>Train D", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_7650", "chunk_text": ">4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.22<br>0.24<br>0.26<br>0.28<br>0.30<br>Final Accuracy<br>Train D*<br>~~Train D* + 1D~~<br>Train D* + 2D<br>~~Train D* + 3D~~||||||\n|2<br>4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.22<br>0.24<br>0.26<br>0.28<br>0.30<br>Final Accuracy<br>Train D*<br>~~Train D* + 1D~~<br>Train D* + 2D<br>~~Train D* + 3D~~||||||\n\n\n|0 .250 Train D*<br>TrainD*+1D<br>0 .225 Train D* + 2D FinalAccuracy<br>TrainD*+3D<br>0200<br>.<br>0175<br>.<br>0150<br>.<br>0125<br>.<br>2 4 6 8 1<br>#T tD t T k|Col2|Col3|Col4|Trai<br>Trai|nD*<br>nD*+1D|\n|---|---|---|---|---|---|\n|2<br>4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.125<br>0.150<br>0.175<br>0.200<br>0.225<br>0.250<br>Final Accuracy<br>~~Train D*~~<br>Train D* + 1D<br>~~Train D* + 2D~~<br>Train D* + 3D||||~~Trai~~<br>Trai|~~n D* + 2D~~<br>n D* + 3D|\n|2<br>4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.125<br>0.150<br>0.175<br>0.200<br>0.225<br>0.250<br>Final Accuracy<br>~~Train D*~~<br>Train D* + 1D<br>~~Train D*", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_8100", "chunk_text": "0.150<br>0.175<br>0.200<br>0.225<br>0.250<br>Final Accuracy<br>~~Train D*~~<br>Train D* + 1D<br>~~Train D* + 2D~~<br>Train D* + 3D||||||\n|2<br>4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.125<br>0.150<br>0.175<br>0.200<br>0.225<br>0.250<br>Final Accuracy<br>~~Train D*~~<br>Train D* + 1D<br>~~Train D* + 2D~~<br>Train D* + 3D||||||\n\n\n\nFigure 6: **Test-Time Documents Varying** : To analyze how robust RAFT is to varying number\n\n- f test-time documents, we study three domains \u2013 NQ, Trivia QA and HotPot QA. In NQ,\nwe find that training with 4 documents leads to optimal performance, and this changes to 3\nand 2 for for Trivia QA and HotPot QA respectively. However, we see that training with\n\n- nly _golden_ documents leads to poor performance.\n\n\ntraining with _D_ _[\u2217]_ + 3 _D_ and it is _D_ _[\u2217]_ + 1 _D_ documents with Hotpot QA. This insight has been\nparticularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ\na training setup consisting of one golden document alongside four distractor documents.\n\n\n**Generalization to a variable number of test-time documents.** We extended our research\nto examine the impact of different quantities of test-time documents on the model\u2019s performance. Specifically, our experiments focused on assessing how models, trained with\nvarying numbers of distractor documents, respond to changes in the number of documents\npresented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distractor documents during training indeed makes the model more resilient to fluctuations in the\nnumber of documents encountered during testing. This ability to maintain consistent performance despite variations in test-time document numbers further validates the robustness of\n\n- ur approach, RAFT . This finding underscores the importance of a well-calibrated training\nenvironment to prepare the model for a range of", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_8550", "chunk_text": " testing. This ability to maintain consistent performance despite variations in test-time document numbers further validates the robustness of\n\n- ur approach, RAFT . This finding underscores the importance of a well-calibrated training\nenvironment to prepare the model for a range of scenarios it may encounter in real-world.\n\n\n**6** **Related Works**\n\n\n**Retrieval-Augmented Language Models** Retrieval-Augmented Language Models (RALMs)\nenhance LLMs by integrating a retrieval module that sources relevant information from\nexternal knowledge bases, significantly improving performance across various NLP tasks,\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,\n2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;\nWang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,\n2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,\ntreating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the\ndecoder-only architecture to include retrieved texts and conducts pre-training from scratch.\nkNN-LM (Khandelwal et al., 2019) interpolates between the LM\u2019s next token distribution\nand distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram\net al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or\nfine-tuned retriever.\n\n\n**Memorization** A key question around large neural language models is whether they truly\n\u201cunderstand\u201d text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern\nmemorization (Carlini et al., 2019; T\u00e4nzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;\n2022) develop methodologies to quantify the extent of memorization in neural models.\n(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_9000", "chunk_text": " et al., 2019;\n2022) develop methodologies to quantify the extent of memorization in neural models.\n(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization\nimpacts the models\u2019 generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)\ndemonstrated the ability of language models to memorize and regurgitate training data,\nraising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).\n\n\n**Finetuning for RAG** More recently, several papers have been exploring the idea of finetuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu\n\n\n9\n\n\nPreprint, Under Review\n\n\net al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning\ndataset for RAG and train a model to perform well on these tasks. In particular, in their\nsettings, at test time, the domain or documents can be different than the training time;\nwhereas our paper studies a slightly opposite scenario where we only care about testing the\nLLM on the same set of documents.\n\n\n**7** **Conclusion**\n\n\nRAFT is a training strategy designed to enhance the model\u2019s performance in answering\nquestions within a specific domain, in \"open-book\" settings. We highlight several crucial\ndesign decisions, such as training the model alongside distractor documents, organizing the\ndataset so a portion lacks golden documents in their context, and formulating answers in a\nchain-of-thought manner with direct quotations from the relevant text. Our evaluations on\nPubMed, HotpotQA, and Gorilla API Bench underline RAFT\u2019s significant potential.\n\n\n**References**\n\n\nAnthropic. Prompt engineering for claude\u2019s long context window. 2023.\n\n\nAsai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Self-rag: Learning to retrieve, generate,\nand critique through self-reflection. _arXiv preprint arXiv:2310.11511_, 2023.\n\n\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican,", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_9450", "chunk_text": " _arXiv preprint arXiv:2310.11511_, 2023.\n\n\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B., Damoc, B., Clark, A., et al. Improving language models by\nretrieving from trillions of tokens. In _International conference on machine learning_, pp.\n2206\u20132240. PMLR, 2022.\n\n\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,\nShyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. _Advances_\n_in neural information processing systems_, 33:1877\u20131901, 2020.\n\n\nCarlini, N., Liu, C., Erlingsson, \u00da., Kos, J., and Song, D. The secret sharer: Evaluating and\ntesting unintended memorization in neural networks. In _28th USENIX Security Symposium_\n_(USENIX Security 19)_, pp. 267\u2013284, 2019.\n\n\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A.,\nBrown, T., Song, D., Erlingsson, U., et al. Extracting training data from large language\nmodels. In _30th USENIX Security Symposium (USENIX Security 21)_, pp. 2633\u20132650, 2021.\n\n\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. Quantifying\nmemorization across neural language models. In _The Eleventh International Conference on_\n_Learning Representations_, 2022.\n\n\nDernoncourt, F. and Lee, J. Y. Pubmed 200k rct: a dataset for sequential sentence classification\nin medical abstracts. _arXiv preprint arXiv:1710.06071_, 2017.\n\n\nFeldman, V. Does", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_9900", "chunk_text": ". Y. Pubmed 200k rct: a dataset for sequential sentence classification\nin medical abstracts. _arXiv preprint arXiv:1710.06071_, 2017.\n\n\nFeldman, V. Does learning require memorization? a short tale about a long tail. In _Proceedings_\n\n_of the 52nd Annual ACM SIGACT Symposium on Theory of Computing_, pp. 954\u2013959, 2020.\n\n\nGuu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language model\npre-training. In _International conference on machine learning_, pp. 3929\u20133938. PMLR, 2020.\n\n\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J.,\nJoulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented\nlanguage models. _Journal of Machine Learning Research_, 24(251):1\u201343, 2023. URL `[http:](http://jmlr.org/papers/v24/23-0037.html)`\n`[//jmlr.org/papers/v24/23-0037.html](http://jmlr.org/papers/v24/23-0037.html)` .\n\n\nJin, Q., Dhingra, B., Liu, Z., Cohen, W. W., and Lu, X. Pubmedqa: A dataset for biomedical\nresearch question answering. _arXiv preprint arXiv:1909.06146_, 2019.\n\n\n10\n\n\nPreprint, Under Review\n\n\nJoshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. _arXiv preprint arXiv:1705.03551_,\n2017.\n\n\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating training data mitigates privacy risks\nin language models. In _International Conference on Machine Learning_, pp. 10697\u201310707.\nPMLR, 2022.\n\n\nKhandelwal, U., Levy, O., Jurafsky,", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_10350", "chunk_text": " data mitigates privacy risks\nin language models. In _International Conference on Machine Learning_, pp. 10697\u201310707.\nPMLR, 2022.\n\n\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memorization: Nearest neighbor language models. _arXiv preprint_\n_arXiv:1911.00172_, 2019.\n\n\nKwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D.,\nPolosukhin, I., Devlin, J., Lee, K., et al. Natural questions: a benchmark for question\nanswering research. _Transactions of the Association for Computational Linguistics_, 7:453\u2013466,\n2019.\n\n\nLazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N. Internet-augmented\nlanguage models through few-shot prompting for open-domain question answering.\n_arXiv preprint arXiv:2203.05115_, 2022.\n\n\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\u00fcttler, H., Lewis, M.,\nYih, W.-t., Rockt\u00e4schel, T., et al. Retrieval-augmented generation for knowledge-intensive\nnlp tasks. _Advances in Neural Information Processing Systems_, 33:9459\u20139474, 2020.\n\n\nLin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy,\nG., Lewis, M., et al. Ra-dit: Retrieval-augmented dual instruction tuning. _arXiv preprint_\n_arXiv:2310.01352_, 2023a.\n\n\nLin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy,\nG., Lewis, M., et al. Ra-dit: Retrieval-augmented dual instruction tuning. _arXiv pre", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_11250", "chunk_text": "2.\n\n\nRam, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K.,\nand Shoham, Y. In-context retrieval-augmented language models. _arXiv preprint_\n_arXiv:2302.00083_, 2023.\n\n\nShi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Sch\u00e4rli, N., and Zhou, D. Large\nlanguage models can be easily distracted by irrelevant context. In _International Conference_\n\n_on Machine Learning_, pp. 31210\u201331227. PMLR, 2023a.\n\n\nShi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., and Zettlemoyer, L.\nDetecting pretraining data from large language models. _arXiv preprint arXiv:2310.16789_,\n2023b.\n\n\n11\n\n\nPreprint, Under Review\n\n\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, V., Smith, N. A., Zettlemoyer, L., Yih, S.,\nand Lewis, M. In-context pretraining: Language modeling beyond document boundaries.\n_arXiv preprint arXiv:2310.10638_, 2023c.\n\n\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.\nReplug: Retrieval-augmented black-box language models. _arXiv preprint arXiv:2301.12652_,\n2023d.\n\n\nT\u00e4nzer, M., Ruder, S., and Rei, M. Memorisation versus generalisation in pre-trained language models. In _Proceedings of the 60th Annual Meeting of the Association for Computational_\n_Linguistics (Volume 1: Long Papers)_, pp. 7564\u20137578, 2022.\n\n\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le,\nQ", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2403.10131_raft_zhang:chunk_11700", "chunk_text": "7578, 2022.\n\n\nVu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung, Y.-H., Zhou, D., Le,\nQ., et al. Freshllms: Refreshing large language models with search engine augmentation.\n_arXiv preprint arXiv:2310.03214_, 2023.\n\n\nWang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B. Instructretro:\nInstruction tuning post retrieval-augmented pretraining. _arXiv preprint arXiv:2310.07713_,\n2023.\n\n\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H.\nSelf-instruct: Aligning language models with self-generated instructions. _arXiv preprint_\n_arXiv:2212.10560_, 2022.\n\n\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al.\nChain-of-thought prompting elicits reasoning in large language models. _Advances in_\n_Neural Information Processing Systems_, 35:24824\u201324837, 2022.\n\n\nWeston, J. and Sukhbaatar, S. System 2 attention (is something you might need too). _arXiv_\n_preprint arXiv:2311.11829_, 2023.\n\n\nXiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R.,\nSankararaman, K. A., Oguz, B., et al. Effective long-context scaling of foundation models.\n_arXiv preprint arXiv:2309.16039_, 2023.\n\n\nXu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E.,\nShoeybi, M., and Catanzaro, B. Retrieval meets long context large language", "token_count": 500, "metadata": {"arxiv_id": "2403.10131", "title": "RAFT: Adapting Language Model to Domain Specific RAG", "authors": ["Tianjun Zhang", "Shishir G. Patil", "Naman Jain", "Sheng Shen", "Matei Zaharia", "Ion Stoica", "Joseph E. Gonzalez"], "year": 2024, "url": "https://arxiv.org/pdf/2403.10131v2"}}
{"chunk_id": "2210.03629_react_yao:chunk_0", "chunk_text": "Published as a conference paper at ICLR 2023\n\n## REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\n\n\nShunyu Yao _[\u2217]_ [*,1], Jeffrey Zhao [2], Dian Yu [2], Nan Du [2], Izhak Shafran [2], Karthik Narasimhan [1], Yuan Cao [2]\n\n\n1Department of Computer Science, Princeton University\n2Google Research, Brain team\n1{shunyuy,karthikn}@princeton.edu\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\n\n\nABSTRACT\n\n\nWhile large language models (LLMs) have demonstrated impressive performance\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\nplan generation) have primarily been studied as separate topics. In this paper, we\nexplore the use of LLMs to generate both reasoning traces and task-specific actions\nin an interleaved manner, allowing for greater synergy between the two: reasoning\ntraces help the model induce, track, and update action plans as well as handle\nexceptions, while actions allow it to interface with and gather additional information\nfrom external sources such as knowledge bases or environments. We apply our\napproach, named ReAct, to a diverse set of language and decision making tasks\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\nimproved human interpretability and trustworthiness. Concretely, on question\nanswering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent\nissues of hallucination and error propagation in chain-of-thought reasoning by\ninteracting with a simple Wikipedia API, and generating human-like task-solving\ntrajectories that are more interpretable than baselines without reasoning traces.\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\nan absolute success rate of 34% and 10% respectively, while being prompted with\n\n     - nly one or two in-context examples.\n\n\n1 INTRODUCTION\n\n\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, ", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_450", "chunk_text": "\n\n     - nly one or two in-context examples.\n\n\n1 INTRODUCTION\n\n\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Consider the example of cooking up a dish in the kitchen. Between any two specific actions, we may\nreason in language in order to track progress (\u201cnow that everything is cut, I should heat up the pot of\nwater\u201d), to handle exceptions or adjust the plan according to the situation (\u201cI don\u2019t have salt, so let\nme use soy sauce and pepper instead\u201d), and to realize when external information is needed (\u201chow do\nI prepare dough? Let me search on the Internet\u201d). We may also act (open a cookbook to read the\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (\u201cWhat\ndish can I make right now?\u201d). This tight synergy between \u201cacting\u201d and \u201creasoning\u201d allows humans\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\nunseen circumstances or facing information uncertainties.\n\n\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\n\n\n_\u2217_\n[Work during Google internship. Projet page with code: https://react-lm.github.io/.](https://react-lm.github.io/)\n\n\n1\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n|\u0014 \u0003+RWVSRW\u00034$<br>4XHVWLRQ \u0003$VLGH\u0003IURP\u0003WKH\u0003$SSOH\u00035HPRWH\u000f\u0003ZKDW\u0003RWKHU\u0003GHYLFH<br>FDQ\u0003FRQWURO \u0003WKH\u0003SURJUDP\u0003$SSOH\u00035HPRWH\u0003ZDV\u0003RULJLQDOO\\<br>GHVLJQHG \u0003WR\u0003LQWHUDFW\u0003", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_900", "chunk_text": "QWURO \u0003WKH\u0003SURJUDP\u0003$SSOH\u00035HPRWH\u0003ZDV\u0003RULJLQDOO\\<br>GHVLJQHG \u0003WR\u0003LQWHUDFW\u0003ZLWK\"|Col2|Col3|\n|---|---|---|\n|**4XHVWLRQ**\u001d$VLGHIURPWKH$SSOH5HPRWH\u000fZKDWRWKHUGHYLFH<br>FDQFRQWUROWKHSURJUDP$SSOH5HPRWHZDVRULJLQDOO\\<br>GHVLJQHGWRLQWHUDFWZLWK\"<br>**\u000b\u0014\f+RWVSRW4$**|**\u000b\u0014\f+RWVSRW4$**|**\u000b\u0014\f+RWVSRW4$**|\n\n\n\n\n\n\n|\u0014D \u00036WDQGDUG<br>$QVZHU \u0003L3RG|Col2|Col3|\n|---|---|---|\n|**\u000b\u0014D\f6WDQGDUG**<br>**$QVZHU**L3RG|**\u000b\u0014D\f6WDQGDUG**|**\u000b\u0014D\f6WDQGDUG**|\n\n\n\n\n\n\n\n\n|\u0014F \u0003$FW\u00102QO\\\u0003<br>$FW\u0003\u0014 \u00036HDUFK>$SSOH\u00035HPRWH@<br>\u0003<br>2EV\u0003\u0014 \u00037KH\u0003$SSOH\u00035HPRWH\u0003LV \u0003D\u0003<br>UHPRWH\u0003FRQWURO \u0003\u00aa<br>$FW\u0003\u0015 \u00036HDUFK>)URQW\u00035RZ @<br>\u0003<br>2EV\u0003\u0015 \u0003&RXOG \u0003QRW\u0003ILQG \u0003>)URQW<br>\u0003<br>5RZ@\u0011\u00036LPLODU \u0003\u00aa<br>$FW\u0003\u0016 \u00036HDUFK>)URQW\u00035RZ<br>\u0003<br>VRIWZDUH @<br>\u0003<br>2EV\u0003\u0016 \u0003)URQW\u00035RZ\u0003LV\u0003D<br>\u0003<br>GLVFRQWLQXHG \u0003PHGL", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_1350", "chunk_text": ">VRIWZDUH @<br>\u0003<br>2EV\u0003\u0016 \u0003)URQW\u00035RZ\u0003LV\u0003D<br>\u0003<br>GLVFRQWLQXHG \u0003PHGLD\u0003FHQWHU<br>\u0003<br>VRIWZDUH\u0003\u00aa<br>$FW\u0003\u0017 \u0003)LQLVK>\\HV@|Col2|Col3|\n|---|---|---|\n|**\u000b\u0014F\f$FW\u00102QO\\**<br>**$FW\u00146HDUFK**>$SSOH5HPRWH@<br>**2EV\u0014**7KH$SSOH5HPRWHLV D<br>UHPRWHFRQWURO\u00aa<br>**$FW\u00156HDUFK**>)URQW5RZ @<br>**2EV\u0015**&RXOGQRWILQG>)URQW<br>5RZ@\u00116LPLODU\u001d\u00aa<br>**$FW\u00166HDUFK**>)URQW5RZ<br>\u000bVRIWZDUH\f @<br>**2EV\u0016**)URQW5RZLVD<br>GLVFRQWLQXHGPHGLDFHQWHU<br>VRIWZDUH\u00aa<br>**$FW\u0017)LQLVK**>\\HV@|**\u000b\u0014F\f$FW\u00102QO\\**|**\u000b\u0014F\f$FW\u00102QO\\**|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: (1) Comparison of 4 prompting methods, (a) Standard, (b) Chain-of-thought (CoT,\nReason Only), (c) Act-only, and (d) ReAct (Reason+Act), solving a HotpotQA (Yang et al., 2018)\nquestion. (2) Comparison of (a) Act-only and (b) ReAct prompting to solve an AlfWorld (Shridhar\net al., 2020b) game. In both domains, we omit in-context examples in the prompt, and only show task\nsolving trajectories generated by the model (Act, Thought) and the environment (Obs).\n\n\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\n2022). However, this \u201cchain-of-thought\u201d reasoning is", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_1800", "chunk_text": "solving trajectories generated by the model (Act, Thought) and the environment (Obs).\n\n\nanswers from questions in arithmetic, commonsense, and symbolic reasoning tasks (Wei et al.,\n2022). However, this \u201cchain-of-thought\u201d reasoning is a static black box, in that the model uses\nits own internal representations to generate thoughts and is not grounded in the external world,\nwhich limits its ability to reason reactively or update its knowledge. This can lead to issues like fact\nhallucination and error propagation over the reasoning process (Figure 1 (1b)). On the other hand,\nrecent work has explored the use of pre-trained language models for planning and acting in interactive\nenvironments (Ahn et al., 2022; Nakano et al., 2021; Yao et al., 2020; Huang et al., 2022a), with\na focus on predicting actions via language priors. These approaches usually convert multi-modal\n\n- bservations into text, use a language model to generate domain-specific actions or plans, and then\nuse a controller to choose or execute them. However, they do not employ language models to reason\nabstractly about high-level goals or maintain a working memory to support acting, barring Huang\net al. (2022b) who perform a limited form of verbal reasoning to reiterate spatial facts about the\ncurrent state. Beyond such simple embodied tasks to interact with a few blocks, there have not been\nstudies on how reasoning and acting can be combined in a synergistic manner for general task solving,\nand if such a combination can bring systematic benefits compared to reasoning or acting alone.\n\n\nIn this work, we present ReAct, a general paradigm to combine reasoning and acting with language\nmodels for solving diverse language reasoning and decision making tasks (Figure 1). ReAct\nprompts LLMs to generate both verbal reasoning traces and actions pertaining to a task in an\ninterleaved manner, which allows the model to perform dynamic reasoning to create, maintain, and\nadjust high-level plans for acting (reason to act), while also interact with the external environments\n(e.g. Wikipedia) to incorporate additional information into reasoning (act to reason).\n\n\n2\n\n\nPublished as a conference paper at ICLR 2023\n\n\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\nquestion answering (HotPotQA, Yang et al., 2018), fact verification (Fever", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_2250", "chunk_text": " as a conference paper at ICLR 2023\n\n\nWe conduct empirical evaluations of ReAct and state-of-the-art baselines on four diverse benchmarks:\nquestion answering (HotPotQA, Yang et al., 2018), fact verification (Fever, Thorne et al., 2018),\ntext-based game (ALFWorld, Shridhar et al., 2020b), and webpage navigation (WebShop, Yao\net al., 2022). For HotPotQA and Fever, with access to a Wikipedia API that the model can interact\nwith, ReAct outperforms vanilla action generation models while being competitive with chain-ofthought reasoning (CoT) (Wei et al., 2022). The best approach overall is a combination of ReAct\nand CoT that allows for the use of both internal knowledge and externally obtained information\nduring reasoning. On ALFWorld and WebShop, two or even one-shot ReAct prompting is able\nto outperform imitation or reinforcement learning methods trained with 10 [3] _\u223c_ 10 [5] task instances,\nwith an absolute improvement of 34% and 10% in success rates respectively. We also demonstrate\nthe importance of sparse, versatile reasoning in decision making by showing consistent advantages\n\n- ver controlled baselines with actions only. Besides general applicability and performance boost,\nthe combination of reasoning and acting also contributes to model interpretability, trustworthiness,\nand diagnosability across all domains, as humans can readily distinguish information from model\u2019s\ninternal knowledge versus external environments, as well as inspect reasoning traces to understand\nthe decision basis of model actions.\n\n\nTo summarize, our key contributions are the following: (1) we introduce ReAct, a novel promptbased paradigm to synergize reasoning and acting in language models for general task solving; (2) we\nperform extensive experiments across diverse benchmarks to showcase the advantage of ReAct in a\nfew-shot learning setup over prior approaches that perform either reasoning or action generation in\nisolation; (3) we present systematic ablations and analysis to understand the importance of acting in\nreasoning tasks, and reasoning in interactive tasks; (4) we analyze the limitations of ReAct under the\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial finetuning\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\nReAct to train and operate on more tasks", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_2700", "chunk_text": "\nprompting setup (i.e. limited support of reasoning and acting behaviors), and perform initial finetuning\nexperiments showing the potential of ReAct to improve with additional training data. Scaling up\nReAct to train and operate on more tasks and combining it with complementary paradigms like\nreinforcement learning could further unlock the potential of large language models.\n\n\n2 REAC T: SYNERGIZING RE ASONING + ACT ING\n\n\nConsider a general setup of an agent interacting with an environment for task solving. At time\nstep _t_, an agent receives an observation _ot \u2208O_ from the environment and takes an action _at \u2208A_\nfollowing some policy _\u03c0_ ( _at|ct_ ), where _ct_ = ( _o_ 1 _, a_ 1 _, \u00b7 \u00b7 \u00b7, ot\u2212_ 1 _, at\u2212_ 1 _, ot_ ) is the _context_ to the agent.\nLearning a policy is challenging when the mapping _ct \ufffd\u2192_ _at_ is highly implicit and requires extensive\ncomputation. For example, the agent shown in Figure 1(1c) is unable to generate the correct final\naction (Act 4) to finish the QA task as it requires complex reasoning over the trajectory context\n(Question, Act 1-3, Obs 1-3). Similarly, the agent shown in Figure 1(2a) fails to comprehend from the\ncontext that sinkbasin 1 does not contain peppershaker 1, thus keep producing hallucinating actions.\n\n\nThe idea of ReAct is simple: we augment the agent\u2019s action space to _A_ [\u02c6] = _A \u222aL_, where _L_ is the\nspace of language. An action \u02c6 _at \u2208L_ in the language space, which we will refer to as a _thought_ - r a\n_reasoning trace_, does not affect the external environment, thus leading to no observation feedback.\nInstead, a thought \u02c6 _at_ aims to compose useful information by reasoning over the current context _ct_,\nand update the context _ct_ +1 = ( _ct,_ \u02c6 _at_ ) to support future reasoning or acting. As shown in Figure 1,\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\n(2b, Act 1; 1d", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_3150", "chunk_text": " _at_ ) to support future reasoning or acting. As shown in Figure 1,\nthere could be various types of useful thoughts, e.g. decomposing task goals and create action plans\n(2b, Act 1; 1d, Thought 1), injecting commonsense knowledge relevant to task solving (2b, Act 1),\nextracting important parts from observations (1d, Thought2, 4), track progress and transit action plans\n(2b, Act 8), handle exceptions and adjust action plans (1d, Thought 3), and so on.\n\n\nHowever, as the language space _L_ is unlimited, learning in this augmented action space is difficult\nand requires strong language priors. In this paper, we mainly focus on the setup where a frozen\nlarge language model, PaLM-540B (Chowdhery et al., 2022) [1], is prompted with few-shot in-context\nexamples to generate both domain-specific actions and free-form language thoughts for task solving\n(Figure 1 (1d), (2b)). Each in-context example is a human trajectory of actions, thoughts, and\nenvironment observations to solve a task instance (see Appendix C). For the tasks where reasoning is\n\n- f primary importance (Figure 1(1)), we alternate the generation of thoughts and actions so that the\ntask-solving trajectory consists of multiple thought-action-observation steps. In contrast, for decision\nmaking tasks that potentially involve a large number of actions (Figure 1(2)), thoughts only need to\n\n\n1We show some GPT-3 (Brown et al., 2020) results in Appendix A.1, which outperforms PaLM-540B.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2023\n\n\nappear sparsely in the most relevant positions of a trajectory, so we let the language model decide the\nasynchronous occurrence of thoughts and actions for itself.\n\n\nSince decision making and reasoning capabilities are integrated into a large language model, ReAct\nenjoys several unique features: **A) Intuitive and easy to design** : Designing ReAct prompts is\nstraightforward as human annotators just type down their thoughts in language on top of their actions\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\nprompt design for each task in Sections 3 and 4. **B)", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_3600", "chunk_text": " their thoughts in language on top of their actions\ntaken. No ad-hoc format choice, thought design, or example selection is used in this paper. We detail\nprompt design for each task in Sections 3 and 4. **B) General and flexible** : Due to the flexible thought\nspace and thought-action occurrence format, ReAct works for diverse tasks with distinct action\nspaces and reasoning needs, including but not limited to QA, fact verification, text game, and web\nnavigation. **C) Performant and robust** : ReAct shows strong generalization to new task instances\nwhile learning solely from one to six in-context examples, consistently outperforming baselines with\n\n- nly reasoning or acting across different domains. We also show in Section 3 additional benefits\nwhen finetuning is enabled, and in Section 4 how ReAct performance is robust to prompt selections.\n**D) Human aligned and controllable** : ReAct promises an interpretable sequential decision making\nand reasoning process where humans can easily inspect reasoning and factual correctness. Moreover,\nhumans can also control or correct the agent behavior on the go by thought editing, as shown in\nFigure 5 in Section 4.\n\n\n3 KNOWLEDGE-INTENSIVE REASONING TASKS\n\n\nWe begin with knowledge-intensive reasoning tasks like multi-hop question answering and fact\nverification. As shown in Figure 1(1d), by interacting with a Wikipedia API, ReAct is able to\nretrieve information to support reasoning, while also use reasoning to target what to retrieve next,\ndemonstrating a synergy of reasoning and acting.\n\n\n3.1 SETUP\n\n\n**Domains** We consider two datasets challenging knowledge retrieval and reasoning: (1) HotPotQA (Yang et al., 2018), a multi-hop question answering benchmark that requires reasoning\n\n- ver two or more Wikipedia passages, and (2) FEVER (Thorne et al., 2018), a fact verification\nbenchmark where each claim is annotated SUPPORTS, REFUTES, or NOT ENOUGH INFO, based\n\n- n if there exists a Wikipedia passage to verify the claim. In this work, we operate in a question-only\nsetup for both tasks, where models only receive the question/claim as input without access to support\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\nan external environment to support reasoning.\n\n\n**Action Space** We design a simple Wikipedia web API with", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_4050", "chunk_text": " only receive the question/claim as input without access to support\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\nan external environment to support reasoning.\n\n\n**Action Space** We design a simple Wikipedia web API with three types of actions to support\ninteractive information retrieval: (1) **search** [entity], which returns the first 5 sentences from\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\nWikipedia search engine, (2) **lookup** [string], which would return the next sentence in the page\ncontaining string, simulating Ctrl+F functionality on the browser. (3) **finish** [answer], which\nwould finish the current task with answer. We note that this action space mostly can only retrieve a\nsmall part of a passage based on exact passage name, which is significantly weaker than state-of-theart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\nand force models to retrieve via explicit reasoning in language.\n\n\n3.2 METHODS\n\n\n**ReAct Prompting** For HotpotQA and Fever, we randomly select 6 and 3 cases [2] from the training\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\nthought), where free-form thoughts are used for various purposes. Specifically, we use a combination\n\n- f thoughts that decompose questions (\u201cI need to search x, find y, then find z\u201d), extract information\nfrom Wikipedia observations (\u201cx was started in 1844\u201d, \u201cThe paragraph does not tell x\u201d), perform\ncommonsense (\u201cx is not y, so z must instead be...\u201d) or arithmetic reasoning (\u201c1844 < 1989\u201d), guide\n\n\n2We find more examples do not improve performance.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n**HotpotQA** **Fever**\n**Prompt Method** _[a]_\n(EM) (Acc)\n\n\nStandard 28.7 57.1\nCoT (Wei et al., 2022) 29.4 56.3\nCoT-SC (Wang et al., 2022a) 33.4 60.4\n\n\nAct 25.7 58.9\n\nReAct ", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_4500", "chunk_text": " 2022) 29.4 56.3\nCoT-SC (Wang et al., 2022a) 33.4 60.4\n\n\nAct 25.7 58.9\n\nReAct 27.4 60.9\n\nCoT-SC _\u2192_ ReAct 34.2 **64.6**\n\nReAct _\u2192_ CoT-SC **35.1** 62.0\n\n\n**Supervised SoTA** _[b]_ 67.5 89.5\n\n\nTable 1: PaLM-540B prompting results on\nHotpotQA and Fever.\n\n\n_a_ HotpotQA EM is 27.1, 28.9, 33.8 for Standard, CoT,\nCoT-SC in Wang et al. (2022b).\n_b_\n(Zhu et al., 2021; Lewis et al., 2020)\n\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|||Met|hod|\n|||~~CoT-S~~<br>ReAct <br>CoT~~-~~S<br>~~ReAct~~|~~C~~ ~~->~~ ~~ReAc~~<br> ~~-~~> CoT~~-~~S<br>C|\n\n\n\n\n\nFigure 2: PaLM-540B prompting results with respect to\nnumber of CoT-SC samples used.\n\n\n\nsearch reformulation (\u201cmaybe I can search/look up x instead\u201d), and synthesize the final answer (\u201c...so\nthe answer is x\u201d). See Appendix C for more details.\n\n\n**Baselines** We systematically ablate ReAct trajectories to build prompts for multiple baselines (with\nformats as Figure 1(1a-1c)): (a) **Standard prompting** (Standard), which removes all thoughts,\nactions, observations in ReAct trajectories. (b) **Chain-of-thought prompting** (CoT) (Wei et al.,\n2022), which removes actions and observations and serve as a reasoning-only baseline. We also\nbuild a self-consistency baseline (CoT-SC) (Wang et al., 2022a;b) by sampling 21 CoT trajectories\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\nconsistently boost performance over CoT. (c) **Acting-only", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_4950", "chunk_text": " 2022a;b) by sampling 21 CoT trajectories\nwith decoding temperature 0.7 during inference and adopting the majority answer, which is found to\nconsistently boost performance over CoT. (c) **Acting-only prompt** (Act), which removes thoughts\nin ReAct trajectories, loosely resembling how WebGPT (Nakano et al., 2021) interacts with the\nInternet to answer questions, though it operates on a different task and action space, and uses imitation\nand reinforcement learning instead of prompting.\n\n\n**Combining Internal and External Knowledge** As will be detail in Section 3.3, we observe that\nthe problem solving process demonstrated by ReAct is more factual and grounded, whereas CoT\nis more accurate in formulating reasoning structure but can easily suffer from hallucinated facts\n\n- r thoughts. We therefore propose to incorporate ReAct and CoT-SC, and let the model decide\nwhen to switch to the other method based on the following heuristics: A) **ReAct** _\u2192_ **CoT-SC** : when\nReAct fails to return an answer within given steps, back off to CoT-SC. We set 7 and 5 steps for\nHotpotQA and FEVER respectively as we find more steps will not improve ReAct performance [3] .\nB) **CoT-SC** _\u2192_ **ReAct** : when the majority answer among _n_ CoT-SC samples occurs less than _n/_ 2\ntimes (i.e. internal knowledge might not support the task confidently), back off to ReAct.\n\n\n**Finetuning** Due to the challenge of manually annotating reasoning traces and actions at scale,\nwe consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories\nwith correct answers generated by ReAct (also for other baselines) to finetune smaller language\nmodels (PaLM-8/62B) to decode trajectories (all thoughts, actions, observations) conditioned on\ninput questions/claims. More details are in Appendix B.1.\n\n\n3.3 RESULTS AND OBSERVATIONS\n\n\n**ReAct outperforms Act consistently** Table 1 shows HotpotQA and Fever results using PaLM540B as the base model with different prompting methods. We note that ReAct is better than Act\n\n- n both tasks, demonstrating the value", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_5400", "chunk_text": "ReAct outperforms Act consistently** Table 1 shows HotpotQA and Fever results using PaLM540B as the base model with different prompting methods. We note that ReAct is better than Act\n\n- n both tasks, demonstrating the value of reasoning to guide acting, especially for synthesizing the\nfinal answer, as shown in Figure 1 (1c-d). Fine-tuning results 3 also confirm the benefit of reasoning\ntraces for more informed acting.\n\n\n3Of all trajectories with correct final answers, those with 7 steps on HotpotQA and 5 steps on FEVER only\ntake up 0.84% and 1.33% respectively.\n\n\n5\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n\n|Col1|Type Definition ReAct CoT|\n|---|---|\n|Success|True positive<br>Correct reasoning trace and facts<br>94%<br>86%<br>Falsepositive<br>Hallucinated reasoning trace or facts<br>6%<br>14%|\n|Failure|Reasoning error<br>Wrong reasoning trace (including failing to recover from repetitive steps)<br>47%<br>16%<br>Search result error<br>Search return empty or does not contain useful information<br>23%<br>-<br>Hallucination<br>Hallucinated reasoning trace or facts<br>0%<br>56%<br>Label ambiguity<br>Right prediction but did not match the label precisely<br>29%<br>28%|\n\n\n\nTable 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their\npercentages in randomly selected examples studied by human.\n\n\n**ReAct vs. CoT** On the other hand, ReAct outperforms CoT on Fever (60.9 vs. 56.3) and slightly\nlags behind CoT on HotpotQA (27.4 vs. 29.4). Fever claims for SUPPORTS/REFUTES might only\ndiffer by a slight amount (see Appendix D.1), so acting to retrieve accurate and up-to-date knowledge\nis vital. To better understand the behavioral difference between ReAct and CoT on HotpotQA, we\nrandomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\nand CoT respectively (thus 200 examples in total), and manually", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_5850", "chunk_text": " behavioral difference between ReAct and CoT on HotpotQA, we\nrandomly sampled 50 trajectories with correct and incorrect answers (judged by EM) from ReAct\nand CoT respectively (thus 200 examples in total), and manually labeled their success and failure\nmodes in Table 2. Some key observations are as follows:\n\n\nA) **Hallucination is a serious problem for CoT**, resulting in much higher false positive rate than\nReAct (14% vs. 6%) in success mode, and make up its major failure mode (56%). In contrast, the\nproblem solving trajectory of ReActis more grounded, fact-driven, and trustworthy, thanks to the\naccess of an external knowledge base.\n\n\nB) **While interleaving reasoning, action and observation steps improves ReAct\u2019s grounded-**\n**ness and trustworthiness, such a structural constraint also reduces its flexibility in formulating**\n**reasoning steps**, leading to more reasoning error rate than CoT. we note that there is one frequent\nerror pattern specific to ReAct, in which the model repetitively generates the previous thoughts and\nactions, and we categorize it as part of \u201creasoning error\u201d as the model fails to reason about what the\nproper next action to take and jump out of the loop [4] .\n\n\nC) **For ReAct, successfully retrieving informative knowledge via search is critical.** Noninformative search, which counts for 23% of the error cases, derails the model reasoning and gives\nit a hard time to recover and reformulate thoughts. This is perhaps an expected trade-off between\nfactuality and flexibility, which motivates our proposed strategies of combining two methods.\n\n\nWe provide examples for each success and failure modes in Appendix E.1. We also find some\nHotpotQA questions may contain outdated answer labels, see Figure 4 for example.\n\n\n**ReAct + CoT-SC perform best for prompting LLMs** Also shown in Table 1, the best prompting\nmethod on HotpotQA and Fever are ReAct _\u2192_ CoT-SC and CoT-SC _\u2192_ ReAct respectively.\nFurthermore, Figure 2 shows how different methods perform with respect to the number of CoT-SC\nsamples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both\nsignificantly and consistently outperform CoT-SC across different number of samples,", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_6300", "chunk_text": " with respect to the number of CoT-SC\nsamples used. While two ReAct + CoT-SC methods are advantageous at one task each, they both\nsignificantly and consistently outperform CoT-SC across different number of samples, reaching\nCoT-SC performance with 21 samples using merely 3-5 samples. These results indicate the value of\nproperly combining model internal knowledge and external knowledge for reasoning tasks.\n\n\n**ReAct performs best for fine-tuning** Figure 3 shows the scaling effect of prompting/finetuning\nfour methods (Standard, CoT, Act, ReAct) on HotpotQA. With PaLM-8/62B, prompting ReAct\nperforms worst among four methods due to the difficulty to learn both reasoning and acting from\nin-context examples. However, when finetuned with just 3,000 examples, ReAct becomes the best\nmethod among the four, with PaLM-8B finetuned ReAct outperforming all PaLM-62B prompting\nmethods, and PaLM-62B finetuned ReAct outperforming all 540B prompting methods. In contrast,\nfinetuning Standard or CoT is significantly worse than finetuning ReAct or Act for both PaLM8/62B, as the former essentially teaches models to memorize (potentially halluincated) knowledge\nfacts, and the latter teaches models how to (reason and) act to access information from Wikipedia, a\nmore generalizable skill for knowledge reasoning. As all prompting methods are still significantly\nfar from domain-specific state-of-the-art approaches (Table 1), we believe finetuning with more\nhuman-written data might be a better way to unleash the power of ReAct.\n\n\n4We suspect that this could be due to the sub-optimal greedy decoding procedure, and future work using\nbetter decoding (e.g. beam search) might help address this issue.\n\n\n6\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n\n\n\n\n|learning = prompt|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||||\n||||||||||||||||\n", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_6750", "chunk_text": "Col13|Col14|Col15|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||||\n||||||||||||||||\n||||||||||||||||\n||||||||||||||||\n||||8|8|8|8|8|8|8|8|8|8|0b|0b|\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Method<br>Standard|\n|---|---|---|---|---|---|---|---|---|\n|||||||||~~CoT~~<br>Act<br>ReAct|\n||||||||||\n||||8|8|8|8|8|8|\n\n\n\nFigure 3: Scaling results for prompting and finetuning on HotPotQA with ReAct (ours) and baselines.\n\n\n4 DECISION MAKING TASKS\n\n\nWe also test ReAct on two language-based interactive decision-making tasks, ALFWorld and\nWebShop, both of which feature complex environments that require agents to act over long horizons\nwith sparse rewards, warranting the need for reasoning to act and explore effectively.\n\n\n**ALFWorld** ALFWorld (Shridhar et al., 2020b) (Figure 1(2)) is a synthetic text-based game designed\nto align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of\ntasks in which an agent needs to achieve a high-level goal (e.g. examine paper under desklamp) by\nnavigating and interacting with a simulated household via text actions (e.g. go to coffeetable 1, take\npaper 2, use desklamp 1). A task instance can have more than 50 locations and take an expert policy\nmore than 50 steps to solve, thus challenging an agent to plan and track subgoals, as well as explore\nsystematically (e.g. check all desks one by one for desklamp). In particular, one challenge built into\nALFWorld is the need to determine likely locations for common household items (e.g. desklamps will\nlikely be on desks, shelfs, or dressers), making this environment a good fit for LLMs to exploit their\npretrained commons", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_7200", "chunk_text": "World is the need to determine likely locations for common household items (e.g. desklamps will\nlikely be on desks, shelfs, or dressers), making this environment a good fit for LLMs to exploit their\npretrained commonsense knowledge. To prompt ReAct, we randomly annotate three trajectories\nfrom the training set for each task type, where each trajectory includes sparse thoughts that (1)\ndecompose the goal, (2) track subgoal completion, (3) determine the next subgoal, and (4) reason via\ncommonsense where to find an object and what to do with it. We show prompts used for ALFWorld\nin Appendix C.4. Following Shridhar et al. (2020b), we evaluate on 134 unseen evaluation games\nin a task-specific setup. For robustness, we construct 6 prompts for each task type through each\npermutation of 2 annotated trajectories from the 3 we annotate. Act prompts are constructed using\nthe same trajectories, but without thoughts \u2014 since task instances are randomly chosen from the\ntraining set, it favors neither ReAct nor Act and provides a fair and controlled comparison to test the\nimportance of sparse thoughts. For baselines, we use BUTLER (Shridhar et al., 2020b), an imitation\nlearning agent trained on 10 [5] expert trajectories for each task type [5] .\n\n\n**WebShop** Can ReAct also interact with noisy real-world language environments for practical\napplications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping\nwebsite environment with 1.18M real-world products and 12k human instructions. Unlike ALFWorld,\nWebshop contains a high variety of structured and unstructured texts (e.g. product titles, descriptions,\nand options crawled from Amazon), and requires an agent to purchase a product based on a user\ninstruction (e.g. \u201cI am looking for a nightstand with drawers. It should have a nickel finish, and\npriced lower than $140\u201d) through web interactions (e.g. search \u201cnightstand drawers\u201d, choose buttons\nsuch as \u201ccolor: modern-nickel-white\u201d or \u201cback to search\u201d). This task is evaluated by average score\n(percentage of desired attributes covered by the chosen product averaged across all episodes) and\nsuccess rate (percentage of episodes where the chosen product satisfies all requirements) on ", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_7650", "chunk_text": "el-white\u201d or \u201cback to search\u201d). This task is evaluated by average score\n(percentage of desired attributes covered by the chosen product averaged across all episodes) and\nsuccess rate (percentage of episodes where the chosen product satisfies all requirements) on 500 test\ninstructions. We formulate Act prompts with actions to search, choose product, choose options,\nand buy, with ReAct prompts additionally reasoning to determine what to explore, when to buy,\nand what products options are relevant to the instruction. See Table 6 for an example prompt, and\nTable 10 for model predictions in the Appendix. We compare to an imitation learning (IL) method\n\n\n5Micheli & Fleuret (2021) finetuned a GPT-2 model on 3553 task instances and achieved a much improved\nperformance than BUTLER, but it is trained on all task types, thus not included as a baseline.\n\n\n7\n\n\nPublished as a conference paper at ICLR 2023\n\n|Method|Pick Clean Heat Cool Look Pick2|All|\n|---|---|---|\n|Act (best of 6)<br>88<br>42<br>74<br>67<br>72<br>**41**<br>45<br>ReAct (avg)<br>65<br>39<br>83<br>76<br>55<br>24<br>57<br>ReAct (best of 6)<br>**92**<br>58<br>**96**<br>86<br>**78**<br>**41**<br>**71**|Act (best of 6)<br>88<br>42<br>74<br>67<br>72<br>**41**<br>45<br>ReAct (avg)<br>65<br>39<br>83<br>76<br>55<br>24<br>57<br>ReAct (best of 6)<br>**92**<br>58<br>**96**<br>86<br>**78**<br>**41**<br>**71**|Act (best of 6)<br>88<br>42<br>74<br>67<br>72<br>**41**<br>45<br>ReAct (avg)<br>65<br>39<br>83<br>76<br>55<br>24<br>57<br>ReAct (best of 6)<br>", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_8100", "chunk_text": "72<br>**41**<br>45<br>ReAct (avg)<br>65<br>39<br>83<br>76<br>55<br>24<br>57<br>ReAct (best of 6)<br>**92**<br>58<br>**96**<br>86<br>**78**<br>**41**<br>**71**|\n|ReAct-IM (avg)<br>ReAct-IM (best of 6)|55<br>59<br>60<br>55<br>23<br>24<br>62<br>**68**<br>87<br>57<br>39<br>33|48<br>53|\n|BUTLER_g_ (best of 8)<br>BUTLER (best of 8)|33<br>26<br>70<br>76<br>17<br>12<br>46<br>39<br>74<br>**100**<br>22<br>24|22<br>37|\n\n\n\nTable 3: AlfWorld task-specific success rates (%). BUTLER and\nBUTLER _g_ results are from Table 4 of Shridhar et al. (2020b). All\nmethods use greedy decoding, except that BUTLER uses beam search.\n\n\n\n|Method|Score SR|\n|---|---|\n|Act<br>ReAct|62.3<br>30.1<br>**66.6**<br>**40.0**|\n|IL<br>IL+RL|59.9<br>29.1<br>62.4<br>28.7|\n|Human<br>82.1<br>59.6<br>Expert|Human<br>82.1<br>59.6<br>Expert|\n\n\nTable 4: Score and success rate (SR) on Webshop. IL/IL+RL taken\nfrom Yao et al. (2022).\n\n\n\ntrained with 1,012 human annotated trajectories, and a imitation + reinforcement learning (IL + RL)\nmethod additionally trained with 10,587 training instructions.\n\n\n**Results** ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On\nALFWorld, the best ReAct trial achieves an average success rate of 71%, significantly outperforming\nthe best Act (45%) and BUTLER (37%) trials. In fact", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_8550", "chunk_text": ") and Webshop (Table 4). On\nALFWorld, the best ReAct trial achieves an average success rate of 71%, significantly outperforming\nthe best Act (45%) and BUTLER (37%) trials. In fact, even the worse ReAct trial (48%) beats\nthe best trial of both methods. Moreover, the advantage of ReAct over Act is consistent across\nsix controlled trials, with relative performance gain ranging from 33% to 90% and averaging 62%.\nQualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals\ninto smaller subgoals, or loses track of the current state of the environment. Example trajectories\ncomparing ReAct and Act can be found in Appendix D.2.1 and Appendix D.2.2.\n\n\nOn Webshop, one-shot Act prompting already performs on par with IL and IL+RL methods. With\nadditional sparse reasoning, ReAct achieves significantly better performance, with an absolute 10%\nimprovement over the previous best success rate. By checking examples, we find that ReAct is more\nlikely to identify instruction-relevant products and options by reasoning to bridge the gap between\nnoisy observations and actions (e.g. \u201cFor \u2018space-saving ottoman bench for living room\u2019, the item\nhas options \u201839x18x18inch\u2019 and \u2018blue\u2019 and seems good to buy.\u201d). However, existing methods are\nstill far from the performance of expert humans (Table 4), who perform significantly more product\nexplorations and query re-formulations that are still challenging for prompting-based methods.\n\n\n**On the value of internal reasoning vs. external feedback** To our knowledge, ReAct is the first\ndemonstration of combined reasoning and action using an LLM applied to an interactive environment\nwithin a closed-loop system. Perhaps the closest prior work is Inner Monologue (IM), from Huang\net al. (2022b), in which actions from an embodied agent are motivated by an eponymous \u201cinner\nmonologue\u201d. **However, IM\u2019s \u201cinner monologue\u201d is limited to observations of the environment**\n**state and what needs to be completed by the agent for the goal to be satisfied.** In contrast, the\nreasoning traces in ReAct for decision making is flexible and sparse, allowing diverse reasoning\ntypes (see Section 2) to be induced for different tasks.\n\n\nTo demonstrate", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_9000", "chunk_text": " by the agent for the goal to be satisfied.** In contrast, the\nreasoning traces in ReAct for decision making is flexible and sparse, allowing diverse reasoning\ntypes (see Section 2) to be induced for different tasks.\n\n\nTo demonstrate the differences between ReAct and IM, and to highlight the importance of internal\nreasoning vs. simple reactions to external feedback, we ran an ablation experiment using a thought\npattern composed of IM-like dense external feedback. As can be seen in Table 3, ReAct substantially\n\n- utperforms IM-style prompting (ReAct-IM) (71 vs. 53 overall success rate), with consistent\nadvantages on five out of six tasks. Qualitatively, we observed that ReAct-IM often made mistakes\nin identifying when subgoals were finished, or what the next subgoal should be, due to a lack of highlevel goal decomposition. Additionally, many ReAct-IM trajectories struggled to determine where\nan item would likely be within the ALFWorld environment, due to a lack of commonsense reasoning.\nBoth shortcomings can be addressed in the ReAct paradigm. More details about ReAct-IM is in\nAppendix B.2. An example prompt for ReAct-IM can be found in Appendix C.4, and an example\ntrajectory in Appendix D.2.3.\n\n\n8\n\n\nPublished as a conference paper at ICLR 2023\n\n\n5 RELATED WORK\n\n\n**Language model for reasoning** Perhaps the most well-known work of using LLMs for reasoning\nis Chain-of-Thought (CoT) (Wei et al., 2022), which reveals the ability of LLMs to formulate their\n\n- wn \u201cthinking procedure\u201d for problem solving. Several follow-up works have since been performed,\nincluding least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shotCoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently,\n(Madaan & Yazdanbakhsh, 2022) systematically studied the formulation and structure of CoT, and\n\n- bserved that the presence of symbols, patterns and texts is crucial to the effectiveness of CoT. Other\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_9450", "chunk_text": ", patterns and texts is crucial to the effectiveness of CoT. Other\nwork has also been extended to more sophisticated reasoning architecture beyond simple prompting.\nFor example Selection-Inference (Creswell et al., 2022) divides the reasoning process into two steps\n\n- f \u201cselection\u201d and \u201cinference\u201d. STaR (Zelikman et al., 2022) bootstraps the reasoning process by\nfinetuning the model on correct rationales generated by the model itself. Faithful reasoning (Creswell\n& Shanahan, 2022) decomposes multi-step reasoning into three steps, each performed by a dedicated\nLM respectively. Similar approaches like Scratchpad (Nye et al., 2021), which finetunes a LM on\nintermediate computation steps, also demonstrate improvement on multi-step computation problems.\nIn contrast to these methods, ReAct performs more than just isolated, fixed reasoning, and integrates\nmodel actions and their corresponding observations into a coherent stream of inputs for the model to\nreason more accurately and tackle tasks beyond reasoning (e.g. interactive decision making).\n\n\n**Language model for decision making** The strong capability of LLMs has enabled them to perform\ntasks beyond language generation, and it is becoming more popular to take advantage of LLMs as a\npolicy model for decision making, especially in interactive environments. WebGPT (Nakano et al.,\n2021) uses an LM to interact with web browsers, navigate through web pages, and infer answers to\ncomplicated questions from ELI5 (Fan et al., 2019). In comparison to ReAct, WebGPT does not\nexplicitly model the thinking and reasoning procedure, instead rely on expensive human feedback for\nreinforcement learning. In conversation modeling, chatbots like BlenderBot (Shuster et al., 2022b)\nand Sparrow (Glaese et al., 2022) and task-oriented dialogue systems like SimpleTOD (Hosseini-Asl\net al., 2020) also train LMs to make decision about API calls. Unlike ReAct, they do not explicitly\nconsider the reasoning procedure either, and also relies on expensive datasets and human feedback\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\ndecision making process only requires language description of the reasoning procedure. [6]\n\n\nLLMS have also been increasingly employed in interactive", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_9900", "chunk_text": " and human feedback\ncollections for policy learning. In contrast, ReAct learns a policy in a much cheaper way, since the\ndecision making process only requires language description of the reasoning procedure. [6]\n\n\nLLMS have also been increasingly employed in interactive and embodied environments for planning\nand decision making. Perhaps most relevant to ReAct in this respect are SayCan (Ahn et al., 2022)\nand Inner Monologue (Huang et al., 2022b), which use LLMs for robotic action planning and decision\nmaking. In SayCan, LLMs were prompted to directly predict possible actions a robot can take, which\nis then reranked by an affordance model grounded on the visual environments for final prediction.\nInner Monologue made further improvements by adding the eponymous \u201cinner monologue\", which is\nimplemented as injected feedback from the environment. To our knowledge, Inner Monologue is the\nfirst work that demonstrates such a closed-loop system, which ReAct builds on. However, we argue\nthat Inner Monologue does not truly comprise of inner thoughts \u2014 this is elaborated in Section 4. We\nalso note that leveraging language as semantically-rich inputs in the process of interactive decision\nmaking has been shown to be successful under other settings (Abramson et al., 2020; Karamcheti\net al., 2021; Huang et al., 2022a; Li et al., 2022). It is becoming more evident that with the help of\nLLMs, language as a fundamental cognitive mechanism will play a critical role in interaction and\ndecision making. What is more, progress in LLMs has also inspired the development of versatile and\ngeneralist agents like Reed et al. (2022).\n\n\n6 CONCLUSION\n\n\nWe have proposed ReAct \u2013 a simple yet effective method for synergizing reasoning and acting in\nlarge language models. Through a diverse set of experiments on multi-hop question-answering, fact\nchecking, and interactive decision-making tasks, we show that ReAct leads to superior performance\nwith interpretable decision traces. Despite the simplicity of our method, complex tasks with large\naction spaces require more demonstrations to learn well, which unfortunately can easily go beyond\nthe input length limit of in-context learning. We explore the fine-tuning approach on HotpotQA\n\n\n6Human feedback can also be incorporated in a complementary manner but we leave it for future work.\n\n\n9\n\n\nPublished", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_10350", "chunk_text": " which unfortunately can easily go beyond\nthe input length limit of in-context learning. We explore the fine-tuning approach on HotpotQA\n\n\n6Human feedback can also be incorporated in a complementary manner but we leave it for future work.\n\n\n9\n\n\nPublished as a conference paper at ICLR 2023\n\n\nwith initial promising results, but learning from more high-quality human annotations will be the\ndesiderata to further improve the performance. Scaling up ReAct with multi-task training and\ncombining it with complementary paradigms like reinforcement learning could result in stronger\nagents that further unlock the potential of LLMs for more applications.\n\n\nACKNOWLEDGMENTS\n\n\nWe thank the support and feedback of many people from Google Brain team and Princeton NLP\nGroup. This work was supported in part by the National Science Foundation under Grant No.\n2107048. Any opinions, findings, and conclusions or recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of the National Science Foundation.\n\n\nREPRODUCIBILITY STATEMENT\n\n\nOur main experiments are done on PaLM (Chowdhery et al., 2022), which is not an openly accessible\nmodel yet. To increase reproducibility, we have included all used prompts in Appendix C, additional\nexperiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct\n[prompting code at https://anonymous.4open.science/r/ReAct-2268/.](https://anonymous.4open.science/r/ReAct-2268/)\n\n\nETHICS STATEMENT\n\n\nReAct prompts large language models to generate more human interpretable, diagnosable, and\ncontrollable task-solving trajectories than previous methods. However, hooking up a large language\nmodel with an action space to interact with external environments (e.g. the web, physical environments) has potential dangers, e.g. looking up inappropriate or private information, or taking harmful\nactions in an environment. Our experiments minimize such risks by limiting the interactions to\nspecific websites (Wikipedia or WebShop) that are free of private information, without any dangerous\nactions in the action space design (i.e. models cannot really buy products on WebShop the research\nbenchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing\nmore extensive experiments in the future.\n\n\nREFERENCES\n\n\nJosh Abramson, Ar", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_10800", "chunk_text": " action space design (i.e. models cannot really buy products on WebShop the research\nbenchmark, or edit Wikipedia). We believe researchers should be aware of such risks before designing\nmore extensive experiments in the future.\n\n\nREFERENCES\n\n\nJosh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita\nChhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim\nHarley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson, So\u02c7na Mokr\u00e1, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne,\nDuncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence,\n[2020. URL https://arxiv.org/abs/2012.05672.](https://arxiv.org/abs/2012.05672)\n\n\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine\nHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally\nJesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee,\nSergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka\nRao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and\nAndy Zeng. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL\n[https://arxiv.org/abs/2204.01691.](https://arxiv.org/abs/2204.01691)\n\n\nBen Alderson-Day and Charles Fernyhough", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_11250", "chunk_text": " in robotic affordances, 2022. URL\n[https://arxiv.org/abs/2204.01691.](https://arxiv.org/abs/2204.01691)\n\n\nBen Alderson-Day and Charles Fernyhough. Inner speech: development, cognitive functions,\nphenomenology, and neurobiology. _Psychological bulletin_, 141(5):931, 2015.\n\n\nAlan Baddeley. Working memory. _Science_, 255(5044):556\u2013559, 1992.\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. _Advances in neural information processing systems_, 33:1877\u20131901, 2020.\n\n\n10\n\n\nPublished as a conference paper at ICLR 2023\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n\n\nAntonia Creswell and Murray Shanahan. Faithful reasoning using large language models, 2022. URL\n[https://arxiv.org/abs/2208.14271.](https://arxiv.org/abs/2208.14271)\n\n\nAntonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large\n[language models for interpretable logical reasoning, 2022. URL https://arxiv.org/abs/](https://arxiv.org/abs/2205.09712)\n[2205.09712.](https://arxiv.org/abs/2205.09712)\n\n\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nLong form question answering. In _Proceedings of the 57th Annual Meeting of the Association_\n_for Computational Linguistics_, pp. 3558\u20133567, Florence, Italy, July 2019. Association for Com[putational", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_11700", "chunk_text": ":\nLong form question answering. In _Proceedings of the 57th Annual Meeting of the Association_\n_for Computational Linguistics_, pp. 3558\u20133567, Florence, Italy, July 2019. Association for Com[putational Linguistics. doi: 10.18653/v1/P19-1346. URL https://aclanthology.org/](https://aclanthology.org/P19-1346)\n[P19-1346.](https://aclanthology.org/P19-1346)\n\n\nCharles Fernyhough. Vygotsky, luria, and the social brain. _Self and social regulation: Social_\n_interaction and the development of social understanding and executive functions_, pp. 56\u201379, 2010.\n\n\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham,\nJonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth\nDathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,\nSo\u02c7na Mokr\u00e1, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,\nWilliam Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and\nGeoffrey Irving. Improving alignment of dialogue agents via targeted human judgements,\n2022. [URL https://storage.googleapis.com/deepmind-media/DeepMind.](https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf)\n[com/Authors-Notes/sparrow/sparrow-final.pdf.](https://storage.googleapis.com/deepmind-media/DeepMind.com/Authors-Notes/sparrow/sparrow-final.pdf)\n\n\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple\nlanguage model for task-oriented dialogue. _Advances in Neural Information Processing Systems_,\n33:20179\u201320191, 2020.\n\n\nWenlong Huang, Pieter Abbeel, Deep", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_12600", "chunk_text": " K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:\n9459\u20139474, 2020.\n\n\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An\nHuang, Ekin Aky\u00fcrek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba,\n[and Yuke Zhu. Pre-trained language models for interactive decision-making, 2022. URL https:](https://arxiv.org/abs/2202.01771)\n[//arxiv.org/abs/2202.01771.](https://arxiv.org/abs/2202.01771)\n\n\n11\n\n\nPublished as a conference paper at ICLR 2023\n\n\nAleksandr Romanovich Luria. Ls vygotsky and the problem of localization of functions. _Neuropsy-_\n_chologia_, 3(4):387\u2013392, 1965.\n\n\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes\n[two to tango, 2022. URL https://arxiv.org/abs/2209.07686.](https://arxiv.org/abs/2209.07686)\n\n\nVincent Micheli and Fran\u00e7ois Fleuret. Language models are few-shot butlers. _arXiv preprint_\n_arXiv:2104.07972_, 2021.\n\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\n[Browser-assisted question-answering with human feedback, 2021. URL https://arxiv.](https://arxiv.org/abs/2112.09332)\n\n[org/abs/2112.09332.](https://arxiv.org/abs/2112.09332)\n\n\nMaxwell Nye,", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_13050", "chunk_text": "://arxiv.](https://arxiv.org/abs/2112.09332)\n\n[org/abs/2112.09332.](https://arxiv.org/abs/2112.09332)\n\n\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and\nAugustus Odena. Show your work: Scratchpads for intermediate computation with language\n[models, 2021. URL https://arxiv.org/abs/2112.00114.](https://arxiv.org/abs/2112.00114)\n\n\nScott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom\nEccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell,\n[Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent, 2022. URL https:](https://arxiv.org/abs/2205.06175)\n[//arxiv.org/abs/2205.06175.](https://arxiv.org/abs/2205.06175)\n\n\nMohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\nLuke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions\nfor everyday tasks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern_\n_recognition_, pp. 10740\u201310749, 2020a.\n\n\nMohit Shridhar, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Yonatan Bisk, Adam Trischler, and Matthew\nHausknecht. Alfworld: Aligning text and embodied environments for interactive learning. _arXiv_\n_preprint arXiv:2010.03768_, 2020b.\n\n\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_13950", "chunk_text": ":2207.00747_, 2022b.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint_\n_arXiv:2201.11903_, 2022.\n\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov,\nand Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. _arXiv preprint arXiv:1809.09600_, 2018.\n\n\n12\n\n\nPublished as a conference paper at ICLR 2023\n\n\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore:\nLanguage models for action generation in text-based games. In _Proceedings of the 2020 Conference_\n\n_on Empirical Methods in Natural Language Processing (EMNLP)_, pp. 8736\u20138754, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.704.\n[URL https://aclanthology.org/2020.emnlp-main.704.](https://aclanthology.org/2020.emnlp-main.704)\n\n\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable\nreal-world web interaction with grounded language agents. _arXiv preprint arXiv:2207.01206_,\n2022.\n\n\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with\n[reasoning, 2022. URL https://arxiv.org/abs/2203.14465.](https://arxiv.org/abs/2203.14465)\n\n\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in\n[large language models, 2022. URL https://arxiv.org/abs/", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_14400", "chunk_text": "hi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in\n[large language models, 2022. URL https://arxiv.org/abs/2205.10625.](https://arxiv.org/abs/2205.10625)\n\n\nYunchang Zhu, Liang Pang, Yanyan Lan, Huawei Shen, and Xueqi Cheng. Adaptive information\nseeking for open-domain question answering. _arXiv preprint arXiv:2109.06747_, 2021.\n\n\n13\n\n\nPublished as a conference paper at ICLR 2023\n\n\nA ADDITIONAL RESULTS\n\n\nA.1 GPT-3 EXPERIMENTS\n\n\nPaLM-540B GPT-3\n\n\nHotpotQA (exact match) 29.4 **30.8**\nALFWorld (success rate %) 70.9 **78.4**\n\n\nTable 5: ReAct prompting results using PaLM-540B vs. GPT-3 (text-davinci-002, greedy decoding).\nOn HotpotQA, we randomly sample a subset of 500 validation questions. On ALFWorld, we use all\n134 unseen validation task instances, and use the best prompt set according to PaLM-540B.\n\n\nWe run additional GPT-3 (Brown et al., 2020) experiments to confirm ReAct prompting performance\nis general across different large language models. As shown in Table 5, GPT-3 (text-davinci-002,\ngreedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly\nbecause it is finetuned with human instruction following. This indicates ReAct prompting is effective\nacross different large language models on different tasks. The code for these experiments are at\n[https://react-lm.github.io/.](https://react-lm.github.io/)\n\n\nA.2 REACT OBTAINS UP-TO-DATE KNOWLEDGE ON HOTPOTQA\n\n\n\n\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|**4XHVWLRQ**\u001d+RZPDQ\\URRPVDUHLQWKHKRWHOWKDWLVKRPHWR<br>WKH&LUTXHGX6ROHLOVKRZ0\\VWHUH", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_15300", "chunk_text": "\u0003<br>2EV\u0003\u0017 \u00030\\VWqUH\u0003 P\\\u0010VWHHU \u0003\u0003LV \u0003<br>RQH\u0003RI \u0003VL[\u00aa<br>$FW \u0003\u0018 \u0003/RRNXS>7UHDVXUH\u0003,VODQG<br>\u0003<br>+RWHO \u0003DQG \u0003&DVLQR@<br>\u0003<br>\u00aa\u00aa HQG \u0003ZLWKRXW\u0003DQVZHU|Col2|Col3|Col4|\n|---|---|---|---|\n|**\u000b\u0014F\f$FW\u00102QO\\**<br>**$FW\u00146HDUFK**>&LUTXHGX6ROHLO @<br>**2EV\u0014**&LUTXHGX6ROHLOLVD<br>&DQDGLDQHQWHUWDLQPHQWFRPSDQ\\\u00aa<br>**$FW\u0015/RRNXS**>0\\VWHUH@<br>**2EV\u0015**1RPRUHUHVXOWV<br>**$FW\u00166HDUFK**>0\\VWHUH\u000b &LUTXHG<br>6ROHLOVKRZ\f @<br>**2EV\u0016**&RXOGQRWILQG\u00aa<br>**$FW\u00176HDUFK**>0\\VWHUH\u000b&LUTXHG<br>6ROHLO\f@<br>**2EV\u0017**0\\VWqUH\u000bP\\\u0010VWHHU\fLV<br>RQHRIVL[\u00aa<br>**$FW\u0018/RRNXS**>7UHDVXUH,VODQG<br>+RWHODQG&DVLQR@<br>\u00aa\u00aa\u000bHQGZLWKRXWDQVZHU|**\u000b\u0014F\f$FW\u00102QO\\**<br>**$FW\u00146HDUFK**>&LUTXHGX6ROHLO @<br>**2EV\u0014**&LUTXHGX6ROHLOLVD<br>&DQDGLDQHQWHUWDLQPHQWFRPSDQ\\\u00aa<br>**$FW\u0015/RRNXS**", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_15750", "chunk_text": "2EV\u0014**&LUTXHGX6ROHLOLVD<br>&DQDGLDQHQWHUWDLQPHQWFRPSDQ\\\u00aa<br>**$FW\u0015/RRNXS**>0\\VWHUH@<br>**2EV\u0015**1RPRUHUHVXOWV<br>**$FW\u00166HDUFK**>0\\VWHUH\u000b &LUTXHG<br>6ROHLOVKRZ\f @<br>**2EV\u0016**&RXOGQRWILQG\u00aa<br>**$FW\u00176HDUFK**>0\\VWHUH\u000b&LUTXHG<br>6ROHLO\f@<br>**2EV\u0017**0\\VWqUH\u000bP\\\u0010VWHHU\fLV<br>RQHRIVL[\u00aa<br>**$FW\u0018/RRNXS**>7UHDVXUH,VODQG<br>+RWHODQG&DVLQR@<br>\u00aa\u00aa\u000bHQGZLWKRXWDQVZHU|**\u000b\u0014F\f$FW\u00102QO\\**|**\u000b\u0014F\f$FW\u00102QO\\**|\n|**\u000b\u0014F\f$FW\u00102QO\\**<br>**$FW\u00146HDUFK**>&LUTXHGX6ROHLO @<br>**2EV\u0014**&LUTXHGX6ROHLOLVD<br>&DQDGLDQHQWHUWDLQPHQWFRPSDQ\\\u00aa<br>**$FW\u0015/RRNXS**>0\\VWHUH@<br>**2EV\u0015**1RPRUHUHVXOWV<br>**$FW\u00166HDUFK**>0\\VWHUH\u000b &LUTXHG<br>6ROHLOVKRZ\f @<br>**2EV\u0016**&RXOGQRWILQG\u00aa<br>**$FW\u00176HDUFK**>0\\VWHUH\u000b&LUTXHG<br>6ROHLO\f@<br>**2EV\u0017**0\\VWqUH\u000bP\\\u0010VWHHU\fLV<br>RQHRIVL[\u00aa<br>**$FW", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_16200", "chunk_text": "\u000b&LUTXHG<br>6ROHLO\f@<br>**2EV\u0017**0\\VWqUH\u000bP\\\u0010VWHHU\fLV<br>RQHRIVL[\u00aa<br>**$FW\u0018/RRNXS**>7UHDVXUH,VODQG<br>+RWHODQG&DVLQR@<br>\u00aa\u00aa\u000bHQGZLWKRXWDQVZHU||||\n\n\n\nFigure 4: Another example HotpotQA question, where the original label is outdated. Only ReAct is\nable to obtain the up-to-date answer thanks to real-world web interaction plus reasoning.\n\nDuring trajectory inspection, we also find that sometimes ReAct does not agree with dataset labels as\nthe labels themselves could be outdated. For example, as shown in Figure 4, the question asks about\nthe size of a hotel, which increased from the HotpotQA construction time. While Standard and CoT\ngive wrong answers due to hallucination, Act fails despite the access of real-world web interaction,\ndue to a lack of reasoning to guide how to interact with the Internet for QA. Only ReAct is able to\nretrieve up-to-date information from the Internet and provide a reasonable answer. Therefore, better\nincorporation of reasoning abilities might benefit recent Internet-augmented language models (Nakano\net al., 2021; Lazaridou et al., 2022; Shuster et al., 2022a) for up-to-date task solving.\n\n\nA.3 HUMAN-IN-THE-LOOP BEHAVIOR CORRECTION ON ALFWORLD\n\n\nWe also explore human-in-the-loop interaction with ReAct, to allow a human to inspect and edit\nReAct\u2019s reasoning traces. Figure 5 shows that by simply removing a hallucinating sentence in Act\n17 and adding some hints in Act 23, ReAct can be made to change its behavior drastically to align\nwith these human thought edits and succeed in the task. From a human perspective, solving such a\ntask becomes significantly easier, from typing tens of actions to only editing a couple of thoughts,\nwhich enables new forms of human-machine collaboration. We note that such a policy edit on-the-go\n\n\n14\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|<RXDUHLQWKHPL", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_16650", "chunk_text": " note that such a policy edit on-the-go\n\n\n14\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|<RXDUHLQWKHPLGGOHRIDURRP\u0011/RRNLQJTXLFNO\\DURXQG\\RX\u000f<br>\\RXVHHDDUPFKDLU\u0014\u000fDFDELQHW\u0017\u000f\u00aaDFDELQHW\u0014\u000fDGUDZHU\u0018\u000f<br>\u00aa\u000fDGUDZHU\u0014\u000fDGUHVVHU\u0014\u000fDJDUEDJHFDQ\u0014\u000fDVDIH\u0014\u000fDVKHOI<br>\u0014\u0015\u000f\u00aa\u000fDVKHOI\u0014\u000fDVLGHWDEOH\u0014\u000fDQGDVRID\u0014\u0011<br>**<RXUWDVNLVWR\u001dSXWWZRNH\\FKDLQLQVDIH\u0011**<br>**$OI:RUOG**|**$OI:RUOG**|**$OI:RUOG**|\n\n\n\nFigure 5: A human-in-the-loop behavior correction example with ReAct in AlfWorld. (a) ReAct\ntrajectory fails due to a hallucinating thought (Act 17). (b) By a human simply editing two thoughts\n(Act 17, 23), the ReAct trajectory produces desirable reasoning traces and actions and succeeds.\n\n\nis difficult for Act and previous RL methods, as a human cannot change the model parameters, and\nchanging a few actions might not edit the rest of the model behavior. This paradigm is also more than\nhuman dialogue to update the goal or subgoal as in Huang et al. (2022b) \u2014 while editing ReAct\nthoughts can do these, it can also modify the model\u2019s internal belief, reasoning styles, or anything the\nflexible thought space supports, for better task solving. We believe this is an exciting direction for\nhuman alignment and leave more systematic study as future work.\n\n\nB EXPERIMENT DETAILS\n\n\nB.1 HOTPOTQA FINETUNING DETAILS\n\n\nFor all finetuning we use a batch size of 64. On PaLM-8B, we finetune ReAct and Act methods\nfor 4 _,_ 000 steps and Standard and CoT methods for 2 _,_ 000 steps. On PaLM-62B, we finetune\nReAct", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_17100", "chunk_text": "8B, we finetune ReAct and Act methods\nfor 4 _,_ 000 steps and Standard and CoT methods for 2 _,_ 000 steps. On PaLM-62B, we finetune\nReAct and Act methods for 4 _,_ 000 steps and Standard and CoT methods for 1 _,_ 000 steps. We\nfind ReAct and Act methods generally benefit from more training steps (and more training data),\nwhile Standard and CoT methods degrade soon after finetuning.\n\n\nB.2 ALFWORLD IM-STYLE DETAILS\n\n\nFor the IM-style ablation, the same expert trajectories used in ReAct are reannotated with dense\nexternal feedback thoughts within these trajectories, that limit ReAct-IM to only think about (1)\ndecomposing the current goal and (2) the current subgoal that needs to be completed. In particular,\nReAct-IM lacks thoughts that (1) determine when a subgoal is completed (2) determine what the\nnext subgoal should be (3) inducing the LLM to refer to its internal pretraining knowledge to identify\nwhere items can be within the environment.\n\n\n15\n\n\nPublished as a conference paper at ICLR 2023\n\n\nC PROMPTS\n\n\nC.1 HOTPOTQA\n\n\nHotpot QA Prompts\n\n\nOriginal\nQuestion What is the elevation range for the area that the eastern sector of the\nColorado orogeny extends into?\nAnswer 1,800 to 7,000 ft\n\n\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\ncharacter Milhouse, who Matt Groening named after who?\n\nAnswer Richard Nixon\n\n\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\n\nSaimaa Gesture?\n\nAnswer The Saimaa Gesture\n\n\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\nAnswer director, screenwriter, actor\n\n\nQuestion Which magazine was started first Arthur\u2019s Magazine or First for Women?\nAnswer Arthur\u2019s Magazine\n\n\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\n\nAnswer Yes\n\n\nAct\n\nQuestion What is the elevation range for the area that the eastern sector of the\nColorado orogeny extends into?\nAction 1 Search[Colorado orogeny]\nObservation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\nColorado and surrounding areas.\nAction", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_17550", "chunk_text": " sector of the\nColorado orogeny extends into?\nAction 1 Search[Colorado orogeny]\nObservation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\nColorado and surrounding areas.\nAction 2 Lookup[eastern sector]\nObservation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called\nthe Central Plains orogeny.\nAction 3 Search[High Plains]\nObservation 3 High Plains refers to one of two distinct land regions:\nAction 4 Search[High Plains (United States)]\nObservation 4 The High Plains are a subregion of the Great Plains. From east to west, the\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\nm).[3]\n\nAction 5 Finish[1,800 to 7,000 ft]\n\n\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\ncharacter Milhouse, who Matt Groening named after who?\nAction 1 Search[Milhouse]\nObservation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\nGroening.\nAction 2 Lookup[named after]\nObservation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\n\nmiddle name was Milhous.\n\nAction 3 Finish[Richard Nixon]\n\n\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\n\nSaimaa Gesture?\n\nAction 1 Search[Adam Clayton Powell]\nObservation 1 Could not find [Adam Clayton Powell]. Similar: [\u2019Adam Clayton Powell\nIII\u2019, \u2019Seventh Avenue (Manhattan)\u2019, \u2019Adam Clayton Powell Jr. State Office\nBuilding\u2019, \u2019Isabel Washington Powell\u2019, \u2019Adam Powell\u2019, \u2019Adam Clayton Powell\n(film)\u2019, \u2019Giancarlo Esposito\u2019].\nAction 2 Search[Adam Clayton Powell (film)]\nObservation 2 Adam Clayton Powell is a 1989 American documentary film directed by\nRichard Kilberg. The film is about the rise and fall of influential\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\nas part of the PBS series The American Experience.\nAction 3 Finish[The Saimaa Gesture]\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_18000", "chunk_text": " The film is about the rise and fall of influential\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\nas part of the PBS series The American Experience.\nAction 3 Finish[The Saimaa Gesture]\n\n\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\nAction 1 Search[Nicholas Ray]\nObservation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\n1979) was an American film director, screenwriter, and actor best known for\n\nthe 1955 film Rebel Without a Cause.\n\nAction 2 Search[Elia Kazan]\nObservation 2 Elia Kazan was an American film and theatre director, producer, screenwriter\n\nand actor.\n\n\n_Continued on next page_\n\n\n16\n\n\nPublished as a conference paper at ICLR 2023\n\n\nHotpot QA Prompts \u2013 Continued from previous page\n\n\nAction 3 Finish[director, screenwriter, actor]\nQuestion Which magazine was started first Arthur\u2019s Magazine or First for Women?\nAction 1 Search[Arthur\u2019s Magazine]\nObservation 1 Arthur\u2019s Magazine (1844-1846) was an American literary periodical published\nin Philadelphia in the 19th century.\nAction 2 Search[First for Women]\nObservation 2 First for Women is a woman\u2019s magazine published by Bauer Media Group in the\nUSA.[1] The magazine was started in 1989.\nAction 3 Finish[Arthur\u2019s Magazine]\n\n\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\nAction 1 Search[Pavel Urysohn]\nObservation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\nmathematician who is best known for his contributions in dimension theory.\nAction 2 Search[Leonid Levin]\nObservation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer\n\nscientist.\n\nAction 3 Finish[yes]\n\n\nCoT\n\nQuestion What is the elevation range for the area that the eastern sector of the\nColorado orogeny extends into?\nThought Let\u2019s think step by step. The eastern sector of Colorado orogeny extends\ninto the High Plains. High Plains rise in elevation from around 1,800 to\n7,000 ft, so the answer", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_18450", "chunk_text": " orogeny extends into?\nThought Let\u2019s think step by step. The eastern sector of Colorado orogeny extends\ninto the High Plains. High Plains rise in elevation from around 1,800 to\n7,000 ft, so the answer is 1,800 to 7,000 ft.\nAnswer 1,800 to 7,000 ft\n\n\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\ncharacter Milhouse, who Matt Groening named after who?\nThought Let\u2019s think step by step. Milhouse was named after U.S. president Richard\nNixon, so the answer is Richard Nixon.\n\nAnswer Richard Nixon\n\n\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\n\nSaimaa Gesture?\n\nThought Let\u2019s think step by step. Adam Clayton Powell (film) is a documentary about\nan African-American politician, not Finnish rock groups. So the documentary\nabout Finnish rock groups must instead be The Saimaa Gesture.\n\nAnswer The Saimaa Gesture\n\n\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\nThought Let\u2019s think step by step. Professions of Nicholas Ray are director,\nscreenwriter, and actor. Professions of Elia Kazan are director, producer,\nscreenwriter, and actor. So profession Nicholas Ray and Elia Kazan have in\ncommon is director, screenwriter, and actor.\nAnswer director, screenwriter, actor\n\n\nQuestion Which magazine was started first Arthur\u2019s Magazine or First for Women?\nThought Let\u2019s think step by step. Arthur\u2019s Magazine was started in 1844. First\nfor Women was started in 1989. 1844 (Arthur\u2019s Magazine) < 1989 (First for\nWomen), so Arthur\u2019s Magazine was started first.\nAnswer Arthur\u2019s Magazine\n\n\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\nThought Let\u2019s think step by step. Pavel Urysohn is a mathematician. Leonid Levin\nis a mathematician and computer scientist. So Pavel Urysohn and Leonid\nLevin have the same type of work.\n\nAnswer Yes\n\n\nReAct\n\nQuestion What is the elevation range for the area that the eastern sector of the\nColorado orogeny extends into?\nThought 1 I need to search Colorado orogeny, find the area that the eastern sector\n\n         - f the Colorado orogeny extends into, then", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_18900", "chunk_text": " range for the area that the eastern sector of the\nColorado orogeny extends into?\nThought 1 I need to search Colorado orogeny, find the area that the eastern sector\n\n         - f the Colorado orogeny extends into, then find the elevation range of the\n\narea.\n\nAction 1 Search[Colorado orogeny]\nObservation 1 The Colorado orogeny was an episode of mountain building (an orogeny) in\nColorado and surrounding areas.\nThought 2 It does not mention the eastern sector. So I need to look up eastern\n\nsector.\n\nAction 2 Lookup[eastern sector]\nObservation 2 (Result 1 / 1) The eastern sector extends into the High Plains and is called\nthe Central Plains orogeny.\nThought 3 The eastern sector of Colorado orogeny extends into the High Plains. So I\nneed to search High Plains and find its elevation range.\nAction 3 Search[High Plains]\nObservation 3 High Plains refers to one of two distinct land regions\nThought 4 I need to instead search High Plains (United States).\nAction 4 Search[High Plains (United States)]\n\n\n_Continued on next page_\n\n\n17\n\n\nPublished as a conference paper at ICLR 2023\n\n\nHotpot QA Prompts \u2013 Continued from previous page\n\n\nObservation 4 The High Plains are a subregion of the Great Plains. From east to west, the\nHigh Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\nm).[3]\nThought 5 High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\nis 1,800 to 7,000 ft.\nAction 5 Finish[1,800 to 7,000 ft]\n\n\nQuestion Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\"\ncharacter Milhouse, who Matt Groening named after who?\nThought 1 The question simplifies to \"The Simpsons\" character Milhouse is named after\nwho. I only need to search Milhouse and find who it is named after.\nAction 1 Search[Milhouse]\nObservation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\nGroening.\nThought 2 The paragraph does not tell who Milhouse", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_19350", "chunk_text": "]\nObservation 1 Milhouse Mussolini Van Houten is a recurring character in the Fox animated\ntelevision series The Simpsons voiced by Pamela Hayden and created by Matt\nGroening.\nThought 2 The paragraph does not tell who Milhouse is named after, maybe I can look up\n\n\"named after\".\n\nAction 2 Lookup[named after]\nObservation 2 (Result 1 / 1) Milhouse was named after U.S. president Richard Nixon, whose\n\nmiddle name was Milhous.\n\nThought 3 Milhouse was named after U.S. president Richard Nixon, so the answer is\n\nRichard Nixon.\n\nAction 3 Finish[Richard Nixon]\n\n\nQuestion Which documentary is about Finnish rock groups, Adam Clayton Powell or The\n\nSaimaa Gesture?\n\nThought 1 I need to search Adam Clayton Powell and The Saimaa Gesture, and find which\ndocumentary is about Finnish rock groups.\nAction 1 Search[Adam Clayton Powell]\nObservation 1 Could not find [Adam Clayton Powell]. Similar: [\u2019Adam Clayton Powell\nIII\u2019, \u2019Seventh Avenue (Manhattan)\u2019, \u2019Adam Clayton Powell Jr. State Office\nBuilding\u2019, \u2019Isabel Washington Powell\u2019, \u2019Adam Powell\u2019, \u2019Adam Clayton Powell\n(film)\u2019, \u2019Giancarlo Esposito\u2019].\nThought 2 To find the documentary, I can search Adam Clayton Powell (film).\nAction 2 Search[Adam Clayton Powell (film)]\nObservation 2 Adam Clayton Powell is a 1989 American documentary film directed by\nRichard Kilberg. The film is about the rise and fall of influential\nAfrican-American politician Adam Clayton Powell Jr.[3][4] It was later aired\nas part of the PBS series The American Experience.\nThought 3 Adam Clayton Powell (film) is a documentary about an African-American\npolitician, not Finnish rock groups. So the documentary about Finnish rock\ngroups must instead be The Saimaa Gesture.\nAction 3 Finish[The Saimaa Gesture]\n\n\nQuestion What profession does Nicholas Ray and Elia Kazan have in common?\nThought 1 I need to search Nicholas Ray and Elia Kazan, find their professions, then\nfind the profession they have in common.\nAction 1 Search[Nicholas Ray]\nObservation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\n1979) was an American film director, screenwriter,", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_19800", "chunk_text": ".\nAction 1 Search[Nicholas Ray]\nObservation 1 Nicholas Ray (born Raymond Nicholas Kienzle Jr., August 7, 1911 - June 16,\n1979) was an American film director, screenwriter, and actor best known for\n\nthe 1955 film Rebel Without a Cause.\n\nThought 2 Professions of Nicholas Ray are director, screenwriter, and actor. I need\nto search Elia Kazan next and find his professions.\nAction 2 Search[Elia Kazan]\nObservation 2 Elia Kazan was an American film and theatre director, producer, screenwriter\n\nand actor.\n\nThought 3 Professions of Elia Kazan are director, producer, screenwriter, and actor.\nSo profession Nicholas Ray and Elia Kazan have in common is director,\nscreenwriter, and actor.\nAction 3 Finish[director, screenwriter, actor]\n\n\nQuestion Which magazine was started first Arthur\u2019s Magazine or First for Women?\nThought 1 I need to search Arthur\u2019s Magazine and First for Women, and find which was\n\nstarted first.\n\nAction 1 Search[Arthur\u2019s Magazine]\nObservation 1 Arthur\u2019s Magazine (1844-1846) was an American literary periodical published\nin Philadelphia in the 19th century.\nThought 2 Arthur\u2019s Magazine was started in 1844. I need to search First for Women\n\nnext.\n\nAction 2 Search[First for Women]\nObservation 2 First for Women is a woman\u2019s magazine published by Bauer Media Group in the\nUSA.[1] The magazine was started in 1989.\nThought 3 First for Women was started in 1989. 1844 (Arthur\u2019s Magazine) < 1989 (First\nfor Women), so Arthur\u2019s Magazine was started first.\nAction 3 Finish[Arthur\u2019s Magazine]\n\n\nQuestion Were Pavel Urysohn and Leonid Levin known for the same type of work?\nThought 1 I need to search Pavel Urysohn and Leonid Levin, find their types of work,\nthen find if they are the same.\nAction 1 Search[Pavel Urysohn]\n\n\n_Continued on next page_\n18\n\n\nPublished as a conference paper at ICLR 2023\n\n\nHotpot QA Prompts \u2013 Continued from previous page\n\n\nObservation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_20250", "chunk_text": " paper at ICLR 2023\n\n\nHotpot QA Prompts \u2013 Continued from previous page\n\n\nObservation 1 Pavel Samuilovich Urysohn (February 3, 1898 - August 17, 1924) was a Soviet\nmathematician who is best known for his contributions in dimension theory.\nThought 2 Pavel Urysohn is a mathematician. I need to search Leonid Levin next and\nfind its type of work.\nAction 2 Search[Leonid Levin]\nObservation 2 Leonid Anatolievich Levin is a Soviet-American mathematician and computer\n\nscientist.\n\nThought 3 Leonid Levin is a mathematician and computer scientist. So Pavel Urysohn\nand Leonid Levin have the same type of work.\nAction 3 Finish[yes]\n\n\n19\n\n\nPublished as a conference paper at ICLR 2023\n\n\nC.2 FEVER\n\n\nFEVER Prompts\n\n\nOriginal Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\n\nthere is NOT ENOUGH INFORMATION.\n\n\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\n\nAnswer SUPPORTS\n\n\nClaim Stranger Things is set in Bloomington, Indiana.\n\nAnswer REFUTES\n\n\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\n\nAnswer NOT ENOUGH INFO\n\n\nAct Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\n\nthere is NOT ENOUGH INFORMATION.\n\n\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\nAction2 Search[Nikolaj Coster-Waldau]\nObservation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\nproducer. He graduated from the Danish National School of Performing Arts\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\nthe film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy\ndrama series Game of Thrones, for which he received two Primetime Emmy\nAward nominations for Outstanding Supporting Actor in a Drama Series..\nCoster-Waldau has appeared in numerous films in his native Denmark and\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\n(2013). In the U.S, his debut film role was in the war film Black Hawk Down\n(2001), playing Medal of Honor recipient Gary Gordon.[2]", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_20700", "chunk_text": " Headhunters (2011) and A Thousand Times Good Night\n(2013). In the U.S, his debut film role was in the war film Black Hawk Down\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\ndetective in the short-lived Fox television series New Amsterdam (2008), and\nappeared in the 2009 Fox television film Virtuality, originally intended as\na pilot.\nAction 2 Finish[SUPPORTS]\n\n\nClaim Stranger Things is set in Bloomington, Indiana.\nAction 1 Search[Stranger Things]\nObservation 1 Stranger Things is an American science fiction horror drama television\nseries created by the Duffer Brothers. Set in the 1980s, primarily in\nthe fictional town of Hawkins, Indiana, the series centers on a number of\nmysteries and supernatural events occurring around the town and their impact\n\n         - n an ensemble of child and adult characters.\n\nAction 2 Finish[REFUTES]\n\n\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\n\nAction 1 Search[Beautiful]\n\nObservation 1 Could not find [Beautiful]. Similar: [\u2019Beautiful\u2019, \u2019Beautiful, Beautiful\u2019,\n\u2019A Beautiful Mind (film)\u2019, \u2019Beautiful (Christina Aguilera song)\u2019, \u2019Life Is\nBeautiful\u2019].\nAction 2 Search[Beautiful (Christina Aguilera song)\nObservation 2 \"Beautiful\" is a song recorded by American singer Christina Aguilera for her\nfourth studio album, Stripped (2002).\nAction 3 Lookup[Billboard Hot 100]\nObservation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\nUnited States, where it was certified Gold for 500,000 units shipped.\nAction 4 Finish[NOT ENOUGH INFO]\n\n\nCoT Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\n\nthere is NOT ENOUGH INFORMATION.\n\n\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\nThought Nikolaj William Coster-Waldau appeared in the 2009 Fox television film\nVirtuality, so he has worked with the Fox Broadcasting Company.\n\nAnswer SUPPORTS\n\n\nClaim Stranger Things is set in Bloomington, Indiana.\nThought Stranger Things is in the fictional town of Hawkins, Indiana, not in\nBloomington, Indiana.\n\nAnswer REFUTES\n\n\n_Continued", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_21150", "chunk_text": " with the Fox Broadcasting Company.\n\nAnswer SUPPORTS\n\n\nClaim Stranger Things is set in Bloomington, Indiana.\nThought Stranger Things is in the fictional town of Hawkins, Indiana, not in\nBloomington, Indiana.\n\nAnswer REFUTES\n\n\n_Continued on next page_\n\n\n20\n\n\nPublished as a conference paper at ICLR 2023\n\n\n_Fever Prompts \u2013 Continued from previous page_\n\n\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\n\nThought The song peaked at number two on the Billboard Hot 100 in the United States,\n\nbut not sure if it was in 2003.\n\nAnswer NOT ENOUGH INFO\n\n\nReAct Determine if there is Observation that SUPPORTS or REFUTES a Claim, or if\n\nthere is NOT ENOUGH INFORMATION.\n\n\nClaim Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\nThought 1 I need to search Nikolaj Coster-Waldau and find if he has worked with the\nFox Broadcasting Company.\nAction 1 Search[Nikolaj Coster-Waldau]\nObservation 1 Nikolaj William Coster-Waldau (born 27 July 1970) is a Danish actor and\nproducer. He graduated from the Danish National School of Performing Arts\nin Copenhagen in 1993,[1] and had his breakthrough role in Denmark with\nthe film Nightwatch (1994). He played Jaime Lannister in the HBO fantasy\ndrama series Game of Thrones, for which he received two Primetime Emmy\nAward nominations for Outstanding Supporting Actor in a Drama Series..\nCoster-Waldau has appeared in numerous films in his native Denmark and\nScandinavia, including Headhunters (2011) and A Thousand Times Good Night\n(2013). In the U.S, his debut film role was in the war film Black Hawk Down\n(2001), playing Medal of Honor recipient Gary Gordon.[2] He then played a\ndetective in the short-lived Fox television series New Amsterdam (2008), and\nappeared in the 2009 Fox television film Virtuality, originally intended as\na pilot.\nThought 2 Because he \"appeared in the 2009 Fox television film Virtuality\", he should\nhave worked with the Fox Broadcasting Company.\nAction 2 Finish[SUPPORTS]\n\n\nClaim Stranger Things is set in Bloomington, Indiana.\nThought 1 I should search for Stranger Things, and see if it is set in Bloomington,\n\nIndiana.\n\nAction", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_21600", "chunk_text": "have worked with the Fox Broadcasting Company.\nAction 2 Finish[SUPPORTS]\n\n\nClaim Stranger Things is set in Bloomington, Indiana.\nThought 1 I should search for Stranger Things, and see if it is set in Bloomington,\n\nIndiana.\n\nAction 1 Search[Stranger Things]\nObservation 1 Stranger Things is an American science fiction horror drama television\nseries created by the Duffer Brothers. Set in the 1980s, primarily in\nthe fictional town of Hawkins, Indiana, the series centers on a number of\nmysteries and supernatural events occurring around the town and their impact\n\n         - n an ensemble of child and adult characters.\n\nThought 2 The observation says that it is set in a \"fictional town of Hawkins,\nIndiana\", so it is not set in Bloomington.\nAction 2 Finish[REFUTES]\n\n\nClaim Beautiful reached number two on the Billboard Hot 100 in 2003.\n\nThought 1 I need to search the song Beautiful and find if it reached number two on the\n\nBillboard Hot 100 in 2003.\n\nAction 1 Search[Beautiful]\n\nObservation 1 Could not find [Beautiful]. Similar: [\u2019Beautiful\u2019, \u2019Beautiful, Beautiful\u2019,\n\u2019A Beautiful Mind (film)\u2019, \u2019Beautiful (Christina Aguilera song)\u2019, \u2019Life Is\nBeautiful\u2019].\nThought 2 From suggestions, I should search \"Beautiful (Christina Aguilera song)\" to\nfind the song.\nAction 2 Search[Beautiful (Christina Aguilera song)]\nObservation 2 \"Beautiful\" is a song recorded by American singer Christina Aguilera for her\nfourth studio album, Stripped (2002).\nThought 3 It does not mention Billboard, so I need to look up \"Billboard Hot 100\" to\n\nfind if it reached number two on it in 2003.\n\nAction 3 Lookup[Billboard Hot 100]\nObservation 3 (Result 1 / 3) The song peaked at number two on the Billboard Hot 100 in the\nUnited States, where it was certified Gold for 500,000 units shipped.\nThought 4 It only says the song peaked at number two on the Billboard Hot 100, but not\n\nif it was in 2003. I am not sure if this claim is true or not.\n\nAction 4 Finish[NOT ENOUGH INFO]\n\n\n21\n\n\nPublished as a conference paper at ICLR 2023\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_22050", "chunk_text": " 100, but not\n\nif it was in 2003. I am not sure if this claim is true or not.\n\nAction 4 Finish[NOT ENOUGH INFO]\n\n\n21\n\n\nPublished as a conference paper at ICLR 2023\n\n\nC.3 WEBSHOP\n\n\nTable 6: Prompts used for the Webshop experiments in Section 4. We use the prompt on the left for\nAct, and use the prompt on the right (add reasoning) for ReAct.\n\n\n**Instruction** : i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars\n\n\nAct ReAct\n\n\n\n**Action** : **search** \n\n\n3 ounce bright citrus deodorant sensitive skin\ufffd\n\n\n\n**Observation:**\n\n- Back to Search \nPage 1 (Total results: 50)\n\n- Next\ufffd\n\n\n- B078GWRC1J\ufffd\n\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\n\n- B078GTKVXY\ufffd\n\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\n\n- B08KBVJ4XN\ufffd\n\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\n2.7 oz, 2-Pack)\n$15.95\n\n\n**Action** : **click** - B078GWRC1J\ufffd\n\n**Observation** :\n\n- Back to Search\ufffd\n\n- Prev\ufffd\n\nscent [assorted scents][bright citrus][calming lavender][ginger\n\n[fresh] simply non-scents]\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\n\n- pack)]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n\n[Description]\n\n[Features]\n\n[Reviews]\n\n[Buy Now]\n\n\n**Action** : **click** [bright citrus", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_22500", "chunk_text": " and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n\n[Description]\n\n[Features]\n\n[Reviews]\n\n[Buy Now]\n\n\n**Action** : **click** [bright citrus]\n**Observation** : You have clicked bright citrus.\n\n\n**Action** : **click** [3 ounce (pack of 1)]\n**Observation** : You have clicked 3 ounce (pack of 1).\n\n\n**Action** : **click** [Buy Now]\n\n\n\nPage 1 (Total results: 50)\n\n - Next\ufffd\n\n\n - B078GWRC1J\ufffd\n\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\n\n - B078GTKVXY\ufffd\n\nGinger Fresh Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\n$10.99\n\n - B08KBVJ4XN\ufffd\n\nBarrel and Oak - Aluminum-Free Deodorant, Deodorant for Men\n, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar\n& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage,\n2.7 oz, 2-Pack)\n$15.95\n\n\n**Action** : **think** [B078GWRC1J and B078GTKVXY are bright citrus\ndeodorant less then 50 dollars. I can check B078GWRC1J first.\n**Observation** : OK.\n\n\n**Action** : **click**  - B078GWRC1J\ufffd\n\n**Observation** :\n\n - Back to Search\ufffd\n\n - Prev\ufffd\n\nscent [assorted scents][bright citrus][calming lavender][ginger\n\n[fresh] simply non-scents]\nsize [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2\n\n - pack)]\nBright Citrus Deodorant by Earth Mama | Natural and Safe for\nSensitive Skin, Pregnancy and Breastfeeding, Contains Organic\nCalendula 3-Ounce\nPrice: $10.99\nRating: N.A.\n\n[Description]\n\n[Features]\n\n[Reviews]\n\n[Buy Now]\n\n\n**Action** : **think** [For 3 ounce bottle of bright citrus deodor", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_22950", "chunk_text": "endula 3-Ounce\nPrice: $10.99\nRating: N.A.\n\n[Description]\n\n[Features]\n\n[Reviews]\n\n[Buy Now]\n\n\n**Action** : **think** [For 3 ounce bottle of bright citrus deodorant\nfor sensitive skin, the item has options \u2019bright citrus\u2019\nand \u20193 ounce (pack of 1)\u2019 and seems good to buy.]\n**Observation** : OK.\n\n\n**Action** : **click** [bright citrus]\n**Observation** : You have clicked bright citrus.\n\n\n**Action** : **click** [3 ounce (pack of 1)]\n**Observation** : You have clicked 3 ounce (pack of 1).\n\n\n**Action** : **click** [Buy Now]\n\n\n22\n\n\n\n**Action** : **search** - 3 ounce bright citrus deodorant sensitive skin\n\n**Observation:**\n\n- Back to Search \n\nPublished as a conference paper at ICLR 2023\n\n\nC.4 ALFWORLD\n\n\nTable 7: An Act prompt on the ALFWorld clean task. No thoughts are provided.\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n_>_ go to fridge 1\nThe fridge 1 is closed.\n_>_ - pen fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n_>_ go to diningtable 1\nOn the diningtable 1", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_23400", "chunk_text": " The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n_>_ go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n_>_ take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n_>_ go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n_>_ clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n_>_ think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.\n\nOK.\n\n_>_ go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n_>_ put lettuce 1 in/on diningtable 1\nYou put the lettuce 1 in/on the diningtable 1.\n\n\n23\n\n\nPublished as a conference paper at ICLR 2023\n\n\nTable 8: An ReAct prompt on the ALFWorld clean task.\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_23850", "chunk_text": "11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n\n - think: To solve the task, I need to find and take a lettuce, then clean it with\nsinkbasin, then put it in diningtable.\n\nOK.\n\n - think: First I need to find a lettuce. A lettuce is more likely to appear in\nfridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13). I\ncan check one by one, starting with fridge 1.\n\nOK.\n\n - go to fridge 1\nThe fridge 1 is closed.\n\n - open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n - think: Now I find a lettuce (1). Next, I need to take it.\n\nOK.\n\n - take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n\n - think: Now I take a lettuce (1). Next, I need to go to sinkbasin", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_24300", "chunk_text": " it.\n\nOK.\n\n - take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n\n - think: Now I take a lettuce (1). Next, I need to go to sinkbasin (1) and clean\n\nit.\n\nOK.\n\n - go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n\n - clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n\n - think: Now I clean a lettuce (1). Next, I need to put it in/on diningtable 1.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n\n - put lettuce 1 in/on diningtable 1\nYou put the lettuce 1 in/on the diningtable 1.\n\n\n24\n\n\nPublished as a conference paper at ICLR 2023\n\n\nTable 9: An ReAct-IM prompt on the ALFWorld clean task.\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_24750", "chunk_text": " sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n\n - think: To solve the task, I need to find and take a lettuce, then clean it with\nsinkbasin, then put it in diningtable.\n\nOK.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - go to fridge 1\nThe fridge 1 is closed.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n\n - think: I need to clean this lettuce (1) using sinkbasin 1.\n\nOK.\n\n - go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n\n - think: I need to clean this lettuce (1) using sinkbasin 1.\n\nOK.\n\n - clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n\n - think: I need to put this cleaned lettuce (1) in/on diningtable 1.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup ", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_25200", "chunk_text": " this cleaned lettuce (1) in/on diningtable 1.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n\n - think: I need to put this cleaned lettuce (1) in/on diningtable 1.\n\nOK.\n\n - put lettuce 1 in/on diningtable 1\nYou put the lettuce 1 in/on the diningtable 1.\n\n\nD TRAJECTORIES\n\n\nD.1 FEVER TRAJECTORIES\n\n\nWe provide some trajectories produced by ReAct, CoTand Acton several randomly selected examples from the FEVER dev set below. Search results are omitted to space.\n\n\n25\n\n\nPublished as a conference paper at ICLR 2023\n\n\nExample 2491 (gt: REFUTES)\n\nReAct\n\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\nThought 1: I need to search Bermuda Triangle and find if it is in the Pacific Ocean.\nAction 1: Search[Bermuda Triangle]\nObservation 1: Reportedly haunted locations...\nThought 2: The observation says that it is in the western part of the North Atlantic\nOcean, so it is not in the Pacific Ocean.\nAction 2: Finish[REFUTES]\nObservation 2: Episode finished\n\nAct\n\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\nAction 1 : Search[Bermuda Triangle]\nObservation 1 : Reportedly haunted locations:...\nAction 2: Finish[REFUTES]\nObservation 2: Episode finished\n\nCoT\n\nClaim: Bermuda Triangle is a loosely-defined region in the Pacific Ocean.\nThought: Let\u2019s think step by step. The Bermuda Triangle is a loosely-defined region\n\nin the Atlantic Ocean. The Pacific Ocean is not the Atlantic Ocean.\n\nAnswer: REFUTES\n\n\nExample 5908 (gt: SUPPORTS)\n\nReAct\n\nClaim: Princess Mononoke is a film.\n\nThought 1: I need to search Princess Mononoke and find if it is a film.\nAction 1: Search[Princess Monon", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_25650", "chunk_text": "8 (gt: SUPPORTS)\n\nReAct\n\nClaim: Princess Mononoke is a film.\n\nThought 1: I need to search Princess Mononoke and find if it is a film.\nAction 1: Search[Princess Mononoke]\n\nObservation 1: Princess Mononoke ...\n\nThought 2: From the observation, it says that Princess Mononoke is a film.\nAction 2: Finish[SUPPORTS]\nObservation 2: Episode finished\n\nAct\n\nClaim: Princess Mononoke is a film.\n\nAction 1: Search[Princess Mononoke]\n\nObservation 1: Princess Mononoke ...\n\nAction 2: Finish[SUPPORTS]\nObservation 2: Episode finished\n\nCoT\n\nClaim: Princess Mononoke is a film.\n\nThought: Let\u2019s think step by step. Princess Mononoke is a movie, and movies are\n\nfilms.\n\nAnswer: SUPPORTS\n\n\nExample 1951 (gt: REFUTES)\n\nReAct\n\nClaim: Soyuz was part of the American space program.\nThought 1: I need to search Soyuz and find if it was part of the American space\n\nprogram.\nAction 1: Search[Soyuz]\nObservation 1: Soyuz is a transliteration...\nThought 2: It does not mention the American space program, so I need to search\n\"American space program\" to find if it was part of it.\nAction 2: Search[American space program]\nObservation 2: The space policy...\nThought 3: It does not mention Soyuz, so I am not sure if this claim is true or not.\nAction 3: Finish[NOT ENOUGH INFO]\nObservation 3: Episode finished\n\nAct\n\nClaim: Soyuz was part of the American space program.\nAction 1: Search[Soyuz]\nObservation 1: Soyuz is a transliteration...\nAction 2: Finish[NOT ENOUGH INFO]\nObservation 2: Episode finished\n\nCoT\n\nClaim: Soyuz was part of the American space program.\nThought: Let\u2019s think step by step. Soyuz is a Russian spacecraft. The American\nspace program is NASA. NASA and Russia have worked together on the\nInternational Space Station.\n\nAnswer: SUPPORTS\n\n\n26\n\n\nPublished as a conference paper at ICLR 2023\n\n\nExample 3208 (gt: REFUTES)\n\nReAct\n\nClaim: Reign Over Me is an American", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_26100", "chunk_text": " have worked together on the\nInternational Space Station.\n\nAnswer: SUPPORTS\n\n\n26\n\n\nPublished as a conference paper at ICLR 2023\n\n\nExample 3208 (gt: REFUTES)\n\nReAct\n\nClaim: Reign Over Me is an American film made in 2010.\nThought 1: I need to search for Reign Over Me and find if it is an American film made\n\nin 2010.\n\nAction 1: Search[Reign Over Me]\nObservation 1: Reign Over Me ...\nThought 2: The observation says that it is an American film made in 2007, so it is not\n\nmade in 2010.\n\nAction 2: Finish[REFUTES]\nObservation 2: Episode finished\n\nAct\n\nClaim: Reign Over Me is an American film made in 2010.\nAction 1: Search[Reign Over Me]\nAction 2: Finish[SUPPORTS]\nObservation 2: Episode finished\n\nCoT\n\nClaim: Reign Over Me is an American film made in 2010.\nThought: Let\u2019s think step by step. First, is Reign Over Me an American film? Yes,\nit is. Second, was it made in 2010? Yes, it was.\n\nAnswer: SUPPORTS\n\n\nD.2 ALFWORLD TRAJECTORIES\n\n\nWe provide an example trajectory of ReAct, Act, and ReAct-IM on the same ALFWorld game,\npick_clean_then_place_in_recep-Knife-None-CounterTop-10/\ntrial_T20190909_110445_675754.\n\n\nD.2.1 REACT TRAJECTORY\n\n\nReAct is able to correctly find the knife, clean it, and then place it on a countertop.\n\n\n27\n\n\nPublished as a conference paper at ICLR 2023\n\n\nReAct ALFWorld trajectory\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_26550", "chunk_text": " 1, a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n\n1.\n\nYour task is to: put a clean knife in countertop.\n\n - think: To solve the task, I need to find and take a knife, then clean it with\nsinkbasin, then put it in countertop.\n\nOK.\n\n - think: First I need to find a knife. A knife is more likely to appear in cabinet\n(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),\nsinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one, starting with\n\ncabinet 1.\n\nOK.\n\n - go to cabinet 1\nOn the cabinet 1, you see a bowl 1.\n\n - go to cabinet 2\n\nThe cabinet 2 is closed.\n\n - go to cabinet 3\nOn the cabinet 3, you see a glassbottle 1.\n\n - go to cabinet 4\nOn the cabinet 4, you see a mug 1.\n\n - go to cabinet 5\n\nThe cabinet 5 is closed.\n\n - go to cabinet 6\n\nThe cabinet 6 is closed.\n\n - go to drawer 1\n\nThe drawer 1 is closed.\n\n - go to drawer 2\n\nThe drawer 2 is closed.\n\n - go to drawer 3\n\nThe drawer 3 is closed.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n\n - think: Now I find a knife (1). Next, I need to take it.\n\nOK.\n\n - take knife 1 from", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_27000", "chunk_text": ", a knife 1, a\nplate 2, a potato 3, and a statue 1.\n\n - think: Now I find a knife (1). Next, I need to take it.\n\nOK.\n\n - take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n\n - think: Now I take a knife (1). Next, I need to go to sinkbasin (1) and clean it.\n\nOK.\n\n - go to sinkbasin 1\nOn the sinkbasin 1, you see a fork 3, a lettuce 3, and a spatula 2.\n\n - clean knife 1 with sinkbasin 1\n\nYou clean the knife 1 using the sinkbasin 1.\n\n - think: Now I clean a knife (1). Next, I need to put it in/on countertop 1.\n\nOK.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - put knife 1 in/on countertop 1\nYou put the knife 1 in/on the countertop 1.\n\n\nD.2.2 ACT TRAJECTORY\n\n\nAct is able to correctly find the knife, but tries to clean it using a sinkbasin before actually going\nto the sinkbasin. From there on, the model is unable to recover, and is stuck repeating the same\nsequence of commands. With ReAct, an existing thought would have concretely identified that the\nknife had been taken, and that the next subgoal was to go to a sinkbasin to clean it.\n\n\n28\n\n\nPublished as a conference paper at ICLR 2023\n\n\nAct ALFWorld trajectory\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sink", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_27450", "chunk_text": " a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n\n1.\n\nYour task is to: put a clean knife in countertop.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\n", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_27900", "chunk_text": " to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n\nD", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_28350", "chunk_text": "1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n\nD.2.3 REACT-IM TRAJECTORY\n\n\nReAct-IM is able to correctly find the knife, but is unable to clean it as well. The LLM then\ngets stuck trying to place this knife on a countertop. One possible issue is the incorrect thought\nthink: I need to find a clean knife., which seems to trick the LLM in believing\nthat the knife is already cleaned. In ReAct, this sequence of subgoals would be concretely captured\nby each thought.\n\n\n29\n\n\nPublished as a conference paper at ICLR 2023\n\n\nReAct-IM ALFWorld trajectory\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet 6, a\ncabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a coffeemachine 1, a\ncountertop 3, a countertop 2, a countertop 1, a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n\n1.\n\nYour task is to: put a clean knife in countertop.\n\n - think: To solve the task, I need to find and take a clean knife, then put it in\n\ncountertop.\n\nOK.\n\n - think: I need to find a clean knife.\n\nOK.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - think: I need to find a clean knife.\n\nOK.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_28800", "chunk_text": ".\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n\n - think: I need to find a clean knife.\n\nOK.\n\n - take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n\n - think: I need to put this knife (1) in/on countertop 1.\n\nOK.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - think: I need to put this knife (1) in/on countertop 1.\n\nOK.\n\n - put knife 1 in/on countertop 1\nYou put the knife 1 in/on the countertop 1.\n\n - think: I need to put this knife (1) in/on countertop 1.\n\nOK.\n\n - put knife 1 in/on countertop 1\nNothing happens.\n\n - think: I need to put this knife (1) in/on countertop 1.\n\nOK.\n\n - put knife 1 in/on countertop 1\nNothing happens.\n\n - think: I need to put this knife (1) in/on countertop 1.\n\nOK.\n\n - put knife 1 in/on countertop 1\nNothing happens.\n\n - think: I need to put this knife (1) in/on countertop 1.\n\nOK.\n\n - put knife 1 in/on countertop 1\nNothing happens.\n\n - think: I need to put this knife (1) in/on countertop 1.\n\nOK.\n\n - put knife 1 in/on countertop 1\nNothing happens.\n\n - think: I need to put this knife (1) in/on countertop 1.\n\nOK.\n\n\n30\n\n\nPublished as a conference paper at ICLR 2023\n\n\nD.3 WEBSHOP TRAJECTORIES\n\n\nTable 10: Example trajectories for Webshop predicted by Act and ReAct. Compared to Act,\nReAct uses reasoning to find products that satisfy all target attributes.\n\n\n**Instruction** : get me a sixteen pack of apple cinnamon freeze dried banana chips, and price lower than 50.00 dollars\n\n\nAct ReAct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Action:", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_29250", "chunk_text": " Act,\nReAct uses reasoning to find products that satisfy all target attributes.\n\n\n**Instruction** : get me a sixteen pack of apple cinnamon freeze dried banana chips, and price lower than 50.00 dollars\n\n\nAct ReAct\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Action:search\u0002sixteenpackapplecinnamonfreezedriedbanana<br>chips\u0003<br>Observation:<br>\u0002BacktoSearch\u0003<br>Page1(Totalresults:50)<br>\u0002Nex t\u0003<br>\u0002B0061IVFZE\u0003<br>Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0 .42 -<br>Ounce(Packof100)<br>$850<br>.<br>\u0002B096H2P6G2\u0003<br>Moon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -<br>(VarietyPack)<br>$1899<br>.<br>\u0002B092JLLYK6\u0003<br>Nature\u2019s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect<br>For School Lunches or an On-The-Go Snack - No Sugar Added,<br>N $1o 2n 9G 9MO, Gluten Free, Nothing Artificial (0 .53oz) 6-Pack<br>.<br>Action:click\u0002B0061IVFZE\u0003<br>Observation:<br>\u0002BacktoSearch\u0003<br>\u0002Prev\u0003<br>flavorname[asianpear][banana][fujiapple&cinnamon][strawberry<br>banana] Brothers-ALL-Natural Fruit Crisps, Strawberry Banana,<br>0 P. r4 ic2 O $u 8n 5c .e (Pack of 100)<br>e : 0<br>Rating: N .A<br>.<br>[Description]<br>[Features]<br>[Reviews]<br>[BuyNow]<br>Action:click[BuyNow]|Action:search\u0002sixteenpackapplecinnamonfreezedriedbanana<br>chips\u0003<br>Observation:<br>\u0002BacktoSearch\u0003<br>Page1(Totalresults:50)<br>\u0002Nex t\u0003<br>\u0002B0061IVFZE\u0003<br>Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0 .42 -<br>Ounce(Packof100)<", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_29700", "chunk_text": ")<br>\u0002Nex t\u0003<br>\u0002B0061IVFZE\u0003<br>Brothers-ALL-Natural Fruit Crisps, Strawberry Banana, 0 .42 -<br>Ounce(Packof100)<br>$850<br>.<br>\u0002B096H2P6G2\u0003<br>Moon Fruit Freeze Dried Fruit Snacks. Fruit Snacks for Kids -<br>(VarietyPack)<br>$1899<br>.<br>\u0002B092JLLYK6\u0003<br>Nature\u2019s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect<br>For School Lunches or an On-The-Go Snack - No Sugar Added,<br>N $1o 2n 9G 9MO, Gluten Free, Nothing Artificial (0 .53oz) 6-Pack<br>.<br>Action: think[B0061IVFZE is strawberry banana, not apple cinnamon.<br>B096H2P6G2 is fruit snacks, not freeze dried banana chips.<br>B092JLLYK6 is banana crisps, not apple cinnamon. I can check<br>B092JLLYK6 first<br>.<br>Observation: OK<br>.<br>Action:click\u0002B092JLLYK6\u0003<br>Observation:<br>\u0002BacktoSearch\u0003<br>\u0002Prev\u0003<br>flavorname[apple][applecinnamon][banana][cantaloupe][peach][pear]<br>[strawberry][strawberrybanana][samplervarietypack][megavariety<br>pack][orchardvarietypack]<br>size [0 .53 ounce (pack of 6)][0 .53 ounce (pack of 8)][0 .53 ounce (pack<br>of16)]<br>Nature\u2019s Turn Freeze-Dried Fruit Snacks - Banana Crisps - Perfect<br>For School Lunches or an On-The-Go Snack - No Sugar Added, Non<br>G PrM icO e:, $G 1l 2u .t 9e 9n Free, Nothing Artificial (0 .53oz) 6-Pack<br>Rating: N .A<br>.<br>[Description]<br>[Features]<br>[Reviews]<br>[BuyNow]<br>Action:think[Forsixteenpackofapplec", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_30150", "chunk_text": " Nothing Artificial (0 .53oz) 6-Pack<br>Rating: N .A<br>.<br>[Description]<br>[Features]<br>[Reviews]<br>[BuyNow]<br>Action:think[Forsixteenpackofapplecinnamonfreezedriedbanana<br>chips, the item has options \u2019apple cinnamon\u2019 and \u20190 .53 ounce (pack of<br>16)\u2019 and seems good to buy.]<br>Observation: OK<br>.<br>Action:click[applecinnamon]<br>Observation: You have clicked apple cinnamon.<br>Action: click[0 .53 ounce (pack of 16)]<br>Observation: You have clicked 0 .53 ounce (pack of 16) .<br>Action:click[BuyNow]|\n|---|---|\n|**Score**: 0.125|**Score**: 1.0|\n\n\n31\n\n\nPublished as a conference paper at ICLR 2023\n\n\nE MORE ANALYSIS\n\n\nE.1 SUCCESS AND FAILURE MODES ANALYSIS\n\n\nWe provide some examples corresponding to the success and error mode analysis given in Sec. 3.3.\nSearch results and non-representative steps are omitted to reduce space.\n\n\nSuccess: True positive\n\nReAct\n\nQuestion: Author David Chanoff has collaborated with a U.S. Navy admiral who served as\nthe ambassador to the United Kingdom under which President?\nThought 1: I need to search David Chanoff and find the U.S. Navy admiral he\n\ncollaborated with.\n\nAction 1: Search[David Chanoff]\n\nObservation 1: David Chanoff is a noted author...\n\nAction 2: Search[William J. Crowe]\n\nObservation 2: William J. Crowe is...\n\nThought 3: William J. Crowe served as the ambassador to the United Kingdom under\n\nPresident Bill Clinton. So the answer is Bill Clinton.\n\nAction 3: Finish[Bill Clinton]\nObservation 3: Episode finished\n\nCoT\n\nQuestion: What was the name of the team that represented an Arizona university in\nthe conference that covers a broad expanse of the western United States,\nincluding Arizona, California, and New Mexico?\nThought: Let\u2019s think step by step. The name of the team that represented an Arizona\nuniversity in the conference that covers a broad expanse of the western\nUnited States, including Arizona, California, and New Mexico is Arizona", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_30600", "chunk_text": " New Mexico?\nThought: Let\u2019s think step by step. The name of the team that represented an Arizona\nuniversity in the conference that covers a broad expanse of the western\nUnited States, including Arizona, California, and New Mexico is Arizona\n\nState Sun Devils.\n\nAnswer: Arizona State Sun Devils\n\n\nSuccess: False positive\n\nReAct\n\nQuestion: Did the board game San Marco or About Time come out first?\nThought 1: I need to search San Marco and About Time, and find which came out first.\nAction 1: Search[San Marco]\n\nObservation 1: San Marco is one of the ... The was built in venice.\n\nThought 2: San Marco is a sestiere of Venice. I need to search About Time next.\nAction 2: Search[About Time]\n\nObservation 2: Could not find [About Time]. Similar: [...].\nThought 3: About Time is a 2013 film. So San Marco came out first.\nAction 3: Finish[San Marco]\nObservation 3: Episode finished\n\nCoT\n\nQuestion: Was Vanderbilt University or Emory University founded first?\nThought: Let\u2019s think step by step. Vanderbilt University was founded in 1873. Emory\nUniversity was founded in 1836. 1873 (Vanderbilt University) > 1836 (Emory\nUniversity), so Vanderbilt University was founded first.\nAnswer: Emory University\n\n\nFailure: Reasoning error\n\nReAct\n\nQuestion: What was the middle name of the actress who was cast alongside Harvey\nFierstein, Linda Hart, Dick Latessa, Clarke Thorell, Mary Bond Davis, Laura\nBell Bundy, Matthew Morrison, Corey Reynolds, and Marissa Jaret Winokur in\nHairspray?\nThought 1: I need to search Harvey Fierstein, Linda Hart, Dick Latessa, Clarke Thorell,\nMary Bond Davis, Laura Bell Bundy, Matthew Morrison, Corey Reynolds, and\nMarissa Jaret Winokur, find the actress that was cast alongside them in\nHairspray, then find its middle name.\nAction 1: Search[Harvey Fierstein]\nObservation 1: Harvey Forbes Fierstein ...\nThought 2: Harvey Fierstein is an actor, playwright, and screenwriter. I need to\n\nsearch Linda Hart next...\n\nAction 2: Search[Linda Hart]...\n\nCoT\n\nQuestion: Who is", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_31050", "chunk_text": " Harvey Forbes Fierstein ...\nThought 2: Harvey Fierstein is an actor, playwright, and screenwriter. I need to\n\nsearch Linda Hart next...\n\nAction 2: Search[Linda Hart]...\n\nCoT\n\nQuestion: Who is older Danny Green or James Worthy?\nThought: Let\u2019s think step by step. Danny Green was born in 1987. James Worthy was\nborn in 1961. 1987 (Danny Green) > 1961 (James Worthy), so Danny Green is\n\n         - lder.\n\nAnswer: Danny Green\n\n\n32\n\n\nPublished as a conference paper at ICLR 2023\n\n\nFailure: Search error\n\nReAct\n\nQuestion: What does the goddess associated with the goddess frigg consists of what\n\ntales?\n\nThought 1: I need to search goddess frigg and find its associated goddess.\nAction 1: Search[goddess frigg]\nObservation 1: Could not find goddess frigg. Similar: [\u2019Frigg\u2019, \u2019Frigg and Freyja common\n\n         - rigin hypothesis\u2019, \u2019Freyja (section Relation to Frigg and other goddesses\nand figures)\u2019, \u2019Eir (redirect from Eir (goddess))\u2019, \u2019Baldr\u2019].\nThought 2: I need to search Frigg instead...\n\n\nFailure: Hallucination\n\nCoT\n\nQuestion: What was the first year a scientific journal published by an organization\nlocated in the Bronx was published?\nThought: Let\u2019s think step by step. The first year a scientific journal published by\nan organization located in the Bronx was published was 1916.\nAnswer: 1916 (Label: 1909)\n\n\nFailure: Label ambiguity\n\nReAct\n\nQuestion: What nationality is the man who developed the dictator game?\nThought 1: I need to search dictator game, find the man who developed it, then find his\nnationality.\nAction 1: Search[dictator game]\nObservation 1: The dictator game is a popular experimental instrument in social psychology\n\nand economics...\n\nAction 3: Search[Amos Tversky]\nObservation 3: Amos Nathan Tversky...\nThought 4: Amos Tversky is an Israeli cognitive and mathematical psychologist. So the\nman who developed the dictator game is Israeli.\nAction 4: Finish[Israeli] (label: Israel-American)\n\nCoT\n\nQuestion: Kurt Fuller from Stockton", "token_count": 500, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2210.03629_react_yao:chunk_31500", "chunk_text": ": Amos Tversky is an Israeli cognitive and mathematical psychologist. So the\nman who developed the dictator game is Israeli.\nAction 4: Finish[Israeli] (label: Israel-American)\n\nCoT\n\nQuestion: Kurt Fuller from Stockton, CA appeared in which American Detective Comedy\nDrama television series created by Steve Franks?\nThought: Let\u2019s think step by step. Kurt Fuller from Stockton, CA appeared in Psych.\nPsych is an American Detective Comedy Drama television series created by\n\nSteve Franks.\n\nAnswer: Psych (label: Psych is an American detective comedy-drama)\n\n\n33\n\n\n", "token_count": 122, "metadata": {"arxiv_id": "2210.03629", "title": "ReAct: Synergizing Reasoning and Acting in Language Models", "authors": ["Shunyu Yao", "Jeffrey Zhao", "Dian Yu", "Nan Du", "Izhak Shafran", "Karthik Narasimhan", "Yuan Cao"], "year": 2022, "url": "https://arxiv.org/pdf/2210.03629v3"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_0", "chunk_text": "## **From Local to Global: A GraphRAG Approach to** **Query-Focused Summarization**\n\n**Darren Edge** **[1\u2020]** **Ha Trinh** **[1\u2020]** **Newman Cheng** **[2]** **Joshua Bradley** **[2]** **Alex Chao** **[3]**\n\n\n**Apurva Mody** **[3]** **Steven Truitt** **[2]** **Dasha Metropolitansky** **[1]** **Robert Osazuwa Ness** **[1]**\n\n\n**Jonathan Larson** **[1]**\n\n\n1Microsoft Research\n2Microsoft Strategic Missions and Technologies\n3Microsoft Office of the CTO\n\n\n_{_ `daedge,trinhha,newmancheng,joshbradley,achao,moapurva,`\n`steventruitt,dasham,robertness,jolarso` _}_ `@microsoft.com`\n\n\n                - These authors contributed equally to this work\n\n\n**Abstract**\n\n\nThe use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs)\nto answer questions over private and/or previously unseen document collections.\nHowever, RAG fails on global questions directed at an entire text corpus, such\nas \u201cWhat are the main themes in the dataset?\u201d, since this is inherently a queryfocused summarization (QFS) task, rather than an explicit retrieval task. Prior\nQFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we\npropose _GraphRAG_, a graph-based approach to question answering over private\ntext corpora that scales with both the generality of user questions and the quantity\n\n     - f source text. Our approach uses an LLM to build a graph index in two stages:\nfirst, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a\nquestion, each community summary is used to generate a partial response, before\nall partial responses are again summarized in a final response to the user. For a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that GraphRAG leads to substantial improvements over a conventional\nRAG baseline for both the comprehensiveness and diversity of generated answers.\n\n\n**1**", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_450", "chunk_text": " a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that GraphRAG leads to substantial improvements over a conventional\nRAG baseline for both the comprehensiveness and diversity of generated answers.\n\n\n**1** **Introduction**\n\n\nRetrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using\nLLMs to answer queries based on data that is too large to contain in a language model\u2019s _context_\n_window_, meaning the maximum number of _tokens_ (units of text) that can be processed by the LLM\nat once (Kuratov et al., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to\na large external corpus of text records and retrieves a subset of records that are individually relevant\nto the query and collectively small enough to fit into the context window of the LLM. The LLM then\n\n\nPreprint. Under review.\n\n\ngenerates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang,\n2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call\n_vector RAG_, works well for queries that can be answered with information localized within a small\nset of records. However, vector RAG approaches do not support _sensemaking_ queries, meaning\nqueries that require global understanding of the entire dataset, such as \u201d _What are the key trends in_\n_how scientific discoveries are influenced by interdisciplinary research over the past decade?_ \u201d\n\n\n_Sensemaking_ tasks require reasoning over \u201c _connections (which can be among people, places, and_\n_events) in order to anticipate their trajectories and act effectively_ \u201d (Klein et al., 2006). LLMs such\nas GPT (Achiam et al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil\net al., 2023) excel at sensemaking in complex domains like scientific discovery (Microsoft, 2023)\nand intelligence analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an\nimplicit and interconnected set of concepts, an LLM can generate a summary that answers the query", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_900", "chunk_text": "Microsoft, 2023)\nand intelligence analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an\nimplicit and interconnected set of concepts, an LLM can generate a summary that answers the query.\nThe challenge, however, arises when the volume of data requires a RAG approach, since vector RAG\napproaches are unable to support sensemaking over an entire corpus.\n\n\nIn this paper, we present **GraphRAG** - a graph-based RAG approach that enables sensemaking over\nthe entirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph,\nwhere nodes correspond to key entities in the corpus and edges represent relationships between those\nentities. Next, it partitions the graph into a hierarchy of communities of closely related entities,\nbefore using an LLM to generate community-level summaries. These summaries are generated in\na bottom-up manner following the hierarchical structure of extracted communities, with summaries\nat higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these\ncommunity summaries provide global descriptions and insights over the corpus. Finally, GraphRAG\nanswers queries through map-reduce processing of community summaries; in the map step, the\nsummaries are used to provide partial answers to the query independently and in parallel, then in the\nreduce step, the partial answers are combined and used to generate a final global answer.\n\n\nThe GraphRAG method and its ability to perform global sensemaking over an entire corpus form\nthe main contribution of this work. To demonstrate this ability, we developed a novel application\n\n- f the LLM-as-a-judge technique (Zheng et al., 2024) suitable for questions targeting broad issues\nand themes where there is no ground-truth answer. This approach first uses one LLM to generate\na diverse set of global sensemaking questions based on corpus-specific use cases, before using a\nsecond LLM to judge the answers of two different RAG systems using predefined criteria (defined\nin Section 3.3). We use this approach to compare GraphRAG to vector RAG on two representative\nreal-world text datasets. Results show GraphRAG strongly outperforms vector RAG when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https://github _._ [com/microsoft/graphrag. In ad-](https://github.com/microsoft/graphrag)\ndition, versions", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_1350", "chunk_text": " when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https://github _._ [com/microsoft/graphrag. In ad-](https://github.com/microsoft/graphrag)\ndition, versions of the GraphRAG approach are also available as extensions to multiple opensource libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), NebulaGraph (NebulaGraph, 2024), and Neo4J (Neo4J, 2024).\n\n\n**2** **Background**\n\n\n**2.1** **RAG Approaches and Systems**\n\n\nRAG generally refers to any system where a user query is used to retrieve relevant information from\nexternal data sources, whereupon this information is incorporated into the generation of a response\nto the query by an LLM (or other generative AI model, such as a multi-media model). The query and\nretrieved records populate a prompt template, which is then passed to the LLM (Ram et al., 2023).\nRAG is ideal when the total number of records in a data source is too large to include in a single\nprompt to the LLM, i.e. the amount of text in the data source exceeds the LLM\u2019s context window.\n\n\nIn canonical RAG approaches, the retrieval process returns a set number of records that are semantically similar to the query and the generated answer uses only the information in those retrieved\nrecords. A common approach to conventional RAG is to use text embeddings, retrieving records\nclosest to the query in vector space where closeness corresponds to semantic similarity (Gao et al.,\n2023). While some RAG approaches may use alternative retrieval mechanisms, we collectively refer\nto the family of conventional approaches as _vector RAG_ . GraphRAG contrasts with vector RAG in\nits ability to answer queries that require global sensemaking over the entire data corpus.\n\n\n2\n\n\nGraphRAG builds upon prior work on advanced RAG strategies. GraphRAG leverages summaries\n\n- ver large sections of the data source as a form of \u201dself-memory\u201d (described in Cheng et al. 2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_1800", "chunk_text": "2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao et al., 2023; Khattab et al., 2022; Shao et al., 2023; Su et al., 2020; Trivedi et al., 2022;\nWang et al., 2024). In particular, GraphRAG is similar to other approaches that use hierarchical\nindexing to create summaries (similar to Kim et al. 2023; Sarthi et al. 2024). GraphRAG contrasts\nwith these approaches by generating a graph index from the source data, then applying graph-based\ncommunity detection to create a thematic partitioning of the data.\n\n\n**2.2** **Using Knowledge Graphs with LLMs and RAG**\n\n\nApproaches to knowledge graph extraction from natural language text corpora include rulematching, statistical pattern recognition, clustering, and embeddings (Etzioni et al., 2004; Kim et al.,\n2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of\nresearch that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022;\nOpenAI, 2023; Tan et al., 2017; Trajanoska et al., 2023; Yao et al., 2023; Yates et al., 2007; Zhang\net al., 2024a). It also adds to a growing body of RAG approaches that use a knowledge graph as\nan index (Gao et al., 2023). Some techniques use subgraphs, elements of the graph, or properties\n\n- f the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023)\n\n- r as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_2250", "chunk_text": "hi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n(e.g., passages, tables) and edges encoding lexical and semantical similarity or structural relationships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of\ngraphs in this context: their inherent _modularity_ (Newman, 2006) and the ability to partition graphs\ninto nested modular communities of closely related nodes (e.g., Louvain, Blondel et al. 2008; Leiden, Traag et al. 2019). Specifically, GraphRAG recursively creates increasingly global summaries\nby using the LLM to create summaries spanning this community hierarchy.\n\n\n**2.3** **Adaptive benchmarking for RAG Evaluation**\n\n\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthese benchmarks are oriented towards vector RAG performance, i.e., they evaluate performance\n\n- n explicit fact retrieval. In this work, we propose an approach for generating a set of questions\nfor evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM\nmethods that use a corpus to generate questions whose answers would be summaries of the corpus,\nsuch as in Xu and Lapata (2021). However, in order to produce a fair evaluation, our method avoids\ngenerating the questions directly from the corpus itself (as an alternative implementation, one can\nuse a subset of the corpus held out from subsequent graph extraction and answer evaluation steps).\n\n\n_Adaptive benchmarking_ refers to the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases. Recent work has used LLMs for adaptive benchmarking\nto ensure relevance, diversity, and alignment with the target application or task (Yuan et al., 2024;\nZhang et al., 2024b). In this work, we propose an adaptive benchmarking approach to generating\nglobal sensemaking queries for the LLM. Our approach builds on prior work in LLM-based persona\ngeneration, where the LLM is used to generate diverse and authentic sets of personas (", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_2700", "chunk_text": " work, we propose an adaptive benchmarking approach to generating\nglobal sensemaking queries for the LLM. Our approach builds on prior work in LLM-based persona\ngeneration, where the LLM is used to generate diverse and authentic sets of personas (Kosinski,\n2024; Salminen et al., 2024; Shin et al., 2024). Our adaptive benchmarking procedure uses persona\ngeneration to create queries that are representative of real-world RAG system usage. Specifically,\n\n- ur approach uses the LLM to infer the potential users would use the RAG system and their use\ncases, which guide the generation of corpus-specific sensemaking queries.\n\n\n**2.4** **RAG evaluation criteria**\n\n\nOur evaluation relies on the LLM to evaluate how well the RAG system answers the generated questions. Prior work has shown LLMs to be good evaluators of natural language generation, including work where LLMs evaluations were competitive with human evaluations (Wang et al., 2023a;\nZheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of\n\n\n3\n\n\n_Indexing Time_ **Pipeline Stage** _Query Time_\n\n\nFigure 1: Graph RAG pipeline using an LLM-derived graph index of source document text. This\ngraph index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims)\nthat have been detected, extracted, and summarized by LLM prompts tailored to the domain of the\ndataset. Community detection (e.g., Leiden, Traag et al., 2019) is used to partition the graph index\ninto groups of elements (nodes, edges, covariates) that the LLM can summarize in parallel at both\nindexing time and query time. The \u201cglobal answer\u201d to a given query is produced using a final round\n\n- f query-focused summarization over all community summaries reporting relevance to that query.\n\n\ngenerated texts such as \u201cfluency\u201d (Wang et al., 2023a) Some of these criteria are generic to vector\nRAG systems and not relevant to global sensemaking, such as \u201ccontext relevance\u201d, \u201cfaithfulness\u201d,\nand \u201canswer relevance\u201d (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can\nquantify relative performance for a given criterion by prompting the LLM to compare", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_3150", "chunk_text": "\u201d, \u201cfaithfulness\u201d,\nand \u201canswer relevance\u201d (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can\nquantify relative performance for a given criterion by prompting the LLM to compare generations\nfrom two different competing models (LLM-as-a-judge, (Zheng et al., 2024)). In this work, we design criteria for evaluating RAG-generated answers to global sensemaking questions and evaluate\n\n- ur results using the comparative approach. We also validate results using statistics derived from\nLLM-extracted statements of verifiable facts, or \u201cclaims.\u201d\n\n\n**3** **Methods**\n\n\n**3.1** **GraphRAG Workflow**\n\n\nFigure 1 illustrates the high-level data flow of the GraphRAG approach and pipeline. In this section,\nwe describe the key design parameters, techniques, and implementation details for each step.\n\n\n**3.1.1** **Source Documents** _\u2192_ **Text Chunks**\n\n\nTo start, the documents in the corpus are split into text chunks. The LLM extracts information from\neach chunk for downstream processing. Selecting the size of the chunk is a fundamental design\ndecision; longer text chunks require fewer LLM calls for such extraction (which reduces cost) but\nsuffer from degraded recall of information that appears early in the chunk (Kuratov et al., 2024; Liu\net al., 2023). See Section A.1 for prompts and examples of the recall-precision trade-offs.\n\n\n**3.1.2** **Text Chunks** _\u2192_ **Entities & Relationships**\n\n\nIn this step, the LLM is prompted to extract instances of important _entities_ and the _relationships_\nbetween the entities from a given chunk. Additionally, the LLM generates short descriptions for the\nentities and relationships. To illustrate, suppose a chunk contained the following text:\n\n\n4\n\n\nNeoChip\u2019s (NC) shares surged in their first week of trading on the NewTech Exchange. However, market analysts caution that the chipmaker\u2019s public debut may\nnot reflect trends for other technology IPOs. NeoChip, previously a private entity,\nwas acquired by Quantum Systems in 2016. The innovative semiconductor firm\nspecializes in low-power processors for wearables and IoT devices.\n\n\nThe LLM is prompted such that it extracts the following:\n\n\n    - The entity `NeoChip`, with description \u201cNeoChip is a publicly", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_3600", "chunk_text": " 2016. The innovative semiconductor firm\nspecializes in low-power processors for wearables and IoT devices.\n\n\nThe LLM is prompted such that it extracts the following:\n\n\n    - The entity `NeoChip`, with description \u201cNeoChip is a publicly traded company specializing\nin low-power processors for wearables and IoT devices.\u201d\n\n\n    - The entity `Quantum Systems`, with description \u201cQuantum Systems is a firm that previ\n    - usly owned NeoChip.\u201d\n\n\n    - A relationship between `NeoChip` and `Quantum Systems`, with description \u201cQuantum Systems owned NeoChip from 2016 until NeoChip became publicly traded.\u201d\n\n\nThese prompts can be tailored to the domain of the document corpus by choosing domain appropriate\nfew-shot exemplars for in-context learning (Brown et al., 2020). For example, while our default\nprompt extracts the broad class of \u201cnamed entities\u201d like people, places, and organizations and is\ngenerally applicable, domains with specialized knowledge (e.g., science, medicine, law) will benefit\nfrom few-shot exemplars specialized to those domains.\n\n\nThe LLM can also be prompted to extract _claims_ about detected entities. _Claims_ are important\nfactual statements about entities, such as dates, events, and interactions with other entities. As\nwith entities and relationships, in-context learning exemplars can provide domain-specific guidance.\nClaim descriptions extracted from the example tetx chunk are as follows:\n\n\n    - NeoChip\u2019s shares surged during their first week of trading on the NewTech Exchange.\n\n\n   - NeoChip debuted as a publicly listed company on the NewTech Exchange.\n\n\n   - Quantum Systems acquired NeoChip in 2016 and held ownership until NeoChip went public.\n\n\nSee Appendix A for prompts and details on our implementation of entity and claim extraction.\n\n\n**3.1.3** **Entities & Relationships** _\u2192_ **Knowledge Graph**\n\n\nThe use of an LLM to extract entities, relationships, and claims is a form of abstractive summarization \u2013 these are meaningful summaries of concepts that, in the case of relationships and claims, may\nnot be explicitly stated in the text. The entity/relationship/claim extraction processes creates multiple instances of a single element because an element is typically detected and extracted multiple\ntimes across documents.\n\n\nIn the final step of the knowledge graph extraction process, these instances of entities and relationships become individual nodes and edges in the graph. Entity descriptions are aggregated and\nsummar", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_4050", "chunk_text": " single element because an element is typically detected and extracted multiple\ntimes across documents.\n\n\nIn the final step of the knowledge graph extraction process, these instances of entities and relationships become individual nodes and edges in the graph. Entity descriptions are aggregated and\nsummarized for each node and edge. Relationships are aggregated into graph edges, where the number of duplicates for a given relationship becomes edge weights. Claims are aggregated similarly.\n\n\nIn this manuscript, our analysis uses exact string matching for _entity matching_ - the task of reconciling different extracted names for the same entity (Barlaug and Gulla, 2021; Christen and Christen,\n2012; Elmagarmid et al., 2006). However, softer matching approaches can be used with minor adjustments to prompts or code. Furthermore, GraphRAG is generally resilient to duplicate entities\nsince duplicates are typically clustered together for summarization in subsequent steps.\n\n\n**3.1.4** **Knowledge Graph** _\u2192_ **Graph Communities**\n\n\nGiven the graph index created in the previous step, a variety of community detection algorithms\nmay be used to partition the graph into communities of strongly connected nodes (e.g., see the\nsurveys by Fortunato (2010) and Jin et al. (2021)). In our pipeline, we use Leiden community\ndetection (Traag et al., 2019) in a hierarchical manner, recursively detecting sub-communities within\neach detected community until reaching leaf communities that can no longer be partitioned.\n\n\n5\n\n\nEach level of this hierarchy provides a community partition that covers the nodes of the graph in a\nmutually exclusive, collectively exhaustive way, enabling divide-and-conquer global summarization.\nAn illustration of such hierarchical partitioning on an example dataset can be found in Appendix B.\n\n\n**3.1.5** **Graph Communities** _\u2192_ **Community Summaries**\n\n\nThe next step creates report-like summaries of each community in the community hierarchy, using\na method designed to scale to very large datasets. These summaries are independently useful as a\nway to understand the global structure and semantics of the dataset, and may themselves be used to\nmake sense of a corpus in the absence of a specific query. For example, a user may scan through\ncommunity summaries at one level looking for general themes of interest, then read linked reports\nat a lower level that provide additional details for each subtopic. Here, however, we focus on their\nutility as", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_4500", "chunk_text": " For example, a user may scan through\ncommunity summaries at one level looking for general themes of interest, then read linked reports\nat a lower level that provide additional details for each subtopic. Here, however, we focus on their\nutility as part of a graph-based index used for answering global queries.\n\n\nGraphRAG generates community summaries by adding various element summaries (for nodes,\nedges, and related claims) to a community summary template. Community summaries from lowerlevel communities are used to generate summaries for higher-level communities as follows:\n\n\n    - _Leaf-level communities_ . The element summaries of a leaf-level community are prioritized\nand then iteratively added to the LLM context window until the token limit is reached.\nThe prioritization is as follows: for each community edge in decreasing order of combined\nsource and target node degree (i.e., overall prominence), add descriptions of the source\nnode, target node, the edge itself, and related claims.\n\n\n    - _Higher-level communities_ . If all element summaries fit within the token limit of the context window, proceed as for leaf-level communities and summarize all element summaries\nwithin the community. Otherwise, rank sub-communities in decreasing order of element\nsummary tokens and iteratively substitute sub-community summaries (shorter) for their\nassociated element summaries (longer) until they fit within the context window.\n\n\n**3.1.6** **Community Summaries** _\u2192_ **Community Answers** _\u2192_ **Global Answer**\n\n\nGiven a user query, the community summaries generated in the previous step can be used to generate\na final answer in a multi-stage process. The hierarchical nature of the community structure also\nmeans that questions can be answered using the community summaries from different levels, raising\nthe question of whether a particular level in the hierarchical community structure offers the best\nbalance of summary detail and scope for general sensemaking questions (evaluated in section 4).\n\n\nFor a given community level, the global answer to any user query is generated as follows:\n\n\n    - _Prepare community summaries_ . Community summaries are randomly shuffled and divided\ninto chunks of pre-specified token size. This ensures relevant information is distributed\nacross chunks, rather than concentrated (and potentially lost) in a single context window.\n\n\n    - _Map community answers_ . Intermediate answers are generated in parallel. The LLM is also\nasked to generate a score between 0-100 indicating how helpful the generated answer is in\nansw", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_4950", "chunk_text": ") in a single context window.\n\n\n    - _Map community answers_ . Intermediate answers are generated in parallel. The LLM is also\nasked to generate a score between 0-100 indicating how helpful the generated answer is in\nanswering the target question. Answers with score 0 are filtered out.\n\n\n    - _Reduce to global answer_ . Intermediate community answers are sorted in descending order\n\n     - f helpfulness score and iteratively added into a new context window until the token limit\nis reached. This final context is used to generate the global answer returned to the user.\n\n\n**3.2** **Global Sensemaking Question Generation**\n\n\nTo evaluate the effectiveness of RAG systems for global sensemaking tasks, we use an LLM to\ngenerate a set of corpus-specific questions designed to asses high-level understanding of a given\ncorpus, without requiring retrieval of specific low-level facts. Instead, given a high-level description\n\n- f a corpus and its purposes, the LLM is prompted to generate personas of hypothetical users of\nthe RAG system. For each hypothetical user, the LLM is then prompted to specify tasks that this\nuser would use the RAG system to complete. Finally, for each combination of user and task, the\nLLM is prompted to generate questions that require understanding of the entire corpus. Algorithm\n1 describes the approach.\n\n\n6\n\n\n**Algorithm 1: Prompting Procedure for Question Generation**\n\n\n1: **Input:** Description of a corpus, number of users _K_, number of tasks per user _N_, number of\nquestions per (user, task) combination _M_ .\n2: **Output:** A set of _K \u2217_ _N \u2217_ _M_ high-level questions requiring global understanding of the corpus.\n3: **procedure** GENERATEQUESTIONS\n4: Based on the corpus description, prompt the LLM to:\n\n1. Describe personas of _K_ potential users of the dataset.\n\n2. For each user, identify _N_ tasks relevant to the user.\n\n3. Specific to each user & task pair, generate _M_ high-level questions that:\n\n\n      - Require understanding of the entire corpus.\n\n      - Do not require retrieval of specific low-level facts.\n\n\n5: Collect the generated questions to produce _K \u2217_ _N \u2217_ _M_ test questions for the dataset.\n6: **end procedure**\n\n\nFor our evaluation, we set _K_", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_5400", "chunk_text": " retrieval of specific low-level facts.\n\n\n5: Collect the generated questions to produce _K \u2217_ _N \u2217_ _M_ test questions for the dataset.\n6: **end procedure**\n\n\nFor our evaluation, we set _K_ = _M_ = _N_ = 5 for a total of 125 test questions per dataset. Table 1\nshows example questions for each of the two evaluation datasets.\n\n\n**3.3** **Criteria for Evaluating Global Sensemaking**\n\n\nGiven the lack of gold standard answers to our activity-based sensemaking questions, we adopt\nthe head-to-head comparison approach using an LLM evaluator that judges relative performance\naccording to specific criteria. We designed three target criteria capturing qualities that are desirable\nfor global sensemaking activities.\n\n\nAppendix F shows the prompts for our head-to-head measures computed using an LLM evaluator,\nsummarized as:\n\n\n    - _Comprehensiveness_ . How much detail does the answer provide to cover all aspects and\ndetails of the question?\n\n\n    - _Diversity_ . How varied and rich is the answer in providing different perspectives and insights\n\n     - n the question?\n\n\n    - _Empowerment_ . How well does the answer help the reader understand and make informed\njudgments about the topic?\n\n\nTable 1: Examples of potential users, tasks, and questions generated by the LLM based on short\ndescriptions of the target datasets. Questions target global understanding rather than specific details.\n\n\n**Dataset** **Example activity framing and generation of global sensemaking** **questions**\n\nPodcast _User_ : A tech journalist looking for insights and trends in the tech industry\ntranscripts _Task_ : Understanding how tech leaders view the role of policy and regulation\n_Questions_ :\n1. Which episodes deal primarily with tech policy and government regulation?\n2. How do guests perceive the impact of privacy laws on technology development?\n3. Do any guests discuss the balance between innovation and ethical considerations?\n4. What are the suggested changes to current policies mentioned by the guests?\n5. Are collaborations between tech companies and governments discussed and how?\n\nNews _User_ : Educator incorporating current affairs into curricula\narticles _Task_ : Teaching about health and wellness\n_Questions_ :\n1. What current topics in health can be integrated into health education curricula?\n2. How do news articles address the concepts of preventive medicine and wellness?\n3. Are there examples of health articles that", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_5850", "chunk_text": " about health and wellness\n_Questions_ :\n1. What current topics in health can be integrated into health education curricula?\n2. How do news articles address the concepts of preventive medicine and wellness?\n3. Are there examples of health articles that contradict each other, and if so, why?\n4. What insights can be gleaned about public health priorities based on news coverage?\n5. How can educators use the dataset to highlight the importance of health literacy?\n\n\n7\n\n\nFurthermore, we use a \u201ccontrol criterion\u201d called _Directness_ that answers _\u201cHow specifically and_\n_clearly does the answer address the question?\u201d_ . In plain terms, directness evaluates the concision\n\n- f an answer in a generic sense that applies to any generated LLM summarization. We include it to\nbehave as a reference against which we can judge the soundness of results for the other criteria. Since\ndirectness is effectively in opposition to comprehensiveness and diversity, we would not expect any\nmethod to win across all four criteria.\n\n\nIn our evaluations, the LLM is provided with the question, the generated answers from two competing systems, and prompted to compare the two answers according to the criterion before giving a\nfinal judgment of which answer is preferred. The LLM either indicates a winner; or, it returns a tie\nif they are fundamentally similar. To account for the inherent stochasticity of LLM generation, we\nrun each comparison with multiple replicates and average the results across replicates and questions.\nAn illustration of LLM assessment for answers to a sample question can be found in Appendix D.\n\n\n**4** **Analysis**\n\n\n**4.1** **Experiment 1**\n\n\n**4.1.1** **Datasets**\n\n\nWe selected two datasets in the one million token range, each representative of corpora that users\nmay encounter in their real-world activities:\n\n\n**Podcast transcripts** . Public transcripts of _Behind the Tech with Kevin Scott_, a podcast featuring\nconversations between Microsoft CTO Kevin Scott and various thought leaders in science and technology (Scott, 2024). This corpus was divided into 1669 _\u00d7_ 600-token text chunks, with 100-token\n\n_\u223c_\n\n- verlaps between chunks ( 1 million tokens).\n\n\n**News articles** . A benchmark dataset comprised of news articles published from September 2013\nto December 2023 in a range of categories, including entertainment, business, sports", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_6300", "chunk_text": "\u223c_\n\n- verlaps between chunks ( 1 million tokens).\n\n\n**News articles** . A benchmark dataset comprised of news articles published from September 2013\nto December 2023 in a range of categories, including entertainment, business, sports, technology,\nhealth, and science (Tang and Yang, 2024). The corpus is divided into 3197 _\u00d7_ 600-token text\n\n_\u223c_\nchunks, with 100-token overlaps between chunks ( 1.7 million tokens).\n\n\n**4.1.2** **Conditions**\n\n\nWe compared six conditions including GraphRAG at four different graph community levels ( **C0**,\n**C1**, **C2**, **C3** ), a text summarization method that applies our map-reduce approach directly to source\ntexts ( **TS** ), and a vector RAG \u201csemantic search\u201d approach ( **SS** ):\n\n\n    - **CO** . Uses root-level community summaries (fewest in number) to answer user queries.\n\n\n    - **C1** . Uses high-level community summaries to answer queries. These are sub-communities\n\n     - f C0, if present, otherwise C0 communities projected downwards.\n\n\n    - **C2** . Uses intermediate-level community summaries to answer queries. These are subcommunities of C1, if present, otherwise C1 communities projected downwards.\n\n\n    - **C3** . Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected downwards.\n\n\n    - **TS** . The same method as in Section 3.1.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n\n\n    - **SS** . An implementation of vector RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\n\n\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\n\n\nThe graph index supporting conditions **C0** - **C3** was created using our generic prompts for entity and\nrelationship extraction, with entity types and few-shot examples tailored to the domain of the data.\n\n\n8\n\n\n**4", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_6750", "chunk_text": " window are created.\n\n\nThe graph index supporting conditions **C0** - **C3** was created using our generic prompts for entity and\nrelationship extraction, with entity types and few-shot examples tailored to the domain of the data.\n\n\n8\n\n\n**4.1.3** **Configuration**\n\n\nWe used a fixed context window size of 8k tokens for generating community summaries, community\nanswers, and global answers (explained in Appendix C). Graph indexing with a 600 token window\n(explained in Section A.2) took 281 minutes for the Podcast dataset, running on a virtual machine\n(16GB RAM, Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz) and using a public OpenAI\nendpoint for `gpt-4-turbo` (2M TPM, 10k RPM).\n\n\nWe implemented Leiden community detection using the graspologic library (Chung et al., 2019).\nThe prompts used to generate the graph index and global answers can be found in Appendix E,\nwhile the prompts used to evaluate LLM responses against our criteria can be found in Appendix F.\nA full statistical analysis of the results presented in the next section can be found in Appendix G.\n\n\n**4.2** **Experiment 2**\n\n\nTo validate the comprehensiveness and diversity results from Experiment 1, we implemented claimbased measures of these qualities. We use the definition of a factual claim from Ni et al. (2024),\nwhich is \u201ca statement that explicitly presents some verifiable facts.\u201d For example, the sentence\n\u201cCalifornia and New York implemented incentives for renewable energy adoption, highlighting the\nbroader importance of sustainability in policy decisions\u201d contains two factual claims: (1) California\nimplemented incentives for renewable energy adoption, and (2) New York implemented incentives\nfor renewable energy adoption.\n\n\nTo extract factual claims, we used **Claimify** (Metropolitansky and Larson, 2025), an LLM-based\nmethod that identifies sentences in an answer containing at least one factual claim, then decomposes\nthese sentences into simple, self-contained factual claims. We applied Claimify to the answers\ngenerated under the conditions from Experiment 1. After removing duplicate claims from each\nanswer, we extracted 47,075 unique claims, with an average of 31 claims per answer.\n\n\nWe defined two metrics, with higher values indicating better performance:\n\n\n1. **Comprehensiveness** :", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_7200", "chunk_text": ". After removing duplicate claims from each\nanswer, we extracted 47,075 unique claims, with an average of 31 claims per answer.\n\n\nWe defined two metrics, with higher values indicating better performance:\n\n\n1. **Comprehensiveness** : Measured as the average number of claims extracted from the answers generated under each condition.\n\n\n2. **Diversity** : Measured by clustering the claims for each answer and calculating the average\nnumber of clusters.\n\n\nFor clustering, we followed the approach described by Padmakumar and He (2024), which involved\nusing Scikit-learn\u2019s implementation of agglomerative clustering (Pedregosa et al., 2011). Clusters\nwere merged through \u201ccomplete\u201d linkage, meaning they were combined only if the maximum distance between their farthest points was less than or equal to a predefined distance threshold. The\ndistance metric used was 1 _\u2212_ ROUGE-L. Since the distance threshold influences the number of\nclusters, we report results across a range of thresholds.\n\n\n**5** **Results**\n\n\n**5.1** **Experiment 1**\n\n\nThe indexing process resulted in a graph consisting of 8,564 nodes and 20,691 edges for the Podcast\ndataset, and a larger graph of 15,754 nodes and 19,520 edges for the News dataset. Table 2 shows\nthe number of community summaries at different levels of each graph community hierarchy.\n\n\n**Global approaches vs. vector RAG** . As shown in Figure 2 and Table 6, global approaches significantly outperformed conventional vector RAG ( **SS** ) in both comprehensiveness and diversity criteria\nacross datasets. Specifically, global approaches achieved comprehensiveness win rates between 7283% (p _<_ .001) for Podcast transcripts and 72-80% (p _<_ .001) for News articles, while diversity win\nrates ranged from 75-82% (p _<_ .001) and 62-71% (p _<_ .01) respectively. Our use of directness as a\nvalidity test confirmed that vector RAG produces the most direct responses across all comparisons.\n\n\n**Empowerment** . Empowerment comparisons showed mixed results for both global approaches versus vector RAG ( **SS** ) and GraphRAG approaches versus source text summarization ( **TS** ). Using an\nLLM to analyze LLM reasoning for this measure indicated that the ability to", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_7650", "chunk_text": " comparisons showed mixed results for both global approaches versus vector RAG ( **SS** ) and GraphRAG approaches versus source text summarization ( **TS** ). Using an\nLLM to analyze LLM reasoning for this measure indicated that the ability to provide specific exam\n\n9\n\n\n**Podcast transcripts**\n\n\n\nSS\n\n\nTS\n\n\nC0\n\n\nC1\n\n\n**C2**\n\n\nC3\n\n\nSS\n\n\nTS\n\n\nC0\n\n\n**C1**\n\n\nC2\n\n\nC3\n\n\n\nSS TS C0 C1 C2 C3\n\n\n**Comprehensiveness**\n\n\nSS TS C0 C1 C2 C3\n\n\n**Comprehensiveness**\n\n\n\nSS\n\n\nTS\n\n\nC0\n\n\nC1\n\n\nC2\n\n\n**C3**\n\n\nSS\n\n\nTS\n\n\nC0\n\n\nC1\n\n\n**C2**\n\n\nC3\n\n\n\nSS TS C0 C1 C2 C3\n\n\n**Diversity**\n\n\n\nSS TS C0 C1 C2 C3\n\n\n**Diversity**\n\n\n\nSS\n\n\nTS\n\n\nC0\n\n\n**C1**\n\n\nC2\n\n\nC3\n\n\n\nSS\n\n\n**TS**\n\n\nC0\n\n\nC1\n\n\nC2\n\n\nC3\n\n\n\nSS TS C0 C1 C2 C3\n\n\n**Empowerment**\n\n\n\nSS TS C0 C1 C2 C3\n\n\n**Directness**\n\n\nSS TS C0 C1 C2 C3\n\n\n**Directness**\n\n\n\n**News articles**\n\n\n\n**SS**\n\n\nTS\n\n\nC0\n\n\nC1\n\n\nC2\n\n\nC3\n\n\n**SS**\n\n\nTS\n\n\nC0\n\n\nC1\n\n\nC2\n\n\nC3\n\n\n\nSS TS C0 C1 C2 C3\n\n\n**Empowerment**\n\n\n\nFigure 2: Head-to-head win rate percentages of (row condition) over (column condition) across two\ndatasets, four metrics, and 125 questions per comparison (each repeated five times and averaged).\nThe overall winner per dataset and metric is shown in bold. Self-win rates were not computed but\nare shown as the expected 50% for reference. All Graph RAG conditions outperformed na\u00a8\u0131ve RAG\n\n- n comprehensiveness and diversity. Conditions C1-C3 also showed slight improvements in answer\ncomprehensiveness and diversity over TS (global text summarization without a graph index).\n\n\nTable 2: Number of context units (community summaries for **C0-C3** and text chunks for **TS** ), corresponding token counts", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_8100", "chunk_text": " in answer\ncomprehensiveness and diversity over TS (global text summarization without a graph index).\n\n\nTable 2: Number of context units (community summaries for **C0-C3** and text chunks for **TS** ), corresponding token counts, and percentage of the maximum token count. Map-reduce summarization of\nsource texts is the most resource-intensive approach requiring the highest number of context tokens.\nRoot-level community summaries ( **C0** ) require dramatically fewer tokens per query (9x-43x).\n\n\n**Podcast Transcripts** **News Articles**\n\n**C0** **C1** **C2** **C3** **TS** **C0** **C1** **C2** **C3** **TS**\n\n**Units** 34 367 969 1310 1669 55 555 1797 2142 3197\n\n**Tokens** 26657 225756 565720 746100 1014611 39770 352641 980898 1140266 1707694\n\n**% Max** 2.6 22.2 55.8 73.5 100 2.3 20.7 57.4 66.8 100\n\n\nples, quotes, and citations was judged to be key to helping users reach an informed understanding.\nTuning element extraction prompts may help to retain more of these details in the GraphRAG index.\n\n\n**Community summaries vs. source texts** . When comparing community summaries to source texts\nusing GraphRAG, community summaries generally provided a small but consistent improvement\nin answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level\nsummaries in the Podcast dataset and low-level community summaries in the News dataset achieved\ncomprehensiveness win rates of 57% (p _<_ .001) and 64% (p _<_ .001), respectively. Diversity win rates\nwere 57% (p=.036) for Podcast intermediate-level summaries and 60% (p _<_ .001) for News low-level\ncommunity summaries. Table 2 also illustrates the scalability advantages of GraphRAG compared\nto source text summarization: for low-level community summaries ( **C3** ), GraphRAG required 2633% fewer context tokens, while for root-level community summaries ( **C0** ), it required over 97%\nfewer tokens. For a", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_8550", "chunk_text": " summarization: for low-level community summaries ( **C3** ), GraphRAG required 2633% fewer context tokens, while for root-level community summaries ( **C0** ), it required over 97%\nfewer tokens. For a modest drop in performance compared with other global methods, root-level\nGraphRAG offers a highly efficient method for the iterative question answering that characterizes\nsensemaking activity, while retaining advantages in comprehensiveness (72% win rate) and diversity\n(62% win rate) over vector RAG.\n\n\n10\n\n\nTable 3: Average number of extracted claims, reported by condition and dataset type. Bolded values\nrepresent the highest score in each column.\n\n\n**Average Number of Claims**\n**Condition**\n\n**News Articles** **Podcast Transcripts**\n\n\nC0 **34.18** 32.21\n\nC1 32.50 32.20\n\nC2 31.62 **32.46**\n\nC3 33.14 32.28\n\nTS 32.89 31.39\n\nSS 25.23 26.50\n\n\n**5.2** **Experiment 2**\n\n\nTable 3 shows the results for the average number of extracted claims (i.e., the claim-based measure\n\n- f comprehensiveness) per condition. For both the News and Podcast datasets, all global search\nconditions ( **C0-C3** ) and source text summarization ( **TS** ) had greater comprehensiveness than vector\nRAG ( **SS** ). The differences were statistically significant (p _<_ .05) in all cases. These findings align\nwith the LLM-based win rates from Experiment 1.\n\n\nTable 4 contains the results for the average number of clusters, the claim-based measure of diversity. For the Podcast dataset, all global search conditions had significantly greater diversity than **SS**\nacross all distance thresholds (p _<_ .05), consistent with the win rates observed in Experiment 1. For\nthe News dataset, however, only **C0** significantly outperformed **SS** across all distance thresholds\n(p _<_ .05). While **C1-C3** also achieved higher average cluster counts than **SS**, the differences were\nstatistically significant only at certain distance thresholds. In Experiment 1, all global search conditions significantly outperformed **SS** in the News dataset \u2013 not just **C0** . However, the differences", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_9000", "chunk_text": " counts than **SS**, the differences were\nstatistically significant only at certain distance thresholds. In Experiment 1, all global search conditions significantly outperformed **SS** in the News dataset \u2013 not just **C0** . However, the differences\nin mean diversity scores between **SS** and the global search conditions were smaller for the News\ndataset than for the Podcast dataset, aligning directionally with the claim-based results.\n\n\nFor both comprehensiveness and diversity, across both datasets, there were no statistically significant\ndifferences observed among the global search conditions or between global search and **TS** .\n\n\nFinally, for each pairwise comparison in Experiment 1, we tested whether the answer preferred by\nthe LLM aligned with the winner based on the claim-based metrics. Since each pairwise comparison\nin Experiment 1 was performed five times, while the claim-based metrics provided only one outcome\nper comparison, we aggregated the Experiment 1 results into a single label using majority voting.\nFor example, if **C0** won over **SS** in three out of five judgments for comprehensiveness on a given\nquestion, **C0** was labeled the winner and **SS** the loser. However, if **C0** won twice, **SS** won once, and\nthey tied twice, then there was no majority outcome, so the final label was a tie.\n\n\nWe found that exact ties were rare for the claim-based metrics. One possible solution is to define a\ntie based on a threshold (e.g., the absolute difference between the claim-based results for condition\nA and condition B must be less than or equal to _x_ ). However, we observed that the results were\nsensitive to the choice of threshold. As a result, we focused on cases where the aggregated LLM\nlabel was not a tie, representing 33% and 39% of pairwise comparisons for comprehensiveness and\ndiversity, respectively. In these cases, the aggregated LLM label matched the claim-based label in\n78% of pairwise comparisons for comprehensiveness and 69-70% for diversity (across all distance\nthresholds), indicating moderately strong alignment.\n\n\n**6** **Discussion**\n\n\n**6.1** **Limitations of evaluation approach**\n\n\nOur evaluation to date has focused on sensemaking questions specific to two corpora each containing\napproximately 1 million tokens. More work is needed to understand how performance generalizes to\ndatasets from", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_9450", "chunk_text": "6.1** **Limitations of evaluation approach**\n\n\nOur evaluation to date has focused on sensemaking questions specific to two corpora each containing\napproximately 1 million tokens. More work is needed to understand how performance generalizes to\ndatasets from various domains with different use cases. Comparison of fabrication rates, e.g., using\napproaches like SelfCheckGPT (Manakul et al., 2023), would also strengthen the current analysis.\n\n\n11\n\n\nTable 4: Average number of clusters across different distance thresholds, reported by condition and\ndataset type. Bolded values represent the highest score in each row.\n\n\n**Average Number of Clusters**\n**Dataset** **Distance Threshold**\n\n**C0** **C1** **C2** **C3** **TS** **SS**\n\n\n\nNews Articles\n\n\nPodcast Transcripts\n\n\n**6.2** **Future work**\n\n\n\n0.5 **23.42** 21.85 21.90 22.13 21.80 17.92\n\n0.6 **21.65** 20.38 20.30 20.52 20.13 16.78\n\n0.7 **20.19** 19.06 19.03 19.13 18.62 15.80\n\n0.8 **18.86** 17.78 17.82 17.79 17.30 14.80\n\n\n0.5 **23.16** 22.62 22.52 21.93 21.14 18.55\n\n0.6 **21.65** 21.33 21.21 20.62 19.70 17.39\n\n0.7 **20.41** 20.04 19.79 19.22 18.08 16.28\n\n0.8 **19.26** 18.77 18.46 17.89 16.66 15.07\n\n\n\nThe graph index, rich text annotations, and hierarchical community structure supporting the current\nGraphRAG approach offer many possibilities for refinement and adaptation. This includes RAG\napproaches that operate in a more local manner, via embedding-based matching of user queries and\ngraph annotations. In particular, we see potential in hybrid RAG schemes that combine embeddingbased matching with just-in-time community report generation before employing our map-reduce\nsummar", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_9900", "chunk_text": " in a more local manner, via embedding-based matching of user queries and\ngraph annotations. In particular, we see potential in hybrid RAG schemes that combine embeddingbased matching with just-in-time community report generation before employing our map-reduce\nsummarization mechanisms. This \u201croll-up\u201d approach could also be extended across multiple levels\n\n- f the community hierarchy, as well as implemented as a more exploratory \u201cdrill down\u201d mechanism\nthat follows the information scent contained in higher-level community summaries.\n\n\n_Broader impacts_ . As a mechanism for question answering over large document collections, there\nare risks to downstream sensemaking and decision-making tasks if the generated answers do not\naccurately represent the source data. System use should be accompanied by clear disclosures of AI\nuse and the potential for errors in outputs. Compared to vector RAG, however, GraphRAG shows\npromise as a way to mitigate these downstream risks for questions of a global nature, which might\n\n- therwise be answered by samples of retrieved facts falsely presented as global summaries.\n\n\n**7** **Conclusion**\n\n\nWe have presented GraphRAG, a RAG approach that combines knowledge graph generation and\nquery-focused summarization (QFS) to support human sensemaking over entire text corpora. Initial\nevaluations show substantial improvements over a vector RAG baseline for both the comprehensiveness and diversity of answers, as well as favorable comparisons to a global but graph-free approach\nusing map-reduce source text summarization. For situations requiring many global queries over the\nsame dataset, summaries of root-level communities in the entity-based graph index provide a data\nindex that is both superior to vector RAG and achieves competitive performance to other global\nmethods at a fraction of the token cost.\n\n\n**Acknowledgements**\n\n\nWe would also like to thank the following people who contributed to the work: Alonso Guevara\nFern\u00b4andez, Amber Hoak, Andr\u00b4es Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez,\nChris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed\nClark, Gabriel Nieves-Ponce, Gaudy Blanco Meneses, Kate Lytvynets, Katy Smith, M\u00b4onica Carvajal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.\n\n\n**References**\n\n\nAchiam, J., Adler, S", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_10350", "chunk_text": ", Kate Lytvynets, Katy Smith, M\u00b4onica Carvajal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.\n\n\n**References**\n\n\nAchiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. _arXiv preprint_\n_arXiv:2303.08774_ .\n\n\n12\n\n\nAnil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,\nHauth, A., et al. (2023). Gemini: a family of highly capable multimodal models. _arXiv preprint_\n_arXiv:2312.11805_ .\n\n\nBaek, J., Aji, A. F., and Saffari, A. (2023). Knowledge-augmented language model prompting for\nzero-shot knowledge graph question answering. _arXiv preprint arXiv:2306.04136_ .\n\n\nBan, T., Chen, L., Wang, X., and Chen, H. (2023). From query tools to causal architects: Harnessing\nlarge language models for advanced causal discovery from data.\n\n\nBarlaug, N. and Gulla, J. A. (2021). Neural networks for entity matching: A survey. _ACM Transac-_\n_tions on Knowledge Discovery from Data (TKDD)_, 15(3):1\u201337.\n\n\nBaumel, T., Eyal, M., and Elhadad, M. (2018). Query focused abstractive summarization: Incorporating query relevance, multi-document coverage, and summary length constraints into seq2seq\nmodels. _arXiv preprint arXiv:1801.07704_ .\n\n\nBlondel, V. D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of\ncommunities in large networks. _Journal of statistical mechanics: theory and experiment_,\n2008(10):P", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_11700", "chunk_text": " Bastian, M. (2014). Forceatlas2, a continuous graph\nlayout algorithm for handy network visualization designed for the gephi software. _PLoS ONE_\n_9(6): e98679. https://doi.org/10.1371/journal.pone.0098679_ .\n\n\nJin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y., and Zhang, W. (2021). A survey of\ncommunity detection approaches: From statistical modeling to deep learning. _IEEE Transactions_\n\n_on Knowledge and Data Engineering_, 35(2):1149\u20131170.\n\n\nKang, M., Kwak, J. M., Baek, J., and Hwang, S. J. (2023). Knowledge graph-augmented language\nmodels for knowledge-grounded dialogue generation. _arXiv preprint arXiv:2305.18846_ .\n\n\nKhattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022).\nDemonstrate-search-predict: Composing retrieval and language models for knowledge-intensive\nnlp. _arXiv preprint arXiv:2212.14024_ .\n\n\nKim, D., Xie, L., and Ong, C. S. (2016). Probabilistic knowledge graph construction: Compositional\nand incremental approaches. In _Proceedings of the 25th ACM International on Conference on_\n_Information and Knowledge Management_, CIKM \u201916, page 2257\u20132262, New York, NY, USA.\nAssociation for Computing Machinery.\n\n\nKim, G., Kim, S., Jeon, B., Park, J., and Kang, J. (2023). Tree of clarifications: Answering ambigu\n - us questions with retrieval-augmented large language models. _arXiv preprint arXiv:2310.14696_ .\n\n\nKlein, G., Moon, B., and Hoffman, R. R. (2006). Making sense of sensemaking 1: Alternative\nperspectives. _IEEE intelligent systems_, 21(4):70\u201373.\n\n\nKosinski, M. (2024). Evaluating large language models in theory of mind tasks. _Proceed", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_12150", "chunk_text": " Making sense of sensemaking 1: Alternative\nperspectives. _IEEE intelligent systems_, 21(4):70\u201373.\n\n\nKosinski, M. (2024). Evaluating large language models in theory of mind tasks. _Proceedings of the_\n_National Academy of Sciences_, 121(45):e2405460121.\n\n\nKuratov, Y., Bulatov, A., Anokhin, P., Sorokin, D., Sorokin, A., and Burtsev, M. (2024). In search\n\n - f needles in a 11m haystack: Recurrent memory finds what llms miss.\n\n\n[LangChain (2024). Langchain graphs. https://langchain-graphrag](https://langchain-graphrag.readthedocs.io/en/latest/) _._ readthedocs _._ io/en/latest/.\n\n\nLaskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via\nincorporating query relevance and transfer learning with transformer models. In _Advances in_\n_Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020,_\n_Ottawa, ON, Canada, May 13\u201315, 2020, Proceedings 33_, pages 342\u2013348. Springer.\n\n\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\u00a8uttler, H., Lewis, M., Yih,\nW.-t., Rockt\u00a8aschel, T., et al. (2020). Retrieval-augmented generation for knowledge-intensive nlp\ntasks. _Advances in Neural Information Processing Systems_, 33:9459\u20139474.\n\n\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. (2023). Lost\nin the middle: How language models use long contexts. arXiv:2307.03172.\n\n\n[LlamaIndex (2024). GraphRAG Implementation with LlamaIndex - V2. https://github](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/cookbooks/GraphRAG_v2.ipynb) _._ com", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_13050", "chunk_text": " on scientific discovery: a preliminary study\nusing gpt-4.\n\n\nMooney, R. J. and Bunescu, R. (2005). Mining knowledge from text using information extraction.\n_SIGKDD Explor. Newsl._, 7(1):3\u201310.\n\n\nNebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented generation with llm based on knowledge graphs. https://www _._ [nebula-graph](https://www.nebula-graph.io/posts/graph-RAG) _._ io/posts/graph-RAG.\n\n\nNeo4J (2024). Get started with graphrag: Neo4j\u2019s ecosystem tools. https://neo4j _._ [com/developer-](https://neo4j.com/developer-blog/graphrag-ecosystem-tools/)\n[blog/graphrag-ecosystem-tools/.](https://neo4j.com/developer-blog/graphrag-ecosystem-tools/)\n\n\nNewman, M. E. (2006). Modularity and community structure in networks. _Proceedings of the_\n_national academy of sciences_, 103(23):8577\u20138582.\n\n\nNi, J., Shi, M., Stammbach, D., Sachan, M., Ash, E., and Leippold, M. (2024). AFaCTA: Assisting\nthe annotation of factual claim detection with reliable LLM annotators. In Ku, L.-W., Martins, A.,\nand Srikumar, V., editors, _Proceedings of the 62nd Annual Meeting of the Association for Compu-_\n_tational Linguistics (Volume 1: Long Papers)_, pages 1890\u20131912, Bangkok, Thailand. Association\nfor Computational Linguistics.\n\n\nOpenAI (2023). Chatgpt: Gpt-4 language model.\n\n\nPadmakumar, V. and He, H. (2024). Does writing with language models reduce content diversity?\n_ICLR_ .\n\n\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,\nPerrot, M., and Duchesn", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_13500", "chunk_text": " M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,\nPerrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in python. _Journal of_\n_Machine Learning Research_, 12:2825\u20132830.\n\n\nRam, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,\nY. (2023). In-context retrieval-augmented language models. _Transactions of the Association for_\n_Computational Linguistics_, 11:1316\u20131331.\n\n\nRanade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented\nnarrative construction. _arXiv preprint arXiv:2310.13848_ .\n\n\nSalminen, J., Liu, C., Pian, W., Chi, J., H\u00a8ayh\u00a8anen, E., and Jansen, B. J. (2024). Deus ex machina\nand personas from large language models: Investigating the composition of ai-generated persona\ndescriptions. In _Proceedings of the CHI Conference on Human Factors in Computing Systems_,\npages 1\u201320.\n\n\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:\nRecursive abstractive processing for tree-organized retrieval. _arXiv preprint arXiv:2401.18059_ .\n\n\nScott, K. (2024). Behind the Tech. https://www _._ microsoft _._ [com/en-us/behind-the-tech.](https://www.microsoft.com/en-us/behind-the-tech)\n\n\nShao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrievalaugmented large language models with iterative retrieval-generation synergy. _arXiv preprint_\n_arXiv:2305.15294_ .\n\n\nShin, J., Hedderich, M. A., Rey, B. J., Lucero, A.,", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_13950", "chunk_text": " models with iterative retrieval-generation synergy. _arXiv preprint_\n_arXiv:2305.15294_ .\n\n\nShin, J., Hedderich, M. A., Rey, B. J., Lucero, A., and Oulasvirta, A. (2024). Understanding humanai workflows for generating personas. In _Proceedings of the 2024 ACM Designing Interactive_\n_Systems Conference_, pages 757\u2013781.\n\n\nShinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. (2024). Reflexion: Language\nagents with verbal reinforcement learning. _Advances in Neural Information Processing Systems_,\n36.\n\n\n15\n\n\nSu, D., Xu, Y., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A question answering and query-focused multi-document summarization system for covid-19 scholarly\ninformation management. _arXiv preprint arXiv:2005.03975_ .\n\n\nTan, Z., Zhao, X., and Wang, W. (2017). Representation learning of large-scale knowledge graphs\nvia entity feature combinations. In _Proceedings of the 2017 ACM on Conference on Information_\n_and Knowledge Management_, CIKM \u201917, page 1777\u20131786, New York, NY, USA. Association for\nComputing Machinery.\n\n\nTang, Y. and Yang, Y. (2024). MultiHop-RAG: Benchmarking retrieval-augmented generation for\nmulti-hop queries. _arXiv preprint arXiv:2401.15391_ .\n\n\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,\nBhargava, P., Bhosale, S., et al. (2023). Llama 2: Open foundation and fine-tuned chat models.\n_arXiv preprint arXiv:2307.09288_ .\n\n\nTraag, V. A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing\nwell-connected communities. _Scientific Reports_, 9(1).\n\n\nTrajan", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_14400", "chunk_text": "ag, V. A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing\nwell-connected communities. _Scientific Reports_, 9(1).\n\n\nTrajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction\nusing large language models. _ArXiv_, abs/2305.04676.\n\n\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-intensive multi-step questions. _arXiv preprint_\n_arXiv:2212.10509_ .\n\n\nWang, J., Liang, Y., Meng, F., Sun, Z., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J. (2023a). Is chatgpt\na good nlg evaluator? a preliminary study. _arXiv preprint arXiv:2303.04048_ .\n\n\nWang, S., Khramtsova, E., Zhuang, S., and Zuccon, G. (2024). Feb4rag: Evaluating federated search\nin the context of retrieval augmented generation. _arXiv preprint arXiv:2402.11891_ .\n\n\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D.\n(2022). Self-consistency improves chain of thought reasoning in language models. _arXiv preprint_\n_arXiv:2203.11171_ .\n\n\nWang, Y., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. (2023b). Knowledge graph\nprompting for multi-document question answering.\n\n\nXu, Y. and Lapata, M. (2021). Text summarization with latent queries. _arXiv preprint_\n_arXiv:2106.00104_ .\n\n\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_15300", "chunk_text": "ausal graph discovery with retrievalaugmented generation based large language models. _arXiv preprint arXiv:2402.15301_ .\n\n\nZhang, Z., Chen, J., and Yang, D. (2024b). Darg: Dynamic evaluation of large language models via\nadaptive reasoning graph. _arXiv preprint arXiv:2406.17271_ .\n\n\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing,\nE., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. _Advances in Neural_\n_Information Processing Systems_, 36.\n\n\nZhu, Y., Wang, X., Chen, J., Qiao, S., Ou, Y., Yao, Y., Deng, S., Chen, H., and Zhang, N. (2024).\nLlms for knowledge graph construction and reasoning: Recent capabilities and future opportunities.\n\n\n17\n\n\n**A** **Entity and Relationship Extraction Approach**\n\n\nThe following prompts, designed for GPT-4, are used in the default GraphRAG initialization\npipeline:\n\n\n[\u2022 Default Graph Extraction Prompt](https://github.com/microsoft/graphrag/blob/6d21ef268377e319a165ca2250bd6841737df1ad/graphrag/prompts/index/entity_extraction.py#L6)\n\n[\u2022 Claim Extraction Prompt](https://github.com/microsoft/graphrag/blob/6d21ef268377e319a165ca2250bd6841737df1ad/graphrag/prompts/index/claim_extraction.py#L1)\n\n\n**A.1** **Entity Extraction**\n\n\nWe do this using a multipart LLM prompt that first identifies all _entities_ in the text, including their\nname, type, and description, before identifying all _relationships_ between clearly related entities,\nincluding the source and target entities and a description of their relationship. Both kinds of element\ninstance are output in a single list of delimited tuples.\n\n\n**A.2** **Self-Reflection**\n\n\n\nThe choice of prompt engineering techniques has a strong impact on the quality of knowledge graph\nextraction (Zhu et al., 2024), and different techniques have different costs in terms of tokens consumed and generated by the model. _", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_15750", "chunk_text": "Reflection**\n\n\n\nThe choice of prompt engineering techniques has a strong impact on the quality of knowledge graph\nextraction (Zhu et al., 2024), and different techniques have different costs in terms of tokens consumed and generated by the model. _Self-reflection_ is a prompt engineering technique where the\nLLM generates an answer, and is then prompted to evaluate its output for correctness, clarity, or\ncompleteness, then finally generate an improved response based on that evaluation (Huang et al.,\n2023; Madaan et al., 2024; Shinn et al., 2024; Wang et al., 2022). We leverage self-reflection in\nknowledge graph extraction, and explore ways how removing self-reflection affects performance\nand cost.\n\n\nUsing larger chunk size is less costly in terms of calls to the LLM. However, the LLM tends to\nextract few entities from chunks of larger size. For example, in a sample dataset (HotPotQA, Yang\net al., 2018), GPT-4 extracted almost twice as many entity references when the chunk size was 600\ntokens than when it was 2400. To address this issue, we deploy a self-reflection prompt engineering\napproach. After entities are extracted from a chunk, we provide the extracted entities back to the\nLLM, prompting it to \u201cglean\u201d any entities that it may have missed. This is a multi-stage process\nin which we first ask the LLM to assess whether all entities were extracted, using a logit bias of\n100 to force a yes/no decision. If the LLM responds that entities were missed, then a continuation\nindicating that \u201cMANY entities were missed in the last extraction\u201d encourages the LLM to detect\nthese missing entities. This approach allows us to use larger chunk sizes without a drop in quality\n(Figure 3) or the forced introduction of noise. We interate self-reflection steps up to a specified\nmaximum number of times.\n\n\n30000\n\n\n\n20000\n\n\n\n\n\n10000\n\n\n0\n\n|Col1|Col2|600chunksize<br>1200chunksize|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||600 chunk size<br>1200 chunk size<br>|600 chunk size<br>1200 chunk size<br>|600 chunk size<br>1200 chunk size<br>|600 chunk", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_16200", "chunk_text": "|---|---|---|---|---|---|\n|||600 chunk size<br>1200 chunk size<br>|600 chunk size<br>1200 chunk size<br>|600 chunk size<br>1200 chunk size<br>|600 chunk size<br>1200 chunk size<br>|\n|||2400 chunk size||||\n|||||||\n\n0 1 2 3\n\n\nNumber of self-reflection iterations performed\n\n\nFigure 3: How the entity references detected in the HotPotQA dataset (Yang et al., 2018)\nvaries with chunk size and self-reflection iterations for our generic entity extraction prompt with\n`gpt-4-turbo` .\n\n\n**B** **Example Community Detection**\n\n\n18\n\n\n(a) Root communities at level 0 (b) Sub-communities at level 1\n\n\nFigure 4: Graph communities detected using the Leiden algorithm (Traag et al., 2019) over the\nMultiHop-RAG (Tang and Yang, 2024) dataset as indexed. Circles represent entity nodes with size\nproportional to their degree. Node layout was performed via OpenORD (Martin et al., 2011) and\nForce Atlas 2 (Jacomy et al., 2014). Node colors represent entity communities, shown at two levels\n\n- f hierarchical clustering: (a) Level 0, corresponding to the hierarchical partition with maximum\nmodularity, and (b) Level 1, which reveals internal structure within these root-level communities.\n\n\n**C** **Context Window Selection**\n\n\nThe effect of context window size on any particular task is unclear, especially for models like\n`gpt-4-turbo` with a large context size of 128k tokens. Given the potential for information to\nbe \u201clost in the middle\u201d of longer contexts (Kuratov et al., 2024; Liu et al., 2023), we wanted to explore the effects of varying the context window size for our combinations of datasets, questions, and\nmetrics. In particular, our goal was to determine the optimum context size for our baseline condition\n( **SS** ) and then use this uniformly for all query-time LLM use. To that end, we tested four context\nwindow sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\nwas universally better for all", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_16650", "chunk_text": " LLM use. To that end, we tested four context\nwindow sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\nwas universally better for all comparisons on comprehensiveness (average win rate of 58.1%), while\nperforming comparably with larger context sizes on diversity (average win rate = 52.4%), and empowerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse\nanswers, we therefore used a fixed context window size of 8k tokens for the final evaluation.\n\n\n19\n\n\n**D** **Example Answer Comparison**\n\n\nTable 5: Example question, answers, and LLM-generated assessments for the News article dataset.\n\n\n**Question** **Which public figures are repeatedly mentioned across various entertainment articles?**\n\n**Graph** The entertainment industry is vast and diverse, encompassing film, television, music, sports, and\n**RAG** digital media. Certain public figures stand out due to their significant contributions and influence\nacross these sectors. The following summary highlights key individuals who are repeatedly\nmentioned in various entertainment articles, reflecting their impact and presence within the industry.\n**Actors and Directors** [...] **Public Figures in Controversy** [...] **Musicians and Executives** [...]\n**Athletes and Coaches** [...] **Influencers and Entrepreneurs** [...]\nThe repeated mention of these figures in entertainment articles signifies their ongoing relevance and\nthe public\u2019s interest in their work. Their influence spans across various aspects of entertainment, from\nshaping cultural narratives in film and television to driving trends in music and digital media. These\nindividuals not only contribute to their respective fields but also influence the broader cultural\nlandscape, often becoming central figures in social discussions and public discourse.\n\n\n**Na\u00a8\u0131ve** Public figures who are repeatedly mentioned across various entertainment articles include Taylor\n**RAG** Swift, Travis Kelce, Britney Spears, and Justin Timberlake. These individuals have been highlighted\nfor various reasons, ranging from their professional achievements to their personal lives.\n**Taylor Swift** [...] **Travis Kelce** [...] **Britney Spears** [...] **Justin Timberlake** [...]\nThese figures are frequently covered due to their high-profile status and the public\u2019s interest in their\ncareers and personal lives. Their activities, whether in music, sports, or personal relationships, have\nsignificant cultural", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_17100", "chunk_text": " [...] **Justin Timberlake** [...]\nThese figures are frequently covered due to their high-profile status and the public\u2019s interest in their\ncareers and personal lives. Their activities, whether in music, sports, or personal relationships, have\nsignificant cultural and economic impacts, as evidenced by the media coverage and public reactions.\n\n\n**LLM** **Comprehensiveness: Winner=1 (Graph RAG)** . Answer 1 is better because it provides a more\n**Decision** comprehensive and detailed list of public figures from a wider range of entertainment sectors,\nincluding film, television, music, sports, gaming, and digital media. It also includes examples of their\ncontributions and the impact they have on their respective fields, as well as mentions of controversies\nand their implications. Answer 2, while detailed in its coverage of a few individuals, is limited to a\nsmaller number of public figures and focuses primarily on their personal lives and relationships rather\nthan a broad spectrum of their professional influence across the entertainment industry.\n\n\n**Diversity: Winner=1 (Graph RAG)** . Answer 1 is better because it provides a more varied and rich\nresponse by covering a wide range of public figures from different sectors of the entertainment\nindustry, including film, television, music, sports, gaming, and digital media. It offers insights into\nthe contributions and influence of these figures, as well as controversies and their impact on public\ndiscourse. The answer also cites specific data sources for each mentioned figure, indicating a diverse\nrange of evidence to support the claims. In contrast, Answer 2 focuses on a smaller group of public\nfigures, primarily from the music industry and sports, and relies heavily on a single source for data,\nwhich makes it less diverse in perspectives and insights.\n\n\n**Empowerment: Winner=1 (Graph RAG)** . Answer 1 is better because it provides a comprehensive\nand structured overview of public figures across various sectors of the entertainment industry,\nincluding film, television, music, sports, and digital media. It lists multiple individuals, providing\nspecific examples of their contributions and the context in which they are mentioned in entertainment\narticles, along with references to data reports for each claim. This approach helps the reader\nunderstand the breadth of the topic and make informed judgments without being misled. In contrast,\nAnswer 2 focuses on a smaller group of public figures and primarily discusses their personal lives and\nrelationships, which may not provide as broad an understanding", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_17550", "chunk_text": " the reader\nunderstand the breadth of the topic and make informed judgments without being misled. In contrast,\nAnswer 2 focuses on a smaller group of public figures and primarily discusses their personal lives and\nrelationships, which may not provide as broad an understanding of the topic. While Answer 2 also\ncites sources, it does not match the depth and variety of Answer 1.\n\n\n**Directness: Winner=2 (Na\u00a8\u0131ve RAG)** . Answer 2 is better because it directly lists specific public\nfigures who are repeatedly mentioned across various entertainment articles, such as Taylor Swift,\nTravis Kelce, Britney Spears, and Justin Timberlake, and provides concise explanations for their\nfrequent mentions. Answer 1, while comprehensive, includes a lot of detailed information about\nvarious figures in different sectors of entertainment, which, while informative, does not directly\nanswer the question with the same level of conciseness and specificity as Answer 2.\n\n\n20\n\n\n**E** **System Prompts**\n\n\n**E.1** **Element Instance Generation**\n\n```\n---Goal--\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify\nall entities of those types from the text and all relationships among the identified entities.\n\n---Steps--1. Identify all entities. For each identified entity, extract the following information:\n- entity n ame: Name of the entity, capitalized\n```\n\n`- entity` ~~`t`~~ `ype:` `One of the following types:` `[` _{_ `entity` ~~`t`~~ `ypes` _}_ `]`\n```\n- entity d escription: Comprehensive description of the entity\u2019s attributes and activities\n\n```\n\n`Format each entity as (\"entity\"` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `<entity` ~~`n`~~ `ame>` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `<entity` `type>` _{_ `tuple`\n`delimiter` _}_ `<entity` ~~`d`~~ `escription>`\n\n```\n2. From the entities identified in step 1, identify all pairs of (source e ntity, target e ntity) that\nare *clearly related* to each other\nFor each pair of related entities, extract the following information:\n- source e ntity: name of the source entity, as identified in step 1", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_18000", "chunk_text": " ntity, target e ntity) that\nare *clearly related* to each other\nFor each pair of related entities, extract the following information:\n- source e ntity: name of the source entity, as identified in step 1\n- target e ntity: name of the target entity, as identified in step 1\n- relationship d escription: explanation as to why you think the source entity and the target entity are\nrelated to each other\n\n- relationship s trength: a numeric score indicating strength of the relationship between the source entity\nand target entity\n\n```\n\n`Format each relationship as (\"relationship\"` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `<source` ~~`e`~~ `ntity>` _{_ `tuple` `delimiter` _}_ `<target`\n`entity>` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `<relationship` ~~`d`~~ `escription>` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `<relationship` ~~`s`~~ `trength>)`\n\n```\n3. Return output in English as a single list of all the entities and relationships identified in steps 1\n```\n\n`and 2.` `Use **` _{_ `record` ~~`d`~~ `elimiter` _}_ `** as the list delimiter.`\n\n\n`4.` `When finished, output` _{_ `completion` ~~`d`~~ `elimiter` _}_\n\n```\n---Examples--\nEntity t ypes: ORGANIZATION,PERSON\n\nInput:\n\nThe Fed is scheduled to meet on Tuesday and Wednesday, with the central bank planning to release its\nlatest policy decision on Wednesday at 2:00 p.m. ET, followed by a press conference where Fed Chair\nJerome Powell will take questions. Investors expect the Federal Open Market Committee to hold its\nbenchmark interest rate steady in a range of 5.25%-5.5%.\n\nOutput:\n\n```\n\n`(\"entity\"` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `FED` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `ORGANIZATION` _{_ `tuple` `delimiter` _}_ `The Fed is the Federal Reserve,`\n```\nwhich is setting interest rates on Tuesday and Wednesday)\n```\n\n_{_ `record` ~~`d", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_18450", "chunk_text": "imiter` _}_ `ORGANIZATION` _{_ `tuple` `delimiter` _}_ `The Fed is the Federal Reserve,`\n```\nwhich is setting interest rates on Tuesday and Wednesday)\n```\n\n_{_ `record` ~~`d`~~ `elimiter` _}_\n`(\"entity\"` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `JEROME POWELL` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `PERSON` _{_ `tuple` `delimiter` _}_ `Jerome Powell is the chair`\n```\nof the Federal Reserve)\n```\n\n_{_ `record` ~~`d`~~ `elimiter` _}_\n`(\"entity\"` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `FEDERAL OPEN MARKET COMMITTEE` _{_ `tuple` `delimiter` _}_ `ORGANIZATION` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `The`\n```\nFederal Reserve committee makes key decisions about interest rates and the growth of the United States\nmoney supply)\n```\n\n_{_ `record` ~~`d`~~ `elimiter` _}_\n`(\"relationship\"` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `JEROME POWELL` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `FED` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `Jerome Powell is the`\n`Chair of the Federal Reserve and will answer questions at a press conference` _{_ `tuple` ~~`d`~~ `elimiter` _}_ `9)`\n_{_ `completion` ~~`d`~~ `elimiter` _}_\n\n\n_...More examples..._\n\n```\n---Real Data--\n```\n\n`Entity` ~~`t`~~ `ypes:` _{_ `entity` ~~`t`~~ `ypes` _}_\n```\nInput:\n\n```\n\n_{_ `input` ~~`t`~~ `ext` _}_\n\n```\nOutput:\n\n```\n\n**E.2** **Community Summary Generation**\n\n```\n---Role--\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information\ndiscovery is the process of identifying and assessing relevant information associated with certain\nentities (e.g., organizations and individuals", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_18900", "chunk_text": "Community Summary Generation**\n\n```\n---Role--\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information\ndiscovery is the process of identifying and assessing relevant information associated with certain\nentities (e.g., organizations and individuals) within a network.\n\n```\n\n21\n\n\n```\n---Goal--\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well\nas their relationships and optional associated claims. The report will be used to inform decision-makers\nabout information associated with the community and their potential impact. The content of this report\nincludes an overview of the community\u2019s key entities, their legal compliance, technical capabilities,\nreputation, and noteworthy claims.\n\n---Report Structure--\nThe report should include the following sections:\n\n- TITLE: community\u2019s name that represents its key entities - title should be short but specific. When\npossible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community\u2019s overall structure, how its entities are related to each\nother, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by\nentities within the community. IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short\nsummary followed by multiple paragraphs of explanatory text grounded according to the grounding rules\nbelow. Be comprehensive.\n\nReturn output as a well-formed JSON-formatted string with the following format:\n```\n\n_{{_\n```\n\"title\": <report t itle>,\n\"summary\": <executive s ummary>,\n\"rating\": <impact s everity r ating>,\n\"rating e xplanation\": <rating e xplanation>,\n\"findings\": [\n```\n\n_{{_\n```\n\"summary\":<insight 1 s ummary>,\n\"explanation\": <insight 1 e xplanation>\n```\n\n_}}_ `,`\n_{{_\n```\n\"summary\":<insight 2 s ummary>,\n\"explanation\": <insight 2 e xplanation>\n```\n\n_}}_\n```\n]\n```\n\n_}}_\n\n```\n---Grounding Rules--\nPoints supported by data should list their data references as follows:\n\n\"This is an example", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_19350", "chunk_text": ">,\n\"explanation\": <insight 2 e xplanation>\n```\n\n_}}_\n```\n]\n```\n\n_}}_\n\n```\n---Grounding Rules--\nPoints supported by data should list their data references as follows:\n\n\"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids);\n<dataset name> (record ids)].\"\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record\nids and add \"+more\" to indicate that there are more.\n\nFor example:\n\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1),\nEntities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)].\"\n\nwhere 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n---Example--\nInput:\n\nEntities\n\nid,entity,description\n5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March\n6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza\n\nRelationships\n\nid,source,target,description\n37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March\n38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza\n39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza\n40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at\nVerdant Oasis Plaza\n\n41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n\n```\n\n22\n\n\n_{{_\n```\n\"title\": \"Verdant Oasis Plaza and Unity March\",\n", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_19800", "chunk_text": " Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n\n```\n\n22\n\n\n_{{_\n```\n\"title\": \"Verdant Oasis Plaza and Unity March\",\n\"summary\": \"The community revolves around the Verdant Oasis Plaza, which is the location of the Unity\nMarch. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of\nwhich are associated with the march event.\",\n\"rating\": 5.0,\n\"rating e xplanation\": \"The impact severity rating is moderate due to the potential for unrest or conflict\nduring the Unity March.\",\n\"findings\": [\n```\n\n_{{_\n```\n\"summary\": \"Verdant Oasis Plaza as the central location\",\n\"explanation\": \"Verdant Oasis Plaza is the central entity in this community, serving as the location for\nthe Unity March. This plaza is the common link between all other entities, suggesting its significance\nin the community. The plaza\u2019s association with the march could potentially lead to issues such as\npublic disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data:\nEntities (5), Relationships (37, 38, 39, 40, 41,+more)]\"\n```\n\n_}}_ `,`\n_{{_\n```\n\"summary\": \"Harmony Assembly\u2019s role in the community\",\n\"explanation\": \"Harmony Assembly is another key entity in this community, being the organizer of the\nmarch at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of\nthreat, depending on their objectives and the reactions they provoke. The relationship between Harmony\nAssembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6),\nRelationships (38, 43)]\"\n```\n\n_}}_ `,`\n_{{_\n```\n\"summary\": \"Unity March as a significant event\",\n\"explanation\": \"The Unity March is a significant event taking place at Verdant Oasis Plaza. This event\nis a key factor in the community\u2019s dynamics and could be a potential source of threat, depending on the\nnature of the march and the reactions it provokes. The relationship between the march and the plaza is\ncrucial in understanding the dynamics of this community. [Data: Relationships (39)]\"\n```\n\n_}}_ `,", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_20250", "chunk_text": " depending on the\nnature of the march and the reactions it provokes. The relationship between the march and the plaza is\ncrucial in understanding the dynamics of this community. [Data: Relationships (39)]\"\n```\n\n_}}_ `,`\n_{{_\n```\n\"summary\": \"Role of Tribune Spotlight\", \"explanation\": \"Tribune Spotlight is reporting on the Unity\nMarch taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention,\nwhich could amplify its impact on the community. The role of Tribune Spotlight could be significant in\nshaping public perception of the event and the entities involved. [Data: Relationships (40)]\"\n```\n\n_}}_\n```\n]\n```\n\n_}}_\n\n```\n---Real Data--\nUse the following text for your answer. Do not make anything up in your answer.\n\nInput:\n\n```\n\n_{_ `input` ~~`t`~~ `ext` _}_\n\n\n_...Report Structure and Grounding Rules Repeated..._\n\n```\nOutput:\n\n```\n\n**E.3** **Community Answer Generation**\n\n```\n---Role--\nYou are a helpful assistant responding to questions about a dataset by synthesizing perspectives from\nmultiple analysts.\n\n---Goal--\nGenerate a response of the target length and format that responds to the user\u2019s question, summarize\nall the reports from multiple analysts who focused on different parts of the dataset, and incorporate any\nrelevant general knowledge.\n\nNote that the analysts\u2019 reports provided below are ranked in the **descending order of helpfulness**.\n\nIf you don\u2019t know the answer, just say so. Do not make anything up.\n\nThe final response should remove all irrelevant information from the analysts\u2019 reports and merge the\ncleaned information into a comprehensive answer that provides explanations of all the key points and\nimplications appropriate for the response length and format.\n\nAdd sections and commentary to the response as appropriate for the length and format. Style the response\nin markdown.\n\nThe response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n\nThe response should also preserve all the data references previously included in the analysts\u2019 reports,\n\n```\n\n23\n\n\n```\nbut do not mention the roles of multiple analysts in the analysis process.\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record\nids and add \"+more\" to indicate that there are more.\n\nFor example:\n\n\"Person", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_20700", "chunk_text": " multiple analysts in the analysis process.\n\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record\nids and add \"+more\" to indicate that there are more.\n\nFor example:\n\n\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2,\n7, 34, 46, 64, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n\nwhere 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.\n\nDo not include information where the supporting evidence for it is not provided.\n\n---Target response length and format--\n```\n\n_{_ `response` ~~`t`~~ `ype` _}_\n\n```\n---Analyst Reports--\n```\n\n_{_ `report` ~~`d`~~ `ata` _}_\n\n\n_...Goal and Target response length and format repeated..._\n\n```\nAdd sections and commentary to the response as appropriate for the length and format. Style the response\nin markdown.\n\nOutput:\n\n```\n\n**E.4** **Global Answer Generation**\n\n```\n---Role--\nYou are a helpful assistant responding to questions about data in the tables provided.\n\n---Goal--\nGenerate a response of the target length and format that responds to the user\u2019s question, summarize\nall relevant information in the input data tables appropriate for the response length and format, and\nincorporate any relevant general knowledge.\n\nIf you don\u2019t know the answer, just say so. Do not make anything up.\n\nThe response shall preserve the original meaning and use of modal verbs such as \"shall\", \"may\" or \"will\".\n\nPoints supported by data should list the relevant reports as references as follows:\n\n\"This is an example sentence supported by data references [Data: Reports (report ids)]\"\n\n```\n\n_Note: the prompts for SS (semantic search) and TS (text summarization) conditions use \u201dSources\u201d in place of \u201dReports\u201d above._\n\n```\nDo not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record\nids and add \"+more\" to indicate that there are more.\n\nFor example:\n\n\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2,\n7, 64, ", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_21150", "chunk_text": "5 most relevant record\nids and add \"+more\" to indicate that there are more.\n\nFor example:\n\n\"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2,\n7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Reports (1, 3)]\"\n\nwhere 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data report in the\nprovided tables.\n\nDo not include information where the supporting evidence for it is not provided.\n\nAt the beginning of your response, generate an integer score between 0-100 that indicates how **helpful**\nis this response in answering the user\u2019s question. Return the score in this format: <ANSWER H ELPFULNESS>\nscore v alue </ANSWER H ELPFULNESS>.\n\n---Target response length and format--\n```\n\n_{_ `response` ~~`t`~~ `ype` _}_\n\n```\n---Data tables--\n```\n\n_{_ `context` ~~`d`~~ `ata` _}_\n\n\n_...Goal and Target response length and format repeated..._\n\n\nOutput:\n\n\n24\n\n\n**F** **Evaluation Prompts**\n\n\n**F.1** **Relative Assessment Prompt**\n\n```\n---Role--\nYou are a helpful assistant responsible for grading two answers to a question that are provided by two\ndifferent people.\n\n---Goal--\nGiven a question and two answers (Answer 1 and Answer 2), assess which answer is better according to\nthe following measure:\n\n```\n\n_{_ `criteria` _}_\n\n```\nYour assessment should include two parts:\n- Winner: either 1 (if Answer 1 is better) and 2 (if Answer 2 is better) or 0 if they are fundamentally\nsimilar and the differences are immaterial.\n\n- Reasoning: a short explanation of why you chose the winner with respect to the measure described above.\n\nFormat your response as a JSON object with the following structure:\n```\n\n_{{_\n```\n\"winner\": <1, 2, or 0>,\n\"reasoning\": \"Answer 1 is better because <your reasoning>.\"\n```\n\n_}}_\n\n```\n---Question--\n```\n\n_{_ `question` _}_\n\n```\n---Answer 1--\n```\n\n_{_ `answer1` _", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_21600", "chunk_text": "ing\": \"Answer 1 is better because <your reasoning>.\"\n```\n\n_}}_\n\n```\n---Question--\n```\n\n_{_ `question` _}_\n\n```\n---Answer 1--\n```\n\n_{_ `answer1` _}_\n\n```\n---Answer 2--\n```\n\n_{_ `answer2` _}_\n\n```\nAssess which answer is better according to the following measure:\n\n```\n\n_{_ `criteria` _}_\n\n```\nOutput:\n\n```\n\n**F.2** **Relative Assessment Metrics**\n\n\n`CRITERIA =` _{_\n```\n\"comprehensiveness\": \"How much detail does the answer provide to cover all the aspects and details of the\nquestion? A comprehensive answer should be thorough and complete, without being redundant or irrelevant.\nFor example, if the question is \u2019What are the benefits and drawbacks of nuclear energy?\u2019, a comprehensive\nanswer would provide both the positive and negative aspects of nuclear energy, such as its efficiency,\nenvironmental impact, safety, cost, etc. A comprehensive answer should not leave out any important points\nor provide irrelevant information. For example, an incomplete answer would only provide the benefits of\nnuclear energy without describing the drawbacks, or a redundant answer would repeat the same information\nmultiple times.\",\n\"diversity\": \"How varied and rich is the answer in providing different perspectives and insights\non the question? A diverse answer should be multi-faceted and multi-dimensional, offering different\nviewpoints and angles on the question. For example, if the question is \u2019What are the causes and effects\nof climate change?\u2019, a diverse answer would provide different causes and effects of climate change, such\nas greenhouse gas emissions, deforestation, natural disasters, biodiversity loss, etc. A diverse answer\nshould also provide different sources and evidence to support the answer. For example, a single-source\nanswer would only cite one source or evidence, or a biased answer would only provide one perspective or\nopinion.\",\n\"directness\": \"How specifically and clearly does the answer address the question? A direct answer should\nprovide a clear and concise answer to the question. For example, if the question is \u2019What is the capital\nof France?\u2019, a direct answer would be \u2019Paris\u2019. A direct answer should not provide any irrelevant or\nunnecessary information that does not answer the question. For example, an indirect answer would be \u2019The\ncapital of France is located", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_22050", "chunk_text": " capital\nof France?\u2019, a direct answer would be \u2019Paris\u2019. A direct answer should not provide any irrelevant or\nunnecessary information that does not answer the question. For example, an indirect answer would be \u2019The\ncapital of France is located on the river Seine\u2019.\",\n\"empowerment\": \"How well does the answer help the reader understand and make informed judgements about\nthe topic without being misled or making fallacious assumptions. Evaluate each answer on the quality of\nanswer as it relates to clearly explaining and providing reasoning and sources behind the claims in the\nanswer.\"\n```\n\n_}_\n\n\n25\n\n\n**G** **Statistical Analysis**\n\n\nTable 6: Pairwise comparisons of six conditions on four metrics across 125 questions and two\ndatasets. For each question and metric, the winning condition received a score of 100, the losing\ncondition received a score of 0, and in the event of a tie, each condition was scored 50. These scores\nwere then averaged over five evaluation runs for each condition. Results of Shapiro-Wilk tests indicated that the data did not follow a normal distribution. Thus, non-parametric tests (Wilcoxon\nsigned-rank tests) were employed to assess the performance differences between pairs of conditions, with Holm-Bonferroni correction applied to account for multiple pairwise comparisons. The\ncorrected p-values that indicated statistically significant differences are highlighted in bold.\n\n\n**Podcast Transcripts** **News Articles**\n**Condition 1** **Condition 2** **Mean 1** **Mean 2** **Z-value** **p-value** **Mean 1** **Mean 2** **Z-value** **p-value**\n\n\n\n**Comprehensiveness**\n\n\n**Diversity**\n\n\n**Empowerment**\n\n\n**Directness**\n\n\n\nC0 TS 50.24 49.76 - 0.06 1 55.52 44.48 - 2.03 0.17\n\nC1 TS 51.92 48.08 - 1.56 0.633 58.8 41.2 - 3.62 **0.002**\n\nC2 TS 57.28 42.72 - 4.1 _**<**_ **0.001** 62.08 37.92 - 5.07 _**<**_ **0.001**\nC3 TS 56.48", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_22500", "chunk_text": "28 42.72 - 4.1 _**<**_ **0.001** 62.08 37.92 - 5.07 _**<**_ **0.001**\nC3 TS 56.48 43.52 - 3.42 **0.006** 63.6 36.4 - 5.63 _**<**_ **0.001**\nC0 SS 71.92 28.08 - 6.2 _**<**_ **0.001** 71.76 28.24 - 6.3 _**<**_ **0.001**\nC1 SS 75.44 24.56 - 7.45 _**<**_ **0.001** 74.72 25.28 - 7.78 _**<**_ **0.001**\nC2 SS 77.76 22.24 - 8.17 _**<**_ **0.001** 79.2 20.8 - 8.34 _**<**_ **0.001**\nC3 SS 78.96 21.04 - 8.12 _**<**_ **0.001** 79.44 20.56 - 8.44 _**<**_ **0.001**\nTS SS 83.12 16.88 - 8.85 _**<**_ **0.001** 79.6 20.4 - 8.27 _**<**_ **0.001**\nC0 C1 53.2 46.8 - 1.96 0.389 51.92 48.08 - 0.45 0.777\n\nC0 C2 50.24 49.76 - 0.23 1 53.68 46.32 - 1.54 0.371\n\nC1 C2 51.52 48.48 - 1.62 0.633 57.76 42.24 - 4.01 _**<**_ **0.001**\nC0 C3 49.12 50.88 - 0.56 1 52.16 47.84 - 0.86 0.777\n\nC1 C3 50.32 49.68", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_22950", "chunk_text": "**\nC0 C3 49.12 50.88 - 0.56 1 52.16 47.84 - 0.86 0.777\n\nC1 C3 50.32 49.68 - 0.66 1 55.12 44.88 - 2.94 **0.016**\n\nC2 C3 52.24 47.76 - 1.97 0.389 58.64 41.36 - 3.68 **0.002**\n\n\nC0 TS 50.24 49.76 - 0.11 1 46.88 53.12 - 1.38 0.676\n\nC1 TS 50.48 49.52 - 0.12 1 54.64 45.36 - 1.88 0.298\n\nC2 TS 57.12 42.88 - 2.84 **0.036** 55.76 44.24 - 2.16 0.184\n\nC3 TS 54.32 45.68 - 2.39 0.1 60.16 39.84 - 4.07 _**<**_ **0.001**\nC0 SS 76.56 23.44 - 7.12 _**<**_ **0.001** 62.08 37.92 - 3.57 **0.003**\nC1 SS 75.44 24.56 - 7.33 _**<**_ **0.001** 64.96 35.04 - 4.92 _**<**_ **0.001**\nC2 SS 80.56 19.44 - 8.21 _**<**_ **0.001** 70.56 29.44 - 6.29 _**<**_ **0.001**\nC3 SS 80.8 19.2 - 8.3 _**<**_ **0.001** 69.12 30.88 - 5.53 _**<**_ **0.001**\nTS SS 82.08 17.92 - 8.43 _**<**_ **0.001** 67.2 32.8 - 4.85 _**", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_23400", "chunk_text": " _**<**_ **0.001**\nTS SS 82.08 17.92 - 8.43 _**<**_ **0.001** 67.2 32.8 - 4.85 _**<**_ **0.001**\nC0 C1 49.76 50.24 - 0.13 1 39.68 60.32 - 3.61 **0.003**\n\nC0 C2 46.32 53.68 - 1.5 0.669 40.96 59.04 - 3.14 **0.012**\n\nC1 C2 44.08 55.92 - 3.27 **0.011** 50.24 49.76 - 0.22 1\n\nC0 C3 44 56 - 2.6 0.065 41.04 58.96 - 3.47 **0.004**\n\nC1 C3 45.44 54.56 - 2.98 **0.026** 49.52 50.48 - 0.01 1\n\nC2 C3 48.48 51.52 - 0.96 1 50.96 49.04 - 0.39 1\n\n\nC0 TS 40.96 59.04 - 4.3 _**<**_ **0.001** 42.24 57.76 - 3.32 **0.012**\nC1 TS 45.2 54.8 - 3.76 **0.002** 50 50 - 0.12 1\n\nC2 TS 47.68 52.32 - 2.2 0.281 49.52 50.48 - 0.22 1\n\nC3 TS 48.72 51.28 - 1.27 1 51.68 48.32 - 1.2 1\n\nC0 SS 42.96 57.04 - 3.71 **0.003** 42.72 57.28 - 3.12 **0.022**\n\nC1 SS 47.68 52.32 - 1.5 0.936 51.36 48.64 - 0.84 ", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_23850", "chunk_text": ".72 57.28 - 3.12 **0.022**\n\nC1 SS 47.68 52.32 - 1.5 0.936 51.36 48.64 - 0.84 1\n\nC2 SS 50.72 49.28 - 0.55 1 49.84 50.16 - 0.2 1\n\nC3 SS 48.96 51.04 - 0.57 1 49.52 50.48 - 0.08 1\n\nTS SS 57.52 42.48 - 4.1 _**<**_ **0.001** 52.88 47.12 - 1.1 1\nC0 C1 48.72 51.28 - 1.23 1 42.4 57.6 - 3.9 **0.001**\n\nC0 C2 46.64 53.36 - 2.54 0.12 44.8 55.2 - 2.16 0.336\n\nC1 C2 49.28 50.72 - 1.73 0.682 52 48 - 1.45 1\n\nC0 C3 47.6 52.4 - 1.78 0.682 44.32 55.68 - 3.45 **0.008**\n\nC1 C3 50 50 0 1 51.44 48.56 - 1.02 1\n\nC2 C3 50.72 49.28 - 0.86 1 50.4 49.6 - 0.22 1\n\n\nC0 TS 44.96 55.04 - 4.09 _**<**_ **0.001** 45.2 54.8 - 3.68 **0.003**\nC1 TS 47.92 52.08 - 2.41 0.126 46.64 53.36 - 2.91 **0.04**\n\nC2 TS 48.8 51.2 - 2.23 0.179 48.32 51.68 - 2.12 0.179\n\nC3 TS 48.08 ", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_24300", "chunk_text": "0.04**\n\nC2 TS 48.8 51.2 - 2.23 0.179 48.32 51.68 - 2.12 0.179\n\nC3 TS 48.08 51.92 - 2.23 0.179 48.32 51.68 - 2.56 0.074\n\nC0 SS 35.12 64.88 - 6.17 _**<**_ **0.001** 41.44 58.56 - 4.82 _**<**_ **0.001**\nC1 SS 40.32 59.68 - 4.83 _**<**_ **0.001** 45.2 54.8 - 3.19 **0.017**\nC2 SS 40.4 59.6 - 4.67 _**<**_ **0.001** 44.88 55.12 - 3.65 **0.003**\nC3 SS 40.48 59.52 - 4.69 _<_ **0.001** 45.6 54.4 - 2.86 **0.043**\nTS SS 43.6 56.4 - 3.96 _**<**_ **0.001** 46 54 - 2.68 0.066\nC0 C1 46.96 53.04 - 2.87 **0.037** 47.6 52.4 - 2.17 0.179\n\nC0 C2 48.4 51.6 - 2.06 0.197 48.48 51.52 - 1.61 0.321\n\nC1 C2 49.84 50.16 - 1 0.952 49.28 50.72 - 1.6 0.321\n\nC0 C3 48.4 51.6 - 1.8 0.29 47.2 52.8 - 2.62 0.071\n\nC1 C3 49.76 50.24 0 1 48.8 51.2 - 1.29 0.321\n\nC2 C3 50 50 0 1 48", "token_count": 500, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2404.16130_graphrag_edge:chunk_24750", "chunk_text": "071\n\nC1 C3 49.76 50.24 0 1 48.8 51.2 - 1.29 0.321\n\nC2 C3 50 50 0 1 48.8 51.2 - 1.84 0.262\n\n\n26\n\n\n", "token_count": 68, "metadata": {"arxiv_id": "2404.16130", "title": "From Local to Global: A Graph RAG Approach to Query-Focused Summarization", "authors": ["Darren Edge", "Ha Trinh", "Newman Cheng", "Joshua Bradley", "Alex Chao", "Apurva Mody", "Steven Truitt", "Dasha Metropolitansky", "Robert Osazuwa Ness", "Jonathan Larson"], "year": 2024, "url": "https://arxiv.org/pdf/2404.16130v2"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_0", "chunk_text": "Preprint.\n\n## - SELF RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\n\n\n**Akari Asai** _[\u2020]_ **, Zeqiu Wu** _[\u2020]_ **, Yizhong Wang** _[\u2020\u00a7]_ **, Avirup Sil** _[\u2021]_ **, Hannaneh Hajishirzi** _[\u2020\u00a7]_\n\n_\u2020_ University of Washington _\u00a7_ Allen Institute for AI _\u2021_ IBM Research AI\n_{_ akari,zeqiuwu,yizhongw,hannaneh _}_ @cs.washington.edu, avi@us.ibm.com\n\n\nABSTRACT\n\n\nDespite their remarkable capabilities, large language models (LLMs) often produce\nresponses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\n\n     - f retrieved passages, regardless of whether retrieval is necessary, or passages are\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\nWe introduce a new framework called **Self-Reflective Retrieval-Augmented Gen-**\n**eration (SELF-RAG)** that enhances an LM\u2019s quality and factuality through retrieval\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\nretrieves passages on-demand, and generates and reflects on retrieved passages\nand its own generations using special tokens, called _reflection_ tokens. Generating\nreflection tokens makes the LM controllable during the inference phase, enabling it\nto tailor its behavior to diverse task requirements. Experiments show that SELFRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\n\n    - utperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in improving\nfactuality and citation accuracy for long-form generations relative to these models. [1]\n\n\n1 INTRODUCTION\n\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_450", "chunk_text": " in improving\nfactuality and citation accuracy for long-form generations relative to these models. [1]\n\n\n1 INTRODUCTION\n\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces **Self-Reflective Retrieval-augmented Generation (SELF-RAG)** to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., _reflection tokens_ ). Reflection tokens are categorized into _retrieval_ and _critique_ tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a **retrieval** token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and then **generating** corresponding task outputs (Step\n2). It then generates critique tokens to **criticize** its own output and choose best one (Step 3) in terms\n\n- f factuality and overall quality. This process differs from conventional RAG (", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_900", "chunk_text": " corresponding task outputs (Step\n2). It then generates critique tokens to **criticize** its own output and choose best one (Step 3) in terms\n\n- f factuality and overall quality. This process differs from conventional RAG (Figure 1 left), which\n\n\n1\n[Our code and trained models are available at https://selfrag.github.io/.](https://selfrag.github.io/)\n\n\n1\n\n\nPreprint.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Retrieval-Augmented Generation (RAG)<br>Prompt How did US states get their names?<br>Step 1: Retrieve K documents<br>O aff t t eh re a f nif t iny d s it va it de us a, le ple ev re sn o na .r e named<br>P Eo mp mul aa r n aa m poe ps b lay s bta at be ys . I an T ee .x as,<br>is u r n m<br>Retriever C isa lali nfo dr in ni a w Sa ps nn ia sm e bd oa kft er a fictional<br>a a h o .<br>Step 2: Prompt LM with K docs and generate<br>Prompt How did US states get their names? +<br>US states got their names from a variety of<br>sources. Eleven states are named after an<br>individual person (e.g, California was named<br>LM after Christopher Columbus) . Some states<br>including Texas and Utah, are named after<br>ContradNicatotirvye American tribe.No information in passages|Ours: Self-reflective Retrieval-Augmented Generation (Self-RAG)<br>Prompt How did US states get their names? Step 1: Retrieve on demand<br>US states got their names from a variety of sources. Retrieve<br>Step 2: Generate segment in parallel<br>Prompt + Prompt + Prompt +<br>cR oe mlev ea n frt om11 o ef s5 o0 s st .ate na Sm upe ps afI trr ee rle ava Nnt ativT ee Ax mas i rs n aa nm tre ibd LarR se ile Sv ea n rgt C a li e1fo Er tn shi p-a l' as nn dta i\u00e1m nye nh oa Ps ri lt s<br>p r n orted e ic e. o ig n s in a sa d 6 c", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_1350", "chunk_text": " rgt C a li e1fo Er tn shi p-a l' as nn dta i\u00e1m nye nh oa Ps ri lt s<br>p r n orted e ic e. o ig n s in a sa d 6 c e n u r . v e a t ially<br>Step 3: Critique outputs and select best segment<br>> ><br>Retrieve Repeat.\u2026 U stS at es sta nte as m g eo s t at rh ee i cr on ma em e frs o mfr o pm e rsa o nva sr . ie ty o 2f 6 s so tu ar tc ee ss a. r1 e1 n ao mf 5 ed0<br>after Native Americans, including Utah<br>.|\n|---|---|\n|**Prompt:**Write an essay of your best summer vacation<br>My best\u2026|**Prompt:**Write an essay of your best summer vacation<br>No Retrieval<br>My best summer vacation is when my family and I embarked on a road trip along \u2026|\n\n\nFigure 1: Overview of SELF-RAG. SELF-RAG learns to retrieve, critique, and generate text passages\nto enhance overall generation quality, factuality, and verifiability.\n\n\nconsistently retrieves a fixed number of documents for generation regardless of the retrieval necessity\n(e.g., the bottom figure example does not require factual knowledge) and never second visits the\ngeneration quality. Moreover, SELF-RAG provides citations for each segment with its self-assessment\n\n- f whether the output is supported by the passage, leading to easier fact verification.\n\n\nSELF-RAG trains an arbitrary LM to generate text with reflection tokens by unifying them as the\nnext token prediction from the expanded model vocabulary. We train our generator LM on a diverse\ncollection of text interleaved with reflection tokens and retrieved passages. Reflection tokens, inspired\nby reward models used in reinforcement learning (Ziegler et al., 2019; Ouyang et al., 2022), are\ninserted offline into the original corpus by a trained _critic_ model. This eliminates the need to host a\ncritic model during training, reducing overhead. The critic model, in part, is supervised on a dataset\n\n- f input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_1800", "chunk_text": " part, is supervised on a dataset\n\n- f input, output, and corresponding reflection tokens collected by prompting a propriety LM (i.e.,\nGPT-4; OpenAI 2023). While we draw inspiration from studies that use control tokens to start and\nguide text generation (Lu et al., 2022; Keskar et al., 2019), our trained LM uses critique tokens to\nassess its own predictions after each generated segment as an integral part of the generation output.\n\n\nSELF-RAG further enables a customizable decoding algorithm to satisfy hard or soft constraints,\nwhich are defined by reflection token predictions. In particular, our inference-time algorithm enables\nus to (1) flexibly adjust retrieval frequency for different downstream applications and (2) customize\nmodels\u2019 behaviors to user preferences by leveraging reflection tokens through segment-level beam\nsearch using the weighted linear sum of the reflection token probabilities as segment score.\n\n\nEmpirical results on six tasks, including reasoning and long-form generation, demonstrate that SELFRAG significantly outperforms pre-trained and instruction-tuned LLMs that have more parameters and\nwidely adopted RAG approaches with higher citation accuracy. In particular, SELF-RAG outperforms\nretrieval-augmented ChatGPT on four tasks, Llama2-chat (Touvron et al., 2023) and Alpaca (Dubois\net al., 2023) on all tasks. Our analysis demonstrates the effectiveness of training and inference with\nreflection tokens for overall performance improvements as well as test-time model customizations\n(e.g., balancing the trade-off between citation previsions and completeness).\n\n\n2 RELATED WORK\n\n\n**Retrieval-Augmented Generation.** Retrieval-Augmented Generation (RAG) augments the input\nspace of LMs with retrieved text passages (Guu et al., 2020; Lewis et al., 2020), leading to large\nimprovements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram\net al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number\n\n\n2\n\n\nPreprint.\n\n\n- f retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\n\n- nce at the", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_2250", "chunk_text": " prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\n\n- nce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\n\n- n top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\nentities. Yet, the improved task performance of such approaches often comes at the expense of\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\nlearn to use retrieval _on-demand_ for diverse instruction-following queries and introduce controlled\ngeneration guided by reflections tokens to further improve generation quality and attributions.\n\n\n**Concurrent RAG work.** A few concurrent works [2] - n RAG propose new training or prompting\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\ninstruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\na summarization model to filter out or compress retrieved passages before using them to prompt the\nLM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_2700", "chunk_text": " question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self-reflection and customizable inference.\n\n\n**Training and generating with critics.** Training LLMs with reinforcement learning (e.g., Proximal\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\n\n- n retrieval and generation, we train our target LM on task examples augmented with reflection\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\nreflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on\nhuman preference alignment during training. Other works use general control tokens to guide LM\ngeneration (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluationguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\n\n\n3 SELF-RAG: LEARNING TO RETRIEVE, GENERATE AND CRITIQUE\n\n\nWe introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1.\nSELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM\u2019s original creativity and versatility. Our end-to-end training\nlets an LM _M_ **generate** text informed by **retrieved** passages, if needed, and **criticize** the output by\nlearning to generate special tokens. These _reflection tokens", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_3150", "chunk_text": " versatility. Our end-to-end training\nlets an LM _M_ **generate** text informed by **retrieved** passages, if needed, and **criticize** the output by\nlearning to generate special tokens. These _reflection tokens_ (Table 1) signal the need for retrieval\n\n- r confirm the output\u2019s relevance, support, or completeness. In contrast, common RAG approaches\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\n\n\n3.1 PROBLEM FORMALIZATION AND OVERVIEW\n\n\nFormally, given input _x_, we train _M_ to sequentially generate textual outputs _y_ consisting of multiple\nsegments _y_ = [ _y_ 1 _, . . ., yT_ ], where _yt_ indicates a sequence of tokens for the _t_ - th segment. [3] Generated\ntokens in _yt_ include text from the original vocabulary as well as the reflection tokens (Table 1).\n\n\n2All work is arXived within a week of this preprint.\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\nsegment unit (i.e., sub-sentence).\n\n\n3\n\n\nPreprint.\n\n\nType Input Output Definitions\n\n\n**Retrieve** _x_ / _x, y_ _{_ yes, no, continue _}_ Decides when to retrieve with _R_\n**ISREL** _x, d_ _{_ **relevant**, irrelevant _}_ _d_ provides useful information to solve _x_ .\n**ISSUP** _x, d, y_ _{_ **fully supported**, partially All of the verification-worthy statement in _y_\nsupported, no support _}_ is supported by _d_ .\n\n|Retrieve|Col2|\n|---|---|\n|**ISREL**||\n|**ISSUP**|**ISSUP**|\n\n**ISUSE** _x, y_ _{_ **5**, 4, 3, 2, 1 _}_ _y_ is a useful response to _x_ .\n\n\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\nits output values. The bottom three rows are three types of **Critique** tokens, and **the bold text** indicates\nthe most desirable critique tokens. _x, y, d_ indicate input, output, and a relevant passage, respectively.\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_3600", "chunk_text": " output values. The bottom three rows are three types of **Critique** tokens, and **the bold text** indicates\nthe most desirable critique tokens. _x, y, d_ indicate input, output, and a relevant passage, respectively.\n\n\n**Algorithm 1** SELF-RAG Inference\n\n\n**Require:** Generator LM _M_, Retriever _R_, Large-scale passage collections _{d_ 1 _, . . ., dN_ _}_\n\n1: **Input:** input prompt _x_ and preceding generation _y<t_, **Output:** next output segment _yt_\n2: _M_ predicts **Retrieve** given ( _x, y<t_ )\n3: **if Retrieve** == Yes **then**\n4: Retrieve relevant text passages **D** using _R_ given ( _x, yt\u2212_ 1) _\u25b7_ Retrieve\n5: _M_ predicts **ISREL** given _x, d_ and _yt_ given _x, d, y<t_ for each _d \u2208_ **D** _\u25b7_ Generate\n6: _M_ predicts **ISSUP** and **ISUSE** given _x, yt, d_ for each _d \u2208_ **D** _\u25b7_ Critique\n7: Rank _yt_ based on **ISREL**, **ISSUP**, **ISUSE** _\u25b7_ Detailed in Section 3.3\n\n|ISREL|givenx dand<br>,<br>and ISUSE gi|Col3|Col4|\n|---|---|---|---|\n|**ISSUP**|**ISSUP**|**ISUSE**|**ISUSE**|\n|d on|**ISREL**|,|**ISSUP**|\n\n8: **else if Retrieve** == No **then**\n9: _Mgen_ predicts _yt_ given _x_ _\u25b7_ Generate\n10: _Mgen_ predicts **ISUSE** given _x, yt_ _\u25b7_ Critique\n\n\n**Inference overview.** Figure 1 and Algorithm 1 present an overview of SELF-RAG at inference. For\nevery _x_ and preceding generation _y<t_, the model decodes a retrieval token to evaluate the utility\n\n- f retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\nstandard LM. If retrieval is", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_4050", "chunk_text": "_ and preceding generation _y<t_, the model decodes a retrieval token to evaluate the utility\n\n- f retrieval. If retrieval is not required, the model predicts the next output segment, as it does in a\nstandard LM. If retrieval is needed, the model generates: a critique token to evaluate the retrieved\npassage\u2019s relevance, the next response segment, and a critique token to evaluate if the information in\nthe response segment is supported by the passage. Finally, a new critique token evaluates the overall\nutility of the response. [4] To generate each segment, SELF-RAG processes multiple passages in parallel\nand uses its own generated reflection tokens to enforce soft constraints (Section 3.3) or hard control\n(Algorithm 1) over the generated task output. For instance, in Figure 1 (right), the retrieved passages\n_d_ 1 is selected at the first time step since _d_ 2 does not provide direct evidence ( **ISREL** is Irrelevant)\nand _d_ 3 output is only partially supported while _d_ 1 are fully supported.\n\n\n**Training overview.** SELF-RAG enables an arbitrary LM to generate text with reflection tokens\nby unifying them as next token predictions from the expanded model vocabulary (i.e., the original\nvocabulary plus reflection tokens). Specifically, we train the generator model _M_ - n a curated corpus\nwith interleaving passages retrieved by a _retriever R_ and reflection tokens predicted by a _critic_ model\n_C_ (summarized in Appendix Algorithm 2). We train _C_ to generate reflection tokens for evaluating\nretrieved passages and the quality of a given task output (Section 3.2.1). Using the critic model, we\nupdate the training corpus by inserting reflection tokens into task outputs offline. Subsequently, we\ntrain the final generator model ( _M_ ) using the conventional LM objective (Section 3.2.2) to enable\n_M_ to generate reflection tokens by itself without relying on the critic at inference time.\n\n\n3.2 SELF-RAG TRAINING\n\n\nHere, we describe the supervised data collection and training of two models, the critic _C_ (Section 3.2.1)\nand the generator _M_ (Section 3.2.2).\n\n\n3.2.1 TRAINING THE CRITIC MODEL\n\n\n**Data collection for critic model.** Manual annotation", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_4500", "chunk_text": " _C_ (Section 3.2.1)\nand the generator _M_ (Section 3.2.2).\n\n\n3.2.1 TRAINING THE CRITIC MODEL\n\n\n**Data collection for critic model.** Manual annotation of reflection tokens for each segment is\nexpensive (Wu et al., 2023). A state-of-the-art LLM like GPT-4 (OpenAI, 2023) can be effectively\n\n\n4We follow Liu et al. (2023a) in using a \u201cperceived\u201d utility value that is independent of retrieved passages.\n\n\n4\n\n\nPreprint.\n\n\n**Input:** Write an essay of your best summer vacation **Input:** How did US states get their names?\n\n\n\n**Output:** My best summer vacation was a magical escape\nto the coastal town of Santorini. The azure waters,\ncharming white-washed building are unforgettable.\n\n\n\n\n\n\n\n**Augmented Output:** No Retrieval My best summer\n\nvacation was a magical escape to the coastal town of\n\nSantorini.           The azure waters, charming white-No Retrieval\n\nwashed building are unforgettable experience **.** Util: 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: SELF-RAG training examples. The left example does not require retrieval while the right\n\n- ne requires retrieval; thus, passages are inserted. More examples are in Appendix Table 4.\n\n\nused to generate such feedback (Liu et al., 2023b). However, depending on such proprietary LMs\ncan raise API costs and diminish reproducibility (Chen et al., 2023). We create supervised data by\nprompting GPT-4 to generate reflection tokens and then distill their knowledge into an in-house _C_ .\nFor each group of reflection tokens, we randomly sample instances from the original training data:\n_{X_ _[sample]_ _, Y_ _[sample]_ _} \u223c{X, Y }_ . As different reflection token groups have their own definitions and\ninput, as shown in Table 1, we use different instruction prompts for them. Here, we use **Retrieve** as\nan example. We prompt GPT-4 with a type-specific instruction (\u201cGiven an instruction, make a\njudgment on whether finding some external documents from the web helps to generate a better\nresponse.\u201d) followed by few-shot demonstrations _I_ the original task input _x_ and output _y_ to predict\nan appropriate", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_4950", "chunk_text": " instruction, make a\njudgment on whether finding some external documents from the web helps to generate a better\nresponse.\u201d) followed by few-shot demonstrations _I_ the original task input _x_ and output _y_ to predict\nan appropriate reflection token as text: _p_ ( _r|I, x, y_ ). Manual assessment reveals that GPT-4 reflection\ntoken predictions show high agreement with human evaluations. We collect 4k-20k supervised\ntraining data for each type and combine them to form training data for _C_ . Appendix Section D shows\nthe full list of instructions, and A.1 contains more details and our analysis.\n\n\n**Critic learning.** After we collect training data _Dcritic_, we initialize _C_ with a pre-trained LM and\ntrain it on _Dcritic_ using a standard conditional language modeling objective, maximizing likelihood:\n\n\nmax E(( _x,y_ ) _,r_ ) _\u223cDcritic_ log _pC_ ( _r|x, y_ ) _, r_ for reflection tokens. (1)\n_C_\n\n\nThough the initial model can be any pre-trained LM, we use the same one as the generator LM\n(i.e., Llama 2-7B; Touvron et al. 2023) for _C_ initialization. The critic achieves a higher than 90%\nagreement with GPT-4-based predictions on most reflection token categories (Appendix Table 5).\n\n\n3.2.2 TRAINING THE GENERATOR MODEL\n\n\n**Data collection for generator.** Given an input-output pair ( _x, y_ ), we augment the original output\n_y_ using the retrieval and critic models to create supervised data that precisely mimics the SELFRAG inference-time process (Section 3.1). For each segment _yt \u2208_ _y_, we run _C_ to assess whether\nadditional passages could help to enhance generation. If retrieval is required, the retrieval special\ntoken **Retrieve** =Yes is added, and _R_ retrieves the top _K_ passages, **D** . For each passage, _C_ further\nevaluates whether the passage is relevant and predicts **ISREL** . If a passage is relevant, _C_ further\nevaluates whether the passage supports the model generation and predicts **ISSUP** . Critique tokens\n\n**ISREL** and **ISSUP** are appended after", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_5400", "chunk_text": " and predicts **ISREL** . If a passage is relevant, _C_ further\nevaluates whether the passage supports the model generation and predicts **ISSUP** . Critique tokens\n\n**ISREL** and **ISSUP** are appended after the retrieved passage or generations. At the end of the output, _y_\n(or _yT_ ), _C_ predicts the overall utility token **ISUSE**, and an augmented output with reflection tokens\nand the original input pair is added to _Dgen_ . See the example training data in Figure 2.\n\n\n**Generator learning.** We train the generator model _M_ by training on the curated corpus augmented\nwith reflection tokens _Dgen_ using the standard next token objective:\n\n\nmax _M_ [E][(] _[x,y,r]_ [)] _[\u223cD][gen]_ [ log] _[ p][M]_ [(] _[y, r][|][x]_ [)] _[.]_ (2)\n\n\nUnlike _C_ training (Eq. 1), _M_ learns to predict the target output as well as the reflection tokens. During\ntraining, we mask out the retrieved text chunks (surrounded by <p> and </p> in Figure 2) for loss\ncalculation and expand the original vocabulary _V_ with a set of reflection tokens _{_ **Critique** _,_ **Retrieve** _}_ .\n\n\n**Connections to prior work on learning with critique.** Recent work incorporates additional\ncritique (feedback) during training, e.g., RLHF (Ouyang et al. 2022) via PPO. While PPO relies on\n\n\n5\n\n\nPreprint.\n\n\nseparate reward models during training, we compute critique offline and directly insert them into the\ntraining corpus, where the generator LM is trained with a standard LM objective. This significantly\nreduces training costs compared to PPO. Our work also relates to prior work that incorporates special\ntokens to control generation (Keskar et al., 2019; Lu et al., 2022; Korbak et al., 2023). Our SELF-RAG\nlearns to generate special tokens _to evaluate its own prediction_ after each generated segment, enabling\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n\n\n3.3 SELF-RAG INFERENCE\n\n\nGenerating reflection tokens to self-evaluate", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_5850", "chunk_text": " special tokens _to evaluate its own prediction_ after each generated segment, enabling\nthe use of a soft re-ranking mechanism or hard constraints at inference (discussed next).\n\n\n3.3 SELF-RAG INFERENCE\n\n\nGenerating reflection tokens to self-evaluate its own output makes SELF-RAG controllable during the\ninference phase, enabling it to tailor its behavior to diverse task requirements. For tasks demanding\nfactual accuracy (Min et al., 2023), we aim for the model to retrieve passages more frequently to\nensure that the output aligns closely with the available evidence. Conversely, in more open-ended\ntasks, like composing a personal experience essay, the emphasis shifts towards retrieving less and\nprioritizing the overall creativity or utility score. In this section, we describe approaches to enforce\ncontrol to meet these distinct objectives during the inference process.\n\n\n**Adaptive retrieval with threshold.** SELF-RAG dynamically decides when to retrieve text passages by\npredicting **Retrieve** . Alternatively, our framework allows a threshold to be set. Specifically, if the probability of generating the **Retrieve** =Yes token normalized over all output tokens in **Retrieve** surpasses a\ndesignated threshold, we trigger retrieval (details in Appendix Section A.3).\n\n\n**Tree-decoding with critique tokens.** At each segment step _t_, when retrieval is required, based either\n\n- n hard or soft conditions, _R_ retrieves _K_ passages, and the generator _M_ processes each passage in\nparallel and outputs _K_ different continuation candidates. We conduct a segment-level beam search\n(with the beam size= _B_ ) to obtain the top- _B_ segment continuations at each timestamp _t_, and return\nthe best sequence at the end of generation. The score of each segment _yt_ with respect to passage _d_ is\nupdated with a critic score _S_ that is the linear weighted sum of the normalized probability of each\n\n**Critique** token type. For each critique token group _G_ (e.g., **ISREL** ), we denote its score at timestamp\n_t_ as _s_ _[G]_ _t_ [, and we compute a segment score as follows:]\n\n\n_f_ ( _yt, d,_ **Critique** ) = _p_ ( _yt|x, d, y<t_ )) + _S_ ( **Critique** ) _,_ where (3)\n\n\n_S_ (", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_6300", "chunk_text": " follows:]\n\n\n_f_ ( _yt, d,_ **Critique** ) = _p_ ( _yt|x, d, y<t_ )) + _S_ ( **Critique** ) _,_ where (3)\n\n\n_S_ ( **Critique** ) =        - _w_ _[G]_ _s_ _[G]_ _t_ [for] _[ G]_ [ =] _[ {]_ **[ I][S][R][EL]** _[,]_ **[ I][S][S][UP]** _[,]_ **[ I][S][U][SE]** _[ }][,]_ (4)\n\n_G\u2208G_\n\n\n_pt_ ( _r_ \u02c6)\nwhere _s_ _[G]_ _t_ [=] ~~\ufffd~~ _NGi_ =1 _[p][t]_ [(] _[r][i]_ [)] [stands for the generation probability of the most desirable reflection token]\n\n_r_ \u02c6 (e.g., **ISREL** =Relevant) for the critique token type _G_ with _N_ _[G]_ distinct tokens (that represent\ndifferent possible values for _G_ ). The weights _w_ _[G]_ in Eq. 4 are hyperparameters that can be adjusted\nat inference time to enable customized behaviors at test time. For instance, to ensure that result\n_y_ is mostly supported by evidence, we can set a weight term for the **ISSUP** score higher, while\nrelatively lowering weights for other aspects. Alternatively, we could further enforce hard constraints\nduring decoding using **Critique** . Instead of using a soft reward function in Eq. 4, we could explicitly\nfilter out a segment continuation when the model generates an undesirable **Critique** token (e.g.,\n\n**ISSUP** =No support) . Balancing the trade-off between multiple preferences has been studied\nin RLHF (Touvron et al., 2023; Wu et al., 2023), which often requires training to change models\u2019\nbehaviors. SELF-RAG tailors an LM with no additional training.\n\n\n4 EXPERIMENTS\n\n\n4.1 TASKS AND DATASETS\n\n\nWe conduct evaluations of our SELF-RAG and diverse baselines on a range of downstream tasks,\nholistically evaluating outputs with metrics designed to assess overall correctness,", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_6750", "chunk_text": " no additional training.\n\n\n4 EXPERIMENTS\n\n\n4.1 TASKS AND DATASETS\n\n\nWe conduct evaluations of our SELF-RAG and diverse baselines on a range of downstream tasks,\nholistically evaluating outputs with metrics designed to assess overall correctness, factuality, and\nfluency. Throughout these experiments, we conduct zero-shot evaluations, where we provide instructions describing tasks without few-shot demonstrations (Wei et al., 2022; Sanh et al., 2022). Details of\n\n- ur experiments\u2019 settings, including test-time instructions, are available in the Appendix Section B.1.\n\n\n**Closed-set tasks** include two datasets, i.e., a fact _verification dataset_ about public health ( **PubHealth** ;\nZhang et al. 2023) and a _multiple-choice reasoning dataset_ created from scientific exams ( **ARC-**\n\n\n6\n\n\nPreprint.\n\n\n**Challenge** ; Clark et al. 2018). We use accuracy as an evaluation metric and report on the test set. We\naggregate the answer probabilities of target classes for both of these datasets (Appendix Section B.2).\n\n\n**Short-form generations tasks** include two open-domain question answering (QA) datasets,\nPopQA (Mallen et al., 2023) and TriviaQA-unfiltered (Joshi et al., 2017), where systems need\nto answer arbitrary questions about factual knowledge. For PopQA, we use the long-tail subset,\nconsisting of 1,399 rare entity queries whose monthly Wikipedia page views are less than 100. As the\nTriviaQA-unfiltered (open) test set is not publicly available, we follow prior work\u2019s validation and\ntest split (Min et al., 2019; Guu et al., 2020), using 11,313 test queries for evaluation. We evaluate\nperformance based on whether gold answers are included in the model generations instead of strictly\nrequiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\n\n\n**Long-form generation tasks** include a biography generation task (Min et al., 2023) and a long-form\nQA task **ALCE-ASQA** Gao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al.,\n2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_7200", "chunk_text": "ao et al. (2023); Stelmakh et al. (2022). We use FactScore (Min et al.,\n2023) to evaluate biographies, and we use official metrics of correctness (str-em), fluency based on\nMAUVE (Pillutla et al., 2021), and citation precision and recall (Gao et al., 2023) for ASQA. [5]\n\n\n4.2 BASELINES\n\n\n**Baselines without retrievals.** We evaluate strong publicly available pre-trained LLMs,\nLlama27B _,_ 13B (Touvron et al., 2023), instruction-tuned models, Alpaca7B _,_ 13B (Dubois et al., 2023)\n(our replication based on Llama2); and models trained and reinforced using private data, ChatGPT (Ouyang et al., 2022) and Llama2-chat13B. For instruction-tuned LMs, we use the official\nsystem prompt or instruction format used during training if publicly available. We also compare our\nmethod to concurrent work, CoVE65B (Dhuliawala et al., 2023), which introduces iterative prompt\nengineering to improve the factuality of LLM generations.\n\n\n**Baselines with retrievals.** We evaluate models augmented with retrieval at test time or during training.\nThe first category includes standard RAG baselines, where an LM (Llama2, Alpaca) generates output\ngiven the query prepended with the top retrieved documents using the same retriever as in our system.\nIt also includes Llama2-FT, where Llama2 is fine-tuned on all training data we use without the\nreflection tokens or retrieved passages. We also report the result of retrieval-augmented baselines\nwith LMs trained with private data: Ret-ChatGPT and Ret-Llama2-chat, which deploy the same\naugmentation technique above, as well as perplexity.ai, an InstructGPT-based production search\nsystem. The second category includes concurrent methods that are trained with retrieved text\npassages, i.e., SAIL (Luo et al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning\ndata with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023)\nto pre-train an LM with", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_7650", "chunk_text": " al., 2023) to instruction-tune an LM on the Alpaca instruction-tuning\ndata with top retrieved documents inserted before instructions, and Toolformer (Schick et al., 2023)\nto pre-train an LM with API calls (e.g., Wikipedia APIs). [6]\n\n\n4.3 EXPERIMENTAL SETTINGS\n\n\n**Training data and settings.** Our training data consists of diverse instruction-following input-output\npairs. In particular, we sample instances from Open-Instruct processed data (Wang et al., 2023) and\nknowledge-intensive datasets (Petroni et al., 2021; Stelmakh et al., 2022; Mihaylov et al., 2018). In\ntotal, we use 150k instruction-output pairs. We use Llama2 7B and 13B (Touvron et al., 2023) as\n\n- ur generator base LM, and we use Llama2 7B as our base critic LM. For the retriever model _R_, we\nuse off-the-shelf Contriever-MS MARCO (Izacard et al., 2022a) by default and retrieve up to ten\ndocuments for each input. More training details are in the Appendix Section B.1.\n\n\n**Inference settings.** As a default configuration, we assign the weight terms **ISREL**, **ISSUP**, **ISUSE**\nvalues of 1.0, 1.0 and 0.5, respectively. To encourage frequent retrieval, we set the retrieval threshold\nto 0.2 for most tasks and to 0 for ALCE (Gao et al., 2023) due to citation requirements. We speed\nup inference using vllm (Kwon et al., 2023). At each segment level, we adopt a beam width of 2.\nFor a token-level generation, we use greedy decoding. By default, we use the top five documents\nfrom Contriever-MS MARCO (Izacard et al., 2022a); for biographies and open-domain QA, we\nuse additional top five documents retrieved by a web search engine, following Luo et al. (2023);\nfor ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all\nbaselines for a fair comparison.\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_8100", "chunk_text": " engine, following Luo et al. (2023);\nfor ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all\nbaselines for a fair comparison.\n\n\n[5https://github.com/princeton-nlp/ALCE](https://github.com/princeton-nlp/ALCE)\n6We report numbers using the results reported in the paper as the implementations are not available.\n\n\n7\n\n\nPreprint.\n\n\nTable 2: Overall experiment results on six tasks. **Bold** numbers indicate the best performance among\nnon-proprietary models, and **gray-colored** bold text indicates the best proprietary model when\nthey outperforms all non-proprietary models. _[\u2217]_ indicates concurrent or recent results reported by\nconcurrent work. \u2013 indicates numbers that are not reported by the original papers or are not applicable.\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n\n\nShort-form Closed-set Long-form generations (with citations)\nPopQA TQA Pub ARC Bio ASQA\nLM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\n\n\n_LMs with proprietary data_\nLlama2-c13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6  -  Ret-Llama2-c13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\nChatGPT 29.3 **74.3** 70.1 **75.3** 71.8 35.3 36.2 68.8  -  \nRet-ChatGPT 50.8 65.7 54.7 **75.3**  - **40.7** **39.9** **79.7** 65.1 **76.6**\nPerplexity.ai  -  -  -  - 71.2  -  -  -  -  \n\n_Baselines without retrieval_\n\n", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_8550", "chunk_text": " **39.9** **79.7** 65.1 **76.6**\nPerplexity.ai  -  -  -  - 71.2  -  -  -  -  \n\n_Baselines without retrieval_\n\nLlama27B 14.7 30.5 34.2 21.8 44.5 7.9 15.3 19.0  -  Alpaca7B 23.6 54.5 49.8 45.0 45.8 18.8 29.4 61.7  -  Llama213B 14.7 38.5 29.4 29.4 53.4 7.2 12.4 16.0  -  Alpaca13B 24.4 61.3 55.5 54.9 50.2 22.9 32.0 70.6  -  CoVE65B *  -  -  -  - 71.2  -  -  -  -  \n\n_Baselines with retrieval_\n\nToolformer*6B  - 48.8  -  -  -  -  -  -  -  Llama27B 38.2 42.5 30.0 48.0 78.0 15.2 22.1 32.0 2.9 4.0\nAlpaca7B 46.7 64.1 40.2 48.0 76.6 30.9 33.3 57.9 5.5 7.2\nLlama2-FT7B 48.7 57.3 64.3 65.8 78.2 31.0 35.8 51.2 5.0 7.5\nSAIL*7B  -  - 69.2 48.4  -  -  -  -  -  Llama213B 45.7 47.0 30.2 26.0 77.5 16.3 20.5 24.7 2.3 3.6\nAlpaca13B 46.1 66.9 51.1 57.6 ", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_9000", "chunk_text": "26.0 77.5 16.3 20.5 24.7 2.3 3.6\nAlpaca13B 46.1 66.9 51.1 57.6 77.7 **34.8** 36.7 56.6 2.0 3.8\n**Our** SELF-RAG 7B 54.9 66.4 72.4 67.3 **81.2** 30.0 35.7 **74.3** 66.9 67.8\n**Our** SELF-RAG 13B **55.8** **69.3** **74.5** **73.1** 80.2 31.7 **37.0** 71.6 **70.3** **71.3**\n\n\n5 RESULTS AND ANALYSIS\n\n\n5.1 MAIN RESULTS\n\n\n**Comparison against baselines without retrieval.** Table 2 (top) presents the baselines without\nretrieval. Our SELF-RAG (bottom two rows) demonstrates a substantial performance advantage\n\n- ver supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\niteratively prompts Llama265B to refine output.\n\n\n**Comparison against baselines with retrieval.** As shown in Tables 2 (bottom), our SELF-RAG also\n\n- utperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\nLM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\npowerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\ntheir non-retrieval baselines. However, we found that these baselines provide limited solutions for\ntasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\nand ARC-Challenge, baselines with retrieval do not improve performance notably from their noretrie", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_9450", "chunk_text": ", we found that these baselines provide limited solutions for\ntasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\nand ARC-Challenge, baselines with retrieval do not improve performance notably from their noretrieval counterparts. We also observe that most baselines with retrieval struggle to improve citation\naccuracy. On ASQA, our model shows significantly higher citation precision and recall than all\nmodels except ChatGPT. Gao et al. (2023) found that ChatGPT consistently exhibits superior efficacy\nin this particular task, surpassing smaller LMs. Our SELF-RAG bridges this performance gap, even\n\n- utperforming ChatGPT in citation precision, which measures whether the model-generated claim is\nfully supported by cited evidence. We also found that on the metrics for factual precision, SELF-RAG\n7B occasionally outperforms our 13B due to the tendency of smaller SELF-RAG to often generate\n\n\n8\n\n\nPreprint.\n\n\n\nPQA Med AS\n\n\n_Training_\nNo Retriever _R_ 43.6 67.8 31.0\n\nNo Critic _C_ 42.6 72.0 18.1\n\n_Test_\n\nNo retrieval 24.7 73.0 \nHard constraints 28.3 72.6 Retrieve top1 41.8 73.1 28.6\nRemove **ISSUP** 44.1 73.2 30.6\n\n\n(a) Ablation\n\n\n\n\n\n(b) Customization\n\n\n\n\n\n(c) Retrieval\n\n\n\n\n\nFigure 3: **Analysis on SELF-RAG:** (a) **Ablation studies** for key components of SELF-RAG training\nand inference based on our 7B model. (b) **Effects of soft weights** - n ASQA citation precision and\nMauve (fluency). (c) **Retrieval frequency** and _normalized_ accuracy on PubHealth and PopQA.\n\n\nprecisely grounded yet shorter outputs. Llama2-FT7B, which is the baseline LM trained on the same\ninstruction-output pairs as SELF-RAG without retrieval or self-reflection and is retrieval-augmented\nat test time only, lags behind SELF-RAG. This result indicates SELF-RAG gains are not solely from\ntraining data and demonstrate the effectiveness of SELF-RAG framework.\n\n\n5.2 ANALYSIS\n\n\n**Ablation studies.**", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_9900", "chunk_text": "\nat test time only, lags behind SELF-RAG. This result indicates SELF-RAG gains are not solely from\ntraining data and demonstrate the effectiveness of SELF-RAG framework.\n\n\n5.2 ANALYSIS\n\n\n**Ablation studies.** We conduct a set of ablations of our framework to identify which factors play\nkey roles. We evaluate two model variants trained differently than our model: _No Retriever_ trains an\nLM using the standard instruction-following method given instruction-output pairs, without retrieved\npassages; _No Critic_ trains an LM trained with input-output pairs that are always augmented with the\ntop one retrieved document without reflection tokens. This is similar to SAIL (Luo et al., 2023), and\nwe use our instruction-output data instead of using the Alpaca dataset (Dubois et al., 2023), as in\nSAIL. We also conduct ablation on our inference-time algorithm, including _No retrieval_ disables\nretrieval during inference; _Hard constraints_ indicates the model performance that retrieves when\n\n**Retrieve** =Yes instead of using the adaptive threshold; _Retrieve top 1_ always retrieves and uses the\ntop one document only, similar to standard RAG approaches; _Remove_ **ISSUP** indicates the model\nperformance that removes **ISSUP** score only during critique-guided beam search in Eq. 4. In this\nablation experiment, we use a training instance size of 50k for a more efficient exploration of training\nvariations. Later in this section, we conduct an analysis of the effect of training data size. We conduct\nthe ablation studies on three datasets, PopQA, PubHealth, and ASQA. On ASQA, we evaluate models\n\n- n sampled 150 instances and exclude ablations involving adaptive or no retrieval processes.\n\n\nWe show in Table 3a the ablation results. The top part of the table shows results for training ablations,\nand the bottom part is for inference ablations. We see that all components play important roles. We\nalso observe a large performance gap between SELF-RAG and No Retriever or Critic baselines across\ntasks, indicating that training an LM with those models largely contributes to the performance gain of\nSELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\nRAG approaches causes a large drop in PopQA and ASQA", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_10350", "chunk_text": " that training an LM with those models largely contributes to the performance gain of\nSELF-RAG. Using the top passages regardless of their relevance (Retrieve top 1) as in conventional\nRAG approaches causes a large drop in PopQA and ASQA, and removing **ISSUP** during the beam\nsearch results hurts performance on ASQA. This demonstrates the effectiveness of SELF-RAG\u2019s\ncapabilities of carefully selecting generations based fine-grained multiple criterion, instead of naively\nusing all of the top passages from the retrieval model or solely depending on relevance scores.\n\n\n**Effects of inference-time customization.** One key benefit of our proposed framework is that it\nenables us to control how much each critique type affects the final generation sampling. We analyze\nthe effects of different parameter weights on the top of our 7B model during inference time on\nASQA, where multiple evaluation aspects are considered. Figure 3b shows the effects of changing\nthe weighting term for **ISSUP**, which criticizes how supported the output is by the text passage. As\nthe figure shows, increasing the weight leads to positive effects on the models\u2019 citation precision\nsince this puts more emphasis on whether model generation is supported by the evidence. On the\n\n\n9\n\n\nPreprint.\n\n\n(a) PopQA\n\n\n\n\n\n\n\n|73<br>72|Col2|\n|---|---|\n|71<br>|71<br>|\n|0<br>100<br>Num of training (k)|0<br>100<br>Num of training (k)|\n\n\n(b) PubHealth\n\n\n\n(c) ASQA (prec)\n\n\n\nPop Bio.\n\n\nS & P 92.5 70.0\n\n**ISREL** 95.0 90.0\n\n**ISSUP** 90.0 85.0\n\n\n(d) Human evaluation on PopQA\nand Bio generation.\n\n\n\nFigure 4: **Training scale and Human analysis:** (a) (b) (c) **Training scale analysis** shows the effect\n\n- f the training data scale on PopQA, PubHealth and ASQA (citation precision), respectively. (d)\n**Human analysis** - n SELF-RAG outputs as well as reflection tokens.\n\n\ncontrary, a larger weight results in lower MAUVE scores: when generation gets longer and more\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\nby Liu et al. (2023a). Our framework lets practitioners choose and customize", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_10800", "chunk_text": " lower MAUVE scores: when generation gets longer and more\nfluent, there are often more claims that are not fully supported by citations, consistent with findings\nby Liu et al. (2023a). Our framework lets practitioners choose and customize models\u2019 behaviors at\ntest time by adjusting such parameters without requiring additional training.\n\n\n**Efficiency and accuracy trade-off.** Using our framework, practitioners can adjust how often retrieval\n\n- ccurs using the token probability of reward tokens. We evaluate how this adaptive threshold affects\n\n- verall accuracy and frequency of retrieval, and we evaluate the performance with varying numbers\n\n- f threshold _\u03b4_ (larger _\u03b4_ results in less retrieval) on PubHealth and PopQA. Figure 3c shows that\nthe model\u2019s retrieval frequencies dramatically change on both datasets. as _\u03b4_ varies. On one hand,\nperformance deterioration by retrieving less is smaller on PubHealth but larger in PopQA.\n\n\n**Effects of training data size.** We conduct an analysis of how the data scale affects the model\u2019s\nperformance. In particular, we randomly sample 5k, 10k, 20k, and 50k instances from our original\n150k training instances, and fine-tune four SELF-RAG 7B variants on those subsets. Then, we compare\nthe model performance on PopQA, PubHealth, and ASQA (citation precision) with our final SELFRAG trained on the full 150k instances. We also evaluate Figures 4a, 4b and 4c shows the models\u2019\nperformance trained on different amount of data. Across all datasets, increasing data size often shows\nupward trajectories and the improvements are significantly larger in PopQA and ASQA, while we do\nnot observed such significant improvements on Llama2-FT7B when increasing the training data from\n50k to 150k. These results also indicate that further expanding the training data of SELF-RAG may\nlead to further improvements, although in this work we limit our training data size to 150k.\n\n\n**Human evaluations.** We conduct small human evaluations on SELF-RAG outputs, as well as the\nreliability of predicted reflection tokens. In particular, we sampled 50 samples from PopQA and Bio\nresults. Following Menick et al. (2022), human annotators evaluate _S&P_, which indicates whether\nthe model output is plausible (i.e., the output is", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_11250", "chunk_text": " In particular, we sampled 50 samples from PopQA and Bio\nresults. Following Menick et al. (2022), human annotators evaluate _S&P_, which indicates whether\nthe model output is plausible (i.e., the output is a reasonable and on-topic response to the question\nas if it were occurring in a conversation) and supported (i.e., the provided evidence is sufficient to\nverify the validity of the answer). For S&P, we do not consider the instances where SELF-RAG\npredicts irrelevant or no support. We then ask our annotators whether the model-predicted\nreflection tokens about **ISREL** and **ISSUP** match their inspections (e.g., whether the _fully supported_\n\n- utput is supported by the cited evidence). Human annotators find SELF-RAG answers are often\nplausible and supported by relevant passages with higher S&P scores on short-form PopQA, which is\nconsistent with Menick et al. (2022). Human annotators also find **ISREL** and **ISSUP** reflection token\npredictions are mostly aligned with their assessments. Appendix Table 6 shows several annotated\nexamples and explanations on assessments.\n\n\n6 CONCLUSION\n\n\nThis work introduces SELF-RAG, a new framework to enhance the quality and factuality of LLMs\nthrough retrieval on demand and self-reflection. SELF-RAG trains an LM to learn to retrieve, generate,\nand critique text passages and its own generation by predicting the next tokens from its original\nvocabulary as well as newly added special tokens, called reflection tokens. SELF-RAG further enables\nthe tailoring of LM behaviors at test time by leveraging reflection tokens. Our holistic evaluations on\nsix tasks using multiple metrics demonstrate that SELF-RAG significantly outperforms LLMs with\nmore parameters or with conventional retrieval-augmented generation approaches.\n\n\n10\n\n\nPreprint.\n\n\nETHICAL CONCERNS\n\n\nThis work aims to improve the factuality of LLM outputs, the lack of which continues to cause numerous real-world problems (e.g., spread of misinformation and provision of incorrect and dangerous\nadvice). While our method shows significant improvements in terms of performance, factuality, and\ncitation accuracy, it can still generate outputs that are not fully supported by the citations. We hope\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\nmodel outputs.\n\n\nACKNOWLEDGMENTS\n\n\nWe thank Sewon Min, Scott Wen", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_11700", "chunk_text": " still generate outputs that are not fully supported by the citations. We hope\nthat explicit self-reflection and fine-grained attribution may help users verify factual errors in the\nmodel outputs.\n\n\nACKNOWLEDGMENTS\n\n\nWe thank Sewon Min, Scott Wen-tau Yih, Sean Welleck, and Kawin Ethayarajh for fruitful discussions\nin the early stages of this work. We thank Sewon Min, Joongwon (Daniel) Kim, and Sandy Kaplan\nfor valuable feedback on the paper, and Tianyu Gao and Weijia Shi for their help on evaluations.\nAkari Asai is supported by the IBM Fellowship. We thank Stability AI for providing computing\nto train and evaluate the LMs in this work, and Microsoft Accelerate Foundation Models Research\nProgram for the access to OpenAI APIs. This work was funded in part by the DARPA MCS program\nthrough NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and gifts from AI2.\n\n\nREFERENCES\n\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning to retrieve reasoning paths over wikipedia graph for question answering. In _International_\n_Conference on Learning Representations_ [, 2020. URL https://openreview.net/forum?](https://openreview.net/forum?id=SJgVHkrYDH)\n[id=SJgVHkrYDH.](https://openreview.net/forum?id=SJgVHkrYDH)\n\n\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. Retrieval-based language models and applications. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics_\n_(Tutorial)_ [, 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.](https://aclanthology.org/2023.acl-tutorials.6)\n\n\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh\nHajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. In _Findings of the Associ-_\n_ation for Computational Linguistics_ [, 2023b. URL https://aclanthology.org/2023.](https://", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_12150", "chunk_text": ", and Wen-tau Yih. Task-aware retrieval with instructions. In _Findings of the Associ-_\n_ation for Computational Linguistics_ [, 2023b. URL https://aclanthology.org/2023.](https://aclanthology.org/2023.findings-acl.225)\n[findings-acl.225.](https://aclanthology.org/2023.findings-acl.225)\n\n\nBernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni, Daniel Andor, Livio Baldini Soares, Jacob\nEisenstein, Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al. Attributed question answering:\nEvaluation and modeling for attributed large language models. _arXiv preprint arXiv:2212.08037_,\n[2022. URL https://arxiv.org/abs/2212.08037.](https://arxiv.org/abs/2212.08037)\n\n\nLingjiao Chen, Matei Zaharia, and James Zou. How is chatgpt\u2019s behavior changing over time? _arXiv_\n_preprint arXiv:2307.09009_ [, 2023. URL https://arxiv.org/abs/2307.09009.](https://arxiv.org/abs/2307.09009)\n\n\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\n_arXiv preprint arXiv:1803.05457_ [, 2018. URL https://arxiv.org/abs/1803.05457.](https://arxiv.org/abs/1803.05457)\n\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-\u00b4\nefficient exact attention with io-awareness. In _Advances in Neural Information Processing Systems_,\n[2022. URL https://openreview.net/forum?id=H4DqfPSibmx.](https://openreview.net/forum?id=H4DqfPSibmx)\n\n\nShehzaad Dhuliawala, Mojtaba Komeili, Jing", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_12600", "chunk_text": "openreview.net/forum?id=H4DqfPSibmx.](https://openreview.net/forum?id=H4DqfPSibmx)\n\n\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. Chain-of-verification reduces hallucination in large language models. _arXiv preprint_\n_arXiv:2309.11495_ [, 2023. URL https://arxiv.org/abs/2309.11495.](https://arxiv.org/abs/2309.11495)\n\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of\nwikipedia: Knowledge-powered conversational agents. In _International Conference on Learning_\n_Representations_ [, 2019. URL https://openreview.net/forum?id=r1l73iRqKm.](https://openreview.net/forum?id=r1l73iRqKm)\n\n\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that\n\n\n11\n\n\nPreprint.\n\n\nlearn from human feedback. _arXiv preprint arXiv:2305.14387_ [, 2023. URL https://arxiv.](https://arxiv.org/abs/2305.14387)\n\n[org/abs/2305.14387.](https://arxiv.org/abs/2305.14387)\n\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\ntext with citations. _arXiv preprint arXiv:2305.14627_ [, 2023. URL https://arxiv.org/abs/](https://arxiv.org/abs/2305.14627)\n[2305.14627.](https://arxiv.org/abs/2305.14627)\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented\nlanguage model pre-training", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_13500", "chunk_text": "2305.06983.](https://arxiv.org/abs/2305.06983)\n\n\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In _Proceedings of the 55th Annual_\n_Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2017. URL\n[https://aclanthology.org/P17-1147.](https://aclanthology.org/P17-1147)\n\n\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.\nCtrl: A conditional transformer language model for controllable generation. _arXiv preprint_\n_arXiv:1909.05858_ [, 2019. URL https://arxiv.org/abs/1909.05858.](https://arxiv.org/abs/1909.05858)\n\n\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher Buckley, Jason\nPhang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences.\nIn _International Conference on Machine Learning_ [, 2023. URL https://openreview.net/](https://openreview.net/forum?id=AT8Iw8KOeC)\n[forum?id=AT8Iw8KOeC.](https://openreview.net/forum?id=AT8Iw8KOeC)\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and\nSlav Petrov. Natural questions: A benchmark for question answering research. _Transactions of_\n_the Association for Computational Linguistics_ [, 2019. URL https://aclanthology.org/](https://aclanthology.org/Q19-1026)\n[Q19-1026.](https://aclanthology.org/Q19-1026)\n\n\nWoosuk Kwon, Zhuohan Li,", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_13950", "chunk_text": " https://aclanthology.org/](https://aclanthology.org/Q19-1026)\n[Q19-1026.](https://aclanthology.org/Q19-1026)\n\n\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating_\n_Systems Principles_ [, 2023. URL https://arxiv.org/abs/2309.06180.](https://arxiv.org/abs/2309.06180)\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8 aschel, Sebastian Riedel, and Douwe Kiela.\u00a8\nRetrieval-augmented generation for knowledge-intensive nlp tasks. In _Advances in Neural Infor-_\n_mation Processing Systems_ [, 2020. URL https://proceedings.neurips.cc/paper/](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)\n[2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)\n\n\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval[augmented dual instruction tuning, 2023. URL https://arxiv.org/abs/2310.01352.](https://arxiv.org/abs/2310.01352)\n\n\nNelson F Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines.\n_arXiv preprint", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_14400", "chunk_text": "0.01352.](https://arxiv.org/abs/2310.01352)\n\n\nNelson F Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search engines.\n_arXiv preprint arXiv:2304.09848_ [, 2023a. URL https://arxiv.org/abs/2304.09848.](https://arxiv.org/abs/2304.09848)\n\n\n12\n\n\nPreprint.\n\n\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment. _arXiv preprint arXiv:2303.16634_, 2023b.\n[URL https://arxiv.org/abs/2303.16634.](https://arxiv.org/abs/2303.16634)\n\n\nXiming Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. QUARK: Controllable text generation with reinforced unlearning.\nIn _Advances in Neural Information Processing Systems_ [, 2022. URL https://openreview.](https://openreview.net/forum?id=5HaIds3ux5O)\n[net/forum?id=5HaIds3ux5O.](https://openreview.net/forum?id=5HaIds3ux5O)\n\n\nHongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox,\nHelen Meng, and James Glass. Sail: Search-augmented instruction learning. _arXiv preprint_\n_arXiv:2305.15225_ [, 2023. URL https://arxiv.org/abs/2305.15225.](https://arxiv.org/abs/2305.15225)\n\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck,", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_15300", "chunk_text": ".org/D18-1260)\n\n[org/D18-1260.](https://aclanthology.org/D18-1260)\n\n\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. A discrete hard EM approach\nfor weakly supervised question answering. In _Proceedings of the 2019 Conference on Empirical_\n_Methods in Natural Language Processing and the 9th International Joint Conference on Natu-_\n_ral Language Processing (EMNLP-IJCNLP)_ [, 2019. URL https://aclanthology.org/](https://aclanthology.org/D19-1284)\n[D19-1284.](https://aclanthology.org/D19-1284)\n\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation. _arXiv preprint arXiv:2305.14251_ [, 2023. URL https:](https://arxiv.org/abs/2305.14251)\n[//arxiv.org/abs/2305.14251.](https://arxiv.org/abs/2305.14251)\n\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. _arXiv preprint arXiv:2112.09332_ [, 2021. URL https:](https://arxiv.org/abs/2112.09332)\n[//arxiv.org/abs/2112.09332.](https://arxiv.org/abs/2112.09332)\n\n\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao,\nYi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\nretrievers. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_16200", "chunk_text": "iktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktaschel,\u00a8\nand Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In _Proceedings_\n\n_of the 2021 Conference of the North American Chapter of the Association for Computational_\n_Linguistics: Human Language Technologies_ [, 2021. URL https://aclanthology.org/](https://aclanthology.org/2021.naacl-main.200)\n[2021.naacl-main.200.](https://aclanthology.org/2021.naacl-main.200)\n\n\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\nand Zaid Harchaoui. MAUVE: Measuring the gap between neural text and human text using\ndivergence frontiers. In _Advances in Neural Information Processing Systems_ [, 2021. URL https:](https://openreview.net/forum?id=Tqx7nJp7PR)\n[//openreview.net/forum?id=Tqx7nJp7PR.](https://openreview.net/forum?id=Tqx7nJp7PR)\n\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. In _Proceedings of the International Conference for High_\n_Performance Computing, Networking, Storage and Analysis_ [, 2020. URL https://dl.acm.](https://dl.acm.org/doi/10.5555/3433701.3433727)\n\n[org/doi/10.5555/3433701.3433727.](https://dl.acm.org/doi/10.5555/3433701.3433727)\n\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language models. _Transactions of the Association_\n_for Computational Linguistics_ [, 2023. URL https://", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_16650", "chunk_text": "ay, Amnon Shashua, Kevin Leyton-Brown, and\nYoav Shoham. In-context retrieval-augmented language models. _Transactions of the Association_\n_for Computational Linguistics_ [, 2023. URL https://arxiv.org/abs/2302.00083.](https://arxiv.org/abs/2302.00083)\n\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,\nStella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training\nenables zero-shot task generalization. In _International Conference on Learning Representations_,\n[2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.](https://openreview.net/forum?id=9Vrb9D0WI4)\n\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. _arXiv preprint arXiv:2302.04761_ [, 2023. URL https://arxiv.org/abs/2302.](https://arxiv.org/abs/2302.04761)\n[04761.](https://arxiv.org/abs/2302.04761)\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klim", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_17100", "chunk_text": "2302.04761)\n[04761.](https://arxiv.org/abs/2302.04761)\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\n\n - ptimization algorithms. _arXiv preprint arXiv:1707.06347_ [, 2017. URL https://arxiv.org/](https://arxiv.org/abs/1707.06347)\n[abs/1707.06347.](https://arxiv.org/abs/1707.06347)\n\n\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H. Chi, Nathanael\nScharli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\u00a8\nIn _Proceedings of the 40th International Conference on Machine Learning_ [, 2023. URL https:](https://proceedings.mlr.press/v202/shi23a.html)\n[//proceedings.mlr.press/v202/shi23a.html.](https://proceedings.mlr.press/v202/shi23a.html)\n\n\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet longform answers. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language_\n_Processing_ [, 2022. URL https://aclanthology.org/2022.emnlp-main.566.](https://aclanthology.org/2022.emnlp-main.566)\n\n\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a largescale dataset for fact extraction and VERification. In _Proceedings of the 2018 Conference of the_\n_North American Chapter of the Association for Computational Linguistics: Human Language Tech-_\n_nologies, Volume 1 (Long Papers)_ [, 2018. URL https://aclanthology.org/N18-1074.](https://aclanthology.org/N18-1074)\n\n\n14\n\n\nPreprint.\n\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov,", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_17550", "chunk_text": "aclanthology.org/N18-1074)\n\n\n14\n\n\nPreprint.\n\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. _arXiv preprint arXiv:2307.09288_ [, 2023. URL https://arxiv.](https://arxiv.org/abs/2307.09288)\n\n[org/abs/2307.09288.](https://arxiv.org/abs/2307.09288)\n\n\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go?\nexploring the state of instruction tuning on open resources. _arXiv preprint arXiv:2306.04751_, 2023.\n[URL https://arxiv.org/abs/2306.04751.](https://arxiv.org/abs/2306.04751)\n\n\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In _International_\n_Conference on Learning Representations_ [, 2022. URL https://openreview.net/forum?](https://openreview.net/forum?id=gEZrGCozdqR)\n[id=gEZrGCozdqR.](https://openreview.net/forum?id=gEZrGCozdqR)\n\n\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\nrewards for language model training. _arXiv preprint arXiv:2306.01693_ [, 2023. URL https:](https://arxiv.org/abs/2306", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_18000", "chunk_text": " Fine-grained human feedback gives better\nrewards for language model training. _arXiv preprint arXiv:2306.01693_ [, 2023. URL https:](https://arxiv.org/abs/2306.01693)\n[//arxiv.org/abs/2306.01693.](https://arxiv.org/abs/2306.01693)\n\n\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. _arXiv preprint arXiv:2305.00633_,\n[2023. URL https://arxiv.org/abs/2305.00633.](https://arxiv.org/abs/2305.00633)\n\n\nFangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms with\n[compression and selective augmentation, 2023. URL https://arxiv.org/abs/2310.](https://arxiv.org/abs/2310.04408)\n[04408.](https://arxiv.org/abs/2310.04408)\n\n\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language\n[models robust to irrelevant context, 2023. URL https://arxiv.org/abs/2310.01558.](https://arxiv.org/abs/2310.01558)\n\n\nXiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of\nattribution by large language models. _arXiv preprint arXiv:2305.06311_ [, 2023. URL https:](https://arxiv.org/abs/2305.06311)\n[//arxiv.org/abs/2305.06311.](https://arxiv.org/abs/2305.06311)\n\n\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen,\nXixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking.\n_arXiv preprint arXiv:2304.037", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_18450", "chunk_text": "ang, Wei Fang, Luc Gaitskell, Thomas Hartvigsen,\nXixin Wu, Danny Fox, Helen Meng, and James Glass. Interpretable unified language checking.\n_arXiv preprint arXiv:2304.03728_ [, 2023. URL https://arxiv.org/abs/2304.03728.](https://arxiv.org/abs/2304.03728)\n\n\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language\n[agent tree search unifies reasoning acting and planning in language models, 2023. URL https:](https://arxiv.org/abs/2310.04406)\n[//arxiv.org/abs/2310.04406.](https://arxiv.org/abs/2310.04406)\n\n\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv_\n_preprint arXiv:1909.08593_ [, 2019. URL https://arxiv.org/abs/1909.08593.](https://arxiv.org/abs/1909.08593)\n\n\n15\n\n\nPreprint.\n\n\nAPPENDIX\n\n\n**A** **SELF-RAG Details** **17**\n\n\nA.1 Reflection Tokens. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\n\nA.2 SELF-RAG Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\n\nA.3 SELF-RAG Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n\n**B** **Experimental Details** **19**\n\n\nB.1 More Details of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n\nB.2 More Details of Evaluations . . . . . .", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_18900", "chunk_text": " of Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n\n\nB.2 More Details of Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n\n**C Results** **20**\n\n\nC.1 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n\n\nC.2 Human Evaluation Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\n\nC.3 Qualitative Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n\n\n**D Full List of Instructions and Demonstrations for GPT-4** **21**\n\n\n16\n\n\nPreprint.\n\n\nA SELF-RAG DETAILS\n\n\nA.1 REFLECTION TOKENS.\n\n\n**Definitions of reflection tokens.** Below, we provide a detailed definition of reflection type and\n\n- utput tokens. The first three aspects will be provided at each segment level, while the final aspect is\n\n- nly given at each output level.\n\n\n- **Retrieval-on-demand** ( **Retrieve** ): Given an input and previous-step generation (if applicable),\nan LM determines whether the continuation requires factual grounding. No indicates retrieval\nis unnecessary as the sequence does not require factual grounding or may not be enhanced by\nknowledge retrieval, Yes indicates retrieval is necessary. We additionally have continue\nto use evidence, which indicates that a model can continue to use the evidence retrieved\npreviously. For instance, a passage may contain rich factual information, and thus SELF-RAG\ngenerates multiple segments based on the passage.\n\n\n- **Relevant** ( **ISREL** ): Retrieved knowledge may not be always relevant to the input. This aspect\nindicates whether the evidence provides useful information (Relevant) or not (Irrelevant).\n\n\n- **Supported** ( **ISSUP** ): Attribution is the concept of whether the output is fully supported by\ncertain evidence (Menick et al., 2022; Bohnet et al.,", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_19350", "chunk_text": "levant) or not (Irrelevant).\n\n\n- **Supported** ( **ISSUP** ): Attribution is the concept of whether the output is fully supported by\ncertain evidence (Menick et al., 2022; Bohnet et al., 2022). This aspect judges how much information in the output is entailed by the evidence. We evaluate attributions in three scale, Fully\nsupported, Partially supported, and No support / Contradictory, following Yue et al. (2023); Nakano et al. (2021).\n\n\n- **Useful** ( **ISUSE** ): Following the definitions from Liu et al. (2023a), we define the perceived utility\nas whether the response is a helpful and informative answer to the query, independently from\nwhether it is in fact factual or not. This can be also viewed as plausibility in Menick et al. (2022).\nFor usefulness, we use a five-scale evaluation (1 is the lowest and 5 is the highest).\n\n\n**Details of GPT-4-based data collections.** We use the instruction and demonstration pairs to prompt\nGPT-4, listed in Section D. Following an official recommendation, we separate instructions and\n\n- utputs with \u201c##\u201d. We use the temperature 1 and set the maximum output token counts to be 200. We\ndiscard instances where GPT-4 does not follow the designated output formats or output sequences\nthat do not match our expected category names. As a result, we collected 1,2594 for **Retrieve**, 11,181\nfor **ISSUP**, 19,317 for relevance, 3,831 for utility.\n\n\n**Manual analysis of the GPT-4 predictions.** The authors of this paper manually assess randomly\nsampled 20 instances for each aspect and check if GPT-4 predictions match their assessments given\nthe same instruction, demonstrations, and test instances. We found our assessments show high\nagreement with GPT-4 predictions, especially for relevance (95%), retrieval necessity (95%), and\nthe degree of support (90%). Agreement was slightly lower in usefulness (80%), mostly due to the\ndisagreement between 1 and 2 or 4 and 5.\n\n\nA.2 SELF-RAG TRAINING\n\n\n**Overview of training.** Algorithm 2 provides a high-level overview of our training.\n\n\n**Full list of seed datasets.** To sample diverse input", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_19800", "chunk_text": "1 and 2 or 4 and 5.\n\n\nA.2 SELF-RAG TRAINING\n\n\n**Overview of training.** Algorithm 2 provides a high-level overview of our training.\n\n\n**Full list of seed datasets.** To sample diverse input-output pairs, we sample instances of the OpenInstruct (Wang et al., 2023) dataset. In particular, we use their ShareGPT, GPT-4 Alpaca, Alpaca,\nOpenAssistant, and FLAN subsets subsets. We also sample instances from a couple of knowledgeintensive datasets, Natural Questions (Kwiatkowski et al., 2019), Wizard of Wikipedia (Dinan et al.,\n2019) and FEVER (Thorne et al., 2018) from the KILT benchmark (Petroni et al., 2021), ASQA (Stelmakh et al., 2022) and multiple QA datasets including ARC-Easy and OpenBookQA (Mihaylov et al.,\n2018). Table 3 shows the full list of training instances, and in total, we use 145,619 instances.\n\n\n**Performance of the Critic** _C_ **.** We evaluate the accuracy of reward predictions by splitting GPT-4\ngenerated feedback into training, development, and test sets. The accuracy of the reward model is\nas follows. Table 5 shows the model performance of predicting GPT-4 judgments. As you can see,\n\n- verall our fine-tuned reward model shows high prediction matching with GPT-4 predicted feedback.\n\n\n17\n\n\nPreprint.\n\n\n**Algorithm 2** SELF-RAG Training\n\n\n1: **Input** input-output data _D_ = _{X, Y }_, generator _M_, _C \u03b8_\n2: Initialize _C_ with a pre-trained LM\n3: Sample data _{X_ _[sample]_ _, Y_ _[sample]_ _} \u223c{X, Y }_ _\u25b7_ **Training Critic LM (Section 3.2.1)**\n4: **for** ( _x, y_ ) _\u2208_ ( _X_ _[sample]_ _, Y_ _[sample]_ ) **do** _\u25b7_ Data collections for _C_\n5: Prompt GPT-4 to collect a reflection token _r_ for ( _x, y_ )\n6: Add _", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_20250", "chunk_text": "_ _, Y_ _[sample]_ ) **do** _\u25b7_ Data collections for _C_\n5: Prompt GPT-4 to collect a reflection token _r_ for ( _x, y_ )\n6: Add _{_ ( _x, y, r_ ) _}_ to _Dcritic_\n7: Update _C_ with next token prediction loss _\u25b7_ Critic learning; Eq. 1\n8: Initialize _M_ with a pre-trained LM _\u25b7_ **Training Generator LM (Section 3.2.2)**\n9: **for** ( _x, y_ ) _\u2208_ ( _X, Y_ ) **do** _\u25b7_ Data collection for _M_ with _Dcritic_\n10: Run _C_ to predict _r_ given ( _x, y_ )\n11: Add ( _x, y, r_ ) to _Dgen_\n12: Update _M_ - n _Dgen_ with next token prediction loss _\u25b7_ Generator LM learning; Eq. 2\n\n|Dataset name|category Data source the number of instances|\n|---|---|\n|||\n|GPT-4 Alpaca<br>Stanford Alpaca<br>FLAN-V2<br>ShareGPT<br>Open Assistant 1<br>Wizard of Wikipedia<br>Natural Questions<br>FEVER<br>OpenBoookQA<br>Arc-Easy<br>ASQA|Instruction-following<br>Open-Instruct<br>26,168<br>Instruction-following<br>Open-Instruct<br>25,153<br>Instruction-following<br>Open-Instruct<br>17,817<br>Instruction-following<br>Open-Instruct<br>13,406<br>Instruction-following<br>Open-Instruct<br>9,464<br>Knowledge-intensive<br>KILT<br>17,367<br>Knowledge-intensive<br>KILT<br>15,535<br>Knowledge-intensive<br>KILT<br>9,966<br>Knowledge-intensive<br>HF Dataset<br>4,699<br>Knowledge-intensive<br>HF Dataset<br>2,147<br>Knowledge-intensive<br>ASQA<br>3,897|\n\n\n\nTable 3: The generator LM _M_ training data statistics.\n\n|baseLM|Col2|Retrieve|Col4|ISSUP|Col6|ISREL|Col8|ISUSE|", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_20700", "chunk_text": "ASQA<br>3,897|\n\n\n\nTable 3: The generator LM _M_ training data statistics.\n\n|baseLM|Col2|Retrieve|Col4|ISSUP|Col6|ISREL|Col8|ISUSE|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama2-7B<br>FLAN-3B|**93.8**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|**93.8**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|**93.8**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|**93.8**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|**93.8**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|**93.8**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|**93.8**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|**93.8**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|**93.", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_21150", "chunk_text": "**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|**93.8**<br>**93.5**<br>80.2<br>**73.5**<br>85.6<br>73.1<br>**82.0**<br>72.1|\n\n\n\nFigure 5: Reward prediction accuracy using GPT-4 predictions as ground-truth predictions.\n\n\nWhile our final model uses Llama2-7B as a base LM, we also train and compare FLAN-3B (Wei\net al., 2022) model on the same data, to investigate the effectiveness of different data sizes affect final\nreward predictions. In most aspects, our reward model shows higher than 80% accuracy, indicating\nthe powerful ability of fine-tuned specialized LMs to evaluate text. While both models show relatively\nlower performance on **ISUSE**, this is because both models often confuse between the two highest\ncases (5 and 4), where human annotators can also disagree.\n\n\n**Details of** _M_ **data creation.** Here, we provide detailed data creation procedures. Algorithm 3\nsummarizes the process. Here we set _yt_ to _y_ for simplification. Once we train the critic model, we\nfirst run it on input data from the aforementioned datasets, to predict whether retrieval is needed or\nnot. For the instances where the critic predicts **Retrieve** =No, we only predict the **ISUSE** given input\nand output. For the instances where the critic predicts **Retrieve** =Yes, we first retrieve passages using\n\n|Retrieve|=No<br>,|\n|---|---|\n|edicts|** Retrieve**|\n\nthe input and the entire output as queries, to find passages that are relevant to the entire output. We\nthen split output sentences using Spacy. [7] For each sentence, we run _C_ to predict whether the retrieval\nis necessary or not, given the input, preceding segments, and the initial retrieved passage. If _C_ predicts\n\n**Retrieve** =No, then do not insert any paragraph at the _t_ th segment. If _C_ predicts **Retrieve** =Yes, then\nwe use the original input and the _t", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_21600", "chunk_text": " passage. If _C_ predicts\n\n**Retrieve** =No, then do not insert any paragraph at the _t_ th segment. If _C_ predicts **Retrieve** =Yes, then\nwe use the original input and the _t_ th segment as a retrieval query to find relevant passages for the\n_t_ - th segment. For each retrieved passage, we predict **ISREL** and **ISSUP** . If there is any passage and\ncontinuation with **ISREL** =Relevant and **ISSUP** =Fully Supported / **ISSUP** =Partially\n\n\n[7https://spacy.io/](https://spacy.io/)\n\n\n18\n\n\nPreprint.\n\n\nSupported, then we sample it as the continuation. If there is more than one passage satisfying this\ncriterion, we use the one with the highest retrieval score. If there are only **ISREL** =Irrelevant or\n\n**ISSUP** =No Support passages, we randomly sample one passage.\n\n\n**Algorithm 3** _Mgen_ Data creation\n\n\n1: **Input** Input-output data _D_ = _X, Y_\n2: **for** ( _x, y_ ) _\u2208{X, Y }_ **do**\n3: Given ( _x, y_ ) _C_ predicts **Retrieve**\n4: **if Retrieve** is predicted **then**\n5: Retrieve relevant passages **D** using _R_ given ( _x, y_ ) _\u25b7_ Retrieve passages\n6: **for** _d \u2208_ **D do**\n7: _C_ predicts **ISREL** for each _d_ _\u25b7_ Predict relevance of passages\n8: _C_ predicts **ISSUP** for each ( _y, d_ ) _\u25b7_ Predict supports of outputs\n\n9: _C_ predicts **ISUSE** for each _d_ _\u25b7_ Predict overall utility ( _t_ = _T_  - nly)\n10: Sample _d_\n11: **else if Retrieve** is not predicted **then**\n12: _C_ predicts **ISUSE** given _x, y_\nAdd augmented ( _x, y, d, r_ ) to _Dgen_\n\n\n**Training examples.** Table 4 show several training examples used for _M_ training.\n\n\nA.3 SELF-RAG INFERENCE\n\n\n**Details of beam-search score calculations", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_22050", "chunk_text": "x, y, d, r_ ) to _Dgen_\n\n\n**Training examples.** Table 4 show several training examples used for _M_ training.\n\n\nA.3 SELF-RAG INFERENCE\n\n\n**Details of beam-search score calculations.** We first compute scores for each critique type by\ntaking the normalized probabilities of desirable tokens. For **ISREL**, we compute the score as follows:\n\n\n_s_ ( **ISREL** ) =\n\n|(<br>p|Col2|Col3|ISREL|RELEVANT)<br>=|Col6|Col7|\n|---|---|---|---|---|---|---|\n|_p_(|** ISREL**|= RELEVANT) +_ p_(|= RELEVANT) +_ p_(|= RELEVANT) +_ p_(|** ISREL**|= IRRELEVANT)|\n\n\n\nFor **ISSUP**, we compute the score as follows:\n\n\n\n_s_ ( **ISREL** ) = _[p]_ [(] **[ I][S][S][UP]** [ =][ F][ULLY][)]\n\n\n\n+ 0 _._ 5 _\u00d7_ _[p]_ [(] **[ I][S][S][UP]** [ =][ P][ARTIALLY][)]\n_S_ _S_\n\n\n\n_S_ _,_\n\n\n\nwhere _S_ = [\ufffd] _t\u2208{_ FULLY _,_ PARTIALLY _,_ NO _}_ _[p]_ [(] **[ I][S][S][UP]** [ =] _[ t]_ [)][. For] **ISUSE** where we have a five-scale score, we\n\ncompute the weighted sum of the scores. We assigns weighted scores of _w_ = _{\u2212_ 1 _, \u2212_ 0 _._ 5 _,_ 0 _,_ 0 _._ 5 _,_ 1 _}_\nto the tokens **ISUSE** = _{_ 1 _,_ 2 _,_ 3 _,_ 4 _,_ 5 _}_, and compute the final scores as follows:\n\n\n\n_S_ _,_\n\n\n\n_s_ ( **ISUSE** ) =\n\n\nwhere _S_ = [\ufffd] _t\u2208{_ 1 _,_ 2 _,_ 3 _,_ 4 _,_ 5 _}_ _[p]_ [(] **[ I][S][U][SE", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_22500", "chunk_text": "\n\n\nwhere _S_ = [\ufffd] _t\u2208{_ 1 _,_ 2 _,_ 3 _,_ 4 _,_ 5 _}_ _[p]_ [(] **[ I][S][U][SE]** [ =] _[ t]_ [)][.]\n\n\n\n5\n\n\n\n\n_wi_ _S_\n_i_\n\n\n\n**Details of adaptive retrieval.** For retrieval based on soft constraints, we trigger retrieval if the\nfollowing condition is satisfied:\n\n|(<br>p|Col2|Col3|Retrieve|YES)<br>=|Col6|Col7|\n|---|---|---|---|---|---|---|\n|_p_(|** Retrieve**|= YES) +_ p_(_p_(|= YES) +_ p_(_p_(|= YES) +_ p_(_p_(|** Retrieve**|= NO)|\n\n\n\nB EXPERIMENTAL DETAILS\n\n\nB.1 MORE DETAILS OF TRAINING\n\n\n**More details of training and computations.** We use 4 Nvidia A100 with 80GB memory to train\n\n- ur models. All models are trained for 3 epochs with a batch size of 128, a peak learning rate of 2e-5\nwith 3% warmup steps, and linear decay afterward. We set the maximum token length to be 2,048\nfor the 7B model, and 1,524 for the 13B model due to the memory constraint. We use Deepspeed\nstage 3 (Rajbhandari et al., 2020) to conduct multi-GPU distributed training, with training precision\nBfloat16 enabled. FlashAttention (Dao et al., 2022) is used to make the long-context training more\nefficient. We run inference of our trained models using 1-2 Quadro RTX 6000 GPUs with 24GB\n\nmemory.\n\n\n19\n\n\nPreprint.\n\n\nB.2 MORE DETAILS OF EVALUATIONS\n\n\n**Retrieval setup details.** By default, we use Contriever-MS MARCO to retrieve the top five\ndocuments from Wikipedia, and use official Wikipedia embeddings based on 2018 English Wikipedia.\nOn PopQA, where question and answer pairs are created based on WikiData in 2022, we found\nthat the 2018 Wikipedia sometimes lacks articles about some entities that have been more recently\nadded to Wikipedia. Therefore, for Pop", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_22950", "chunk_text": ".\nOn PopQA, where question and answer pairs are created based on WikiData in 2022, we found\nthat the 2018 Wikipedia sometimes lacks articles about some entities that have been more recently\nadded to Wikipedia. Therefore, for PopQA, we used the December 2020 preprocessed Wikipedia\ncorpus provided by Izacard et al. (2022b) and generated document embeddings. [8] The issues of\nperformance variance from different Wikipedia dumps have been reported by prior work (Asai et al.,\n2020; Izacard et al., 2022b). Yet, we observe limited effectiveness of such off-the-shelf retrieval\nmodels trained primarily on knowledge-intensive tasks for open-ended generation (e.g., instruction\nfollowing). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al.,\n2023b) or joint training of retrieval and LM components (Lin et al., 2023), while we leave exploring\nthe effectivess of such appraoches for future work. For bio generation and open-domain QA tasks,\nwe additionally retrieve five documents using Google Programmable Search [9] and search documents\nfrom English Wikipedia. As this API only provides snippets, we retrieve Wikipedia introductory\nparagraphs for the corresponding entities.\n\n\n**Detailed experimental settings for individual datasets.** For OpenQA datasets, we set the maximum new token number to 100 tokens. For closed-set tasks (PubHealth and ARC-C), we set the\nmaximum new token length to 50 for all baselines. For SELF-RAG inference on PubHealth and\nARC-C, instead of determining the output with the highest score 4 as in other tasks, we aggregate the\nscores for each option and select the answer option with the highest score. We found in zero-shot\nsettings of fact checking, some LLMs can generate capitalized class labels (e.g., True) while our\ngold labels are lower-cased. Therefore, across different LMs, for fact checking, we lowercase the\npredictions. In multiple choice tasks, we found some models generate answers in slightly different\nways (e.g., (A) instead of A). We slightly modify instructions for each LLM to avoid such format\nviolations, and further conduct string matching between each candidate and model predictions if\nformat violations still remain. After that processing, in closed set tasks, model predictions match\n\n- ne of the gold classes in almost all cases. For", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_23400", "chunk_text": " avoid such format\nviolations, and further conduct string matching between each candidate and model predictions if\nformat violations still remain. After that processing, in closed set tasks, model predictions match\n\n- ne of the gold classes in almost all cases. For ALCE, we found that Llama2-chat tend to generate\nsignificantly lower outputs than other models (e.g., on average, their output is nearly 100 token, while\nChatGPT generates 40 tokens on average), resulting in inflated str-em scores. We limit the maximum\ngeneration length to 100 tokens for all baselines to avoid this issue, rather than the original 300\ntokens in the ALCE paper. Consequently, all of the baseline output length is within 30-60 tokens.\nFor FactScore, we set the maximum new token length to 500 for baselines and 200 for SELF-RAG at\neach segment level.\n\n\n**Task-specific instructions.** Table 5 shows the list of the instructions used during evaluations. For\nOpen-domain QA, we do not provide explicit instructions.\n\n\nC RESULTS\n\n\nC.1 ANALYSIS\n\n\n**Reliance on parametric- and non-parametric memories.** We conduct analysis on how frequently\nmodel answers come from retrieved passages (non-parametric memories) or their own parametric\nmemories. On two open-domain QA datasets, TriviaQA and PopQA, we conduct the following\nanalysis: 1) sample query models successfully answer correctly, 2) for each query in this group,\ncheck whether the matched ground-truth answer is a sub-string of the retrieved passage or not. We\nevaluate SELF-RAG 7B, Alpaca 7B, Alpaca 13B, and Llama2-Chat-13B. We found that SELF-RAG\nsignificantly less frequently generates answers that are not included in the provided evidence; in\nparticular, in Alpaca 30B, 20% of the correct predictions are not included in the provided passages,\nfollowed by Llama2-chat 13B (18%) and Alpaca (15%), while it is only 2% in SELF-RAG. When\nretrieved passages are not relevant, SELF-RAG generates **ISREL** =Irrelevant, indicating that the\nfollowing answers may not be factually grounded, while those instruction-tuned models continue to\ngenerate plausible answers.\n\n\n[8https://github.com/facebookresearch", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_23850", "chunk_text": " are not relevant, SELF-RAG generates **ISREL** =Irrelevant, indicating that the\nfollowing answers may not be factually grounded, while those instruction-tuned models continue to\ngenerate plausible answers.\n\n\n[8https://github.com/facebookresearch/atlas](https://github.com/facebookresearch/atlas)\n[9https://programmablesearchengine.google.com/about/](https://programmablesearchengine.google.com/about/)\n\n\n20\n\n\nPreprint.\n\n\nC.2 HUMAN EVALUATION EXAMPLES\n\n\nTable 6 shows examples with human evaluations on S&P and correctness of **ISREL** and **ISSUP**\nreflection tokens.\n\n\nC.3 QUALITATIVE EXAMPLES\n\n\nTable 7 shows several examples predicted by our SELF-RAG (13B). The first example is the model\n\n- utput to an ASQA question. The first reference states that Emperor Constantine made Sunday a\nday of rest from labor, and further the second citation supports the fact that the official adoption\n\n- f Sunday as a day of rest by Constantine in AD 321. In the second example, the model predicts\nContradictory to the first output as the output says the person has served as the CEO since 2010,\nwhile the passage says he stepped down as CEO in 2015. Indicating those factual contradictions\nas reflection tokens enables to enforcement of hard control and also verification of model outputs\neasily. In the third example, while the generation is mostly correct, SELF-RAG predicts Partially\nSupport to the statement listing the name of the songs, as they were not explicitly mentioned.\n\n\nD FULL LIST OF INSTRUCTIONS AND DEMONSTRATIONS FOR GPT-4\n\n\nHere, we show the instructions and demonstrations used to prompt GPT-4 to collect reflection tokens.\nTable 8 shows the instructions and demonstrations for the initial retrieval token. Table 9 shows\nthe instruction and demonstrations used to collect the three-way output tokens for **Retrieve** given\ninstruction, preceding sentences, and previously retrieved passages. Due to the longer demonstration\nand test input, we only use a single demonstration. Table 10 shows an instruction and demonstrations\nused to collect the three-way output tokens for **ISREL** . Table 11 shows an instruction and demonstrations used to collect the three-way output tokens for **ISREL** . Table 12 shows an instruction and\ndemonstrations used to collect the five-way output tokens for **ISUSE** .\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_24300", "chunk_text": " . Table 11 shows an instruction and demonstrations used to collect the three-way output tokens for **ISREL** . Table 12 shows an instruction and\ndemonstrations used to collect the five-way output tokens for **ISUSE** .\n\n\n21\n\n\n|ISREL|Col2|Ta<br>.|\n|---|---|---|\n|for|**ISUSE**|**ISUSE**|\n\n\nPreprint.\n\n\n**Input** Identify the two types of computer memory. (from gpt4 ~~a~~ lpaca)\n**Output** [ **Retrieve** =Yes]<p>Computer memory Computer memory \u2013 In computing, memory refers to the\ncomputer hardware integrated circuits that store information for immediate use in a computer; it is synonymous\nwith the term \u201cprimary storage\u201d. Computer memory operates at a high speed, for example random-access memory\n(RAM), as a distinction from storage that provides slow-to-access information but offers higher capacities. If\nneeded, contents of the computer memory can be transferred to secondary storage; a very common way of doing\nthis is through a memory management technique called \u201cvirtual memory\u201d. An archaic synonym for memory is\nstore. The term \u201cmemory\u201d, meaning \u201cprimary storage\u201d or \u201cmain memory\u201d, is<p>[ **ISREL** =Relevant]Primary\nmemory, also known as main memory, is the memory that the computer can directly access and manipulate.\n\n[ **ISSUP** =Fully Supported] [ **Retrieve** =Yes]<p>Computer memory\u2013often associated with addressable\nsemiconductor memory, i.e. integrated circuits consisting of silicon-based transistors, used for example as\nprimary storage but also other purposes in computers and other digital electronic devices. There are two main\nkinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory\n(used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware\nsuch as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access\nmemory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM)\nthat is fast but energy-consuming, offering lower memory<p>[ **ISREL** =Relevant]Primary memory can be\naccessed quickly, but it is volatile, meaning that the data is lost if the power is turned off.[ **ISSUP** =Partially\nSupported]...[ **ISUSE**", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_24750", "chunk_text": "ISREL** =Relevant]Primary memory can be\naccessed quickly, but it is volatile, meaning that the data is lost if the power is turned off.[ **ISSUP** =Partially\nSupported]...[ **ISUSE** :5]\n\n\n**Input** Give an example of a situation where you have to delegate tasks (from gpt4 ~~a~~ lpaca)\n**Output** [ **Retrieve** =No]The manager realizes that there is an extensive list of tasks that need to be completed in\n\n - rder to make the event a success.[ **Retrieve** =Yes]<p>Delegation\u2013Delegation is the assignment of any responsibility or authority to another person (normally from a manager to a subordinate) to carry out specific activities. It\nis one of the core concepts of management leadership. However, the person who delegated the work remains\naccountable for the outcome of the delegated work. Delegation empowers a subordinate to make decisions, i.e. it\nis a shifting of decision-making authority from one organizational level to a lower one. Delegation, if properly\ndone, is not fabrication. The opposite of effective delegation is micromanagement, where a manager provides too\nmuch input,<p>[ **ISREL** =Relevant]This includes tasks such as creating a guest list, designing and sending invitations, booking a venue, coordinating with vendors, planning the events program, and monitoring the budget.[\u00b4\n\n**ISSUP** =Partially Supported][ **Retrieve** =No]Since it is not possible for the manager to handle everything\nalone, they must delegate tasks to their team members.[ **Retrieve** =No]The manager would then assign each\nteam member specific responsibilities based on their individual skillsets and strengths.[ **Retrieve** =Yes]<p>Joint\napplication design \u2013 Subject Matter Experts: These are the business users, the IS professionals, and the outside\nexperts that will be needed for a successful workshop. This group is the backbone of the meeting<p>[ **ISREL**\n=Irrelevant]For instance, the teams graphic designer might be tasked with designing the invitations, while\u00b4\nthe event coordinator would be in charge of planning the program and working with vendors...[ **Retrieve** =No]By\ndelegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also\nempowering their team members to take on responsibilities and contribute to the success of", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_25200", "chunk_text": " with vendors...[ **Retrieve** =No]By\ndelegating tasks, the manager can ensure that all necessary tasks are completed in a timely manner, while also\nempowering their team members to take on responsibilities and contribute to the success of the event.[ **ISUSE**\n=5]\n\n\n**Input** Synthesize a poem based on this prompt:\u201cThe sun is setting\u201d (from stanford ~~a~~ lpaca)\n**Output** [ **Retrieve** =No]The stars begin to twinkle bright the trees, scattered throughout the night;\n\n[ **Retrieve** =No]The stars begin to twinkle bright the trees, scattered throughout the night [ **Retrieve** =No] They\nflicker like a silent dance, A peaceful moment in its trance. [ **Retrieve** =No] They flicker like a silent dance, A\npeaceful moment in its trance. [ **Retrieve** =No] A chorus of crickets sing a lullaby, To every creature that roams\nthe sky [ **ISUSE** =5]\n\n\n**Input** when did walking dead season 7 come out (from nq)\n**Output** [ **Retrieve** =Yes]<p>The Walking Dead (season 7)\u2013The Walking Dead (season 7) The seventh season of\n\u201cThe Walking Dead\u201d, an American post-apocalyptic horror television series on AMC, premiered on October 23,\n2016, and concluded on April 2, 2017, consisting of 16 episodes. Developed for television by Frank Darabont,\nthe series is based on the eponymous series of comic books by Robert Kirkman, Tony Moore, and Charlie Adlard.\n...<p>[ **ISREL** =Relevant]October 23, 2016[ **ISSUP** =Fully Supported][ **ISUSE** =5]\n\n\n**Input** Given a chat history separated by new lines, generates an informative, knowledgeable and engaging\nresponse: Hello, I love German shepherd dogs and you? \u2013 My favorite dog, they are officially known as German\nShepherd Dogs? \u2013 Yes, I have a German shepherd in my house and she is pregnant (from wow)\n**Output** hey also are easily trained, obedient, and intelligent.[ **Retrieve** =Yes]<p>German Shepherd \u2013 a simple\ndifference of opinion, it is the fundamental issue of the breeds essential conformation and movement. ... They\u00b4\nare known for being", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_25650", "chunk_text": " also are easily trained, obedient, and intelligent.[ **Retrieve** =Yes]<p>German Shepherd \u2013 a simple\ndifference of opinion, it is the fundamental issue of the breeds essential conformation and movement. ... They\u00b4\nare known for being easy to train and good for performing tasks and following instructions. They are especially\nwell known for their police work, being used for tracking criminals, patrolling troubled areas and detecting and\nholding suspects. <p>[ **ISREL** =Relevant]Because of this, they are often used for disability assistance and\nfor police work. [ **ISSUP** =Partially Supported][ **ISUSE** =2]\n\n\nTable 4: List of the training examples.\n\n\n22\n\n\n|<p>[|ISREL|\n|---|---|\n|**ISSUP**|=Par|\n\n\nPreprint.\n\n\n**Dataset** **Instruction**\n\n\nARC-C Given four answer candidates, A, B, C and D, choose the best answer choice. Please answer\nwith the capitalized alphabet only, without adding any extra phrase or period.\nPubHealth Is the following statement correct or not? Say true if it\u2019s correct; otherwise, say false. Don\u2019t\ncapitalize or add periods, just say \u201ctrue\u201d or \u201cfalse\u201d.\n\nBio Generation Tell me a bio about [Person Name]\n\nASQA (baseline) Instruction: Write an accurate, engaging, and concise answer for the given question using only\nthe provided search results (some of which might be irrelevant) and cite them properly. Use\nan unbiased and journalistic tone. Always cite for any factual claim. When citing several\nsearch results, use [1][2][3]. Cite at least one document and at most three documents in each\nsentence. If multiple documents support the sentence, only cite a minimum sufficient subset of\nthe documents.\n\nASQA (ours) Answer the following question. The question may be ambiguous and have multiple correct\nanswers, and in that case, you have to provide a long-form answer including all correct answers.\n\n\nTable 5: Full list of instructions used during zero-shot evaluations. For open-domain QA, we don\u2019t\nuse any task specific instruction and simply use the original questions as input query.\n\n\n23\n\n\nPreprint.\n\n\n**Input** What genre is A Question and Answer Guide to Astronomy? (PopQA)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_26100", "chunk_text": "\n\n\nPreprint.\n\n\n**Input** What genre is A Question and Answer Guide to Astronomy? (PopQA)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] A Question and Answer Guide to Astronomy is a non-fiction book that falls under the genre of Astronomy and Cosmology. [ **ISSUP** =Fully\nSupported][ **ISUSE** =5]\n\n**Reference [1]** A Question and Answer Guide to Astronomy is a book about astronomy and cosmology, and is\nintended for a general audience. The book was written by Pierre-Yves Bely, Carol Christian, and Jean-Rene\nRoy, and published in English by Cambridge University Press in 2010. It was originally written in French. The\ncontent within the book is written using a question and answer format. It contains some 250 questions, which\nThe Science Teacher states each are answered with a concise and well-formulated essay that is informative and\nreadable. The Science Teacher review goes on to state that many of the answers given in the book are little gems\n\n - f science writing. The Science Teacher summarizes by stating that each question is likely to be thought of by a\nstudent, and that the answers are informative, well constructed, and thorough. The book covers information about\nthe planets, the Earth, the Universe, practical astronomy, history, and awkward questions such as astronomy in\nthe Bible, UFOs, and aliens. Also covered are subjects such as the Big Bang, comprehension of large numbers,\nand the Moon illusion.\n\n**S&P** Supported and Plausible.\n**Correctness of** **ISREL and** **ISSUP :** Both **ISREL** and **ISSUP** are correct, as the Reference [1] provides\ndetailed information of the book including the genre, and the model output is fully supported by the reference.\n**Input** Who is the author of The Lie? (PopQA)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] The author of The Lie is Sam Harris.[\n**ISSUP** =Fully Supported][ **ISUSE** =5]\n\n**Reference [1]** Lying is a 2011 long-form essay book by American author and neuroscience expert Sam Harris.\nHarris argues that we can radically simplify our lives and improve society", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_26550", "chunk_text": "** =Fully Supported][ **ISUSE** =5]\n\n**Reference [1]** Lying is a 2011 long-form essay book by American author and neuroscience expert Sam Harris.\nHarris argues that we can radically simplify our lives and improve society by merely telling the truth in situations\nwhere others often lie.\n\n**S&P** Plausible but not supported.\n\n**Correctness of** **ISREL and** **ISSUP** Both **ISREL** and **ISSUP** are incorrect, as the Reference [1] is about a\ndifferent book whose title is similar to The Lie.\n\n\n**Input** Tell me a bio about Ronaldinho (Bio)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] Ronaldinho is a retired professional\nfootballer from Brazil.[ **ISSUP** =Fully Supported][ **Retrieve** =Continue]He played as an attacking\nmidfielder and was known for his technical skills, creativity, dribbling ability, and accuracy from free\nkicks[ **Retrieve** =Continue]He won two FIFA World Player of the Year awards and a Ballon d\u2019Or.[ **ISUSE** =5]\n\n**Reference [1]** Ronaldo de Assis Moreira (born 21 March 1980), commonly known as Ronaldinho Gaucho or\u00b4\nsimply Ronaldinho, is a Brazilian retired professional footballer who played mostly as an attacking midfielder,\nbut was also deployed as a winger. Considered one of the best players of his generation and regarded by many as\n\n - ne of the greatest of all time, Ronaldinho won two FIFA World Player of the Year awards and a Ballon d\u2019Or. A\nglobal icon of the sport, he was renowned for his technical skills, creativity, dribbling ability and accuracy from\nfree-kicks, as well as his use of tricks, feints, no-look passes and overhead...\n\n**S&P** Plausible and fully supported.\n\n**Correctness of** **ISREL and** **ISSUP** Both **ISREL** and **ISSUP** are correct.\n\n\n**Input** Tell me a bio about Sergei Bodrov (Bio)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] Sergei Vladimirovich Bodrov\n(born June 28, 1948) is", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_27000", "chunk_text": ")\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] Sergei Vladimirovich Bodrov\n(born June 28, 1948) is a Russian film director, screenwriter, and producer.[ **ISSUP** =Fully\nSupported][ **Retrieve** =Continue]He was born in Moscow, Russia.[ **Retrieve** =Continue]He is the president of the Russian Guild of Film Producers.[ **ISUSE** =5]\n\n**Reference [1]** Sergei Vladimirovich Bodrov (born June 28, 1948) is a Russian film director, screenwriter, and\nproducer. In 2003 he was the President of the Jury at the 25th Moscow International Film Festival.\n\n**S&P** Plausible but only partially supported. The the Reference [1] does not explicitly mention the person is born\nin Moscow and there is no description about the Russian Guild of Film Producers.\n**Correctness of** **ISREL and** **ISSUP** **ISREL** and **ISSUP** are correct as the reference document is highly relevant\nand the first sentence is fully supported.\n\n\nTable 6: Examples of human evaluation results.\n\n\n24\n\n\nPreprint.\n\n\n**Input** When did sunday became the seventh day of the week (asqa)\n**Output** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] In 321 AD, the Roman Emperor Constantine the Great made Sunday a day of rest from labor, stating: \u201cOn the venerable Day of the Sun let the\nmagistrates and people residing in cities rest, and let all workshops be closed\u201d.[ **ISSUP** =Fully Supported]\n\n[ **Retrieve** =Yes] <p> **Reference [2]** <p>[ **ISREL** =Relevant] The official adoption of Sunday as a day of\nrest by Constantine the Great in AD 321 made Sunday the seventh day of the week. **[2]** [ **ISSUP** =Fully\nSupported][ **ISUSE** =5]\n\n**Reference [1]** Emperor Constantine then enacted the first Sunday Laws, for \u201cthe venerable Day of the Sun\u201d\nin 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_27450", "chunk_text": "1]** Emperor Constantine then enacted the first Sunday Laws, for \u201cthe venerable Day of the Sun\u201d\nin 321 A.D. On March 7, 321, the Roman emperor Constantine issued a decree making Sunday a day of rest\nfrom labor, stating: We find a good description of this in sources such as the book Rest Days which states:\nEarly Christian observance of both the spiritual seventh-day sabbath and a Lords Day assembly is evidenced in\u00b4\nIgnatius\u00b4s letter to the Magnesians ca. 110.\n**Reference [2]** Despite the official adoption of Sunday as a day of rest by Constantine, the seven-day week and\nthe nundial cycle continued to be used side-by-side until at least the Calendar of 354 and probably later. ... The\nfact that the canon had to be issued at all is an indication that adoption of Constantine\u2019s decree of 321 was still\nnot universal ...\n\n\n**Input** Tell me a bio about Rory Tapner. (bio generation)\n**Output** <p> **Reference [1]** <p>[ **ISREL** =Relevant] Rory Tapner is a British businessman who has served as\nthe Chief Executive Officer of Coutts, a private banking and wealth management company, since 2010.[ **ISSUP**\n=Contradictory], **Reference [2]** <p>[ **ISREL** =Relevant] Rory Tapner was born on 30 September 1959\nin London, England.[ **ISSUP** =Fully Supported]\n\n**Reference [1]** Rory Tapner Rory Tapner is a UK businessman who, in September 2010 was appointed CEO of\nCoutts, the Wealth Management division of Royal Bank of Scotland Group. In February 2015 he stepped down\nas CEO when he was replaced in the role by Alison Rose\n**Reference [2]** Holdings (from August 2016); and the Council of the University of Buckingham (from July 2017).\nRory Tapner Rory Tapner (born 30 September 1959) is a UK businessman ...\n\n\n**Input** Tell me a bio about G. Venugopa. (bio generation)\n**Output (ranked 1)** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] G. Venugopal is a popular playback singer in the Malayalam film industry.", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_27900", "chunk_text": "Output (ranked 1)** [ **Retrieve** =Yes] <p> **Reference [1]** <p>[ **ISREL** =Relevant] G. Venugopal is a popular playback singer in the Malayalam film industry. [ **Retrieve** =Continue] He has sung over 300 songs in\n\n|]<p>[|ISREL|\n|---|---|\n|** Retrieve**|=Con|\n\nMalayalam, Tamil, and Hindi movies.[ **ISSUP** =Partially Supported] [ **Retrieve** =Yes] <p> **Reference**\n\n**[2]** <p>G. Venugopal was born on December 10, 1960 in India.[ **ISSUP** =Fully Supported]\n\n**Reference [1]** \u2019G. Venugopal G Venugopal (born 10 December 1960) is an Indian playback singer known for his\nwork in Malayalam films. He started his singing career in the film \u201cOdaruthammaava Aalariyaam\u201d (1984). Since\nthen he has sung in more than 300 films; and has over 500 private albums to his credit. The many awards he has\nwon include the coveted Best Playback Singer (Kerala State) which he won thrice, the Film Critics award and\nthe Kerala Kaumudi Gallop Poll award (twice). He won The Kerala State award for Best Playback singer (2004)\nthird time for the song \u201daadedee..aadaadedee\u201d from the film \u201dUllam\u201d.Venugopal\u2019\n**Reference [2]** Kerala State Film Awards: Kerala Film Critics Award Kerala State Government Award (Professional Drama): Asianet Film Awards: Kerala Kaumadi Gallup Poll Award: Academic G. Venugopal G Venugopal\n(born 10 December 1960) is an Indian playback singer known for his work in Malayalam films.\n\n\nTable 7: Examples of outputs.\n\n\n25\n\n\nPreprint.\n\n\n\n\n\nTable 8: Instructions and demonstrations for **Retrieve** aspect given the input only.\n\n\n26\n\n\nPreprint.\n\n\n\n\n\nTable 9: Instructions and demonstrations for **Retrieve** aspect given the input, preceding generations,\nand retrieved passages.\n\n\n27\n\n\nPreprint.\n\n\n\n\n\nTable 10: Instructions and demonstrations for **ISREL** aspect given the input only.\n\n\n28\n\n\nPreprint.\n\n\n\n\n\nTable 11: Instructions and demonstrations for **ISSUP** tokens.\n\n\n29\n\n\nPreprint.\n\n\n\n\n\nTable 12:", "token_count": 500, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2310.11511_self_rag_asai:chunk_28350", "chunk_text": "\n\n\nPreprint.\n\n\n\n\n\nTable 10: Instructions and demonstrations for **ISREL** aspect given the input only.\n\n\n28\n\n\nPreprint.\n\n\n\n\n\nTable 11: Instructions and demonstrations for **ISSUP** tokens.\n\n\n29\n\n\nPreprint.\n\n\n\n\n\nTable 12: Instructions and demonstrations for **ISUSE** tokens.\n\n\n30\n\n\n", "token_count": 62, "metadata": {"arxiv_id": "2310.11511", "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "authors": ["Akari Asai", "Zeqiu Wu", "Yizhong Wang", "Avirup Sil", "Hannaneh Hajishirzi"], "year": 2023, "url": "https://arxiv.org/pdf/2310.11511v1"}}
{"chunk_id": "2309.15217_ragas_es:chunk_0", "chunk_text": "## **Ragas: Automated Evaluation of Retrieval Augmented Generation**\n\n**Shahul Es** _[\u2020]_ **, Jithin James** _[\u2020]_ **, Luis Espinosa-Anke** _[\u2217\u2662]_ **, Steven Schockaert** _[\u2217]_\n\n_\u2020_ Exploding Gradients\n_\u2217_ CardiffNLP, Cardiff University, United Kingdom\n_\u2662_ AMPLYFI, United Kingdom\nshahules786@gmail.com,jamesjithin97@gmail.com\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n\n\n\n**Abstract**\n\n\nWe introduce **Ragas** ( **R** etrieval **A** ugmented\n**G** eneration **As** sessment), a framework for\nreference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG\nsystems are composed of a retrieval and an\nLLM based generation module, and provide\nLLMs with knowledge from a reference textual\ndatabase, which enables them to act as a natural language layer between a user and textual\ndatabases, reducing the risk of hallucinations.\nEvaluating RAG architectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to\nidentify relevant and focused context passages,\nthe ability of the LLM to exploit such passages\nin a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite\n\n  - f metrics which can be used to evaluate these\n\ndifferent dimensions _without having to rely on_\n_ground truth human annotations_ . We posit that\nsuch a framework can crucially contribute to\nfaster evaluation cycles of RAG architectures,\nwhich is especially important given the fast\nadoption of LLMs.\n\n\n**1** **Introduction**\n\n\nLanguage Models (LMs) capture a vast amount\n\n- f knowledge about the world, which allows them\nto answer questions without accessing any external sources. This idea of LMs as repositories of\nknowledge emerged shortly after the introduction\n\n- f BERT (Devlin et al., 2019) and became more\nfirmly established with the introduction of ever\nlarger LMs (Roberts et al., 2020). While the most\nrecent Large Language Models (LLMs) capture\nenough knowledge to rival human performance\nacross a wide variety", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_450", "chunk_text": "firmly established with the introduction of ever\nlarger LMs (Roberts et al., 2020). While the most\nrecent Large Language Models (LLMs) capture\nenough knowledge to rival human performance\nacross a wide variety of question answering benchmarks (Bubeck et al., 2023), the idea of using\nLLMs as knowledge bases still has two fundamental limitations. First, LLMs are not able to answer\nquestions about events that have happened after\nthey were trained. Second, even the largest models\n\n\n\nstruggle to memorise knowledge that is only rarely\nmentioned in the training corpus (Kandpal et al.,\n2022; Mallen et al., 2023). The standard solution\nto these issues is to rely on _Retrieval Augmented_\n_Generation (RAG)_ (Lee et al., 2019; Lewis et al.,\n2020; Guu et al., 2020). Answering a question\nthen essentially involves retrieving relevant passages from a corpus and feeding these passages,\nalong with the original question, to the LM. While\ninitial approaches relied on specialised LMs for\nretrieval-augmented language modelling (Khandelwal et al., 2020; Borgeaud et al., 2022), recent work\nhas suggested that simply adding retrieved documents to the input of a standard LM can also work\nwell (Khattab et al., 2022; Ram et al., 2023; Shi\net al., 2023), thus making it possible to use retrievalaugmented strategies in combination with LLMs\nthat are only available through APIs.\nWhile the usefulness of retrieval-augmented\nstrategies is clear, their implementation requires\na significant amount of tuning, as the overall performance will be affected by the retrieval model,\nthe considered corpus, the LM, or the prompt formulation, among others. Automated evaluation of\nretrieval-augmented systems is thus paramount. In\npractice, RAG systems are often evaluated in terms\n\n- f the language modelling task itself, i.e. by measuring perplexity on some reference corpus. However, such evaluations are not always predictive\n\n- f downstream performance (Wang et al., 2023c).\nMoreover, this evaluation strategy relies on the LM\nprobabilities, which are not accessible for some\nclosed models (e.g. ChatGPT and GPT-4). Question answering is", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_900", "chunk_text": " performance (Wang et al., 2023c).\nMoreover, this evaluation strategy relies on the LM\nprobabilities, which are not accessible for some\nclosed models (e.g. ChatGPT and GPT-4). Question answering is another common evaluation task,\nbut usually only datasets with short extractive answers are considered, which may not be representative of how the system will be used.\nTo address these issues, in this paper we present\nRagas [1], a framework for the automated assessment\n\n\n1Ragas is available at [https://github.com/](https://github.com/explodinggradients/ragas)\n[explodinggradients/ragas.](https://github.com/explodinggradients/ragas)\n\n\n- f retrieval augmented generation systems. We focus on settings where reference answers may not be\navailable, and where we want to estimate different\nproxies for correctness, in addition to the usefulness of the retrieved passages. The Ragas frame[work provides an integration with both llama-index](https://github.com/jerryjliu/llama_index)\n[and Langchain, the most widely used frameworks](https://github.com/langchain-ai/langchain)\nfor building RAG solutions, thus enabling devel\n- pers to easily integrate Ragas into their standard\nworkflow.\n\n\n**2** **Related Work**\n\n\n**Estimating faithfulness using LLMs** The problem of detecting hallucinations in LLM generated\nresponses has been extensively studied (Ji et al.,\n2023). Several authors have suggested the idea\n\n- f predicting factuality using a few-shot prompting strategy (Zhang et al., 2023). Recent analyses, however, suggest that existing models struggle\nwith detecting hallucination when using standard\nprompting strategies (Li et al., 2023; Azaria and\nMitchell, 2023). Other approaches rely on linking\nthe generated responses to facts from an external\nknowledge base (Min et al., 2023), but this is not\nalways possible.\nYet another strategy is to inspect the probabilities assigned to individual tokens, where we would\nexpect the model to be less confident in hallucinated answers than in factual ones. For instance,\nBARTScore (Yuan et al., 2021) estimates factuality\nby looking at the conditional probability of the generated text given the input. Kadavath et al. (2022)\nuse a variation of this idea. Starting", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_1350", "chunk_text": ",\nBARTScore (Yuan et al., 2021) estimates factuality\nby looking at the conditional probability of the generated text given the input. Kadavath et al. (2022)\nuse a variation of this idea. Starting from the observation that LLMs provide well-calibrated probabilities when answering multiple-choice questions,\nthey essentially convert the problem of validating\nmodel generated answers into a multiple-choice\nquestion which asks whether the answer is true or\nfalse. Rather than looking at the output probabilities, Azaria and Mitchell (2023) propose to train\na supervised classifier on the weights from one of\nthe hidden layers of the LLM, to predict whether a\ngiven statement is true or not. While the approach\nperforms well, the need to access the hidden states\n\n- f the model makes it unsuitable for systems that\naccess LLMs through an API.\nFor models that do not provide access to token\nprobabilities, such as ChatGPT and GPT-4, different methods are needed. SelfCheckGPT (Manakul\net al., 2023) addresses this problem by instead sampling multiple answers. Their core idea is that\n\n\n\nfactual answers are more stable: when an answer is\n\nfactual, we can expect that different samples will\ntend to be semantically similar, whereas this is less\nlikely to be the case for hallucinated answers.\n\n\n**Automated evaluation of text generation systems**\nLLMs have also been leveraged to automatically\nevaluate other aspects of generated text fragments,\nbeyond factuality. For instance, GPTScore (Fu\net al., 2023) uses a prompt that specifies the considered aspect (e.g. fluency) and then scores passages\nbased on the average probability of the generated\ntokens, according to a given autoregressive LM.\nThis idea of using prompts was previously also\nconsidered by Yuan et al. (2021), although they\nused a smaller fine-tuned LM (i.e. BART) and did\nnot observe a clear benefit from using prompts. An\n- ther approach directly asks ChatGPT to evaluate\na particular aspect of the given answer by providing a score between 0 and 100, or by providing a\nrating on a 5-star scale (Wang et al., 2023a). Remarkably, strong results can be obtained in this\nway, although it comes with the limitation of being", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_1800", "chunk_text": " 0 and 100, or by providing a\nrating on a 5-star scale (Wang et al., 2023a). Remarkably, strong results can be obtained in this\nway, although it comes with the limitation of being\nsensitive to the design of the prompt. Rather than\nscoring individual answers, some authors have also\nfocused on using an LLM to select the best answer\namong a number of candidates (Wang et al., 2023b),\ntypically to compare the performance of different\nLLMs. However, care is needed with this approach,\nas the order in which the answers is presented can\ninfluence the result (Wang et al., 2023b).\nIn terms of how ground truth answers or, more\ngenerally, generations, have been typically used\nin the literature, most approaches have relied on\nthe availability of one or more reference answers.\nFor instance, BERTScore (Zhang et al., 2020)\nand MoverScore (Zhao et al., 2019) use contextualised embeddings, produced by a pre-trained\nBERT model, to compare the similarity between\nthe generated answer and the reference answers.\nBARTScore (Yuan et al., 2021) similarly uses reference answers to compute aspects such as precision\n(estimated as the probability of generating the generated answer given the reference) and recall (estimated as the probability of generating the reference\ngiven the generated answer).\n\n\n**3** **Evaluation Strategies**\n\n\nWe consider a standard RAG setting, where given a\nquestion _q_, the system first retrieves some context\n_c_ ( _q_ ) and then uses the retrieved context to generate\nan answer _as_ ( _q_ ). When building a RAG system,\n\n\nwe usually do not have access to human-annotated\ndatasets or reference answers. We therefore fo\ncus on metrics that are fully self-contained and\nreference-free. We focus in particular three quality\naspects, which we argue are of central importance.\nFirst, **Faithfulness** refers to the idea that the answer should be grounded in the given context. This\nis important to avoid hallucinations, and to ensure\nthat the retrieved context can act as a justification\nfor the generated answer. Indeed, RAG systems are\n\n- ften used in applications where the factual consistency of the generated text w.r.t. the grounded\nsources is highly important, e", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_2250", "chunk_text": "\nthat the retrieved context can act as a justification\nfor the generated answer. Indeed, RAG systems are\n\n- ften used in applications where the factual consistency of the generated text w.r.t. the grounded\nsources is highly important, e.g. in domains such as\nlaw, where information is constantly evolving. Sec\n- nd, **Answer Relevance** refers to the idea that the\ngenerated answer should address the actual question that was provided. Finally, **Context Relevance**\nrefers to the idea that the retrieved context should\n\nbe focused, containing as little irrelevant information as possible. This is important given the cost\nassociated with feeding long context passages to\nLLMs. Moreover, when context passages are too\nlong, LLMs are often less effective in exploiting\nthat context, especially for information that is provided in the middle of the context passage (Liu\net al., 2023).\nWe now explain how these three quality aspects\ncan be measured in a fully automated way, by\nprompting an LLM. In our implementation and\nexperiments, all prompts are evaluated using the\ngpt-3.5-turbo-16k model, which is available\nthrough the OpenAI API [2] .\n\n\n**Faithfulness** We say that the answer _as_ ( _q_ ) is\nfaithful to the context _c_ ( _q_ ) if the claims that are\nmade in the answer can be inferred from the con\ntext. To estimate faithfulness, we first use an LLM\nto extract a set of statements, _S_ ( _as_ ( _q_ )). The aim\n\n- f this step is to decompose longer sentences into\nshorter and more focused assertions. We use the\nfollowing prompt for this step [3] :\n\n\n_Given a question and answer, create one_\n\n_or more statements from each sentence_\n_in the given answer._\n_question:_ [question]\n_answer:_ [answer]\n\n\nwhere [question] and [answer] refer to the\ngiven question and answer. For each statement _si_\n\n\n[2https://platform.openai.com](https://platform.openai.com)\n3To help clarify the task, we include a demonstration as\npart of the prompt. This demonstration is not explicitly shown\nin the listing of the prompts throughout this paper.\n\n\n\nin _S_, the LLM determines if _si_ can be inferred from\n", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_2700", "chunk_text": " help clarify the task, we include a demonstration as\npart of the prompt. This demonstration is not explicitly shown\nin the listing of the prompts throughout this paper.\n\n\n\nin _S_, the LLM determines if _si_ can be inferred from\n_c_ ( _q_ ) using a verification function _v_ ( _si, c_ ( _q_ )). This\nverification step is carried out using the following\n\nprompt:\n\n\n_Consider the given context and following_\n_statements, then determine whether they_\n_are supported by the information present_\n_in the context. Provide a brief explana-_\n_tion for each statement before arriving_\n_at the verdict (Yes/No). Provide a final_\n_verdict for each statement in order at the_\n_end in the given format. Do not deviate_\n_from the specified format._\n_statement:_ [statement 1]\n\n...\n\n_statement:_ [statement _n_ ]\n\n\nThe final faithfulness score, _F_, is then computed\n\n_[|]_\nas _F_ = _[|]_ _|_ _[V]_ _S|_ [, where] _[ |][V][ |]_ [ is the number of statements]\n\nthat were supported according to the LLM and _|S|_\nis the total number of statements.\n\n\n**Answer relevance** We say that the answer _as_ ( _q_ )\nis relevant if it directly addresses the question in\nan appropriate way. In particular, our assessment\n\n- f answer relevance does not take into account fac\ntuality, but penalises cases where the answer is\nincomplete or where it contains redundant information. To estimate answer relevance, for the given\nanswer _as_ ( _q_ ), we prompt the LLM to generate _n_\npotential questions _qi_ based on _as_ ( _q_ ), as follows:\n\n\n_Generate a question for the given answer._\n_answer_ : [answer]\n\n\nWe then obtain embeddings for all questions using the text-embedding-ada-002 model, available from the OpenAI API. For each _qi_, we calculate the similarity sim( _q, qi_ ) with the original\nquestion _q_, as the cosine between the corresponding embeddings. The answer relevance score, AR,\nfor question _q_ is then computed as:\n\n\n\nThis metric evaluates how closely the generated\nanswer aligns with the initial question or instruction.\n\n\n**Context relevance** The context _c_ ( _q_ )", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_3150", "chunk_text": ". The answer relevance score, AR,\nfor question _q_ is then computed as:\n\n\n\nThis metric evaluates how closely the generated\nanswer aligns with the initial question or instruction.\n\n\n**Context relevance** The context _c_ ( _q_ ) is considered relevant to the extent that it exclusively contains information that is needed to answer the question. In particular, this metric aims to penalise the\n\n\n\nAR = [1]\n\n\n_n_\n\n\n\n_n_\n\n- sim( _q, qi_ ) (1)\n\n\n_i_ =1\n\n\ninclusion of redundant information. To estimate\n\ncontext relevance, given a question _q_ and its context _c_ ( _q_ ), the LLM extracts a subset of sentences,\n_Sext_, from _c_ ( _q_ ) that are crucial to answer _q_, using\nthe following prompt:\n\n\n_Please extract relevant sentences from_\n_the provided context that can potentially_\n_help answer the following question. If no_\n_relevant sentences are found, or if you_\n_believe the question cannot be answered_\n_from the given context, return the phrase_\n_\"Insufficient Information\". While extract-_\n_ing candidate sentences you\u2019re not al-_\n_lowed to make any changes to sentences_\n_from given context._\n\n\nThe context relevance score is then computed as:\n\n\nnumber of extracted sentences\nCR = (2)\ntotal number of sentences in _c_ ( _q_ )\n\n\n**4** **The WikiEval Dataset**\n\n\nTo evaluate the proposed framework, we ideally\nneed examples of question-context-answer triples\nwhich are annotated with human judgments. We\ncan then verify to what extent our metrics agree\nwith human assessments of faithfulness, answer\n\nrelevance and context relevance. Since we are not\n\naware of any publicly available datasets that could\nbe used for this purpose, we created a new dataset,\nwhich we refer to as _WikiEval_ [4] . To construct the\n\ndataset, we first selected 50 Wikipedia pages covering events that have happened since the start of\n2022 [5] . In selecting these pages, we prioritised\nthose with recent edits. For each of the 50 pages,\nwe then asked ChatGPT to suggest a question that\ncan be answered based on the introductory section\n\n- f the page, using the following prompt:\n\n\n_Your task is to formulate a question from_\n_given context satisfying the rules given_\n\n_below:_\n\n_1. The question should be fully answered_\n_from the given", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_3600", "chunk_text": " answered based on the introductory section\n\n- f the page, using the following prompt:\n\n\n_Your task is to formulate a question from_\n_given context satisfying the rules given_\n\n_below:_\n\n_1. The question should be fully answered_\n_from the given context._\n_2. The question should be framed from_\n_a part that contains non-trivial informa-_\n\n_tion._\n\n_3. The answer should not contain any_\n\n\n[4https://huggingface.co/datasets/](https://huggingface.co/datasets/explodinggradients/WikiEval)\n[explodinggradients/WikiEval](https://huggingface.co/datasets/explodinggradients/WikiEval)\n\n5That is, beyond the reported training cutoff of the model\nwe used in our experiments.\n\n\n\n_links._\n\n_4. The question should be of moderate_\n_difficulty._\n_5. The question must be reasonable and_\n_must be understood and responded to by_\n\n_humans._\n\n_6. Do not use phrases that \u2019provided con-_\n_text\u2019, etc in the question_\n\n_context:_\n\n\nWe also used ChatGPT to answer the generated\nquestion, when given the corresponding introductory section as context, using the following prompt:\n\n\n_Answer the question using the informa-_\n_tion from the given context._\n_question:_ [question]\n_context:_ [context]\n\n\nAll questions were annotated along the three considered quality dimensions by two annotators. Both\nannotators were fluent in English and were given\nclear instructions about the meaning of the three\nconsidered quality dimensions. For faithfulness\nand context relevance, the two annotators agreed in\naround 95% of cases. For answer relevance, they\nagreed in around 90% of the cases. Disagreements\nwere resolved after a discussion between the anno\ntators.\n\n\n**Faithfulness** To obtain human judgements about\nfaithfulness, we first used ChatGPT to answer the\nquestion without access to any additional context.\nWe then asked the annotators to judge which of the\ntwo answers was the most faithful (i.e. the standard\n\n- ne or the one generated without context), given\nthe question and corresponding Wikipedia page.\n\n\n**Answer relevance** We first used ChatGPT to\n\n- btain candidate answers with lower answer rel\nevance, using the following prompt:\n\n\n_Answer the given question in an incom-_\n_plete manner._\n_question:_ [question]\n\n\nWe then asked", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_4050", "chunk_text": " relevance** We first used ChatGPT to\n\n- btain candidate answers with lower answer rel\nevance, using the following prompt:\n\n\n_Answer the given question in an incom-_\n_plete manner._\n_question:_ [question]\n\n\nWe then asked human annotators to compare this\nanswer, and indicate which of the two answers had\nthe highest answer relevance.\n\n\n**Context relevance** To measure this aspect, we\nfirst added additional sentences to the context by\nscraping back-links to the corresponding Wikipedia\npage. In this way, we were able to add information\nto the context that was related but less relevant for\n\n\n**Faith.** **Ans. Rel.** **Cont. Rel.**\n\n\nRagas **0.95** **0.78** **0.70**\nGPT Score 0.72 0.52 0.63\n\nGPT Ranking 0.54 0.40 0.52\n\n\nTable 1: Agreement with human annotators in pairwise\ncomparisons of faithfulness, answer relevance and context relevance, using the WikEval dataset (accuracy).\n\n\nanswering the question. For the few pages with\n- ut any back-links, we instead used ChatGPT to\ncomplete the given context.\n\n\n**5** **Experiments**\n\n\nTable 1 analyses the agreement between the metrics proposed in Section 3 and the human assessments from the proposed WikiEval dataset. Each\nWikiEval instance requires the model to compare\ntwo answers or two context fragments. We count\nhow often the answer/context preferred by the\nmodel (i.e. with highest estimated faithfulness, answer relevance, or context relevance) coincides\nwith the answer/context preferred by the human\nannotators. We report the results in terms of accuracy (i.e. the fraction of instances on which the\nmodel agrees with the annotators).\nTo put the results in context, we compare our\nproposed metrics (shown as _Ragas_ in Table 1) with\ntwo baseline methods. For the first method, shown\nas _GPT Score_, we ask ChatGPT to assign a score\nbetween 0 and 10 for the three quality dimensions.\nTo this end, we use a prompt that describes the\nmeaning of the quality metric and then asks to\nscore the given answer/context in line with that\ndefinition. For instance, for evaluating faithfulness,\nwe used the following prompt:\n\n\n_Faithfulness", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_4500", "chunk_text": " end, we use a prompt that describes the\nmeaning of the quality metric and then asks to\nscore the given answer/context in line with that\ndefinition. For instance, for evaluating faithfulness,\nwe used the following prompt:\n\n\n_Faithfulness measures the information_\n_consistency of the answer against the_\n_given context. Any claims that are made_\n\n_in the answer that cannot be deduced_\n\n_from context should be penalized._\n_Given an answer and context, assign a_\n_score for faithfulness in the range 0-10._\n_context_ : [context]\n\n_answer_ : [answer]\n\n\nTies, where the same score is assigned by the LLM\nto both answer candidates, were broken randomly.\nThe second baseline, shown as _GPT Ranking_, instead asks ChatGPT to select the preferred answer/\n\n\ncontext. In this case, the prompt again includes\na definition of the considered quality metric. For\ninstance, for evaluating answer relevance, we used\nthe following prompt:\n\n\n_Answer Relevancy measures the degree_\n_to which a response directly addresses_\n_and is appropriate for a given question._\n_It penalizes the present of redundant in-_\n_formation or incomplete answers given a_\n_question. Given an question and answer,_\n_rank each answer based on Answer Rele-_\n\n_vancy._\n_question_ : [question]\n_answer 1_ : [answer 1]\n\n_answer 2_ : [answer 2]\n\n\nThe results in Table 1 show that our proposed\nmetrics are much closer aligned with the human\njudgements than the predictions from the two baselines. For faithfulness, the Ragas prediction are in\ngeneral highly accurate. For answer relevance, the\nagreement is lower, but this is largely due to the\nfact that the differences between the two candidate\n\nanswers are often very subtle. We found context\nrelevance to be the hardest quality dimension to\nevaluate. In particular, we observed that ChatGPT\n\n- ften struggles with the task of selecting the sentences from the context that are crucial, especially\nfor longer contexts.\n\n\n**6** **Conclusions**\n\n\nWe have highlighted the need for automated\nreference-free evaluation of RAG systems. In particular, we have argued the need for an evaluation\nframework that can assess faithfulness (i.e. is the\nanswer grounded in the retrieved context), answer\nrelevance (i.e. does the answer address the question) and context relevance (i.e.", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_4950", "chunk_text": " have argued the need for an evaluation\nframework that can assess faithfulness (i.e. is the\nanswer grounded in the retrieved context), answer\nrelevance (i.e. does the answer address the question) and context relevance (i.e. is the retrieved\ncontext sufficiently focused). To support the devel\n- pment of such a framework, we have introduced\n_WikiEval_, a dataset which human judgements of\nthese three different aspects. Finally, we have also\ndescribed Ragas, our implementation of the three\nconsidered quality aspects. This framework is easy\nto use and can provide deverlopers of RAG systems with valuable insights, even in the absence\n\n- f any ground truth. Our evaluation on WikiEval\nhas shown that the predictions from Ragas are\nclosely aligned with human predictions, especially\nfor faithfulness and answer relevance.\n\n\n**References**\n\n\n[Amos Azaria and Tom M. Mitchell. 2023. The inter-](https://doi.org/10.48550/arXiv.2304.13734)\n[nal state of an LLM knows when its lying.](https://doi.org/10.48550/arXiv.2304.13734) _CoRR_,\nabs/2304.13734.\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n[2022. Improving language models by retrieving from](https://proceedings.mlr.press/v162/borgeaud22a.html)\n[trillions of tokens. In](https://proceedings.mlr.press/v162/borgeaud22a.html) _International Conference on_\n_Machine Learning, ICML 2022, 17-23 July 2022, Bal-_\n_timore, Maryland, USA", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_5850", "chunk_text": "3929\u20133938. PMLR.\n\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. _ACM Comput-_\n_ing Surveys_, 55(12):1\u201338.\n\n\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\n\n\n\n[Kaplan. 2022. Language models (mostly) know what](https://doi.org/10.48550/arXiv.2207.05221)\n[they know.](https://doi.org/10.48550/arXiv.2207.05221) _CoRR_, abs/2207.05221.\n\n\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\n[Wallace, and Colin Raffel. 2022. Large language](https://doi.org/10.48550/arXiv.2211.08411)\n[models struggle to learn long-tail knowledge.](https://doi.org/10.48550/arXiv.2211.08411) _CoRR_,\nabs/2211.08411.\n\n\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n[Zettlemoyer, and Mike Lewis. 2020. Generalization](https://openreview.net/forum?id=HklBjCEKvH)\n[through memorization: Nearest neighbor language](https://openreview.net/forum?id=HklBjCEKvH)\n[models", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_6750", "chunk_text": "/hash/6b493230205f780e1bc26945df7481e5-Abstract.html) _Advances in Neu-_\n_ral Information Processing Systems 33: Annual Con-_\n_ference on Neural Information Processing Systems_\n_2020, NeurIPS 2020, December 6-12, 2020, virtual_ .\n\n\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\n[Nie, and Ji-Rong Wen. 2023. Halueval: A large-](https://doi.org/10.48550/arXiv.2305.11747)\n[scale hallucination evaluation benchmark for large](https://doi.org/10.48550/arXiv.2305.11747)\n[language models.](https://doi.org/10.48550/arXiv.2305.11747) _CoRR_, abs/2305.11747.\n\n\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\n[Liang. 2023. Lost in the middle: How language](http://arxiv.org/abs/2307.03172)\n[models use long contexts.](http://arxiv.org/abs/2307.03172)\n\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\n[When not to trust language models: Investigating](https://aclanthology.org/2023.acl-long.546)\n[effectiveness of parametric and non-parametric mem-](https://aclanthology.org/2023.acl-long.546)\n\n[ories. In](https://aclanthology.org/2023.acl-long.546) _Proceedings of the 61st Annual Meeting of_\n_the Association for Computational Linguistics (Vol-_\n_ume 1: Long Papers)_, pages 9802\u20139822, Toronto,\nCanada. Association for Computational Linguistics.\n\n\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\n[2023. Selfcheckgpt: Zero-resource black-box hal-](https://doi.org/10.48550/arXiv.2303.08896)\n[luc", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_7650", "chunk_text": " Processing (EMNLP)_, pages 5418\u20135426,\nOnline. Association for Computational Linguistics.\n\n\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\n[Wen-tau Yih. 2023. REPLUG: retrieval-augmented](https://doi.org/10.48550/arXiv.2301.12652)\n[black-box language models.](https://doi.org/10.48550/arXiv.2301.12652) _CoRR_, abs/2301.12652.\n\n\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie\n[Zhou. 2023a. Is chatgpt a good NLG evaluator? A](https://doi.org/10.48550/arXiv.2303.04048)\n[preliminary study.](https://doi.org/10.48550/arXiv.2303.04048) _CoRR_, abs/2303.04048.\n\n\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n[2023b. Large language models are not fair evaluators.](https://doi.org/10.48550/arXiv.2305.17926)\n_CoRR_, abs/2305.17926.\n\n\nShufan Wang, Yixiao Song, Andrew Drozdov, Aparna\nGarimella, Varun Manjunatha, and Mohit Iyyer.\n[2023c. KNN-LM does not improve open-ended text](https://doi.org/10.48550/arXiv.2305.14625)\n[generation.](https://doi.org/10.48550/arXiv.2305.14625) _CoRR_, abs/2305.14625.\n\n\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\n\n[Bartscore: Evaluating generated text as text genera-](https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_8100", "chunk_text": " Neubig, and Pengfei Liu. 2021.\n\n[Bartscore: Evaluating generated text as text genera-](https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html)\n[tion. In](https://proceedings.neurips.cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html) _Advances in Neural Information Processing_\n_Systems 34: Annual Conference on Neural Informa-_\n_tion Processing Systems 2021, NeurIPS 2021, De-_\n_cember 6-14, 2021, virtual_, pages 27263\u201327277.\n\n\nTianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei\nFang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,\nDanny Fox, Helen Meng, and James R. Glass. 2023.\n[Interpretable unified language checking.](https://doi.org/10.48550/arXiv.2304.03728) _CoRR_,\nabs/2304.03728.\n\n\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\n[Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu-](https://openreview.net/forum?id=SkeHuCVFDr)\n[ating text generation with BERT. In](https://openreview.net/forum?id=SkeHuCVFDr) _8th International_\n_Conference on Learning Representations, ICLR 2020,_\n_Addis Ababa, Ethiopia, April 26-30, 2020_ . OpenReview.net.\n\n\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris[tian M. Meyer, and Steffen Eger. 2019. MoverScore:](https://doi.org/10.18653/v1/D19-1053)\n[Text generation evaluating with contextualized em-](https://doi.org/10.18653/v1/D19-1053)\n[beddings and earth mover distance. In](https://doi.org/10.18653/v1/D19-1053", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_8550", "chunk_text": " evaluating with contextualized em-](https://doi.org/10.18653/v1/D19-1053)\n[beddings and earth mover distance. In](https://doi.org/10.18653/v1/D19-1053) _Proceedings_\n\n_of the 2019 Conference on Empirical Methods in_\n_Natural Language Processing and the 9th Interna-_\n_tional Joint Conference on Natural Language Pro-_\n_cessing (EMNLP-IJCNLP)_, pages 563\u2013578, Hong\nKong, China. Association for Computational Linguistics.\n\n\n\n**A** **Examples from WikiEval**\n\n\nTables 2, 3 and 4 show examples from the WikiEval\ndataset, focusing in particular on answers with high\nand low faithfulness (Table 2), high and low answer\nrelevance (Table 3), and high and low context relevance (Table 4).\n\n\n**Question** **Context** **Answer**\n\n\n\nWho directed the film Oppenheimer and who stars\nas J. Robert Oppenheimer\nin the film?\n\n\n\nOppenheimer is a 2023 biographical thriller film written\nand directed by Christopher Nolan. Based on the 2005\nbiography American Prometheus by Kai Bird and Martin J. Sherwin, the film chronicles the life of J. Robert\nOppenheimer, a theoretical physicist who was pivotal in\ndeveloping the first nuclear weapons as part of the Manhattan Project, and thereby ushering in the Atomic Age.\nCillian Murphy stars as Oppenheimer, with Emily Blunt\nas Oppenheimer\u2019s wife Katherine \"Kitty\" Oppenheimer.\n\n\n\n_High Faithfulness_ : Christopher\nNolan directed the film Oppenheimer. Cillian Murphy stars as J.\nRobert Oppenheimer in the film.\n\n_Low_ _Faithfulness_ : James\nCameron directed the film Oppenheimer. Tom Cruise stars as J.\nRobert Oppenheimer in the film.\n\n\n\nTable 2: Example from WikiEval, showing answers with high and low faithfulness.\n\n\n**Question** **Answer**\n\n\n\nWhen is the scheduled\n\nlaunch date and time for\nthe PSLV-C56 mission,\nand where will it be\n\nlaunched from?\n\n\n\n_High answer relevance_ : The PSLV-C56 mission is scheduled to be launched on Sunday, 30\nJuly 2023 at 06:30 IST / 01:00 UTC. It", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_9000", "chunk_text": " will it be\n\nlaunched from?\n\n\n\n_High answer relevance_ : The PSLV-C56 mission is scheduled to be launched on Sunday, 30\nJuly 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space\nCentre, Sriharikota, Andhra Pradesh, India.\n\n_Low answer relevance_ : The scheduled launch date and time for the PSLV-C56 mission have\nnot been provided.The PSLV-C56 mission is an important space mission for India. It aims to\nlaunch a satellite into orbit to study weather patterns.\n\n\n\nTable 3: Example from WikiEval, showing answers with high and low answer relevance.\n\n\n**Question** **Context**\n\n\n\nWhen was the Chimnabai\nClock Tower completed,\nand who was it named af\nter?\n\n\n\n_High context relevance_ : The Chimnabai Clock Tower, also known as the Raopura Tower, is\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\nin 1896 and named in memory of Chimnabai I (1864\u20131885), a queen and the first wife of\nSayajirao Gaekwad III of Baroda State.\n\n_Low context relevance_ : The Chimnabai Clock Tower, also known as the Raopura Tower, is\na clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed\nin 1896 and named in memory of Chimnabai I (1864\u20131885), a queen and the first wife of\nSayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style.\nHistory. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai\nI (1864\u20131885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was\ninaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of\nGaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost\n\n- f 25,000 (equivalent to 9.2 million or USD 120,000 in", "token_count": 500, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2309.15217_ragas_es:chunk_9450", "chunk_text": " rule of\nGaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost\n\n- f 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).\n\n\n\nTable 4: Example from WikiEval, showing answers with high and low context relevance.\n\n\n", "token_count": 72, "metadata": {"arxiv_id": "2309.15217", "title": "Ragas: Automated Evaluation of Retrieval Augmented Generation", "authors": ["Shahul Es", "Jithin James", "Luis Espinosa-Anke", "Steven Schockaert"], "year": 2023, "url": "https://arxiv.org/pdf/2309.15217v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_0", "chunk_text": "## - TAKE A STEP BACK: EVOKING REASONING VIA AB\n\n### STRACTION IN LARGE LANGUAGE MODELS\n\n**Huaixiu Steven Zheng** _[\u2217]_ **Swaroop Mishra** _[\u2217]_ **Xinyun Chen** **Heng-Tze Cheng**\n**Ed H. Chi** **Quoc V Le** **Denny Zhou**\n\n\nGoogle DeepMind\n\n\nABSTRACT\n\n\nWe present STEP-BACK PROMPTING, a simple prompting technique that enables\nLLMs to do abstractions to derive high-level concepts and first principles from\ninstances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning\npath towards the solution. We conduct experiments of STEP-BACK PROMPTING\nwith PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM,\nKnowledge QA, and Multi-Hop Reasoning. For instance, STEP-BACK PROMPTING improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7%\nand 11% respectively, TimeQA by 27%, and MuSiQue by 7%.\n\n\n_The purpose of abstraction is not to be vague, but to create a new semantic level in which one can be_\n_absolutely precise. \u2014 Edsger W. Dijkstra_\n\n\n1 INTRODUCTION\n\n\nThe field of natural language processing (NLP) is witnessing a ground-breaking revolution because\n\n- f the Transformer-based (Vaswani et al., 2017) large language models (LLMs) (Devlin et al., 2018;\nRaffel et al., 2020; Brown et al., 2020; Anil et al., 2023). Scaling up the model size and pre-training\ncorpus (Hoffmann et al., 2022; Chowdhery et al., 2022) has brought remarkable improvement in model\ncapabilities and sample efficiency with insights from the scaling law (Kaplan et al., 2020; Hoffmann\net al., 2022), as well as emergent abilities (Wei et al., 2022a) such as multi-step reasoning (Wei et al.,\n2022b; Zhou et al., 2022) and instruction following (Mishra et al., 202", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_450", "chunk_text": " as well as emergent abilities (Wei et al., 2022a) such as multi-step reasoning (Wei et al.,\n2022b; Zhou et al., 2022) and instruction following (Mishra et al., 2022b; Wei et al., 2021).\n\n\nFigure 1: Strong Performance of STEP-BACK PROMPTING: our proposed Abstraction-and-Reasoning\nscheme leads to a substantial improvement in a wide range of challenging tasks in STEM, Knowledge\nQA and Multi-Hop Reasoning requiring complex (often multi-hop) reasoning.\n\n\n_\u2217_ Equal Contribution\n\n\n1\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nDespite the great advancements, complex multi-step reasoning remains challenging for even the state\n- f-the-art LLMs. Lightman et al. (2023) show that process-supervision with step-by-step verification\nis a promising remedy to improve the correctness of intermediate reasoning steps. Techniques such\nas Chain-of-Thought (Wei et al., 2022b) were introduced to produce a coherent series of intermediate\nreasoning steps to increase the success rate of following the right decoding path. Inspired by the\nfact that when faced with challenging tasks humans often step back and do abstractions to arrive at\nhigh-level principles to guide the process, we propose STEP-BACK PROMPTING to ground reasoning\n\n- n abstractions to reduce the chance of making errors in the intermediate reasoning steps.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of STEP-BACK PROMPTING with two steps of Abstraction and Reasoning\nguided by concepts and principles. _Top_ : an example of MMLU high-school physics (Hendrycks et al.,\n2020) where the first principle of Ideal Gas Law is retrieved via abstraction. _Bottom_ : an example\nfrom TimeQA (Chen et al., 2021) where the high-level concept of education history is a result of the\nabstraction. _Left_ : PaLM-2L (Anil et al., 2023) fails to answer the original question. Chain-of-Thought\nprompting (Wei et al., 2022b; Kojima et al., 2022) ran into errors during intermediate reasoning\nsteps (highlighted as red). _Right_ : PaLM-2L (Anil et al., 2023) successfully answers the question via", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_900", "chunk_text": "b; Kojima et al., 2022) ran into errors during intermediate reasoning\nsteps (highlighted as red). _Right_ : PaLM-2L (Anil et al., 2023) successfully answers the question via\nSTEP-BACK PROMPTING.\n\n\nAmong many of the cognitive skills, abstraction (Lachmy et al., 2022) is ubiquitous to humans\u2019\nability to process vast amounts of information and derive general principles. For example, Kepler\ncompressed thousands of measurements into Kepler\u2019s three laws of planetary motion, which precisely\ndescribe the orbits of planets around the Sun (Russell, 1964). In critical decision-making, humans\nfind abstraction to be helpful since it provides a broader view of the environment. This work explores\nhow LLMs can tackle complex tasks involving many low-level details through a two-step process\n\n- f abstraction-and-reasoning. The first step is to show LLMs how to step back through in-context\nlearning \u2013 prompting them to derive high-level abstractions such as concepts and principles for\na specific example. The second step is to leverage the reasoning ability to reason on top of the\nhigh-level concepts and principles. We use few-shot exemplar demonstrations to execute STEP-BACK\nPROMPTING on LLMs.\n\n\nWe experiment across a range of tasks involving domain specific reasoning such as Physics and Chemistry, knowledge-intensive question answering requiring factual knowledge, multi-hop commonsense\nreasoning. We observe significant performance improvements (up to 27%) in PaLM-2L (Anil et al.,\n\n\n2\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\n2023) demonstrating the efficacy of STEP-BACK PROMPTING in tackling complex tasks, which are\n\n- therwise challenging due to the amount of details needed for reasoning. Figure 1 shows a summary\n\n- f all the key results presented in this paper. Some the tasks are very challenging: both PaLM-2L and\nGPT-4 achieve only _\u223c_ 40% accuracy on TimeQA and MuSiQue. Chain-of-Thought prompting leads\nto a minor improvement on a few tasks, while STEP-BACK PROMPTING improves the performance\n\n- f PaLM-2L across the board: 7% and 11% on MMLU Physics and Chemistry, 27% on TimeQA,\nand 7% on MuSiQue", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_1350", "chunk_text": "-BACK PROMPTING improves the performance\n\n- f PaLM-2L across the board: 7% and 11% on MMLU Physics and Chemistry, 27% on TimeQA,\nand 7% on MuSiQue.\n\n\nWe conduct a variety of analyses and find that STEP-BACK PROMPTING leads to strong performance\nimprovements (up to 36%) over chain-of-thought (CoT) prompting (Wei et al., 2022b) and \u201ctake-adeep-breath\u201d (TDB) prompting (Yang et al., 2023). We perform a qualitative evaluation where we\nfind that Step-Back fixes a large portion of errors of the base model (up to _\u223c_ 40%) while introducing\na small portion of new errors (max _\u223c_ 12%). We also conduct an error analysis and find that majority\n\n- f the errors made by STEP-BACK PROMPTING is attributed to the intrinsic limitations of reasoning\ncapabilities of LLMs while abstraction skills are relatively easy to demonstrate to LLMs, pointing\n\n- ut the direction for future improvements of methods alike STEP-BACK PROMPTING.\n\n\n2 STEP-BACK PROMPTING\n\n\nSTEP-BACK PROMPTING is motivated by the observation that many tasks contain a lot of details,\nand it is hard for LLMs to retrieve relevant facts to tackle the task. As shown in the first example\n(top) in Figure 2, for a Physics question of \u201c _What happens to the pressure, P, of an ideal gas if the_\n_temperature is increased by a factor of 2 and the volume is increased by a factor of 8 ?_ \u201d, the LLM can\ndeviate from the first principle of Ideal Gas Law when reasoning directly on the question. Similarly, a\nquestion of \u201c _Estella Leopold went to which school between Aug 1954 and Nov 1954?_ \u201d is very hard to\naddress directly given the detailed time range constraint. In both cases, asking a step-back question\nhelps the model to solve the problem effectively.\n\n\nWe define a step-back question as a derived question from the original question at a higher level of\nabstraction. For instance, instead of directly asking \u201c _which school Estella Leopold went to during a_\n_specific period_ \u201d, a step-back question (Figure 2 bottom) would ask about the \u201c _education", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_1800", "chunk_text": " higher level of\nabstraction. For instance, instead of directly asking \u201c _which school Estella Leopold went to during a_\n_specific period_ \u201d, a step-back question (Figure 2 bottom) would ask about the \u201c _education history_ \u201d,\nwhich is a high-level concept encompasses the original question. Answering the step-back question\n\n- f \u201c _Estella Leopold\u2019s education history_ \u201d in this case will provide all the necessary information to\nreason about \u201c _which school Estella Leopold went to during a specific period_ \u201d. The premise is that\nthe step-back question is typically much easier. Grounding the reasoning on top of such abstractions\nhelps to avoid reasoning errors in the intermediate steps such as the example shown in Figure 2 (left)\nfrom Chain-of-Thought. In short, STEP-BACK PROMPTING consists two simple steps:\n\n\n- **Abstraction** : Instead of addressing the question directly, we first prompt the LLM to ask a generic\nstep-back question about a higher-level concept or principle, and retrieve relevant facts about the\nhigh-level concept or principle. The step-back question is unique for each task in order to retrieve\nthe most relevant facts.\n\n- **Reasoning** : Grounded on the facts regarding the high-level concept or principle, the LLM can\nreason about the solution to the original question. We term this as _Abstraction-grounded Reasoning_ .\n\n\nIn the following sections, we present an empirical study of STEP-BACK PROMPTING on a range of\nchallenging tasks covering STEM, Knowledge QA, and Multi-Hop Reasoning involving complex\nreasoning.\n\n\n3 EXPERIMENTAL SETUP\n\n\nHere we define the tasks and models we experiment with. We also describe our evaluation metric and\nthe baselines we consider.\n\n\n3.1 TASKS\n\n\nWe experiment with the following diverse tasks: (a) STEM, (b) Knowledge QA, and (c) Multi-Hop\nReasoning. We describe below the datasets we consider (see Appendix B for more details).\n\n\n3\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\n    - **STEM** : We evaluate MMLU and GSM8K for STEM tasks. MMLU (Hendrycks et al., 2020)\ncontains a series of benchmarks across diverse domains to evaluate the model\u2019s language\nunderstanding. We consider the high school physics", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_2250", "chunk_text": " MMLU and GSM8K for STEM tasks. MMLU (Hendrycks et al., 2020)\ncontains a series of benchmarks across diverse domains to evaluate the model\u2019s language\nunderstanding. We consider the high school physics and chemistry portions of MMLU\nbecause of the deep reasoning involved.\n\n    - **Knowledge QA** : We consider TimeQA (Chen et al., 2021) since it contains complex\nqueries that require challenging time-sensitive knowledge. We also experiment with SituatedQA (Zhang & Choi, 2021), another challenging open-retrieval QA dataset requiring the\nmodel to answer questions given temporal or geographical contexts.\n\n    - **Multi-Hop Reasoning** : We experiment with MuSiQue (Trivedi et al., 2022), a hard multihop\nreasoning dataset created via composable pairs of single-hop questions, and StrategyQA\n(Geva et al., 2021) with open-domain questions that demand some strategy to solve.\n\n\n3.2 MODELS\n\n\nWe use the following state-of-the-art LLMs: instruction-tuned PaLM-2L (Anil et al., 2023), GPT4 (OpenAI, 2023), and Llama2-70B (Touvron et al., 2023).\n\n\n3.3 EVALUATION\n\n\nConventional evaluation metrics such as accuracy, F1 score have limitations specifically for evaluating\nthe generations of state-of-the-art LLMs since these models often generate long-form answers which\nare hard to capture. We instead conduct an evaluation using the PaLM-2L model where we few-shot\nprompt the model to identify equivalence between target answers and the model predictions. Few-shot\nexamples, prompts and other details used for this evaluation are in Appendix C.\n\n\n3.4 BASELINE METHODS\n\n\n    - **PaLM-2L, PaLM-2L 1-shot** : PaLM-2L is either queried directly with the question or has a\nsingle demonstration exemplar of question-answer included in the prompt.\n\n    - **PaLM-2L + CoT, PaLM-2L + CoT 1-shot** : PaLM-2L model is queried with zero-shot\nCoT prompting (Kojima et al., 2022): \u201c _Let\u2019s think step by step_ \u201d is appended to the question.\nFor 1-shot, One demonstration example", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_2700", "chunk_text": " PaLM-2L model is queried with zero-shot\nCoT prompting (Kojima et al., 2022): \u201c _Let\u2019s think step by step_ \u201d is appended to the question.\nFor 1-shot, One demonstration example of a question and answer pair is provided in the\nprompt, where the answer is in the style of CoT (Wei et al., 2022b).\n\n    - **PaLM-2L + TDB** : Zero-shot prompting with \u201c _Take a deep breath and work on this problem_\n_step-by-step._ \u201d (Yang et al., 2023) prepended to the question.\n\n    - **PaLM-2L + RAG** : For Sections 5 and 6, we use retrieval-augmented generation (RAG)\nwhere the retrieved passage is used as context by the LLM.\n\n    - **GPT-4 and Llama2-70B** : we run GPT-4 and Llama2-70B on MMLU tasks for all methods.\nIn addition, we also run GPT-4 on all baselines for all tasks.\n\n\nWe do not use RAG for STEM tasks, because of the inherent reasoning nature of the tasks contrary to\nthe other fact-seeking datasets. All inferences are done using greedy decoding.\n\n\n4 STEM\n\n\nWe evaluate STEP-BACK PROMPTING on STEM tasks (Hendrycks et al., 2020) to gauge the efficacy\n\n- f our method on reasoning in highly specialized domains. We explain below our experimental setup,\nresult, and analysis of applying STEP-BACK PROMPTING on the MMLU high-school Physics and\nChemistry, and GSM8K benchmarks.\n\n\n4.1 STEP-BACK PROMPTING\n\n\nQuestions in the MMLU benchmarks require deeper reasoning. Furthermore, they also require\nunderstanding and application of formulae which are often physics and chemistry principles and\nconcepts. In this case, we first demonstrate to the model abstraction skills in the form of concepts\nand first principles such as _Newton\u2019s first law of motion_, _Doppler effect_, and _Gibbs free energy_ etc.\n\n\n4\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nTable 1: Strong performance of STEP-BACK PROMPTING on MMLU tasks across three model\nfamilies. CoT: zero-shot Chain of Thought prompting (Kojima et", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_3150", "chunk_text": " Enables Reasoning Via Abstraction in Large Language Models\n\n\nTable 1: Strong performance of STEP-BACK PROMPTING on MMLU tasks across three model\nfamilies. CoT: zero-shot Chain of Thought prompting (Kojima et al., 2022), TDB: Take a Deep\nBreath prompting (Yang et al., 2023).\n\n\nMethod MMLU Physics MMLU Chemistry\n\n\nPaLM-2L 66.4% (0.8%) 70.9% (0.9%)\nPaLM-2L 1-shot 64% (1.6%) 75.6% (0.4%)\nPaLM-2L + CoT 65% (2%) 75.3% (1.5%)\nPaLM-2L + CoT 1-shot 61.5% (1.8%) 76.6% (1%)\nPaLM-2L + TDB 65.7% (0.7%) 73.8% (1.1%)\nPaLM-2L + Step-Back (ours) **73.2%** (1.9%) **81.8** % (1.4%)\n\n\nGPT-4 69.4% (2.0%) 80.9% (0.7%)\nGPT-4 1-shot 78.4% (2.4%) 80.5% (1.6%)\nGPT-4 + CoT 82.9% (0.5%) 85.3% (1.0%)\nGPT-4 + CoT 1-shot 79.3% (1.0%) 82.8% (0.5%)\nGPT-4 + TDB 74.4% (4.0%) 81.5% (1.3%)\nGPT-4 + Step-Back (ours) **84.5%** (1.2%) **85.6** % (1.4%)\n\n\nLlama2-70B 51.9% (3.6%) 55.7% (2.1%)\nLlama2-70B 1-shot 57.3% (1.6%) 58.5% (2.5%)\nLlama2-70B + CoT 59.", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_3600", "chunk_text": "7% (2.1%)\nLlama2-70B 1-shot 57.3% (1.6%) 58.5% (2.5%)\nLlama2-70B + CoT 59.3% (2.0%) 64.1% (1.2%)\nLlama2-70B + CoT 1-shot 59.6% (2.0%) **68.1%** (1.4%)\nLlama2-70B + TDB 60.4% (2.1%) 63.6% (1.9%)\nLlama2-70B + Step-Back (ours) **64.8%** (1.5%) 66.7% (1.6%)\n\n\nThe implicit step-back question here is \u201c _what are the physics or chemistry principles and concepts_\n_involved in solving this task?_ \u201d. We provide demonstrations to the model to recite the relevant\nprinciples for solving the task from its own knowledge (see Appendix D.1 for few-shot exemplars).\n\n\n4.2 RESULTS\n\n\n\nTable 1 illustrates model performance across Accuracy\nvarious setups across three model families: 0.75\nPaLM-2L, GPT-4, and Llama2-70B. Average ac- 0.74\ncuracy over 5 evaluation runs is reported along\n\n0.73\n\nwith standard deviations (in the parentheses).\n\n0.72\n\nPaLM-2L baseline performance is 66 _._ 4% and\n70.9% on Physics and Chemistry, respectively. 0.71\nWe find that CoT and TDB zero-shot prompt- 0.70\n\n1 2 3 4 5\n\ning do not significantly increase model performance, which could be due to the inherent dif\nNumber of Shots\n\nficulty and deep reasoning associated with these\ntasks. PaLM-2L 1-shot and PaLM-2L + CoT\n1-shot do not improve against the baseline much Figure 3: Ablation study of STEP-BACK PROMPTeither, highlighting the challenge of demonstrat- ING accuracy using PaLM-2L on MMLU highing the reasoning steps to the model. In con- school Physics against the number of few shot\ntrast, STEP-BACK PROMPTING significantly im- exempl", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_4050", "chunk_text": " demonstrat- ING accuracy using PaLM-2L on MMLU highing the reasoning steps to the model. In con- school Physics against the number of few shot\ntrast, STEP-BACK PROMPTING significantly im- exemplars: robust performance with respect to a\nproves model performance: +7% and +11% varying number of shots.\ncompared to PaLM-2L. Similarly, with GPT4 and Llama2-70B models, STEP-BACK PROMPTING is very competitive among all the baseline\nmethods we tested, showing that STEP-BACK PROMPTING is model-agnostic. We present the results\n\n- f GSM8K in Appendix A.1.\n\n\n\nAccuracy\n\n\n\n0.75\n\n\n0.74\n\n\n0.73\n\n\n0.72\n\n\n0.71\n\n\n0.70\n\n\n\n1 2 3 4 5\n\n\n\nNumber of Shots\n\n\n\nFigure 3: Ablation study of STEP-BACK PROMPTING accuracy using PaLM-2L on MMLU highschool Physics against the number of few shot\nexemplars: robust performance with respect to a\nvarying number of shots.\n\n\n\n4.3 ABLATION AND ANALYSIS\n\n\n**Few-shot Ablation** : First, in Figure 3, we observe that STEP-BACK PROMPTING is robust to the\nnumber of few-shot exemplars of (question, principles) pairs used as demonstrations. Adding more\n\n\n5\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nTable 2: Strong performance of STEP-BACK PROMPTING on Knowledge QA tasks. CoT: Chain of\nThought prompting, TDB: Take a Deep Breath prompting, RAG: retrieval-augmented generation.\nSTEP-BACK PROMPTING results in significant performance improvements.\n\n\nMethod TimeQA TQA Easy TQA Hard SituatedQA\n\n\nPaLM-2L 41.5% 42.6% 40.4% 54.3% (0.3%)\nPaLM-2L 1-shot 40.7% 41.7% 39.1% 51.8% (0.6%)\nPaLM-2L + CoT 40.8% 41.8% 39.8% 56.4% (0.2%)\nPaLM-2L + CoT 1-shot 38.", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_4500", "chunk_text": "%)\nPaLM-2L + CoT 40.8% 41.8% 39.8% 56.4% (0.2%)\nPaLM-2L + CoT 1-shot 38.1% 39.3% 36.8% 54% (0.8%)\nPaLM-2L + TDB 40.9% 42.6% 39.1% 54% (0.5%)\nPaLM-2L + RAG 57.4% 67.8% 46.8% 59.3% (0.4%)\nPaLM-2L + Step-Back (ours) 66% 70.4% 61.6% 57.5% (0.3%)\nPaLM-2L + Step-Back + RAG (ours) **68.7%** **75.2%** **62.3%** 61% (0.4%)\n\n\nGPT-4 45.6% 48.9% 42.6% **63.2%** (0.4%)\n\n\ndemonstration examples beyond a single example does not lead to further improvements. This\nindicates that the task of retrieving the relevant principles and concepts is relatively easy through\nin-context learning and a single demonstration suffices. Therefore, we use a single exemplar for\nfew-shot prompting throughout the paper except the ablation studies.\n\n\n**Error Analysis** : Comparing the predictions of STEP-BACK PROMPTING to the baseline PaLM-2L\nmodel for MMLU high-school Physics: we find that STEP-BACK PROMPTING corrects 20 _._ 5% errors\nfrom the baseline while introducing 11 _._ 9% errors.\n\n\nTo further understand where the errors come from in STEP-BACK PROMPTING, we annotate all the\nwrong predictions of STEP-BACK PROMPTING in the test set, and categorize them into 5 classes (see\nAppendix E.1 for examples in each class):\n\n\n- **Principle Error** : The error happens at the step of Abstraction, where the first principles generated\nby models are wrong or incomplete.\n\n- **Factual Error** : There is at least one factual error when the model recites its own factual knowledge\n\n- **Math Error** : There is", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_4950", "chunk_text": " step of Abstraction, where the first principles generated\nby models are wrong or incomplete.\n\n- **Factual Error** : There is at least one factual error when the model recites its own factual knowledge\n\n- **Math Error** : There is at least one math error in the intermediate steps when math calculations are\ninvolved in deriving the final answer.\n\n- **Context Loss** : There is at least one error where the model response loses context from the question,\nand deviates from addressing the original question\n\n- **Reasoning Error** : We define Reasoning Error as when the model makes at least one error in the\nintermediate Reasoning steps before arriving at the final answer.\n\n\nAll five types of errors are happening during the\nReasoning step except _Principle Error_ which\npoints to the failure of the Abstraction step. As\nshown in Figure 4 (right), _Principle Error_ comprises only a small fraction of the errors the\nmodel makes: more than 90% of the errors happen at the Reasoning step. Among the four error types during Reasoning, _Reasoning Error_\nand _Math Error_ are the major error categories.\nThis corroborates with the finding in the ablation study above that very few exemplars are\nneeded to demonstrate to LLMs the Abstraction skill. Reasoning step is still the bottleneck Figure 4: Error Analysis of STEP-BACK PROMPT\n- f how well STEP-BACK PROMPTING can per- ING on MMLU high-school Physics: five classes\nform tasks such as MMLU requiring complex - f errors Step-Back makes with Reasoning being\nreasoning. For MMLU Physics specifically, the the dominating class.\nReasoning and Math skills are critical for solving the problems successfully: even if the first principles are retrieved correctly, deep reasoning and\nmath are involved to derive a correct final answer through a typical multi-step reasoning process.\n\n\n6\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\n5 KNOWLEDGE QA\n\n\nWe evaluate STEP-BACK PROMPTING on question-answering benchmarks requiring intensive factual\nknowledge. Knowledge QA has been challenging for LLMs. In this section, we first describe the\nexperimental setup, followed by results and analysis on STEP-BACK PROMPTING.\n\n\n5.1 STEP-BACK PROMPTING\n\n\nWe evaluate STEP-BACK PROMPTING on TimeQA (Chen et al", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_5400", "chunk_text": " this section, we first describe the\nexperimental setup, followed by results and analysis on STEP-BACK PROMPTING.\n\n\n5.1 STEP-BACK PROMPTING\n\n\nWe evaluate STEP-BACK PROMPTING on TimeQA (Chen et al., 2021) and SituatedQA (Zhang &\nChoi, 2021) in the Knowledge QA category. We first show the LLMs how to do Abstraction through\nin-context demonstrations. The step-back question \u201c _What was Estella Leopold\u2019s education history_ \u201d\nin Figure 2 is generated by the LLM through few-shot demonstrations (see Appendix D.2 for details).\nGiven the knowledge-intensive nature of these queries, we use retrieval augmentation (RAG) in\ncombination with STEP-BACK PROMPTING. The step-back question is used to retrieve relevant facts,\nwhich work as additional context (see Table 14 for the prompt) to ground the final reasoning step.\n\n\n5.2 RESULTS\n\n\nWe evaluate the models on the test set of TimeQA. As shown in Table 2, the baseline models of\nGPT-4 and PaLM-2L achieved 45 _._ 6% and 41 _._ 5%, highlighting the difficulty of the task. Applying\neither CoT or TDB zero-shot (and one-shot) prompting to the baseline model shows no improvement.\nIn contrast, augmenting the baseline model by regular retrieval augmentation (RAG) improves the\naccuracy to 57 _._ 4%, highlighting the fact-intensive nature of the task. The result of Step-Back + RAG\nshows the effectiveness of going back to a high-level concept, which enables much more reliable\nretrieval augmentation: the accuracy on TimeQA achieves a remarkable 68 _._ 7%.\n\n\nNext, we segment TimeQA into the Easy and Hard difficulty levels provided in the original dataset. As\nexpected, all methods perform worse on the Hard subset. While RAG can improve the Easy accuracy\nfrom 42 _._ 6% to 67 _._ 8%, the improvement is much smaller on the Hard accuracy: 40 _._ 4% to 46 _._ 8%. This\nis where STEP-BACK PROMPTING shines by retrieving facts regarding high-level concepts to ground\nthe final reasoning: Step-Back + RAG further improves the Hard accuracy to 62 _._ 3%, outperforming\nGPT", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_5850", "chunk_text": " This\nis where STEP-BACK PROMPTING shines by retrieving facts regarding high-level concepts to ground\nthe final reasoning: Step-Back + RAG further improves the Hard accuracy to 62 _._ 3%, outperforming\nGPT-4\u2019s 42 _._ 6% from GPT-4. We hypothesize that facts regarding the high-level concepts (such as\n_education history_ ) are much more accessible than the low-level details.\n\n\nOn the SituatedQA benchmark, we observe a moderate quality gain from 54 _._ 3% to our best method\n\n- f Step-Back + RAG (61%) with a small gap to GPT-4\u2019s 63 _._ 2%. Similar to TimeQA, prompting\ntechniques such as CoT and TDB don\u2019t help significantly for SituatedQA.\n\n\nFigure 5: Ablation and error analysis of STEP-BACK PROMPTING on TimeQA. _Left_ : ablation against\nthe number of few-shot exemplars. _Right_ : four classes of errors Step-Back makes with Reasoning\nand RAG being the dominant error sources.\n\n\n5.3 ABLATION AND ANALYSIS\n\n\n**Few-shot Ablation** : We observe in Figure 5 (left) that the performance of STEP-BACK PROMPTING\n\n- n TimeQA is robust to the number of exemplars used in demonstration, highlighting again the\nsample efficiency of in-context learning Abstraction skills for models like PaLM-2L.\n\n\n7\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nTable 3: Results of STEP-BACK PROMPTING on Multi-Hop Reasoning. CoT: Chain of Thought\nprompting, TDB: Take a Deep Breath prompting, RAG: retrieval augmentation generation. The\naverage accuracy is over 5 evaluation runs with the standard deviations included in the parentheses.\n\n\nMethod MuSiQue StrategyQA\n\n\nPaLM-2L 35.5% (3%) 82.8% (0.7%)\nPaLM-2L 1-shot 29.0% (0.5%) 76.6% (0.5%)\nPaLM-2L + CoT 38.7% (3.2%) 83.6% (0.4%)\nPaLM-2L + CoT 1-shot 38.5% (", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_6300", "chunk_text": ".5%)\nPaLM-2L + CoT 38.7% (3.2%) 83.6% (0.4%)\nPaLM-2L + CoT 1-shot 38.5% (2.2%) 76.8% (1.4%)\nPaLM-2L + TDB 39.0% (2.3%) 82.7% (0.9%)\nPaLM-2L + RAG 39.6% (2.8%) 84.2% (0.5%)\nPaLM-2L + Step-Back (ours) 42.6% (3.1%) 82.7% (0.4%)\nPaLM-2L + Step-Back + RAG (ours) **42.8** % (2.0%) **86.4%** (1%)\n\n\nGPT-4 38.5% (0.2%) 78.3% (1.1%)\n\n\n**Error Analysis:** Figure 5 (right) shows the breakdown of all the remaining errors made by STEPBACK PROMPTING on TimeQA. Similar to Section 4.3, we categorize the errors into\n\n\n- **StepBack** : The step-back question generated is not helpful in solving the task.\n\n- **RAG** : RAG fails to retrieve relevant information despite that the step-back question is on target.\n\n- **Scoring Error** : The evaluation by the judge model made a mistake.\n\n- **Reasoning Error** : The retrieved context is relevant, but the model still fails to reason through the\ncontext to arrive at the right answer.\n\n\nWe find that the StepBack rarely fails. In contrast, we find more than half of the errors are due to\nreasoning errors. Additionally, 45% of errors are due to failure in retrieving the right information\ndespite that Abstraction provided by step-back makes it a much easier task. This reflects the difficulty\nlevel of the TimeQA task. Additional error analysis of TimeQA is in Appendix A.\n\n\n6 MULTI-HOP REASONING\n\n\nWe evaluate STEP-BACK PROMPTING on challenging Multi-Hop reasoning benchmark MuSiQue\n(Trivedi et al., 2022) and StrategyQA (Geva et al., 2021). We follow the same protocol as Section 5\nto implement STEP-B", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_6750", "chunk_text": "ING on challenging Multi-Hop reasoning benchmark MuSiQue\n(Trivedi et al., 2022) and StrategyQA (Geva et al., 2021). We follow the same protocol as Section 5\nto implement STEP-BACK PROMPTING.\n\n\nTable 3 shows performance of various baselines on the dev set of MuSiQue and StrategyQA.\nBaseline performance of PaLM-2L and GPT-4 are low (35 _._ 5% and 38 _._ 5% for PaLM-2L and GPT-4\nrespectively) in MuSiQue since it is a hard multihop reasoning benchmark. In contrast, StrategyQA\nhas stronger baselines (82 _._ 8% and 78 _._ 3% for PaLM-2L and GPT-4 respectively) probably because\nit is a binary classification task. CoT and TDB improve model performance a bit in the case of\n\n_\u223c_\nMuSiQue ( 3% and 3.5% respectively) which can be attributed to the inherent reasoning nature\n\n- f this task where these methods are shown to be helpful. In the case of StrategyQA, there is no\nsignificant performance gain with CoT and TDB which could be due to the high baseline performance\nin this task, with limited scope for these prompting methods to improve performance. Often, 1-shot\nperformance is significantly lower than their zero-shot methods, which could be attributed to potential\nexample bias (Zhao et al., 2021; Parmar et al., 2023). RAG improves model performance ( _\u223c_ 4%\nand 2% for MuSiQue and StrategyQA respectively.) STEP-BACK PROMPTING with the power\n\n- f abstraction produces the best performance of all methods: 42 _._ 8% in MuSiQue and 86 _._ 4% in\nStrategyQA, significantly outperforming GPT-4 on both tasks. We present a detailed error analysis\n\n- n StrategyQA in Appendix A.3.\n\n\n7 DISCUSSION\n\n\nAbstraction helps humans to solve complex tasks by removing irrelevant details and distilling highlevel concepts and principles to guide the problem-solving process. STEP-BACK PROMPTING breaks\n\n\n8\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ncomplex tasks such as knowledge-intensive QA, multi-hop reasoning,", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_7200", "chunk_text": " highlevel concepts and principles to guide the problem-solving process. STEP-BACK PROMPTING breaks\n\n\n8\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ncomplex tasks such as knowledge-intensive QA, multi-hop reasoning, and science questions into two\nseparate steps of Abstraction and Reasoning. We demonstrate through empirical experiments that\nAbstraction is an easy skill for the LLMs such as PaLM-2L via sample-efficient in-context learning.\nGrounding on the high-level concepts and principles, LLMs can leverage their intrinsic Reasoning\ncapabilities to derive the solution. This reduces the chance of reasoning failures in the intermediate\nsteps and is shown to improve the performance on a wide range of complex reasoning tasks. Despite\nthe success, through error analysis, we find that Reasoning is still one of the hardest skills for LLMs\nto acquire: it is still the dominant failure mode even after the large reduction of task complexity by\nSTEP-BACK PROMPTING.\n\n\nNevertheless, Abstraction is neither necessary nor possible in all scenarios. For instance, the task\ncan be as simple as _who was the president of the United States in 2000?_, in which case there is\nno such need to step back and ask a high-level question as the answer to such questions is readily\navailable. Questions such as _what is the speed of light?_ point to the first principles themselves. Doing\nAbstraction in this case would not make a difference either.\n\n\n8 RELATED WORK\n\n\n8.1 PROMPTING\n\n\nFew-shot prompting (Brown et al., 2020; Liu et al., 2023; Mishra et al., 2022a; Wei et al., 2022b)\nhas significantly improved model performance across a range of tasks without requiring updating\nany model parameters. Our work STEP-BACK PROMPTING is in the same category as the chain-ofthought prompting (Wei et al., 2022b) and scratchpad (Nye et al., 2021) owing to its simplicity and\ngeneric nature. But our approach is focused on the key idea of abstraction which is inspired from the\nfact that taking a step back often helps humans in performing complex tasks. Our work is also related\nto the recitation-augmented language models (Sun et al., 2022); however in contrast to their work, we\nexplicitly perform step-back and abstraction, with optional use", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_7650", "chunk_text": " humans in performing complex tasks. Our work is also related\nto the recitation-augmented language models (Sun et al., 2022); however in contrast to their work, we\nexplicitly perform step-back and abstraction, with optional use of retrieval augmentation depending\n\n- n the nature of the task at hand.\n\n\n8.2 DECOMPOSITION\n\n\nDecomposing a task into simpler tasks and solving these tasks to complete the original task has\nbeen an effective way (Zhou et al., 2022; Patel et al., 2022; Khot et al., 2022; Press et al., 2022) to\nimprove model performance on complex tasks. Several prompting methods have been successful in\nthis regard. Our work STEP-BACK PROMPTING, in contrast, is on making the question more abstract\nand high-level, which is different from decomposition that is often a low-level breakdowns of the\n\n- riginal question. For instance, a generic question for _which employer did Steve Jobs work for in_\n_1990?_ could be _what is the employment history of Steve Jobs?_ While decomposition would lead\nto sub-questions such as _What was Steve Jobs doing in 1990?_, _Was Steve Jobs employed in 1990?_\nand _If Steve Jobs was employed, who was his employer?_ Furthermore, abstract questions such as\n_what is the employment history of Steve Jobs?_ are often generic in nature to have a many-to-one\nmapping since many questions (e.g. _which employer did Steve Jobs work for in 1990?_ and _which_\n_employer did Steve Jobs work for in 2000?_ ) can have the same abstract question. This is in contrast\nto decomposition where there is often a one-to-many mapping since there are multiple decomposed\nsub-problems necessary to solve a given question.\n\n\n9 CONCLUSION\n\n\nWe introduce STEP-BACK PROMPTING as a simple yet generic method to elicit deep reasoning via\nabstraction in large language models. Experimentation on LLMs across fact-seeking, commonsense\nreasoning and domain-specific reasoning benchmarks shows that STEP-BACK PROMPTING significantly improves model performance. We hypothesize that abstraction helps models to hallucinate\nless and reason better, probably reflecting the true nature of the model which are often hidden\nwhile responding to the original question without abstraction. We hope our work will inspire more\n", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_8100", "chunk_text": " improves model performance. We hypothesize that abstraction helps models to hallucinate\nless and reason better, probably reflecting the true nature of the model which are often hidden\nwhile responding to the original question without abstraction. We hope our work will inspire more\nhuman-inspired approaches to elicit the hidden potential of large language models.\n\n\n9\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nACKNOWLEDGEMENT\n\n\nWe thank Andrew Dai, Adams Yu and Hannah Rashkin of Google DeepMind for their insightful\nfeedback on this paper.\n\n\nREFERENCES\n\n\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. _arXiv_\n_preprint arXiv:2305.10403_, 2023.\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. _Advances in neural information processing systems_, 33:1877\u20131901, 2020.\n\n\nWenhu Chen, Xinyi Wang, and William Yang Wang. A dataset for answering time-sensitive questions.\n_arXiv preprint arXiv:2108.06314_, 2021.\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.\n\n\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies. _Transactions of_\n_the Association for", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_8550", "chunk_text": "va, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies. _Transactions of_\n_the Association for Computational Linguistics_, 9:346\u2013361, 2021.\n\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint_\n_arXiv:2009.03300_, 2020.\n\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.\n\n\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\n_arXiv preprint arXiv:2001.08361_, 2020.\n\n\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish\nSabharwal. Decomposed prompting: A modular approach for solving complex tasks. _arXiv_\n_preprint arXiv:2210.02406_, 2022.\n\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. _Advances in neural information processing systems_, 35:\n22199\u201322213, 2022.\n\n\nRoyi Lachmy, Valentina Pyatkin, Avshalom Manevich, and Reut Tsarfaty. Draw me a flower:\nProcessing and grounding abstraction in natural language. _Transactions of the Association for_\n_Computational Linguistics_, 2022.\n\n\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever,", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_9450", "chunk_text": " Swaroop Mishra, Mor Geva, and Chitta Baral. Don\u2019t blame the annotator: Bias\nalready starts in the annotation instructions. In _Proceedings of the 17th Conference of the European_\n_Chapter of the Association for Computational Linguistics_, pp. 1771\u20131781, 2023.\n\n\nPruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. Is a question decomposition unit\nall we need? In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language_\n_Processing_, pp. 4553\u20134569, 2022.\n\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. _arXiv preprint arXiv:2210.03350_,\n2022.\n\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. _The Journal of Machine Learning Research_, 21(1):5485\u20135551, 2020.\n\n\nJohn L Russell. Kepler\u2019s laws of planetary motion: 1609\u20131666. _The British journal for the history of_\n_science_, 2(1):1\u201324, 1964.\n\n\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\nmodels. _arXiv preprint arXiv:2210.01296_, 2022.\n\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, H", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_9900", "chunk_text": "u, Jude Fernandes, Jeremy Fu,\nWenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023.\n\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop\nquestions via single-hop question composition. _Transactions of the Association for Computational_\n_Linguistics_, 10:539\u2013554, 2022.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing_\n_systems_, 30, 2017.\n\n\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint_\n_arXiv:2109.01652_, 2021.\n\n\n11\n\n\nStep-Back Prompting Enables Reasoning", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_10350", "chunk_text": " M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint_\n_arXiv:2109.01652_, 2021.\n\n\n11\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.\n_arXiv preprint arXiv:2206.07682_, 2022a.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in_\n_Neural Information Processing Systems_, 35:24824\u201324837, 2022b.\n\n\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen.\nLarge language models as optimizers. _arXiv preprint arXiv:2309.03409_, 2023.\n\n\nMichael Zhang and Eunsol Choi. Situatedqa: Incorporating extra-linguistic contexts into qa. In\n_Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pp.\n7371\u20137387, 2021.\n\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving\nfew-shot performance of language models. In _International Conference on Machine Learning_, pp.\n12697\u201312706. PMLR, 2021.\n\n\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,\u00a8\nClaire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning\nin large language models. _arXiv preprint arXiv:2205.10625_, 2022.\n\n\n12\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nTable 4: Step-Back Prompting on", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_10800", "chunk_text": " models. _arXiv preprint arXiv:2205.10625_, 2022.\n\n\n12\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nTable 4: Step-Back Prompting on GSM8K. CoT: zero-shot Chain of Thought prompting (Kojima\net al., 2022), TDB: Take a Deep Breath prompting (Yang et al., 2023), DP: Decomposed Prompting\n(Khot et al., 2022). The Table reports the average accuracy over 5 evaluation runs, with standard\ndeviations in the parentheses.\n\n\nMethod GSM8K\n\n\nPaLM-2L 75.8% (0.2%)\nPaLM-2L 1-shot **84.5%** (0.4%)\nPaLM-2L + CoT **84.4%** (0.2%)\nPaLM-2L + CoT 1-shot 81% (0.2%)\nPaLM-2L + TDB 82.2% (0.2%)\nPaLM-2L + DP 82.2% (0.08%)\n\n\nPaLM-2L + Step-Back (ours) **84.3%** (0.2%)\n\n\nA GSM8K RESULTS, AND ERROR ANALYSIS\n\n\nA.1 GSM8K RESULTS\n\n\nWe present in Table 4 the results of STEP-BACK PROMPTING on GSM8K along with other strong\nbaselines from PaLM-2L runs. We observe that STEP-BACK PROMPTING achieved competitive\nperformance together with zero-shot CoT and 1-shot standard prompting. We hypothesize that\nthe simplicity of principles (e.g. addition, subtraction, etc.) in GSM8K makes it not absolutely\nnecessary to retrieve the principles first before reasoning. Nonetheless, we still find that STEP-BACK\nPROMPTING is the most competitive among all the prompting methods we tested, including the \u201cTake\na Deep Breath\u201d prompting optimized for GSM8K in Yang et al. (2023) and Decomposed Prompting\nin Khot et al. (2022).\n\n\nA.2 TIMEQA ERROR ANALYSIS\n\n\nWe conduct error analysis to understand where STEP-BACK PROMPTING fixes the errors the baseline\nmodels make. Figure 6 shows that compared to the predictions of baseline PaLM-2L", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_11250", "chunk_text": "2022).\n\n\nA.2 TIMEQA ERROR ANALYSIS\n\n\nWe conduct error analysis to understand where STEP-BACK PROMPTING fixes the errors the baseline\nmodels make. Figure 6 shows that compared to the predictions of baseline PaLM-2L, STEP-BACK\nPROMPTING can fix 39 _._ 9% of the predictions where the baseline prediction is wrong, while causing\n5 _._ 6% errors.Furthermore, Step-Back + RAG fixes 21 _._ 6% errors coming from RAG. The % of errors\nintroduced by STEP-BACK PROMPTING to RAG is still relatively low (6 _._ 3%). Together, this shows\nthat the STEP-BACK PROMPTING is helpful most of the time, signifying the need and effectiveness\n\n- f doing Abstraction before directly addressing the original question.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Error Analysis of Step-Back Prompting on TimeQA. _Left_ : Step-Back + RAG vs Baseline\npredictions. _Right_ : Step-Back RAG vs RAG predictions. Step-Back + RAG can fix 39 _._ 9% of\nthe predictions where the baseline prediction is wrong while causing 5 _._ 6% errors. Furthermore,\nStep-Back + RAG fixes 21 _._ 6% errors coming from RAG. The % of errors introduced by STEP-BACK\nPROMPTING to RAG is still relatively low (6 _._ 3%).\n\n\n13\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nTable 5: Stats of the evaluation datasets used in this paper.\n\n\nDomain Dataset Split Number of Examples\n\n\nSTEM MMLU high-school Physics Test 151\nMMLU high-school Chemistry Test 203\nGSM8K Test 1319\n\n\nKnowledge QA TimeQA Test 5226\nTimeQA Easy Test 2613\nTimeQA Hard Test 2613\nSituatedQA Test 2901\n\n\nMulti-hop Reasoning MuSiQue Dev 2417\nStrategyQA Dev 229\n\n\nA.3 STRATEGYQA ERROR ANALYSIS\n\n\nFigure 7 shows the error analysis of StrategyQA on the predictions of Step-Back + RAG against the\nbaseline model and the raw retrieval augmentation variant of PaLM-2L. Compared to the baseline,\nStep-Back + RAG can turn 15 _._ 4%", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_11700", "chunk_text": " StrategyQA on the predictions of Step-Back + RAG against the\nbaseline model and the raw retrieval augmentation variant of PaLM-2L. Compared to the baseline,\nStep-Back + RAG can turn 15 _._ 4% wrong predictions into correct predictions, while leading to 6 _._ 1%\nerrors the other way around. Furthermore, Step-Back + RAG fixes 12 _._ 7% errors coming from RAG.\nThe errors introduced to RAG by Step-Back are just 4 _._ 4%.\n\n\nB DATASET DETAILS\n\n\nTable 5 shows the split and number of examples used for evaluations in TimeQA, StrategyQA,\nMMLU, and GSM8K.\n\n\nC EVALUATION DETAILS\n\n\nC.1 FEW-SHOT EXAMPLES FOR EVALUATION WITH PALM-2L\n\n\nGiven the model free-form outputs and the target label, we use one positive and one negative output\nas few-shot examples to demonstrate to the scoring model how to score the output. Table 6 illustrates\nthe prompt we used for the scoring model. We parse out the \u201cYes\u201d or \u201cNo\u201d answer from the scoring\nmodel output as a TRUE or FALSE score of the model output.\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Error Analysis of Step-Back Prompting on StrategyQA. _Left_ : Step-Back + RAG vs Baseline\npredictions. _Right_ : Step-Back + RAG vs RAG predictions. Step-Back + RAG is able to turn\n15 _._ 4% wrong predictions into correct predictions, while leading to 6 _._ 1% errors the other way around.\nFurthermore, Step-Back + RAG fixes 12 _._ 7% errors coming from RAG. The errors introduced to RAG\nby Step-Back are just 4 _._ 4%.\n\n\n14\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nAre the following two answers to the given question equivalent? Do not\nconsider whether the answers are right or wrong, but only whether they\nare equivalent. Directly state \u201dYes\u201d or \u201dNo\u201d.\n**Question** : Which title was conferred to Anna Muzychuk in 2007?\n**Answer 1** : Anna Muzychuk was conferred the title of International\nMaster (IM) in 2007. She earned the title by scoring three norms in rapid\nchess", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_12150", "chunk_text": " conferred to Anna Muzychuk in 2007?\n**Answer 1** : Anna Muzychuk was conferred the title of International\nMaster (IM) in 2007. She earned the title by scoring three norms in rapid\nchess tournaments.\n\n**Answer 2** : International Master\n**Answer 1 (short)** : International Master\n**Answer 2 (short)** : International Master\n**Are the two answers equivalent?** Yes\n**Question** : What state is Seattle located in?\n**Answer 1** : Seattle is in Washington State.\n**Answer 2** : The answer is George Washington.\n**Answer 1 (short)** : Washington State\n**Answer 2 (short)** : George Washington\n**Are the two answers equivalent?** No\n**Question** : _<_ Question _>_\n**Answer 1** : _<_ Model Output _>_\n**Answer 2** : _<_ Target Label _>_\n\n\nTable 6: Illustration of few shot evaluation with the PaLM-2L model.\n\n\nMMLU Physics/Chemistry First-Principle Prompt\n\n\nYou are an expert at Physics/Chemistry. You are given\na Physics/Chemistry problem. Your task is to extract the\nPhysics/Chemistry concepts and principles involved in solving\nthe problem. Here are a few examples:\n\n\nQuestion: _<_ Question Example1 _>_\nPrinciples Involved: _<_ Principles Example1 _>_\n\n...\nQuestion: _<_ Question Example5 _>_\nPrinciples Involved: _<_ Principles Example5 _>_\nQuestion: _<_ Question _>_\nPrinciples Involved:\n\n\nTable 7: Prompt of extracting the underlying principles involved in MMLU physics and chemistry\nquestions.\n\n\nC.2 HYPER-PARAMETERS FOR EVALUATION WITH PALM-2L\n\n\nWe use PaLM-2L as the scoring model for evaluation. We experiment with different sampling\ntemperatures, and find that _T_ = 1 gives us a highly-accurate evaluation. For example, we sampled\n100 test examples and the model predictions, and manually rated the correctness of the model scoring.\nWe found that out of 4 trials, the model scoring agrees with human ratings 97%, 98%, 99% and 99%\n\n- f the time.\n\n\nD PROMPTS AND FEW SHOT EXAMPLES\n\n\nD.1 STEM\n\n\nFor MMLU high", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_12600", "chunk_text": " 4 trials, the model scoring agrees with human ratings 97%, 98%, 99% and 99%\n\n- f the time.\n\n\nD PROMPTS AND FEW SHOT EXAMPLES\n\n\nD.1 STEM\n\n\nFor MMLU high-school Physics and Chemistry, we first prompt the model to generate the first\nprinciples behind the question. Using the generated first principles, we further prompt the model to\ngenerate the final answer through few-shot demonstrations The prompt generating first principles is\nshown in Table 7 for MMLU high-school Physics and Chemistry.\n\n\n15\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nMMLU Physics/Chemistry Final Answer Prompt\n\n\nYou are an expert at Physics/Chemistry. You are given a\nPhysics/Chemistry problem and a set of principles involved in\nsolving the problem. Solve the problem step by step by following the\nprinciples. Here are a few examples:\n\n\nQuestion: _<_ Question Example1 _>_\nPrinciples: _<_ Principles Example1 _>_\nAnswer: _<_ Answer Example1 _>_\n\n...\nQuestion: _<_ Question Example5 _>_\nPrinciples: _<_ Principles Example5 _>_\nAnswer: _<_ Answer Example5 _>_\nQuestion: _<_ Question _>_\nPrinciples: _<_ Principles _>_\nAnswer:\n\n\nTable 8: Prompt of querying the model for final answer with first principles behind the question in\nMMLU high-school Physics and Chemistry.\n\n\nAfter extracting the first principles of solving a particular question, we formulate the prompt in\nTable 8 to query the model for the final answer.\n\n\nTables 9-10 show one demonstration exemplar of Question-Principles-Answer triplets for MMLU\nhigh-school Physics and Chemistry, respectively. For GSM8K, given the simplicity of the principles,\nwe directly combine the principles and the solution in the demonstration exemplar. Table 11 shows\nthe exemplar we used in the paper.\n\n\nD.2 KNOWLEDGE QA\n\n\nWe use the following prompting in Table 12 to demonstrate to the LLM on asking a step-back question\nfor TimeQA and SituatedQA including up to 5 exemplar demonstrations of pairs of Original Question\nand Step-back Question.\n\n\nTable 13 shows 5 exemplars from the Train split of TimeQA and SituatedQA as demonstrations of\nasking step-back questions.\n\n\nThe step-back", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_13050", "chunk_text": " including up to 5 exemplar demonstrations of pairs of Original Question\nand Step-back Question.\n\n\nTable 13 shows 5 exemplars from the Train split of TimeQA and SituatedQA as demonstrations of\nasking step-back questions.\n\n\nThe step-back question is extracted from the model output using the prompt. Using the step-back\nquestion, we do retrieval augmentation. Using both the retrieval augmentations from the original\nquestion and the step-back question, we formulate the final prompt to query the model for the final\nanswer, as shown in Table 14.\n\n\nD.3 MULTI-HOP REASONING\n\n\nFor Multi-Hop Reasoning, we use the same prompting template as in Knowledge QA to ask the\nstep-back question, and query for the final answer given the retrieval augmentations. Table 15 shows\n5 demonstration exemplars for asking step-back questions from the Train split of MuSiQue and\nStrategyQA.\n\n\nD.4 BASELINE PROMPTS\n\n\nFor standard zero-shot and few-shot prompting of the baseline model, we formulate the prompt using\nthe template in Table 16 with up to 1 exemplars.\n\n\nD.5 CHAIN OF THOUGHT (COT)\n\n\nFor zero-shot CoT prompting, we simply append _Let\u2019s think step by step._ to the question to query the\nmodel.\n\n\n16\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nQuestion A spherical conductor carries a net charge. How is this charge distributed on\nthe sphere?\n0) The charge is evenly distributed on the surface.\n1) The charge resides on the surface only; the distribution of charge on the\nsurface depends on what other charged objects are near the sphere.\n2) The charge moves continually within the sphere.\n3) The charge is distributed uniformly throughout the sphere.\n\nPrinciples Coulomb\u2019s Law: the force between two charged particles is proportional to the\nproduct of their charges and inversely proportional to the square of the distance\nbetween them, F = k * q1 * q2 / r, where F is the electric force, k is a constant,\nq1 and q2 are the charges the particles carry, and r is the distance between them.\n\nAnswer Using the Principles of Coulomb\u2019s Law, we can solve the problem as following:\nStep 1: Apply Coulomb\u2019s Law to find out how charges are distributed on the\nsurface.\nIn the case of a spherical conductor, the charges on the surface will rep", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_13500", "chunk_text": " of Coulomb\u2019s Law, we can solve the problem as following:\nStep 1: Apply Coulomb\u2019s Law to find out how charges are distributed on the\nsurface.\nIn the case of a spherical conductor, the charges on the surface will repel each\n\n          - ther. The further apart the charges are, the less force they will exert on each\n\n          - ther. Therefore, the charges will distribute themselves evenly on the surface\n\n          - f the sphere, as this is the configuration that minimizes the repulsive force\nbetween them.\n\n\nStep 2: Apply Coulomb\u2019s Law to find out what happens if there are other\ncharges present.\nThe distribution of charge on the surface may also be affected by the presence\n\n          - f other charged objects near the sphere. For example, if a negatively charged\n\n          - bject is brought near a positively charged sphere, the negative charges on the\nsphere will be repelled and will move to the opposite side of the sphere. This\nwill result in a non-uniform distribution of charge on the surface of the sphere.\n\n\nTherefore, the correct answer is 1) The charge resides on the surface only; the\ndistribution of charge on the surface depends on what other charged objects are\nnear the sphere.\n\n\nTable 9: Few-shot demonstration exemplar for MMLU high-school Physics.\n\n\n17\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nQuestion A sample of an unknown chloride compound was dissolved in water, and\nthen titrated with excess Pb(NO3)2 to create a precipitate. After drying, it is\ndetermined there are 0.0050 mol of precipitate present. What mass of chloride\nis present in the original sample?\n0) 0.177 g\n1) 0.355 g\n2) 0.522 g\n3) 0.710 g\n\nPrinciples Precipitation reactions: Precipitation reactions occur when two soluble salts are\nmixed and form an insoluble product, called a precipitate. The precipitate can\nbe separated from the solution by filtration or centrifugation.\nMolar mass: The molar mass of a substance is the mass of one mole of that\nsubstance. The molar mass is expressed in grams per mole (g/mol).\nLimiting reactant: The limiting reactant is the reactant that is completely consumed in a chemical reaction. The amount of product formed", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_13950", "chunk_text": " of one mole of that\nsubstance. The molar mass is expressed in grams per mole (g/mol).\nLimiting reactant: The limiting reactant is the reactant that is completely consumed in a chemical reaction. The amount of product formed is determined by\nthe amount of limiting reactant.\n\nAnswer Assuming the unknown chloride compound is MCl, where M represents the\nmetal cation, the balanced chemical equation for the precipitation reaction is:\n_Pb_ ( _NO_ 3)2( _aq_ ) + 2 _MCl_ ( _aq_ ) _\u2212\u2192_ _PbCl_ 2( _s_ ) + 2 _MNO_ 3( _aq_ )\n\n\nSince _Pb_ ( _NO_ 3)2 is in excess, MCl is the limiting reactant. The stoichiometry\n\n         - f the reaction indicates that 2 moles of MCl produce 1 mole of _PbCl_ 2\nprecipitate. Therefore, 0.0050 mol of _PbCl_ 2 corresponds to 0.010 mol of MCl.\n\n\nThe mass of chloride in the original sample can be calculated using the molar\nmass of chloride (35.45 g/mol):\n0.010 mol Cl - 35.45 g/mol = 0.355 g Cl\n\n\nThe correct answer is 1) 0.355 g.\n\n\nTable 10: Few-shot demonstration exemplar for MMLU high-school Chemistry.\n\n\nQuestion Bella bought stamps at the post office. Some of the stamps had a snowflake\ndesign, some had a truck design, and some had a rose design. Bella bought 11\nsnowflake stamps. She bought 9 more truck stamps than snowflake stamps, and\n13 fewer rose stamps than truck stamps. How many stamps did Bella buy in all?\n\nAnswer Let us find and apply the math principles to solve the problem step by step:\n\n\nStep 1. Addition: Calculate the number of truck stamps.\nBella bought 11 snowflake stamps. She bought 9 more truck stamps than\nsnowflake stamps: there are 11 + 9 = 20 truck stamps.\n\n\nStep 2. Subtraction: Calculate the number of rose stamps.\nBella bought 13 fewer rose stamps than truck stamps: there are 20 - 13 = 7 rose\nstamps.\n\n\nStep 3. Addition: Calculate the total", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_14400", "chunk_text": "Step 2. Subtraction: Calculate the number of rose stamps.\nBella bought 13 fewer rose stamps than truck stamps: there are 20 - 13 = 7 rose\nstamps.\n\n\nStep 3. Addition: Calculate the total number of stamps in all three colors.\nBella bought 11 snowflake stamps, 20 truck stamps, 7 rose stamps: there are 11\n+ 20 + 7 = 38 stamps in total.\n\n\nConclusion: Bella bought 38 stamps in all.\n\n\nTable 11: Step-Back few-shot demonstration exemplar for GSM8K.\n\n\n18\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Step-Back Prompt\n\n\nYou are an expert at world knowledge. Your task is to step back and\nparaphrase a question to a more generic step-back question, which is\neasier to answer. Here are a few examples:\n\n\nOriginal Question: _<_ Original Question Example1 _>_\nStepback Question: _<_ Stepback Question Example1 _>_\n\n...\nOriginal Question: _<_ Original Question Example5 _>_\nStepback Question: _<_ Stepback Question Example5 _>_\nOriginal Question: _<_ Original Question _>_\nStepback Question:\n\n\nTable 12: Prompt of asking step-back question in Knowledge QA tasks.\n\n\ndataset Original Question Step-back Question\n\n\nTimeQA Which position did Knox Cunningham Which positions have Knox Cunninghold from May 1955 to Apr 1956? ham held in his career?\n\n\nTimeQA Who was the spouse of Anna Karina Who were the spouses of Anna Karina?\nfrom 1968 to 1974?\n\n\nTimeQA Which team did Thierry Audel play for Which teams did Thierry Audel play for\nfrom 2007 to 2008? in his career?\n\n\nTimeQA What was the operator of GCR Class What were the operators of GCR Class\n11E from 1913 to Dec 1922? 11E in history?\n\n\nTimeQA Which country did Sokolovsko belong Which countries did Sokolovsko belong\nto from 1392 to 1525? to in history?\n\n\nSituatedQA when was the last time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_14850", "chunk_text": " time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_15300", "chunk_text": "studio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine of philosophy of language\n\n\nwhich baseball team won the world\n\nseries in 2015 baseball?\n\n\n\nStrategyQA Could the members of The Police what can the members of The Police\nperform lawful arrests? do?\n\n\nStrategyQA Would a Monoamine Oxidase candy What are the effects of Monoamine\nbar cheer up a depressed friend? Oxidase?\n\n\nStrategyQA Would a dog respond to bell before Would a dog respond to bell before\nGrey seal? Grey seal?\n\n\nStrategyQA Is shrimp scampi definitely free of what is shrimp scampi made of?\nplastic?\n\n\nStrategyQA Do the anchors on Rede Globo What languages do the anchors on\nspeak Chinese? Rede Globo speak?\n\n\nTable 15: Few-shot demonstration exemplars for asking step-back questions in MuSiQue and StrategyQA.\n\n\n20\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nBaseline few-shot Prompt\n\n\nYou are an expert of world knowledge and physics. Your task is to solve\nthe following question. Here are a few examples:\n\n\nQuestion: _<_ Question Example _>_\nAnswer: _<_ Answer Example _>_\nQuestion: _<_ Question _>_\nAnswer:\n\n\nTable 16: Prompt of querying the baseline model for final answer with few-shot demonstration\nexemplars.\n\n\nFor few-shot CoT prompting, we use the same template as the Baseline prompting in Sec. D.4 by\nreplacing the few-shot examples using CoT responses, as shown in Tables 18, 19, 20, 21, and 22.\n\n\nD.6 TAKE A DEEP BREATH (TDB)\n\n\nWe study the zero-shot prompting found in Yang et al. (2023): we take _Take a deep breath and work_\n\n_on this problem step-by-step_, and prepend it to the question.\n\n\nE EXAMPLES OF ERROR ANALYSIS AND WINS OF STEP-BACK PROMPTING\n\n\nE.1 MMLU ERROR ANALYSIS\n\n\nIn Tables 23-27, we show one example for each of the 5 error categories we identified through error\nanalysis on STEP-BACK PROMPTING.\n\n\nE.2 EXAMPLE WINS FROM STEP-BACK PROMPTING\n\n\nTables 28- ", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_15750", "chunk_text": " 23-27, we show one example for each of the 5 error categories we identified through error\nanalysis on STEP-BACK PROMPTING.\n\n\nE.2 EXAMPLE WINS FROM STEP-BACK PROMPTING\n\n\nTables 28- 30, 31 32, 33 and 34 illustrate the some successful examples of STEP-BACK PROMPTING\n\n- n MMLU-Physics, MMLU-Chemistry, TimeQA, SituatedQA, and StrategyQA respectively.\n\n\n21\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset Question Answer\n\n\n\nMMLU A point charge, Q1 = +1 mC, is fixed\nPhysics at the origin. How much work is\nrequired to move another charge, Q2\n= +8 _\u00b5_ C, from the point (0, 4 meters)\nto the point (3 meters, 0)?\n0) 3.5 J 1) 6.0 J 2) 22.5 J 3) 40 J\n\n\nMMLU A sample of an unknown chloride\nChemistry compound was dissolved in water, and then titrated with excess\nPb(NO3)2 to create a precipitate.\nAfter drying, it is determined there\nare 0.0050 mol of precipitate present.\nWhat mass of chloride is present in\nthe original sample?\n0) 0.177 g 1) 0.355 g 2) 0.522 g 3)\n0.710 g\n\n\nGSM8K Bella bought stamps at the post office. Some of the stamps had a\nsnowflake design, some had a truck\ndesign, and some had a rose design.\nBella bought 11 snowflake stamps.\nShe bought 9 more truck stamps\nthan snowflake stamps, and 13 fewer\nrose stamps than truck stamps. How\nmany stamps did Bella buy in all?\n\n\n\nThe work required to move a charge in an\nelectric field is equal to the change in potential\nenergy of the charge. The potential energy of a\ncharge in an electric field is given by: U = k *\nQ1 * Q2 / r where: k is the Coulomb constant,\nQ1 and Q2 are the charges, r is the distance\nbetween the charges. In this case, the initial\npotential energy of Q2 is: U1 = k * Q1 * Q", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_16200", "chunk_text": " where: k is the Coulomb constant,\nQ1 and Q2 are the charges, r is the distance\nbetween the charges. In this case, the initial\npotential energy of Q2 is: U1 = k * Q1 * Q2 /\n4 meters\n\n\nThe final potential energy of Q2 is: U2 = k *\nQ1 * Q2 / 3 meters\n\n\nThe change in potential energy is: \u2206U = U2 U1 = k * Q1 * Q2 * (1/3 meters - 1/4 meters)\n\n\nPlugging in the values, we get: \u2206U = (9 _\u00d7_\n10 [9] _N \u2217m_ [2] _/C_ [2] ) _\u00d7_ (1 _\u00d7_ 10 _[\u2212]_ [3] _C_ ) _\u2217_ (8 _\u00d7_ 10 _[\u2212]_ [6] _C_ ) _\u2217_\n(1 _/_ 3 _m \u2212_ 1 _/_ 4 _m_ ) = 6 _J_ . The answer is 1) 6.0 J.\n\n\n0.0050 moles of lead chloride precipitate\ncontain 0.010 moles of chloride ions due to the\n\n2:1 mole ratio between chloride ions and lead\n\nchloride.\n\n\nThe mass of 0.010 moles of chloride ions is\n0.355 g.\n\n\nTherefore, the mass of chloride present in the\n\n- riginal sample is 0.355 g.\n\n\nThe number of truck stamps is 11 + 9 = 20. The\nnumber of rose stamps is 20 - 13 = 7. Bella\nbought 11 + 20 + 7 = 38 stamps in all.\n\n\n\nTimeQA Who was the spouse of Anna Karina Anna Karina\u2019s spouse from 1968 to 1974 was\nfrom 1968 to 1974? French actor Pierre Fabre.\n\n\nSituatedQA what is the biggest hotel in las vegas The biggest hotel in Las Vegas, Nevada as of\nnv as of November 28, 1993 November 28, 1993 was the Excalibur Hotel &\nCasino.\n\n\n\nMuSiQue What year saw the creation of the\nregion where the county of Hertfordshire is located?\n\n\n\n1994\n\n\n\nStrategyQA Would a Monoamine Oxidase candy A Monoamine", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_16650", "chunk_text": "3 was the Excalibur Hotel &\nCasino.\n\n\n\nMuSiQue What year saw the creation of the\nregion where the county of Hertfordshire is located?\n\n\n\n1994\n\n\n\nStrategyQA Would a Monoamine Oxidase candy A Monoamine Oxidase (MAO) candy bar\nbar cheer up a depressed friend? would not cheer up a depressed friend. MAO\nis an enzyme that breaks down neurotransmitters such as serotonin, dopamine, and norepinephrine. These neurotransmitters are associated with mood, so inhibiting MAO can lead to\nan increase in their levels.\n\n\nTable 17: Demonstration exemplar for standard prompting.\n\n\n22\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset Question Answer\n\n\n\nMMLU A point charge, Q1 = +1 mC, is\nPhysics fixed at the origin. How much\nwork is required to move another\ncharge, Q2 = +8 _\u00b5_ C, from the\npoint (0, 4 meters) to the point\n(3 meters, 0)?\n0) 3.5 J\n1) 6.0 J\n2) 22.5 J\n3) 40 J\n\n\n\nSure, let\u2019s think step by step.\n\n\n1. Calculate the electric potential energy of Q2\nat the initial position. The electric potential\nenergy (U) of a point charge Q2 at a distance\nr from another point charge Q1 is given by the\nfollowing equation: U = k * Q1 * Q2 / r, where k\nis Coulomb\u2019s constant.\n\n\nTherefore, the electric potential energy of Q2 at\nthe initial position is:\n_U_ initial = (9 _\u00d7_ 10 [9] _Nm_ [2] _/C_ [2] ) _\u2217_ (1 _\u00d7_ 10 _[\u2212]_ [3] _C_ ) _\u2217_ (8 _\u00d7_\n10 _[\u2212]_ [6] _C_ ) _/_ 4 _m_ = 18 J\n\n\n2. Calculate the electric potential energy of Q2 at\nthe final position. The electric potential energy of\nQ2 at the final position is:\n_U_ final = (9 _\u00d7_ 10 [9] _Nm_ [2] _/C_ [2] ) _\u2217_ (1 _\u00d7_", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_17100", "chunk_text": " electric potential energy of\nQ2 at the final position is:\n_U_ final = (9 _\u00d7_ 10 [9] _Nm_ [2] _/C_ [2] ) _\u2217_ (1 _\u00d7_ 10 _[\u2212]_ [3] _C_ ) _\u2217_ (8 _\u00d7_\n10 _[\u2212]_ [6] _C_ ) _/_ 3 _m_ = 24 J\n\n\n3. Calculate the work required to move Q2 from\nthe initial position to the final position. The work\n(W) required to move a charge from one position\nto another is equal to the change in electric\npotential energy (\u2206U) between the two positions.\nW = \u2206U = _U_ final - _U_ initial = 24 J - 18 J = 6 J\n\n\nTherefore, the answer is 1) 6.0 J.\n\n\n\nTable 18: Demonstration exemplar for CoT prompting for MMLU Physics.\n\n\n23\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset Question Answer\n\n\n\nMMLU A sample of an unknown chloChemistry ride compound was dissolved in\nwater, and then titrated with excess Pb(NO3)2 to create a precipitate. After drying, it is determined there are 0.0050 mol of\nprecipitate present. What mass\n\n        - f chloride is present in the original sample?\n0) 0.177 g\n1) 0.355 g\n2) 0.522 g\n3) 0.710 g\n\n\n\nHere\u2019s the step-by-step solution:\nIdentify the precipitate: The precipitate formed\nwhen a chloride compound reacts with lead nitrate\n(Pb(NO3)2) is lead chloride (PbCl2).\n\n\nDetermine the mole ratio: The balanced chemical\nequation for the reaction between a chloride\ncompound (represented as MCl, where M is the\nunknown metal) and lead nitrate is: 2 _MCl_ +\n_Pb_ ( _NO_ 3)2 _\u2212\u2192_ 2 _MNO_ 3 + _PbCl_ 2\nFrom the balanced equation, you can see that 2\nmoles of chloride ions (Cl-) react with 1 mole of\nlead nitrate to produce 1 mole of lead", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_17550", "chunk_text": "NO_ 3 + _PbCl_ 2\nFrom the balanced equation, you can see that 2\nmoles of chloride ions (Cl-) react with 1 mole of\nlead nitrate to produce 1 mole of lead chloride\nprecipitate. Additionally, 1 mole of lead chloride\ncontains 2 moles of chloride ions.\n\n\nCalculate the moles of chloride ions: Since\n0.0050 moles - f lead chloride precipitate\nwere formed, there must have been 2 - 0.0050 =\n0.010 moles of chloride ions in the original sample.\n\n\nCalculate the mass of chloride ions: The molar\nmass of chlorine (Cl) is 35.45 g/mol. Therefore,\nthe mass of 0.010 moles of chloride ions is:\n\nmass = moles - molar mass = 0.010 mol - 35.45\ng/mol = 0.355 g\n\n\nTherefore, the mass of chloride present in the original sample is 0.355 g. The correct answer is option\n1.\n\n\n\nTable 19: Demonstration exemplar for CoT prompting for MMLU Chemistry.\n\n\ndataset Question Answer\n\n\n\nGSM8K Bella bought stamps at the post\n\n       - ffice. Some of the stamps had\na snowflake design, some had\na truck design, and some had\na rose design. Bella bought\n11 snowflake stamps. She\nbought 9 more truck stamps than\nsnowflake stamps, and 13 fewer\nrose stamps than truck stamps.\nHow many stamps did Bella buy\nin all?\n\n\n\nHere are the steps to solve the problem:\n\n\nStep 1: Find the number of truck stamps. Adding\n9 to the number of snowflake stamps gives the\nnumber of truck stamps: 11 + 9 = 20\n\n\nStep 2: Find the number of rose stamps. Subtracting 13 from the number of truck stamps gives the\nnumber of rose stamps: 20 - 13 = 7\n\n\nStep 3: Add the number of snowflake, truck, and\nrose stamps to find the total number of stamps\nBella bought: 11 + 20 + 7 = 38\n\n\nSolution: Bella bought 38 stamps in all.\n\n\n\nTable 20: Demonstration exemplar for CoT prompting for GSM8K.\n\n\n24\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_18000", "chunk_text": " 7 = 38\n\n\nSolution: Bella bought 38 stamps in all.\n\n\n\nTable 20: Demonstration exemplar for CoT prompting for GSM8K.\n\n\n24\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset Question Answer\n\n\nTimeQA Who was the spouse of Anna Ka- Step 1: Identify Anna Karina\u2019s spouses:Jean-Luc\nrina from 1968 to 1974? Godard (1961\u20131965).\nPierre Fabre (1968\u20131974)\nDaniel Duval (1978\u20131981)\nDennis Berry (1982\u20131994)\n\n\nStep 2: Determine which spouse was married to\nAnna Karina from 1968 to 1974.\n\nJean-Luc Godard was married to Anna Karina\nfrom 1961 to 1965, so he is not the correct answer.\nPierre Fabre was married to Anna Karina from\n1968 to 1974, so he is the correct answer.\nDaniel Duval was married to Anna Karina from\n1978 to 1981, so he is not the correct answer.\nDennis Berry was married to Anna Karina from\n1982 to 1994, so he is not the correct answer.\n\n\nTherefore, Anna Karina\u2019s spouse from 1968 to\n1974 was Pierre Fabre.\n\n\nSituatedQA what is the biggest hotel in las ve- Sure, let\u2019s think step by step:\ngas nv as of November 28, 1993 What were the largest hotels in Las Vegas in 1993?\nAccording to a 1993 article in the Las Vegas\nReview-Journal, the largest hotels in Las Vegas\nat the time were:\nMGM Grand (5,005 rooms) Excalibur (4,008\nrooms) The Mirage (3,049 rooms) Flamingo\nHilton (3,565 rooms) Caesars Palace (2,577 rooms)\nWhich of these hotels was the largest on November\n28, 1993? The MGM Grand opened in December\n1993, so it would not have been the largest hotel\n\n                       - n November 28, 1993. The Excalibur opened in\nJune 1990, so it would have been the largest hotel\n\n                       - n November 28, 1993.\nTherefore, the largest hotel in Las Vegas on November 28, 1993 was the Ex", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_18450", "chunk_text": " Excalibur opened in\nJune 1990, so it would have been the largest hotel\n\n                       - n November 28, 1993.\nTherefore, the largest hotel in Las Vegas on November 28, 1993 was the Excalibur.\n\n\nTable 21: Demonstration exemplar for CoT prompting for TimeQA and SituatedQA.\n\n\n25\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset Question Answer\n\n\n\nMuSiQue What year saw the creation of\nthe region where the county of\nHertfordshire is located?\n\n\nStrategyQA Would a Monoamine Oxidase\ncandy bar cheer up a depressed\nfriend?\n\n\n\nThe county of Hertfordshire is in the \u2019East of England\u2019 region. The \u2019East of England\u2019 region was\ncreated in 1994. So, the answer is 1994.\n\n\nNo, a Monoamine Oxidase (MAO) candy bar\nwould not cheer up a depressed friend. In fact,\nit could be dangerous for them to consume. Here\u2019s\na step-by-step explanation:\n1. MAO is an enzyme that breaks down neurotransmitters like serotonin, dopamine, and norepinephrine. These neurotransmitters are important\nfor regulating mood, and low levels of them are\nassociated with depression.\n2. MAO inhibitors (MAOIs) are a type of medication that blocks the action of MAO, which can increase levels of these neurotransmitters in the brain.\nMAOIs are sometimes used to treat depression, but\nthey can have serious side effects, including interactions with certain foods and medications.\n3. A MAO candy bar would not contain enough\nMAOI to have a therapeutic effect on depression.\nIn fact, it is unlikely that a candy bar could contain\nany MAOI at all, as these medications are typically\ntaken in pill form.\nIn conclusion, a MAO candy bar would not cheer\nup a depressed friend and could even be dangerous\nfor them to consume.\n\n\n\nTable 22: Demonstration exemplar for CoT prompting for MuSiQue and StrategyQA.\n\n\n26\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nQuestion An alarm whose frequency is 400 Hz is dropped out of a third-floor window. The\nstudent who drops it measures the frequency with a very sensitive oscilloscope.\nThe measured frequency\n0) appears higher than 400 Hz and the frequency increase as", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_18900", "chunk_text": " alarm whose frequency is 400 Hz is dropped out of a third-floor window. The\nstudent who drops it measures the frequency with a very sensitive oscilloscope.\nThe measured frequency\n0) appears higher than 400 Hz and the frequency increase as it falls\n1) appears higher than 400 Hz and the frequency decreases as it falls\n\n2) appears lower than 400 Hz and the frequency decreases as it falls\n3) appears lower than 400 Hz and the frequency increases as it falls\n\nPrinciples Doppler Effect: the effect produced by the motion of a wave source, observer, or\nboth that causes a change in wave frequency perceived by an observer relative\nto the frequency produced by the source. The Doppler effect is observed with\nall types of waves, including electromagnetic and acoustic waves. It was named\nafter Austrian physicist Christian Doppler.\n\nAnswer Using the Principles of Doppler Effect, we can solve the problem as following:\n\n\nStep 1: Apply Doppler Effect to find out the measured frequency.\nThe Doppler Effect is the change in frequency of a wave as measured by an\n\n          - bserver who is moving relative to the source of the wave. In this case, the\nsource of the wave is the alarm, and the observer is the student who drops it.\n\n\nAs the alarm falls, the distance between the alarm and the student decreases.\nThis means that the frequency of the wave as measured by the student will\nincrease. Therefore, the measured frequency will appear higher than 400 Hz,\nand the frequency will increase as the alarm falls.\n\n\nTherefore, the correct answer is 0) appears higher than 400 Hz and the frequency\nincrease as it falls.\n\n\nTable 23: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Factual Error** . In this case, the Factual error leads to the wrong answer despite that the Principle and\nReasoning are all sound.\n\n\n27\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nQuestion An astronaut lands on a planet whose mass and radius are each twice that of\nEarth. If the astronaut weighs 800 N on Earth, how much will he weigh on this\nplanet?\n0) 200 N\n1) 400 N\n\n2) 800 N\n3) 1600 N\n\nPrinciples Weight: W = m * g, where W is the weight, m is the", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_19350", "chunk_text": " on this\nplanet?\n0) 200 N\n1) 400 N\n\n2) 800 N\n3) 1600 N\n\nPrinciples Weight: W = m * g, where W is the weight, m is the mass and g is the\nacceleration due to gravity\nAcceleration due to gravity: g = G * m / _r_ [2], where G is the gravitational constant,\nm is the mass of the object and r is the radius of the object\n\nAnswer Using the Principles of Newton\u2019s law of universal gravitation and Weight, we\ncan solve the problem as following:\n\n\nStep 1: Apply Newton\u2019s law of universal gravitation to find out the acceleration\ndue to gravity on the planet.\nThe acceleration due to gravity on a planet is given by the formula:\ng = G * m / _r_ [2], where g is the acceleration due to gravity, G is the gravitational\nconstant, m is the mass of the planet, and r is the radius of the planet.\n\n\nIn this problem, we are given the following information:\nm = 2 * _mearth_\nr = 2 * _rearth_\n\n\nPlugging these values into the formula, we get:\ng = G * 2 * _mearth_ / (2 _\u2217_ _rearth_ ) [2]\ng = G * _mearth_ / _rearth_ [2]\n\n\nThis is the same as the acceleration due to gravity on Earth.\n\n\nStep 2: Apply Weight to find out the weight of the astronaut on the planet.\nThe weight of an object is given by the formula:\nW = m * g, where W is the weight, m is the mass of the object, and g is the\nacceleration due to gravity.\n\n\nIn this problem, we are given the following information:\nm = 800 N / _gearth_\n\ng = _gearth_\n\n\nPlugging these values into the formula, we get:\nW = 800 N / _gearth \u2217_ _gearth_\nW = 800 N\n\n\nTherefore, the astronaut will weigh 800 N on the planet. The correct answer is\n2) 800 N.\n\n\nTable 24: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Math Error** . As highlighted, the math error leads to directly a wrong answer with a factor of 2 off", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_19800", "chunk_text": ".\n\n\nTable 24: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Math Error** . As highlighted, the math error leads to directly a wrong answer with a factor of 2 off.\n\n\n28\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nQuestion Two spheres of net charge +5e and -6e briefly come into contact. Afterward,\nwhich of the following is a possible combination of net charges for the two\nspheres?\n0) +3e and -4e\n1) +1.5e and -2.5e\n2) Both of the above\n3) None of the above\n\nPrinciples Law of conservation of charge: the total charge in a closed system remains\n\nconstant.\nCoulomb\u2019s Law: the mutual repulsion of like charges demands that the charges\nbe as far apart as possible.\n\nAnswer Using the Principles of Conservation of charge, we can solve the problem as\nfollowing:\n\n\nStep 1: Apply Conservation of charge to find out the possible combination of\nnet charges for the two spheres.\nThe total charge of the two spheres before they come into contact is +5e - 6e =\n\n          - 1e. After they come into contact, the total charge of the two spheres must still\nbe -1e.\n\n\nStep 2: Find out the possible combination of net charges for the two spheres.\nThe possible combination of net charges for the two spheres are:\n(1) +3e and -4e\n(2) +1.5e and -2.5e\n\n\nTherefore, the correct answer is 2) Both of the above.\n\n\nTable 25: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Principle Error** . In this case, Coulomb\u2019s Law is not needed for solving this problem. Instead, the\nCharge Quantization Principle is missing from the retrieved principles, and is in fact needed to rule\n\n- ut option (2) of fractional charges.\n\n\n29\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nQuestion The circumference of a helium-filled balloon is measured for three different\nconditions: at room temperature, after being in a warm oven for 30 minutes,\nand after being in a freezer for 30 minutes. A student plotting the circumference\ncubed _C", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_20250", "chunk_text": " of a helium-filled balloon is measured for three different\nconditions: at room temperature, after being in a warm oven for 30 minutes,\nand after being in a freezer for 30 minutes. A student plotting the circumference\ncubed _C_ [3] as a function of temperature T, should expect to find which of the\nfollowing?\n0) a cubic relationship between _C_ [3] and T\n1) an indirect relationship between _C_ [3] and T\n2) an extrapolated temperature T where _C_ [3] reaches zero\n3) a maximum _C_ [3] as the temperature T increases\n\nPrinciples Ideal Gas Law: PV = nRT, where P is the pressure, V is the volume, n is\nthe number of moles of gas, R is the ideal gas constant, T is the absolute\ntemperature.\nVolume of a sphere: _V_ = 4 _/_ 3 _\u2217_ _\u03c0 \u2217_ _r_ [3], where r is the radius.\nAbsolute temperature: Absolute temperature is always written as kelvin.\nAbsolute zero: Absolute zero is the lowest limit of the kelvin scale and is the\nlowest possible temperature (- 273.15 \u00b0C).\n\nAnswer Using the Principles of Charles\u2019s Law and Gay-Lussac\u2019s Law, we can solve the\nproblem as following:\nStep 1: Apply Charles\u2019s Law to find out the relationship between the volume\nand temperature of the gas. Charles\u2019s Law states that the volume of a fixed\nmass of a gas is directly proportional to the absolute temperature of the gas\nwhen pressure is kept constant. This means that if the temperature of the gas\nincreases, the volume of the gas will also increase.\n\n\nStep 2: Apply Gay-Lussac\u2019s Law to find out the relationship between the\npressure and temperature of the gas. Gay-Lussac\u2019s Law states that the pressure\n\n          - f a fixed mass of a gas is directly proportional to the absolute temperature of\nthe gas when volume is kept constant. This means that if the temperature of the\ngas increases, the pressure of the gas will also increase.\n\n\nStep 3: Apply Charles\u2019s Law and Gay-Lussac\u2019s Law to find out the relationship\nbetween the circumference and temperature of the gas.The circumference of the\nballoon is directly proportional to the volume of the gas. This is because the\ncircumference of", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_20700", "chunk_text": " Charles\u2019s Law and Gay-Lussac\u2019s Law to find out the relationship\nbetween the circumference and temperature of the gas.The circumference of the\nballoon is directly proportional to the volume of the gas. This is because the\ncircumference of the balloon is the distance around the outside of the balloon,\nand the volume of the gas is the amount of space that the gas takes up.\n\n\nThe pressure of the gas is directly proportional to the temperature of the gas.\nThis is because the pressure of the gas is the force that the gas exerts on the\nwalls of the balloon, and the temperature of the gas is the measure of the\naverage kinetic energy of the gas molecules.\n\n\nTherefore, the circumference of the balloon is directly proportional to the\ntemperature of the gas. This means that if the temperature of the gas increases,\nthe circumference of the balloon will also increase.\n\n\nStep 4: Apply the relationship between the circumference and temperature\n\n         - f the gas to find out the relationship between the circumference cubed and\ntemperature of the gas. The circumference cubed is directly proportional to the\ntemperature of the gas. This is because the circumference cubed is the volume\n\n          - f the balloon, and the volume of the balloon is directly proportional to the\ntemperature of the gas.\n\n\nTherefore, the student should expect to find a cubic relationship between _C_ [3]\nand T. The correct answer is 0) a cubic relationship between _C_ [3] and T.\n\n\nTable 26: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Context Loss** . In this case, at the end of the Reasoning chain, the model forgot the original question,\nand lose the context to apply the reasoning to the question.\n\n\n30\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nQuestion Physics students are checking the constant acceleration equations of kinematics\nby measuring the velocity of a tennis ball that is dropped and falls 6 meters\nand then passes through a photogate. The predicted velocity is 20% above the\nvelocity measured by the photogate. Which of the following best describes the\ncause of the large percent difference?\n0) The ball changes its shape while falling.\n1) The acceleration of gravity varies as the ball is falling.\n2) Air resistance increases the acceleration of the ball.\n3) The acceleration of the balls varies with the velocity.\n\nPr", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_21150", "chunk_text": " percent difference?\n0) The ball changes its shape while falling.\n1) The acceleration of gravity varies as the ball is falling.\n2) Air resistance increases the acceleration of the ball.\n3) The acceleration of the balls varies with the velocity.\n\nPrinciples Vertical motion: y = 0.5 * g * _t_ [2], where y is the vertical displacement, g is the\nacceleration due to gravity, t is the time.\nFree fall: is a special case of motion with constant acceleration, in which the\nacceleration is due to gravity.\nAcceleration to Velocity Formula: v = u + a * t, where v is the final speed, u is\nthe initial velocity, a is the acceleration, and t is the time.\n\nAnswer Using the Principles of Vertical motion and Horizontal motion, we can solve\nthe problem as following:\n\n\nStep 1: Apply the Vertical motion equation to find the time it takes for the ball\nto fall 6 meters.\ny = 0.5 * g * _t_ [2]\n6 m = 0.5 * 9.8 m/ _s_ [2]          - _t_ [2]\n\nt = 1.12 s\n\n\nStep 2: Apply the Horizontal motion equation to find the velocity of the ball\nwhen it passes through the photogate.\nx = v * t\n\nv = x / t\n\nv = 6 m / 1.12 s\n\nv = 5.36 m/s\n\n\nStep 3: Compare the predicted velocity with the velocity measured by the\nphotogate. The predicted velocity is 20% above the velocity measured by the\nphotogate, so the predicted velocity is 1.2 * 5.36 m/s = 6.43 m/s.\nStep 4: Find the cause of the large percent difference. The predicted velocity is\n6.43 m/s, but the velocity measured by the photogate is 5.36 m/s. The difference\nis 1.07 m/s. This difference is caused by air resistance. Air resistance is a force\nthat opposes the motion of an object through the air. It is caused by the friction\nbetween the object and the air molecules. The greater the surface area of the\n\n           - bject, the greater the air resistance. The tennis ball has a large surface area, so\nit experiences a lot of air resistance.", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_21600", "chunk_text": " by the friction\nbetween the object and the air molecules. The greater the surface area of the\n\n           - bject, the greater the air resistance. The tennis ball has a large surface area, so\nit experiences a lot of air resistance. This air resistance slows down the tennis\nball and causes the difference between the predicted velocity and the velocity\nmeasured by the photogate.\nTherefore, the correct answer is 0) The ball changes its shape while falling.\n\n\nTable 27: Error example of STEP-BACK PROMPTING on MMLU high-school Physics in the class of\n**Reasoning Error** . In this case, the model was able to correctly identify air resistance is the cause of\nthe velocity difference, but made the error at the final reasoning step of deriving the right answer.\n\n\n31\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nMMLU-Physics **Original Question** : What happens to the pressure, P, of an ideal gas if\nthe temperature is increased by a factor of 2 and the volume is increased\nby a factor of 8 ?\n0) P decreases by a factor of 16.\n1) P decreases by a factor of 4.\n2) P decreases by a factor of 2.\n3) P increases by a factor of 4.\n**Original Answer [Incorrect]** : The correct answer is 0. The ideal gas\nlaw is PV = nRT. If the temperature is increased by a factor of 2 and the\nvolume is increased by a factor of 8, then the pressure will decrease by a\nfactor of 16.\n**Stepback Question** : What are the physics principles behind this\nquestion?\n**Principles** :\nIdeal gas law: PV = nRT, where P is the pressure, V is the volume, n is\nthe number of moles, R is the gas constant, and T is the temperature.\n**Final Answer** : Using the Principles of Ideal gas law, we can solve the\nproblem as following:\nStep 1: Apply Ideal gas law to find out the pressure of an ideal gas. The\nideal gas law states that PV = nRT, where P is the pressure, V is the\nvolume, n is the number of moles, R is the gas constant, and T is the\ntemperature.\n\n\nStep 2: Apply Ideal gas law to find out the pressure of", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_22050", "chunk_text": " nRT, where P is the pressure, V is the\nvolume, n is the number of moles, R is the gas constant, and T is the\ntemperature.\n\n\nStep 2: Apply Ideal gas law to find out the pressure of an ideal gas when\nthe temperature is increased by a factor of 2 and the volume is increased\nby a factor of 8. If the temperature is increased by a factor of 2, then T becomes 2T. If the volume is increased by a factor of 8, then V becomes 8V.\n\n\nSubstituting these values into the ideal gas law, we get: P(8V) = nR(2T)\n\n\nDividing both sides by 8V, we get: P = nR(2T) / 8V\n\n\nWe can see that the pressure has decreased by a factor of 4.\n\n\nTherefore, the correct answer is 1) P decreases by a factor of 4.\n\n\nTable 28: Illustration of wins of STEP-BACK PROMPTING on the MMLU-Physics dataset.\n\n\n32\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nMMLU-Physics **Original Question** : A liquid flows at a constant flow rate through a pipe\nwith circular cross-sections of varying diameters. At one point in the\npipe, the diameter is 2 cm and the flow speed is 18 m/s. What is the flow\nspeed at another point in this pipe, where the diameter is 3 cm?\n0) 4 m/s\n1) 6 m/s\n2) 8 m/s\n3) 12 m/s\n**Original Answer [Incorrect]** : The answer is 3) 12 m/s A step-by-step\nsolution is below: The flow rate is constant, so the product of the speed\nand the cross-sectional area is constant. The cross-sectional area is\nproportional to the square of the diameter, so the speed is inversely\nproportional to the square of the diameter. The ratio of the speeds is the\ninverse of the ratio of the squares of the diameters, so the speed at the\nsecond point is 18 _m/s \u2217_ (2 _cm/_ 3 _cm_ ) [2] = 12 m/s.\n**Stepback Question** : What are the physics principles behind this", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_22500", "chunk_text": " so the speed at the\nsecond point is 18 _m/s \u2217_ (2 _cm/_ 3 _cm_ ) [2] = 12 m/s.\n**Stepback Question** : What are the physics principles behind this\nquestion?\n**Principles** :\nConservation of Mass: the amount of mass flowing through the pipe at\ntwo different points must be equal, or A1 * v1 = A2 * v2, where A1 is\nthe area of cross-section one, v1 is the velocity at point 1, A2 is the area\n\n               - f cross-section two, v2 is the velocity at point 2.\nArea of Circle: _A_ = _\u03c0 \u2217_ _r_ [2], where r is the radius of the circle.\n**Final Answer** : Using the Principles of Continuity equation, we can\nsolve the problem as following:\nStep 1: Apply the Continuity equation to find out the flow speed at\nanother point in this pipe. The continuity equation states that the product\n\n               - f the area and the velocity is constant at all points in a pipe. In this\nproblem, we are given the following information:\n_A_ 1 = _\u03c0 \u2217_ (2 _cm_ ) [2] = 4 _\u03c0cm_ [2]\nv1 = 18 m/s\n_A_ 2 = _\u03c0 \u2217_ (3 _cm_ ) [2] = 9 _\u03c0cm_ [2]\n\n\nWe can use the continuity equation to find v2:\nA1 * v1 = A2 * v2\n4 _\u03c0cm_ [2] _\u2217_ 18 _m/s_ = 9 _\u03c0cm_ [2] _\u2217_ _v_ 2\nv2 = 8 m/s\n\n\nTherefore, the flow speed at another point in this pipe is 8 m/s.\nThe correct answer is 2) 8 m/s.\n\n\nTable 29: Illustration of wins of STEP-BACK PROMPTING on the MMLU-Physics dataset.\n\n\n33\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nMMLU-Physics **Original Question** : A 2 _\u00b5_ F capacitor is connected directly to a battery. When\nthe capacitor is fully charged, it stores 600 _\u00b5_ C of", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_22950", "chunk_text": " in Large Language Models\n\n\ndataset example\n\n\nMMLU-Physics **Original Question** : A 2 _\u00b5_ F capacitor is connected directly to a battery. When\nthe capacitor is fully charged, it stores 600 _\u00b5_ C of charge. An experimenter\nreplaces the 2 _\u00b5_ F capacitor with three 18 _\u00b5_ F capacitors in series connected to\nthe same battery. Once the capacitors are fully charged, what charge is stored\n\n          - n each capacitor?\n0) 100 _\u00b5_ C\n1) 200 _\u00b5_ C\n2) 600 _\u00b5_ C\n3) 1800 _\u00b5_ C\n**Original Answer [Incorrect]** : The answer is 100 _\u00b5_ C. The total capacitance of\nthe three 18 _\u00b5_ F capacitors in series is 1 / (1 / 18 + 1 / 18 + 1 / 18) = 6 _\u00b5_ F. The\ncharge stored on the three capacitors in series is 6 _\u00b5_ F * V = 600 _\u00b5_ C. The charge\nstored on each capacitor is 600 _\u00b5_ C / 3 = 200 _\u00b5_ C.\n**Stepback Question** : What are the physics principles behind this question?\n**Principles** :\nCapacitors: Q = C *V, where Q is the charge, C is the capacitance and V is the\nvoltage.\nCapacitors in series: C = 1 / (1/C1 + 1/C2 + ... + 1/Cn), where C is the\nequivalent capacitance and C1, C2, Cn are the capacitances of each capacitor.\nCapacitor in parallel: C = C1 + C2 + ... + Cn, where C is the equivalent\ncapacitance and C1, C2, Cn are the capacitances of each capacitor.\n**Final Answer** : Using the Principles of Capacitance and Capacitors in series,\nwe can solve the problem as following:\nStep 1: Apply Capacitance to find out the voltage of the battery.\nWe know that the capacitance of the 2 _\u00b5_ F capacitor is 2 _\u00b5_ F and the charge\nstored on the capacitor is 600 _\u00b5_ C. Using the formula for capacitance, we can", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_23400", "chunk_text": " the battery.\nWe know that the capacitance of the 2 _\u00b5_ F capacitor is 2 _\u00b5_ F and the charge\nstored on the capacitor is 600 _\u00b5_ C. Using the formula for capacitance, we can\nfind the voltage of the battery:\nC = Q / V\n2 _\u00b5_ F = 600 _\u00b5_ C / V\nV = 300 V\n\n\nStep 2: Apply Capacitors in series to find out the equivalent capacitance of the\nthree 18 _\u00b5_ F capacitors in series.\nWe know that the capacitance of each of the three capacitors is 18 _\u00b5_ F. Using the\nformula for capacitors in series, we can find the equivalent capacitance of the\nthree capacitors:\n1 / _Ceq_ = 1 / _C_ 1 + 1 / _C_ 2 + 1 / _C_ 3\n1 / _Ceq_ = 1 / 18 _\u00b5_ F + 1 / 18 _\u00b5_ F + 1 / 18 _\u00b5_ F\n1 / _Ceq_ = 3 / 18 _\u00b5_ F\n_Ceq_ = 6 _\u00b5_ F\n\n\nStep 3: Apply Capacitance to find out the charge stored on each capacitor.\nWe know that the equivalent capacitance of the three capacitors is 6 _\u00b5_ F and the\nvoltage of the battery is 300 V. Using the formula for capacitance, we can find\nthe charge stored on each capacitor:\nC = Q / V\n6 _\u00b5_ F = Q / 300 V\nQ = 1800 _\u00b5_ C\n\n\nTherefore, the charge stored on each capacitor is 1800 _\u00b5_ C. The correct\nanswer is 3) 1800 _\u00b5_ C.\n\n\nTable 30: Illustration of wins of STEP-BACK PROMPTING on the MMLU-Physics dataset.\n\n\n34\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nMMLU- **Original Question** : Which of the following indicates that a reaction is\nChemistry spontaneous?\n0) at equilibrium there are more products than reactants\n1) the value of \u2206 _G_ [0] is greater than zero\n2) the value of \u2206 _S_ [0]", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_23850", "chunk_text": " reaction is\nChemistry spontaneous?\n0) at equilibrium there are more products than reactants\n1) the value of \u2206 _G_ [0] is greater than zero\n2) the value of \u2206 _S_ [0] is greater than zero\n3) the value of Keq is less than one\n**Original Answer [Incorrect]** : 2) the value of \u2206S- is greater than zero.\nA reaction is spontaneous if the value of \u2206S- is greater than zero. This is\nbecause a positive \u2206S- indicates that the reaction results in an increase in\nentropy, which is a measure of disorder. A spontaneous reaction is one that\n\n          - ccurs without the input of energy, and an increase in entropy is a natural\ntendency of the universe.\n**Stepback Question** : What are the chemistry principles behind this question?\n**Principles** :\nSpontaneous reactions: A spontaneous reaction is a reaction that occurs without\nthe input of energy.\n**Final Answer** : A reaction is spontaneous if the change in Gibbs free energy is\nnegative (\u2206 _G <_ 0). The value of \u2206 _G_ [0] is greater than zero indicates that the\nreaction is not spontaneous.\n\n\nThe correct answer is 0) at equilibrium there are more products than reactants.\n\n\nMMLU- **Original Question** : Potassium-40 is a minor isotope found in naturally ocChemistry curring potassium. It is radioactive and can be detected on simple radiation\ncounters. How many protons, neutrons, and electrons does potassium-40 have\nwhen it is part of K2SO4?\n0) 21 neutrons, 19 protons, 18 electrons\n1) 20 neutrons, 19 protons, 19 electrons\n2) 21 neutrons, 19 protons, 19 electrons\n3) 19 neutrons, 19 protons, 19 electrons\n**Original Answer [Incorrect]** : The correct answer is 2).\nPotassium-40 has 19 protons and 21 neutrons. When it is part of K2SO4, it has\n19 electrons.\n\n**Stepback Question** : What are the chemistry principles behind this question?\n**Principles** :\nAtomic number: The atomic number of an element is the number of protons in\nthe nucleus of an atom of that element", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_24300", "chunk_text": "\n19 electrons.\n\n**Stepback Question** : What are the chemistry principles behind this question?\n**Principles** :\nAtomic number: The atomic number of an element is the number of protons in\nthe nucleus of an atom of that element.\n**Final Answer** : The atomic number of potassium is 19, so all potassium atoms\nhave 19 protons. The mass number of potassium-40 is 40, so it has 40 - 19 =\n21 neutrons. When potassium is part of a compound, it loses one electron to\nbecome a potassium ion. Therefore, potassium-40 has 19 - 1 = 18 electrons\nwhen it is part of K2SO4.\nThe correct answer is 0) 21 neutrons, 19 protons, 18 electrons.\n\n\nTable 31: Illustration of wins of STEP-BACK PROMPTING on the MMLU-Chemistry dataset.\n\n\n35\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nTimeQA **Original Question** : Carlos Ascues played for which team from 2013 to\n2014?\n**Original Answer [Incorrect]** : Carlos Ascues played for Benfica from\n2013 to 2014.\n**Stepback Question** : Which teams did Carlos Ascues play for?\n**Final Answer [Correct]** : From 2013 to 2014, Carlos Ascues played for\nthe Greek team Panetolikos.\n\n\nTimeQA **Original Question** : Seth Nana Twumasi played for which team between\nJan 2011 and Feb 2011?\n**Original Answer [Incorrect]** : Seth Nana Twumasi played for the FC\nHonka team between Jan 2011 and Feb 2011.\n**Stepback Question** : Which teams did Seth Nana Twumasi play for?\n**Final Answer [Correct]** : Northampton. He was on a loan transfer from\nPeterborough.\n\n\nTimeQA **Original Question** : What position did Carmel Hanna take in May 2002?\n**Original Answer [Incorrect]** : According to the article, Carmel Hanna\nwas appointed as Minister of State for Children and the Family in May\n2002. She held this position until 2005.\n**Stepback Question** : What positions did Carmel Hanna hold?\n**Final Answer [Correct]** : From the context, we can see that Carmel\n", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_24750", "chunk_text": " and the Family in May\n2002. She held this position until 2005.\n**Stepback Question** : What positions did Carmel Hanna hold?\n**Final Answer [Correct]** : From the context, we can see that Carmel\nHanna was named minister of Employment and Learning (2001). We\ncan then infer that she held the position of Minister for Employment and\nLearning in May 2002.\n\n\nTable 32: Illustration of wins of STEP-BACK PROMPTING on the TimeQA dataset.\n\n\n36\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nSituatedQA **Original Question** : when was the last time miss texas won miss america\nas of November 29, 2010\n**Original Answer [Incorrect]** : Laura Leigh Covington was the last Miss\nTexas to win the Miss America competition. This happened in 2000.\n**Stepback Question** : which years did miss texas win miss america\n**Final Answer [Correct]** : According to Wikipedia, the last time Miss\nTexas won Miss America was in 1975.\n\n\nSituatedQA **Original Question** : when is the last time congress passed a complete\nbudget as of 2021\n**Original Answer [Incorrect]** : The last time Congress passed a complete\nbudget as of 2021 was on January 19, 2018.\n**Stepback Question** : what are the years that congress passed a complete\nbudget as of 2021\n**Final Answer [Correct]** : The last time Congress passed a complete budget as of 2021 was December 27, 2020, the Consolidated Appropriations\nAct, 2021.\n\n\nSituatedQA **Original Question** : when was the last time unc did not make the ncaa\ntournament as of 2003\n**Original Answer [Incorrect]** : The last time UNC did not make the\nNCAA tournament was 1990.\n**Stepback Question** : which years did unc not make the ncaa tournament\nas of 2003\n**Final Answer [Correct]** : The correct answer is 2002-03. The context\nshows the standings of North Carolina Tar Heels men\u2019s basketball. From\nthe standings, we can see that this team has 19 wins and 16 losses. 19-16\nis not good enough to make the NCAA tournament, which is mentioned\nin the context.", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_25200", "chunk_text": " Tar Heels men\u2019s basketball. From\nthe standings, we can see that this team has 19 wins and 16 losses. 19-16\nis not good enough to make the NCAA tournament, which is mentioned\nin the context. From the standings, we can also see that this is the\nstanding for 2002-03 season. So the last time UNC did not make the\nNCAA tournament as of 2003 is 2002-03.\n\n\nTable 33: Illustration of wins of STEP-BACK PROMPTING on the SituatedQA dataset.\n\n\n37\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nStrategyQA **Original Question** : Could you drive a Rowe 550 to the 2008 Summer\nOlympics?\n**Original Answer [Incorrect]** : The 2008 Summer Olympics was held in\nBeijing, China. The Rowe 550 is an all-electric car. The Rowe 550 has a\nrange of 70 miles on a single charge and a top speed of about 30 mph.\nThe distance between Beijing and the U.K. is about 4000 miles.\n**Stepback Question** : What kind of vehicle is a Rowe 550?\n**Final Answer [Correct]** : The Roewe 550 was released in 2008; the\n2008 Summer Olympics were held from August 8-24, 2008. So you\ncould drive a Rowe 550 to the 2008 Summer Olympics.\n\n\nStrategyQA **Original Question** : what channel does thursday night football come on\nregular tv as of 2017?\n**Original Answer [Incorrect]** : Thursday Night Football is a weekly\nseries of National Football League (NFL) games broadcast on Thursday\nnights. It started in 2006 on NFL Network. Under a new agreement\nbetween the NFL and CBS, CBS will air 50% of the Thursday Night\nFootball games and NFL Network will air the other 50%.\n**Stepback Question** : which channel is thursday night football on as of\n2017?\n**Final Answer [Correct]** : In 2017, Thursday Night Football was aired\n\n            - n CBS, NBC, and NFL Network.\n\n\nStrategyQA **Original Question** : who is the present president of assam sahitya sabha\nas of 2018?\n**Original Answer [Incorrect]** : Dr. Nagen Sa", "token_count": 500, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2310.06117_step_back_zheng:chunk_25650", "chunk_text": " - n CBS, NBC, and NFL Network.\n\n\nStrategyQA **Original Question** : who is the present president of assam sahitya sabha\nas of 2018?\n**Original Answer [Incorrect]** : Dr. Nagen Saikia\n**Stepback Question** : who lead assam sahitya sabha as of 2018?\n**Final Answer [Correct]** : The answer is Paramananda Rajbongshi. As\nper a news from 2019, The Asam Sahitya Sabha, which was established\nin 1917, in Sibsagar, Assam, had got Paramananda Rajbongshi as its\npresident. So, as of 2018, the president of assam sahitya sabha is Paramananda Rajbongshi.\n\n\nTable 34: Illustration of wins of STEP-BACK PROMPTING on the StrategyQA dataset.\n\n\n38\n\n\n", "token_count": 194, "metadata": {"arxiv_id": "2310.06117", "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models", "authors": ["Huaixiu Steven Zheng", "Swaroop Mishra", "Xinyun Chen", "Heng-Tze Cheng", "Ed H. Chi", "Quoc V Le", "Denny Zhou"], "year": 2023, "url": "https://arxiv.org/pdf/2310.06117v2"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_0", "chunk_text": "## **RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented** **Verification and Refinement**\n\n**Jinhao Jiang** **[1]** **, Jiayi Chen** _[\u2217]_ **[2]** _[\u2217]_ **, Junyi Li** **[4]** _[\u2217]_ **, Ruiyang Ren** **[1]** **, Shijie Wang** **[3]**\n\n**Wayne Xin Zhao** **[1]** _[\u2020]_ **, Yang Song** **[5]** _[\u2020]_ **, Tao Zhang** **[5]**\n\n1Gaoling School of Artificial Intelligence, Renmin University of China.\n2Wuhan University of Science and Technology. 3Northeastern University at Qinhuangdao.\n4Department of Computer Science, National University of Singapore. 5BOSS Zhipin, Beijing, China.\njiangjinhao@ruc.edu.cn, batmanfly@gmail.com\n\n\n\n**Abstract**\n\n\nExisting large language models (LLMs) show\nexceptional problem-solving capabilities but\nmight struggle with complex reasoning tasks.\nDespite the successes of chain-of-thought and\ntree-based search methods, they mainly depend\n\n  - n the internal knowledge of LLMs to search\n\n  - ver intermediate reasoning steps, limited to\ndealing with simple tasks involving fewer reasoning steps. In this paper, we propose **RAG-**\n**Star**, a novel RAG approach that integrates the\nretrieved information to guide the tree-based\ndeliberative reasoning process that relies on\nthe inherent knowledge of LLMs. By leveraging Monte Carlo Tree Search, RAG-Star iteratively plans intermediate sub-queries and\nanswers for reasoning based on the LLM itself.\nTo consolidate internal and external knowledge,\nwe propose an retrieval-augmented verification\nthat utilizes query- and answer-aware reward\nmodeling to provide feedback for the inherent\nreasoning of LLMs. Our experiments involving Llama-3.1-8B-Instruct and GPT-4o demonstrate that RAG-Star significantly outperforms\nprevious RAG and reasoning methods.\n\n\n**1** **Introduction**\n\n\nDespite the excellent capabilities of large language\nmodels (LLMs) (Zhao et al., 2023b), they still face\nsignificant challenges in complex reasoning tasks\n( _e.g.,_ multi-hop question answering), which", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_450", "chunk_text": "Introduction**\n\n\nDespite the excellent capabilities of large language\nmodels (LLMs) (Zhao et al., 2023b), they still face\nsignificant challenges in complex reasoning tasks\n( _e.g.,_ multi-hop question answering), which often\ngo beyond simple, single-step problem-solving,\ndemanding a deeper level of cognitive reasoning\nacross multiple facts, sources, or contexts (Huang\net al., 2024; Suzgun et al., 2023). Great efforts have\nbeen made to improve the reasoning effectiveness\n\n- f LLMs by conducing step-by-step reasoning, exemplified by chain-of-thought (CoT) (Wei et al.,\n2022). However, as the number of reasoning steps\ngrows, LLMs are often prone to introduce logical\n\n\n_\u2217_ Equal contributions.\n\n_\u2020_ Corresponding author.\n\n\n\nerrors, factual hallucinations, or inconsistent statements (Wei et al., 2022; Lyu et al., 2023).\n\nIn fact, step-by-step reasoning in the autoregressive generation paradigm can be described\nas akin to \u201cSystem 1\u201d, a mode of thinking which is\nfast, instinctive but less accurate (Kahneman, 2011).\nConversely, solving complex reasoning problems\nrequires more in-depth, deliberative, and logical\nthinking, known as the \u201cSystem 2\u201d mode, which requires conscious effort to conduct massive strategic\ndecision-making (Kahneman, 2011). To enhance\nthe \u201cSystem 2\u201d reasoning capabilities of LLMs,\nprior studies have proposed to conduct deliberative\ngeneration by leveraging basic tree search algorithms ( _e.g.,_ Monte Carlo Tree Search (Silver et al.,\n2017)). However, LLMs in these studies mainly\ndepend on their _internal knowledge_ to search over\nintermediate reasoning steps, limited to handling\nproblems with relatively simple reasoning process.\nTo leverage external knowledge in model reasoning,\nextensive research has sought to augment LLMs\nwith external information sources ( _a.k.a._ retrievalaugmented generation, RAG) (Lewis et al., 2020b;\nYao et al., 2022), while existing efforts mainly consider sequential reasoning structure, which cannot\nnaturally support more complex reasoning structure\nlike MCTS. Thus, we raise the following research\nquestion: _Can RAG enhance the deliberative rea-_\n_soning capabilities of", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_900", "chunk_text": " while existing efforts mainly consider sequential reasoning structure, which cannot\nnaturally support more complex reasoning structure\nlike MCTS. Thus, we raise the following research\nquestion: _Can RAG enhance the deliberative rea-_\n_soning capabilities of LLMs?_\n\nIn light of this, in this paper, we propose **RAG-**\n**Star**, a novel RAG-enhanced framework designed\nto improve multi-step reasoning capabilities of\nLLMs with deliberative planning. As the major\ntechnical contribution, RAG-Star can fully exploit\nthe internal knowledge of LLMs to plan the multistep reasoning, and meanwhile integrating the external retrieval to guide the internal reasoning process. To achieve this goal, we first introduce a\ntree-based search algorithm ( _i.e.,_ Monte Carlo Tree\nSearch, MCTS) with LLMs to search over possible\nplans for solving the problem at hand where a com\n\nplete plan is composed of a sequence of sub-queries\nand corresponding answers. Starting from the input\nquestion (root node), RAG-Star iteratively generates and selects an appropriate sub-query and its answer (intermediate node), which aims to maximally\nexplore the optimal sub-query path towards the final answer solely based on the inherent knowledge\n\n- f LLMs. Second, different from existing deliberation methods (Wang et al., 2024a; Yao et al., 2023),\nRAG-Star proposes retrieval-augmented verification that involves both query- and answer-aware\nreward modeling, fully exploiting external sources\nto guide the internal deliberative reasoning. In our\napproach, instead of directly interfering in the reasoning process of LLMs, we consider employing\nRAG to refine the derived reasoning steps in MCTS,\nwhich can effectively reduce the conflicts between\ninherent and external knowledge, which has been\na common issue when using RAG methods (Wang\net al., 2024b; Gao et al., 2023).\nWe conduct extensive experiments to verify the\neffectiveness of RAG-Star based on Llama-3.1-8B\nInstruct and GPT-4o. Our method outperforms the\nbaselines by up to 18.98% and 16.19% on average\nfor Llama-3.1-8B and GPT-4o, respectively.\nOur main contributions can be summarized as:\n\n_\u2022_ We", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_1350", "chunk_text": "baselines by up to 18.98% and 16.19% on average\nfor Llama-3.1-8B and GPT-4o, respectively.\nOur main contributions can be summarized as:\n\n_\u2022_ We propose RAG-Star that leverages external\nretrieval to enahnce the deliberative reasoning of\nLLMs based on their internal knowledge.\n\n_\u2022_ We design an effective retrieval-augmented\nverification and refinement to evaluate and correct\nthe inherent reasoning process.\n\n_\u2022_ We conduct extensive experiments on several datasets, where RAG-Star significantly outperforms existing RAG and reasoning methods.\n\n\n**2** **Related Work**\n\n\n**Retrieval-Augmented LLMs.** Augmenting large\nlanguage models (LLMs) with retrieval has been\nextensively studied in existing literature (Lewis\net al., 2020a; Borgeaud et al., 2022; Guu et al.,\n2020), which incorporates a differentiable retriever\nto provide external sources for LLMs. Furthermore, LLMs have made significant advancements\nin many reasoning tasks, such as code generation (OpenAI, 2023), math word problems (Zhu\net al., 2023) and question answering (Brown et al.,\n2020). Chain-of-thought (CoT) has been reported\nas an emergent ability of LLMs when they are large\nenough (Wei et al., 2022), which encourages LLMs\nto generate explicit intermediate reasoning steps\n\n\n\nin reasoning rather than simply providing answers\ndirectly. To elicit or improve the multi-step reasoning capability of LLMs, several approaches seek\nto harness the strengths of both CoT and retrieval\n\n- n knowledge-intensive complex reasoning tasks,\nsuch as multi-hop question answering (Yao et al.,\n2022; Zhao et al., 2023a). The rationales gained\nfrom reasoning enhance the retrieval of more relevant information, while the retrieved knowledge\nimproves the factuality of intermediate reasoning\nsteps. However, these approaches primarily take\nretrieved documents as direct input to the model,\neasily suffering from knowledge conflicts between\nthe parametric knowledge of LLMs and the external sources. In contrast, our RAG-Star framework\nintegrates tree-based search to fully explore the solution space and repurpose the retrieval information\nas external guidance to the reasoning process.\n\n\n**", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_1800", "chunk_text": "\nthe parametric knowledge of LLMs and the external sources. In contrast, our RAG-Star framework\nintegrates tree-based search to fully explore the solution space and repurpose the retrieval information\nas external guidance to the reasoning process.\n\n\n**Enhancing LLMs with Search.** Applying search\n\n- n top of LLMs has been a topic of much interest. Several recent works have explored search\nalgorithms to improve the performance of LLMs\nduring the inference stage (Wang et al., 2024a;\nZhang et al., 2024). The bitter lesson (Sutton,\n2019) famously suggests that two forms of scaling, _i.e.,_ learning and search, supersede all other\napproaches. Many studies have proven that scaling the inference-time computation can lead to\nsubstantial improvements in the performance of\nLLMs without training (Brown et al., 2024; Snell\net al., 2024). These search algorithms, where\nmultiple branches of outcomes are explored during search, have been widely applied in reinforcement learning algorithms (Hart et al., 1968; Silver\net al., 2017) and many real-world applications such\nas AlphaGo (Silver et al., 2016) for their good\nexploration-exploitation trade-off. However, these\napproaches mainly rely on the internal knowledge\n\n- f LLMs to search potential solutions, which might\nnot be optimal and leads to a amount of rollouts,\nsignificantly slowing down the decoding process.\nIn this paper, we leverage the external retrieval\nsources to enhance the deliberative search process\nwith LLMs, effectively differentiate the internal\nreasoning and external retrieval.\n\n\n**3** **Preliminary**\n\n\nIn this section, we will first formally define our task\nand then introduce Monte Carlo Tree Search which\n\nis used in our proposed RAG-Star approach.\n\n\n**Task Formulation.** In this work, we mainly focus on open-domain multi-hop question answering (Chen et al., 2019; Yang et al., 2018), which\nrequires multiple steps of reasoning across different documents to answer questions. Previous work\ntypically adopts an iterative _reason-then-generate_\npipeline (Wei et al., 2022; Huang and Chang, 2023).\nAt each step, the LLM first infers an intermediate\nsub-query based on the current situation and then\n", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_2250", "chunk_text": " an iterative _reason-then-generate_\npipeline (Wei et al., 2022; Huang and Chang, 2023).\nAt each step, the LLM first infers an intermediate\nsub-query based on the current situation and then\ngenerates possible answers to the query. Formally,\ngiven a natural language input question, at the _t_ - th\nstep, the LLM _M\u03b8_ (parameterized by _\u03b8_ ) first deliberately reasons about a sub-query _qt_, followed\nby generating an answer _at_ based on its inherent\nknowledge. In some literature (Yao et al., 2022;\nAsai et al., 2024), retrieval-augmented generation\n(RAG) has been employed to improve the factuality of intermediate reasoning steps. For each subquery _qt_, the retriever retrieves top- _K_ documents\n_Dt_ = _{dt,k}_ _[K]_ _k_ =1 [from an external large-scale cor-]\npus, _e.g.,_ Wikipedia, supplying them to the LLM\n\nto generate more accurate answers.\n\n\n**Monte Carlo Tree Search (MCTS).** In existing literature (Zelikman et al., 2024; Zhang et al., 2024),\nMCTS builds a search tree _T_ based on a policy\nmodel _\u03c0\u03b8_, which is usually the target LLM _M\u03b8_ .\nEach node _st_ = [ _qt, at, N_ ( _st_ ) _, V_ ( _st_ )] represents a\nstate comprising the sub-query _qt_, its answer _at_,\nthe number of visits _N_ ( _st_ ), and the value function\n(expected reward) _V_ ( _st_ ) for accurately answering\nquestions, except that the root node _s_ 0 = [ _q_ 0] only\ncontains the original input question _q_ 0, and each\nedge is an action aiming to generate the next subquery. During the search process, MCTS runs for\nmultiple simulations. For the _t_ - th simulation, it\nconducts four operations to expand the tree:\n\n_\u2022 Selection_ aims to select a node with the highest\nUCT (Upper Confidence bounds applied to Trees)\nscore (Kocsis and Szepesv\u00e1ri, 2006) starting from\nthe root node _s", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_2700", "chunk_text": " the tree:\n\n_\u2022 Selection_ aims to select a node with the highest\nUCT (Upper Confidence bounds applied to Trees)\nscore (Kocsis and Szepesv\u00e1ri, 2006) starting from\nthe root node _s_ 0. The UCT score of a child node\nwith state _st_ is calculated as follows:\n\n\n\nexpanded child node _st_ +1 until the task is solved\nand obtain a reward _r_ based on the rollout results.\n\n_\u2022 Backpropagation_  - peration leverages the reward _r_ - f the child node to update the expected\nreward _V_ ( _st_ ) of nodes along the path from the root\nnode to the current node:\n\n\n_Nnew_ ( _st_ ) = _Nold_ ( _st_ ) + 1 _,_ (2)\n\n_Vnew_ ( _st_ ) = _[V][old]_ [(] _[s][t]_ [)] _[N][old]_ [(] _[s][t]_ [)][ +] _[ r]_ _,_ (3)\n\n_Nnew_ ( _st_ )\n\n\nwhere _Nold_ ( _st_ ) and _Vold_ ( _st_ ) are the number of visits and value function at last iteration, respectively.\n\n\n**4** **Approach**\n\n\n**4.1** **Overview**\n\n\nRAG has been an indispensable technique to address the inherent knowledge limitations of LLMs,\neffectively integrating requisite information and\ngrounding to reliable sources (Lewis et al., 2020a;\nGuu et al., 2020). However, existing work mainly\nutilizes RAG to provide supplementary knowledge,\nwhile overlooking a thorough investigation of RAG\n\n- n enhancing the inherent reasoning capabilities of\nLLMs. To address this, we propose RAG-Star, a\nframework to fully harness the potential of internal knowledge in LLMs for multi-step reasoning\nguided by the external retrieval.\nOur RAG-Star framework contains two major\ntechnical steps. First, we propose _tree-based sub-_\n_query generation_ to perform deliberative reasoning with MCTS, totally relying on the inherent\nknowledge of LLMs. Second, we design _retrieval-_\n_augmented verification_ capitalizing on RAG to assist in guiding the reasoning based on the external\nknowledge. Under this framework, RAG-Star first", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_3150", "chunk_text": " on the inherent\nknowledge of LLMs. Second, we design _retrieval-_\n_augmented verification_ capitalizing on RAG to assist in guiding the reasoning based on the external\nknowledge. Under this framework, RAG-Star first\nselects a node from the tree to explore (Section 4.2),\nthen generates the next sub-query and answers for\n\n- btaining new child nodes (Section 4.3), and computes a reward to the expanded nodes (Section 4.4).\nFinally, it backpropagates the reward to update their\nparent nodes on the tree (Section 4.4). This process\nwill iterate until the task is solved. Next, we will\ndescribe each step in detail.\n\n\n**4.2** **Node Selection**\n\n\nTo answer multi-hop questions, our framework will\niterate the tree-based search process multiple times\nto gradually generate inference solutions in a stepby-step way. In our work, the solution is composed\n\n- f a sequence of intermediate sub-queries and the\n\n\n\n_UCT_ ( _st_ ) = _V_ ( _st_ ) + _w_\n\n\n\n\n\n\n\nln _N_ ( _p_ )\n\n(1)\n_N_ ( _st_ ) _[,]_\n\n\n\nwhere _w_ controls the exploration and exploitation,\nand _p_ is the parent node of the current node _st_ .\n\n_\u2022 Expansion_ explores multiple child nodes\n_{st_ +1 _}_ from the selected node _st_ through repeated\nsampling based on the policy model _\u03c0\u03b8_ .\n\n_\u2022 Simulation_ aims to perform rollout for each\n\n\nFigure 1: Overall framework of our proposed RAG-Star approach.\n\n\n\nassociated answers. At each iteration, it first selects\nan appropriate node from the current tree for the\nnext exploration or expansion. The selection operation is based on the node values computed through\nthe reward modeling and backpropagation steps.\nSpecifically, starting from the root node _s_ 0 ( _i.e.,_\nthe input question _q_ 0), our RAG-Star model selects one node with the highest score from its child\nnodes, and then sequentially selects the next best\nchild node layer-by-layer along the tree until reaching a leaf node, _i.e.,_ the terminal state indicating the\nfinal answer. To better balance exploration and exploitation, we use the UCT algorithm (Kocsis and\nSzepesv\u00e1", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_3600", "chunk_text": " layer-by-layer along the tree until reaching a leaf node, _i.e.,_ the terminal state indicating the\nfinal answer. To better balance exploration and exploitation, we use the UCT algorithm (Kocsis and\nSzepesv\u00e1ri, 2006) to calculate the score of each\nnode according to its number of visits _N_ ( _s_ ) and\nexpected reward _V_ ( _s_ ) in Eq. 1.\n\n\n**4.3** **Plan Expansion**\n\n\nAfter selecting the current node, it expands the\nsearch tree by repeatively sampling multiple child\nnodes as _plan_ based on the policy model _\u03c0\u03b8_ . Specially, the expansion process involves two steps,\n_i.e.,_ sub-query generation and answer deduction.\n\n\n**Sub-query Planning.** To generate the next subquery as plan, our approach first builds the context information by concatenating states from the\nroot node to the current selected node, and then\ninstructs the policy model to sample the next subquery based on the context information. Formally,\ngiven the node _st_, we can extract a path from the\nroot node _s_ 0 to the current node _st_, denoted by\n_H_ = _{q_ 0; _\u27e8q_ 1 _, a_ 1 _\u27e9_ ; _..._ ; _\u27e8qt, at\u27e9}_, where _q_ 0 is the\n\n- riginal input question and each _\u27e8qi, ai\u27e9_ pair denotes the planned sub-query and its answer verified by our retrieval-augmented varification (Section 4.4). We convert this path into the context\ninformation, and feed it to the policy model _\u03c0\u03b8_ to\n\n\n\ngenerate the next sub-query _qt_ +1 = _\u03c0\u03b8_ ( _H_ ). During\ninference, we employ repeated sampling to sample\nsub-queries by _mq_ times to fully exploit the policy\nmodel\u2019s inherent capabilities and obtain _mq_ new\nexpanded sub-queries.\n\n\n**Answer Deduction.** After planning the sub-query,\nwe further instruct the policy model to generate\nan answer to explore the internal knowledge of\nLLMs. Specially, for each planned sub-query _qt_ +1,\nwe directly feed the historical context _H_ and subquery into the policy model to generate a candidate\nanswer by leveraging the inherent", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_4050", "chunk_text": " to explore the internal knowledge of\nLLMs. Specially, for each planned sub-query _qt_ +1,\nwe directly feed the historical context _H_ and subquery into the policy model to generate a candidate\nanswer by leveraging the inherent knowledge encoded in its parameters as follows:\n\n\n_at_ +1 = _\u03c0\u03b8_ ( _H, qt_ +1) _._ (4)\n\n\nIn this process, we do not consider the external\nknowledge from RAG to avoid knowledge conflicts. We aim to fully exploit the potential of the\ninternal knowledge of LLMs without interference\nfrom external information, differing from previous\nretrieval-augmented work (Lewis et al., 2020b; Yao\net al., 2022) that might suffer from knowledge conflicts and interference. After obtaining the answer,\nwe can store each _\u27e8qt_ +1 _, at_ +1 _\u27e9_ pair in the corresponding node state, which will be subsequently\nused for reward modeling.\nWhen completing the plan expansion process,\nwe can obtain _mq_ child nodes for every parent node,\neach of which contains a sub-query _qt_ +1 and its\n\nanswer _at_ +1.\n\n\n**4.4** **Reward Modeling and Backpropagation**\n\n\nTraditional MCTS methods require to perform expensive rollout from the current node until the task\nends to evaluate the expanded nodes. In our work,\nfollowing previous work on process-supervised reward modeling (Setlur et al., 2024; Lightman et al.,\n\n\n2024), we propose _retrieval-augmented verifica-_\n_tion and refinement_ by using external knowledge\nto verify the consistency between the model output\nand retrieved information. Specifially, we employ\nreward models to assign an estimated reward _r_ to\nthe expanded node, which effectively quantifies the\neffectiveness of the policy model in successfully\nanswering the input question if continually reasoning from the current node. Next, we introduce\nthe involved two kinds of reward scores, namely\n_answer-aware reward_ and _query-aware reward_ .\n\n\n**Answer-aware Reward** . We first introduce the\nanswer-aware reward in the verification process.\nFirst, given a sub-query _qt_ +1, we follow existing\nmethods (Lewis et al., 2020a) to retrieve top- _K_\ndocuments _Dt_ +1 = _{dt_ +1 _,k}_", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_4500", "chunk_text": "First, given a sub-query _qt_ +1, we follow existing\nmethods (Lewis et al., 2020a) to retrieve top- _K_\ndocuments _Dt_ +1 = _{dt_ +1 _,k}_ _[K]_ _k_ =1 [from the exter-]\nnal corpus. Based on the retrieved documents, we\nthen employ the reward model to assign an _answer-_\n_aware reward ra_ to the currently generated answer\n_at_ +1 from the internal knowledge of LLMs. Specifically, there are overall three cases for the knowledge consistency between _at_ +1 and _Dt_ +1 with different rewards:\n\n\n\n_ra_ =\n\n\n\n\uf8f1\uf8f41 _,_ if _at_ +1 cannot be verified by _Dt_ +1\n\n\n2 _,_ if _at_ +1 is in conflict with _Dt_ +1\n\n\uf8f2\n\n\uf8f4\uf8f33 _,_ if _at_ +1 is aligned with _Dt_ +1\n\n\n\nNote that in the second case ( _i.e., at_ +1 is in conflict\nwith _Dt_ +1), we assign a moderate score 2 to the answer because we will refine _at_ +1 with a new potential answer \u02dc _at_ +1 from the external knowledge _Dt_ +1\nto support the policy model to continually reason\nfrom the current node. However, if the answer _at_ +1\ncannot be verified by the external knowledge, we\nwill assign the lowest score 1 to the answer, avoiding the policy model from exploring the potentially\nrisky solution space.\n\n\n**Query-aware Reward** . In addition to evaluating\nthe consistency of the generated answer with external knowledge, we employ the reward model\nto provide a _query-aware reward rq_ for measuring the plausibility of the planned sub-query\n_qt_ +1 based on the historical context information\nfrom the root node to current node, _i.e., H_ =\n_{q_ 0; _\u27e8q_ 1 _, a_ 1 _\u27e9_ ; _..._ ; _\u27e8qt, at\u27e9}_ . If the sub-query evaluated by the reward model is logically inconsistent\nwith the history plan, the score _rq_ is set to 0; otherwise, it is set to 1. Therefore, the final reward _r_", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_4950", "chunk_text": " at\u27e9}_ . If the sub-query evaluated by the reward model is logically inconsistent\nwith the history plan, the score _rq_ is set to 0; otherwise, it is set to 1. Therefore, the final reward _r_ for\nthe expanded node _st_ +1 is computed as _r_ = _ra \u00b7 rq_ .\nThis step aims to prevent the policy model from\n\n\n\ncontinuing to reason along illogical sub-queries.\nAfter obtaining the final reward for the newly\nexpanded node, we backpropagate the reward to update the value of nodes from the root node _s_ 0 to the\ncurrent node _st_ +1. For each node _s_ 0 _, s_ 1 _, ..., st_ +1 in\nthe path, its number of visits _N_ ( _s_ ) and the value\n_V_ ( _s_ ) will be updated according to Eq. 2. These updated values are used in the UCT algorithm in Eq. 1\nto guide the node selection at the next iteration.\n\n\n**4.5** **Reward Model Training**\n\n\nIn the reward modeling process, the capacity of the\nreward model critically influences the search process and ultimate answer accuracy. However, utilizing close-source model API or very large LLMs\nincurs substantial computational costs for deployment. Hence, we adopt a knowledge distillation\ntechnique to transfer capabilities from an advanced\nLLM, which usually has more parameters, to a relatively smaller model. This involves two phases:\ndata synthesis and instruction fine-tuning.\nDuring data synthesis, we mix up training sets\nfrom our evaluation datasets to maintain diversity.\nFirst, we adopt in-context learning to instruct the\npolicy model to generate a CoT format solution\nand then break down into multiple sub-steps, each\nincorporating the input question, accumulated reasoning paths, and a sub-query specific to the current\nstep. To further ensure diversity, only one random\nstep from each sample is selected for subsequent\ninstruction data creation. We then employ a more\nadvanced LLM ( _i.e.,_ GPT-4o-mini) combined with\na retrieval system to evaluate the sub-query and its\nanswer for each step (Section 4.4), and filter the\n\n- utput that fails to meet the format criteria. Finally,\nwe compile a dataset of intermediate steps and their\nquery and answer rewards from an advanced L", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_5400", "chunk_text": "-query and its\nanswer for each step (Section 4.4), and filter the\n\n- utput that fails to meet the format criteria. Finally,\nwe compile a dataset of intermediate steps and their\nquery and answer rewards from an advanced LLM.\nIn the instruction fine-tuning phase, we utilize the\nsynthetic samples to fine-tune a smaller LLM ( _i.e.,_\nLlama-3.1-8B-Instruct), thereby enhancing its capabilities in reward modeling.\n\n\n**5** **Experiments**\n\n\n**5.1** **Experimental Setup**\n\n\n**Datasets and Evaluation Metrics.** We select\n\nfour typical complex multi-hop question-answering\ndatasets, _i.e.,_ HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MusiQue (Trivedi\net al., 2022), and StrategyQA (Geva et al., 2021).\nFor evaluation metrics, we use Exact Match (EM),\nF1 score, and Cover Exact Match (Cover EM),\n\n\nwhere Cover EM measures whether the ground\ntruth answer is covered in the generated answer.\nWe randomly select 100 samples from the whole\nvalidation sets of each dataset as our final test set\nfor all baselines and our method.\n\n\n**Baselines.** We compare **RAG-Star** to the following two types of baselines based on GPT-4o and\nLlama-3.1-8B-Instruct:\n\n  - **Vanilla prompting methods** including direct\nprompting, Chain-of-Thought (CoT), and standard\nRAG. Direct prompting instructs the model to directly generate answers and CoT incorporates intermediate reasoning steps, which are all based on\nthe inherent knowledge of LLMs. Standard RAG\nfirst retrieves documents from Wikipedia based on\nDPR (Karpukhin et al., 2020) as prompts and then\ngenerates the final answers.\n\n  - **Improved RAG methods** including Iterative\nRAG (Xu et al., 2024), Judge-then-retrieve (Asai\net al., 2024), and Generate-then-retrieve (Wang\net al., 2023). We reimplement all of these baselines\nin our experiments. Iterative RAG iteratively decomposes the input question into sub-queries for\nre", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_5850", "chunk_text": "4), and Generate-then-retrieve (Wang\net al., 2023). We reimplement all of these baselines\nin our experiments. Iterative RAG iteratively decomposes the input question into sub-queries for\nretrieval and generation, ultimately ensembling all\nintermediate answers; Judge-then-retrieve first decides whether the retrieval is needed, autonomously\ndeciding to utilize either internal or external knowledge to aid in generation; Generate-then-retrieve\nfirst generates an initial answer used for retrieving\nmore documents relevant to the question and then\ngenerates the final answer based on documents.\n\n\n**Implementation Details.** We use a closed-source\nmodel (GPT-4o) and an open-source model (Llama3.1-8B-Instruct) as our policy models to measure\nthe performance of the RAG-Star framework. For\nthe reward models, we use GPT-4o-mini and a\nfine-tuned Llama-3.1-8B-Instruct. For HotpotQA,\nwe only use the abstract of articles in Wikipedia\n2017 dump as the retrieval corpus following Yang\net al. (2018), while for other datasets, we use the\nwhole articles in Wikipedia 2018 dump (Karpukhin\net al., 2020). Moreover, for the retrieval model, we\nuse FAISS for index building and BGE-large-env1.5 (Xiao et al., 2023) for dense passage retrieval.\nFor all retrieval-based baselines, we retrieve top-5\ndocuments and employ greedy search for decoding\nwith a temperature of 0. For RAG-Star, we set the\nmaximum number of simulations to 50 and a maxi\nmum of 6 layers. In UCT algorithm, the weight _w_\nto control the exploration and exploitation is set to\n\n\n\n0 _._ 2. We also retrieve top-5 documents and sample\nthree sub-queries at a time ( _mq_ = 3) with temperature 1 _._ 0 and top- _p_ sampling where _p_ = 1 _._ 0. For\nanswer generation, we sample an answer using a\ntemperature of 0 _._ 9 and top- _p_ sampling set to 1 _._ 0.\n\n\n**5.2** **Main Results**\n\n\nTable 1 shows the results of RAG-Star", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_6300", "chunk_text": " sample an answer using a\ntemperature of 0 _._ 9 and top- _p_ sampling set to 1 _._ 0.\n\n\n**5.2** **Main Results**\n\n\nTable 1 shows the results of RAG-Star and other\n\nbaselines across four representative multi-hop question answering datasets.\n\nFirstly, it can be observed that relatively smaller\nmodels ( _e.g.,_ Llama-3.1-8B-Instruct) show limited\nperformance on these knowledge-intensive reasoning tasks, achieving below 10% across three metrics in MusiQue. Although the Chain-of-Thought\ntechnique can slightly improve the answer recall\n( _e.g.,_ Cover EM scores of Llama-3.1-8B-Instruct\nand GPT-4o in MusiQue increase from 3.0% and\n19.0% to 16.0% and 27.0%, respectively), the\nmodel is prone to generating substantial irrelevant information in the output, decreasing the overall performance ( _e.g.,_ F1 score of Llama-3.1-8BInstruct drops from 21.9% to 7.1% on 2WikiMultihopQA).\n\nSecondly, based on the standard RAG, GPT-4o\nachieves substantial improvement in HotpotQA\n( _e.g.,_ Cover EM increases from 47.0% to 57.0%)\nbut exhibits a large decline in StrategyQA ( _e.g.,_\nCover EM from 73.0% to 62.0%), suggesting a potential conflict between external sources and internal knowledge of LLMs. We speculate the reason\nmight be that using the retrieved information directly as input incorporates some noises and makes\nthe LLM lost in the useful information. There\nfore, by controlling the utilization of internal and\nexternal knowledge, Judge-then-Retrieve can significantly alleviate this issue ( _e.g.,_ Cover EM from\n62.0% to 74.0% in StrategyQA). However, these\napproaches still present limited or even negative improvements in complex tasks ( _e.g.,_ Cover EM from\n19.0% to 16.0% in MusiQue), necessitating effective methods to consolidate external and internal\n\nknowledge.\n\nFinally, our approach outperforms all baselines\nacross most metrics in four datasets. RAG-Star\n\n", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_6750", "chunk_text": "19.0% to 16.0% in MusiQue), necessitating effective methods to consolidate external and internal\n\nknowledge.\n\nFinally, our approach outperforms all baselines\nacross most metrics in four datasets. RAG-Star\n\nintroduces a \u201cSystem 2\u201d-like slow and deliberative thinking process and employs RAG to verify\nand guide the multi-step reasoning process. By\nemploying retrieval-augmented verification, the reward model can effectively encourage the model\ntowards plausible sub-query nodes or avert from\n\n\n**HotpotQA** **2WikiMultihopQA** **MusiQue** **StrategyQA**\n**Method**\n\n**EM** **CEM** **F1** **EM** **CEM** **F1** **EM** **CEM** **F1** **EM** **CEM** **F1**\n\n\nLlama-3.1-8B-Instruct 14.0 25.0 26.0 9.0 29.0 21.9 2.0 3.0 3.9 63.0 65.0 63.0\n\n\n+ Chain-of-Tought 20.0 38.0 26.3 4.0 32.0 7.1 4.0 16.0 6.6 55.0 69.0 55.0\n+ Standard RAG 40.0 48.0 52.8 17.0 23.0 26.1 11.0 11.0 15.5 63.0 64.0 63.0\n\n+ Iterative RAG 26.0 31.0 36.9 22.0 23.0 26.0 7.0 11.0 15.9 61.0 63.0 61.0\n\n+ Generate-then-Retrieve 34.0 44.0 49.4 21.0 30.0 26.6 13.0 17.0 19.4 63.0 67.0 63.0\n+ Judge-then-Retrieve 39.0 48.0 53.9 18.0 26.0 26.8 10.0 10.0 16", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_7200", "chunk_text": " 67.0 63.0\n+ Judge-then-Retrieve 39.0 48.0 53.9 18.0 26.0 26.8 10.0 10.0 16.0 58.0 63.0 58.0\n\n\n+ RAG-Star w Llama RM 42.0 44.0 54.4 34.0 38.0 42.0 13.0 18.0 22.2 **71.0** **72.0** **71.0**\n\n+ RAG-Star w GPT RM **46.0** **49.0** **60.0** **38.0** **43.0** **46.8** **22.2** **27.0** **30.7** 67.6 69.0 67.6\n\n\nGPT-4o 43.0 47.0 56.7 36.0 42.0 45.7 13.0 19.0 24.3 70.0 73.0 70.0\n\n\n+ Chain-of-Tought 36.0 49.0 56.8 38.0 55.0 53.9 20.0 27.0 29.6 37.0 79.0 37.0\n+ Standard RAG 47.0 57.0 63.7 25.0 26.0 31.2 14.0 18.0 20.6 45.0 62.0 45.0\n\n+ Iterative RAG 47.0 **59.0** 63.3 19.0 24.0 26.3 15.0 26.0 25.5 32.0 74.0 32.0\n\n+ Generate-then-Retrieve 44.0 57.0 62.0 29.0 36.0 37.5 23.0 28.0 31.0 50.0 68.0 50.0\n+ Judge-then-Retrieve 44.0 50.0 58.6 28.0 29.0 32.2 14.0 16.", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_7650", "chunk_text": "50.0 68.0 50.0\n+ Judge-then-Retrieve 44.0 50.0 58.6 28.0 29.0 32.2 14.0 16.0 22.8 **72.0** 74.0 **72.0**\n\n\n+ RAG-Star w Llama RM **48.0** 54.0 66.3 47.0 **68.0** **62.8** 25.0 36.0 39.0 61.0 **86.0** 61.0\n\n+ RAG-Star w GPT RM **48.0** 57.0 **68.6** **48.0** 63.0 61.7 **29.0** **40.0** **43.5** 60.0 81.0 60.0\n\n\nTable 1: Evaluation results on four representative multi-hop question answering tasks. \u201cRM\u201d is short for reward\nmodel. The **bold** and underline fonts denote the best and second best results in each dataset, respectively.\n\n\n**GPT-4o** **Llama3.1-8B**\n**Method**\n\n**CEM** **F1** **CEM** **F1**\n\n\nRAG-Star (Ours) 84.0 68.3 75.0 73.3\n\n\nw/o Query Score 82.0 68.0 71.0 69.0\nw/o Answer Score 80.0 66.3 66.0 65.3\n\nw/o Retrieval 78.0 67.3 67.0 66.0\nw/o Refine 77.0 68.2 70.0 68.1\n\n\nTable 2: Ablation study in StrategyQA.\n\n\n\npotential risky nodes. For example, equipped with\n\n- ur RAG-Star framework, Llama-3.1-8B-Instruct\nachieves higher scores in two challenging reasoning datasets, _i.e.,_ 2WikiMultihopQA and MusiQue,\nsignificantly beyond all baseline methods.\n\n\n**5.3** **Further Analysis**\n\n\nWe report further analysis in StrategyQA with randomly selected 100 samples \u2013 we have similar findings in other datasets.\n\n\n**Ablation Study.** To validate", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_8100", "chunk_text": "Que,\nsignificantly beyond all baseline methods.\n\n\n**5.3** **Further Analysis**\n\n\nWe report further analysis in StrategyQA with randomly selected 100 samples \u2013 we have similar findings in other datasets.\n\n\n**Ablation Study.** To validate the effectiveness of\n\n- ur proposed framework, we conduct an ablation\nanalysis of its key design elements. We design\nfour variants: (1) _w/o Retrieval_ removes the retrieved documents in reward modeling; (2) _w/o_\n_Refine_ does not refine the conflict answer with retrieved documents in reward modeling; (3) _w/o_\n_Query Reward_ removes the query-aware reward _rq_\nfor scoring; and (4) _w/o Answer Reward_ removes\nthe answer-aware reward _ra_ for scoring. We show\nthe results in Table 2. It is clear that all the vari\n\n\nFigure 2: Cover EM performance on the StrategyQA\n_w.r.t._ the number of simulations ( **Left** ) or the number of\ntraining data ( **Right** ).\n\n\nants perform worse than the original method, indicating the effectiveness of each component in\n\n- ur framework. Specifically, the performance of\n_w/o Retrieval_ drops significantly for Llama-3.1-8B,\nindicating that using external knowledge for verification can be highly beneficial for the inherent\nreasoning of LLMs. Similarly, _w/o Refine_ leads to a\ndecline in model performance, which highlights the\nimportance of repurposing external sources for correcting the errors in the model\u2019s reasoning process.\nMoreover, both _w/o Query Reward_ and _w/o Answer_\n_Reward_ variants lead to a substantial performance\ndecline, which suggests that the consistency and\nlogical plausibility of intermediate sub-queries and\nanswers are both critical for the model to plan the\ncorrect path towards the final answer.\n\n\n**Effect of Simulation Scaling.** Typically, scaling the simulation iterations will lead to a higher\n\n\n|Col1|Col2|\u2026|\n|---|---|---|\n|Life Hits is a 2006<br>drama film directe<br>by** Christian**\u2026|<br>  d|<br>  d|\n\n\n\nFigure 3: A qualitative example showing the deliberative reasoning process of RAG-Star in 2WikiMultihopQA.\n\n\n\nlevel of task-solving capability. To explore the relationship between simulation scaling and the", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_8550", "chunk_text": " d|<br>  d|\n\n\n\nFigure 3: A qualitative example showing the deliberative reasoning process of RAG-Star in 2WikiMultihopQA.\n\n\n\nlevel of task-solving capability. To explore the relationship between simulation scaling and the final\nperformance of RAG-Star, we test our model under\ndifferent maximum simulation iterations. Specifically, we vary the maximum simulation rounds in\na set _{_ 10 _,_ 20 _,_ 30 _,_ 40 _,_ 50 _,_ 60 _}_, and evaluate Llama\nand GPT-4o in StrategyQA with GPT-4o-mini as\nthe reward model. The results are presented in the\nFigure 2. We can see that as the maximum number\n\n- f simulation increases, the model\u2019s performance\ngradually improves, although the average time consumed also rises to some extent. This highlights\nthat scaling the test-time computation can further\npromote more thorough exploration and exploitation by the policy model within the search space.\nHowever, as the number of simulations further increases, the performance of the policy model tends\nto be saturated. Due to the limitation of inherent\n\nknowledge, the policy model cannot benefit a lot\nfrom conducting more simulations.\n\n\n**Effect of Reward Model.** In our framework, the\nreward model is used to assess the logical plausibility of the sub-query and the consistency between\nthe output answer and external sources. In this part,\nwe aim to explore how to train open-source reward\nmodels ( _i.e.,_ Llama-3.1-8B-Instruct) to achieve performance comparable to closed-source LLMs ( _i.e.,_\nGPT-4o-mini) by varying amounts of training data\nfrom 20K to 80K. Specifically, we employ different\namounts of training data to fine-tune Llama-3.1-8BInstruct and use the fine-tuned model to evaluate\nthe sub-query and its answer. As shown in Figure 2, we can see that as the amount of training\ndata increases, the reward model can achieve more\naccurate verification quality, significantly benefiting the planning and reasoning of the policy model.\nHowever, the performance gains tend to saturate at\nlater stages, necessitating instruction tuning data\n\n\n\nwith higher diversity and quality.\n\n\n**5.4** **Case Study**\n\n\nTo facilitate understanding of the entire workflow\n\n- f our proposed R", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_9000", "chunk_text": ".\nHowever, the performance gains tend to saturate at\nlater stages, necessitating instruction tuning data\n\n\n\nwith higher diversity and quality.\n\n\n**5.4** **Case Study**\n\n\nTo facilitate understanding of the entire workflow\n\n- f our proposed RAG-Star, we present a qualitative analysis in 2WikiMultihopQA. Throughout\nthe search process, the LLM initializes the input\nquestion as root node and conducts multiple simulations, eventually reaching the terminal leaf node,\nwhich can be vividly represented as a tree. As\nshown in Figure 3, after selecting the first query\n( _i.e., Who is the director of \u201cLife Hits\u201d?_ ), the model\nexpands multiple children nodes by repeated sampling. At the next iteration, the model refines the\ngenerated answer ( _i.e., 1998_ ) for the sub-query\n(\u201c _When was Christian born?_ \u201d) based on retrieved\n\ndocuments and the reward model returns an overall\n\nscore of 2. By iterating the multi-step reasoning\nand retrieval-augmented verification processes for\nseveral rounds, the model outputs the final answer\n( _i.e., Life Hits_ ). In the task-solving process, the policy model generates an answer to the current subquery based on its internal knowledge, which might\nbe erroneous due to the limited pre-training corpus\nin time or the memorization mistakes. Therefore,\nthe external knowledge can be beneficial to validate\nthe correctness of inherent knowledge of LLMs, effectively guiding the model to plan a reasonable\npath.\n\n\n**6** **Conclusion**\n\n\nIn this work, we proposed RAG-Star, a novel RAG\napproach for leveraging external retrieval technique\nto enhance the multi-step reasoning capabilities of\nLLMs. RAG-Star employed Monte Carlo Tree\nSearch to search intermediate sub-queries and corresponding answers. Moreover, RAG-Star introduced retrieval-augmented verification to evaluate\n\n\nthe plausibility and consistency of the planned subqueries and answers based on a query-aware and\nan answer-aware reward. At each iteration, RAGStar conducted node selection, plan expansion, reward modeling, and reward backpropagation sequentially to consolidate the internal knowledge of\nLLMs and external knowledge from RAG. Extensive experiments on several datasets showed that\n\n- ur proposed RAG-Star outperforms the traditional\nRAG and reasoning methods.\n\n\n**Limitations**\n\n\nDespite the great efforts that we have made, the\nexperimental", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_9450", "chunk_text": " external knowledge from RAG. Extensive experiments on several datasets showed that\n\n- ur proposed RAG-Star outperforms the traditional\nRAG and reasoning methods.\n\n\n**Limitations**\n\n\nDespite the great efforts that we have made, the\nexperimental analysis is still limited due to the\nmassive computational cost of tree-based search\napproaches. We will investigate into more types\n\n- f complex reasoning tasks and datasets. In our\nmodel, we only leverage Monte Carlo Tree Search\nto conduct our deleberative reasoning process. we\nmay consider investigate more kinds of search algorithms to verify the generalization and robustness\n\n- f our proposed framework. Moreover, the performance of our model is affected by the feedback\nquality provided by the reward model. Therefore, a\nwell-trained and performant reward model is important for guiding the reasoning process. We will consider other fine-tuning strategies and more LLMs\nin reward modeling.\n\n\n**References**\n\n\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\nIn _The Twelfth International Conference on Learning_\n_Representations, ICLR 2024, Vienna, Austria, May_\n_7-11, 2024_ . OpenReview.net.\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from trillions of tokens. In _International conference on ma-_\n_chine learning_, pages 2206\u20132240. PMLR.\n\n\nBradley C. A. Brown, Jordan Juravsky, Ryan Saul\nEhrlich, Ronald Clark, Quoc V. Le, Christopher R\u00e9,\n[and Azalia Mirhoseini. 2024. Large language mon-](https://doi.org/10.48550/ARXIV.2407.21787)\n[keys: Scaling inference compute with repeated sam-](https://doi.org/10.48550/ARXIV.2407.21787)\n[pling.](https://doi.org/10.48550", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_9900", "chunk_text": ".2407.21787)\n[keys: Scaling inference compute with repeated sam-](https://doi.org/10.48550/ARXIV.2407.21787)\n[pling.](https://doi.org/10.48550/ARXIV.2407.21787) _CoRR_, abs/2407.21787.\n\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\n\n\n\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In _Ad-_\n_vances in Neural Information Processing Systems 33:_\n_Annual Conference on Neural Information Process-_\n_ing Systems 2020, NeurIPS 2020, December 6-12,_\n_2020, virtual_ .\n\n\nJifan Chen, Shih-Ting Lin, and Greg Durrett. 2019.\n\n[Multi-hop question answering via reasoning chains.](http://arxiv.org/abs/1910.02610)\n_CoRR_, abs/1910.02610.\n\n\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,\nJinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen\nWang. 2023. Retrieval-augmented generation for\nlarge language models: A survey. _arXiv preprint_\n_arXiv:2312.10997_ .\n\n\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? A question answering benchmark with\nimplicit reasoning strategies. _Trans. Assoc. Comput._\n_Linguistics_, ", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_12150", "chunk_text": ". Challenging\nbig-bench tasks and whether chain-of-thought can\nsolve them. In _Findings of the Association for Com-_\n_putational Linguistics: ACL 2023, Toronto, Canada,_\n_July 9-14, 2023_, pages 13003\u201313051. Association for\nComputational Linguistics.\n\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition.\n_Trans. Assoc. Comput. Linguistics_, 10:539\u2013554.\n\n\nChaojie Wang, Yanchen Deng, Zhiyi Lv, Zeng Liang,\n[Jujie He, Shuicheng Yan, and Bo An. 2024a. Q*:](https://doi.org/10.48550/ARXIV.2406.14283)\n[Improving multi-step reasoning for llms with deliber-](https://doi.org/10.48550/ARXIV.2406.14283)\n[ative planning.](https://doi.org/10.48550/ARXIV.2406.14283) _CoRR_, abs/2406.14283.\n\n\nFei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen,\nand Sercan \u00d6 Ar\u0131k. 2024b. Astute rag: Overcoming imperfect retrieval augmentation and knowledge\nconflicts for large language models. _arXiv preprint_\n_arXiv:2410.07176_ .\n\n\nLiang Wang, Nan Yang, and Furu Wei. 2023.\nQuery2doc: Query expansion with large language\nmodels. In _Proceedings of the 2023 Conference on_\n_Empirical Methods in Natural Language Process-_\n_ing, EMNLP 2023, Singapore, December 6-10, 2023_,\npages 9414\u20139423. Association for Computational\nLinguistics.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. 2022. Chain-of-thought prompting\nelicits reasoning in large language models. In _Ad-_\n_vances in Neural Information Processing Systems 35", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_13050", "chunk_text": ", Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. _arXiv preprint_ .\n\n\nEric Zelikman, Georges Harik, Yijia Shao, Varuna\nJayasiri, Nick Haber, and Noah D. Goodman. 2024.\n[Quiet-star: Language models can teach themselves](https://doi.org/10.48550/ARXIV.2403.09629)\n[to think before speaking.](https://doi.org/10.48550/ARXIV.2403.09629) _CoRR_, abs/2403.09629.\n\n\nDi Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong\nLi, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco\nPavone, Yuqiang Li, et al. 2024. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. _arXiv preprint arXiv:2410.02884_ .\n\n\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\nQin, and Lidong Bing. 2023a. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\nIn _Proceedings of the 61st Annual Meeting of the_\n_Association for Computational Linguistics (Volume_\n_1: Long Papers), ACL 2023, Toronto, Canada, July_\n_9-14, 2023_, pages 5823\u20135840. Association for Computational Linguistics.\n\n\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023b. A survey of large language models. _CoRR_,\nabs/2303.18223.\n\n\nXinyu Zhu, Jun", "token_count": 500, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2412.12881_deliberative_rag_jiang:chunk_13500", "chunk_text": "ang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023b. A survey of large language models. _CoRR_,\nabs/2303.18223.\n\n\nXinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang,\nYongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2023. Solving math word problems via\ncooperative reasoning induced language models. In\n\n\n\n_Proceedings of the 61st Annual Meeting of the As-_\n_sociation for Computational Linguistics (Volume 1:_\n_Long Papers), ACL 2023, Toronto, Canada, July 9-14,_\n_2023_, pages 4471\u20134485. Association for Computational Linguistics.\n\n\n", "token_count": 168, "metadata": {"arxiv_id": "2412.12881", "title": "RAG-Star: Enhancing Deliberative Reasoning with Retrieval Augmented Verification and Refinement", "authors": ["Jinhao Jiang", "Jiayi Chen", "Junyi Li", "Ruiyang Ren", "Shijie Wang", "Wayne Xin Zhao", "Yang Song", "Tao Zhang"], "year": 2024, "url": "https://arxiv.org/pdf/2412.12881v1"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_0", "chunk_text": "## **ColBERT: Efficient and Effective Passage Search via** **Contextualized Late Interaction over BERT**\n\n\n\nOmar Khatab\n\nStanford University\n\n         - khatab@stanford.edu\n\n\n**ABSTRACT**\n\n\nRecent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed\nto fine-tuning deep language models (LMs) for document ranking.\nWhile remarkably effective, the ranking models based on these LMs\nincrease computational cost by orders of magnitude over prior approaches, particularly as they must feed each query\u2013document pair\nthrough a massive neural network to compute a single relevance\nscore. To tackle this, we present ColBERT, a novel ranking model\nthat adapts deep LMs (in particular, BERT) for efficient retrieval.\nColBERT introduces a _late interaction_ architecture that independently encodes the query and the document using BERT and then\nemploys a cheap yet powerful interaction step that models their\nfine-grained similarity. By delaying and yet retaining this finegranular interaction, ColBERT can leverage the expressiveness of\ndeep LMs while simultaneously gaining the ability to pre-compute\ndocument representations offline, considerably speeding up query\nprocessing. Beyond reducing the cost of re-ranking the documents\nretrieved by a traditional model, ColBERT\u2019s _pruning-friendly_ interaction mechanism enables leveraging vector-similarity indexes\nfor end-to-end retrieval directly from a large document collection.\nWe extensively evaluate ColBERT using two recent passage search\ndatasets. Results show that ColBERT\u2019s effectiveness is competitive\nwith existing BERT-based models (and outperforms every nonBERT baseline), while executing two orders-of-magnitude faster\nand requiring four orders-of-magnitude fewer FLOPs per query.\n\n\n**ACM Reference format:**\nOmar Khatab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In _Proceedings_\n\n_of Proceedings of the 43rd International ACM SIGIR Conference on Research_\n_and Development in Information Retrieval, Virtual Event, China, July 25\u201330,_\n_2020 (SIGIR \u201920),_ 10 pages.\nDOI: 10.1145/3397271.3401075\n\n\n**1** **INTRODUCTION**\n\n\nOver the past few years, the Information Retrieval (IR) community\nhas witnessed the introduction of a host of neural", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_450", "chunk_text": " pages.\nDOI: 10.1145/3397271.3401075\n\n\n**1** **INTRODUCTION**\n\n\nOver the past few years, the Information Retrieval (IR) community\nhas witnessed the introduction of a host of neural ranking models,\nincluding DRMM [7], KNRM [4, 36], and Duet [20, 22]. In contrast\n\n\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\n\n- n the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permited. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\n_SIGIR \u201920, Virtual Event, China_\n\u00a9 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n978-1-4503-8016-4/20/07...$15.00\n\nDOI: 10.1145/3397271.3401075\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|~~**Bag-of-Words (BoW) Model**~~<br>**BoW Model with NLU Augmentation**<br>**Neural Matching Model**|~~**Bag-of-Words (BoW) Model**~~<br>**BoW Model with NLU Augmentation**<br>**Neural Matching Model**|~~**Bag-of-Words (BoW) Model**~~<br>**BoW Model with NLU Augmentation**<br>**Neural Matching Model**|BERT-lare|BERT-lare|BERT-lare|\n|<br>~~**Deep Language Model**~~<br>**ColBERT (ours)**|<br>~~**Deep Language Model**~~<br>**ColBERT (ours)**|<br>~~**Deep Language Model**~~<br>**ColBERT (ours)**|<br>~~**Deep Language Model**~~<br>**ColBERT (ours)**", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_900", "chunk_text": "~~<br>**ColBERT (ours)**|<br>~~**Deep Language Model**~~<br>**ColBERT (ours)**|<br>~~**Deep Language Model**~~<br>**ColBERT (ours)**|<br>~~**Deep Language Model**~~<br>**ColBERT (ours)**|<br>~~**Deep Language Model**~~<br>**ColBERT (ours)**|\n\n\n\n10 [1]\n\n\n0.15 0.20 0.25 0.30 0.35 0.40\n\n\nMRR@10\n\n\n**Figure 1: Effectiveness (MRR@10) versus Mean Qery La-**\n**tency (log-scale) for a number of representative ranking**\n**models on MS MARCO Ranking [24]. Te figure also shows**\n**ColBERT. Neural re-rankers run on top of the official BM25**\n**top-1000 results and use a Tesla V100 GPU. Methodology and**\n**detailed results are in \u00a74.**\n\n\nto prior learning-to-rank methods that rely on hand-crafed features, these models employ embedding-based representations of\nqueries and documents and directly model _local interactions_ (i.e.,\nfine-granular relationships) between their contents. Among them,\na recent approach has emerged that _fine-tunes_ deep pre-trained\nlanguage models (LMs) like ELMo [29] and BERT [5] for estimating\nrelevance. By computing deeply-contextualized semantic representations of query\u2013document pairs, these LMs help bridge the\npervasive vocabulary mismatch [21, 42] between documents and\nqueries [30]. Indeed, in the span of just a few months, a number\n\n- f ranking models based on BERT have achieved state-of-the-art\nresults on various retrieval benchmarks [3, 18, 25, 39] and have\nbeen proprietarily adapted for deployment by Google [1] and Bing [2] .\nHowever, the remarkable gains delivered by these LMs come\nat a steep increase in computational cost. Hofstater\u00a8 _et al._ [9] and\nMacAvaney _et al._ [18] observe that BERT-based models in the literature are 100-1000\u00d7 more computationally expensive than prior\nmodels\u2014some of which are arguably _not_ inexpensive to begin with\n\n[", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_1350", "chunk_text": "\nMacAvaney _et al._ [18] observe that BERT-based models in the literature are 100-1000\u00d7 more computationally expensive than prior\nmodels\u2014some of which are arguably _not_ inexpensive to begin with\n\n[13]. Tis quality\u2013cost tradeoff is summarized by Figure 1, which\ncompares two BERT-based rankers [25, 27] against a representative\nset of ranking models. Te figure uses MS MARCO Ranking [24],\na recent collection of 9M passages and 1M queries from Bing\u2019s\nlogs. It reports retrieval effectiveness (MRR@10) on the official\nvalidation set as well as average query latency (log-scale) using a\nhigh-end server that dedicates one Tesla V100 GPU per query for\nneural re-rankers. Following the _re-ranking_ setup of MS MARCO,\nColBERT (re-rank), the Neural Matching Models, and the Deep LMs\nre-rank the MS MARCO\u2019s official top-1000 documents per query.\n\n\n1htps://blog.google/products/search/search-language-understanding-bert/\n2htps://azure.microsof.com/en-us/blog/bing-delivers-its-largest-improvementin-search-experience-using-azure-gpus/\n\n\n\n10 [5]\n\n\n10 [4]\n\n\n10 [3]\n\n\n10 [2]\n\n\n\nMatei Zaharia\n\nStanford University\nmatei@cs.stanford.edu\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|s|Col2|\n|---|---|\n|||\n|||\n|||\n|||\n|||\n|||\n\n\n\n**Query** **Document**\n\n\n\n**Query** **Document**\n\n\n\n**Query**\n\n\n\n**Query** **Document**\n\n\n\n**(c) All-to-all Interaction**\n\n\n\n**(a) Representation-based Similarity** **(b) Query-Document Interaction** **(c) All-to-all Interaction** **(d) Late Interaction**\n\n_(e.g., DSSM, SNRM)_ _(e.g., DRMM, KNRM, Conv-KNRM)_ _(e.g., BERT)_ _(i.e., the proposed ColBERT)_\n\n\n\n_(e.g., DSSM, SNRM)_\n\n\n\n**(b) Query-Document Interaction**\n\n_(e.g., DRMM, KNRM, Conv-KNRM)_ _(e.g., BERT)_\n\n\n\n_(e.g., DRMM, KNRM, Conv-KNRM)_\n\n\n\n**Figure 2: Schematic diagrams", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_1800", "chunk_text": "Document Interaction**\n\n_(e.g., DRMM, KNRM, Conv-KNRM)_ _(e.g., BERT)_\n\n\n\n_(e.g., DRMM, KNRM, Conv-KNRM)_\n\n\n\n**Figure 2: Schematic diagrams illustrating query\u2013document matching paradigms in neural IR. Te figure contrasts existing**\n**approaches (sub-figures (a), (b), and (c)) with the proposed late interaction paradigm (sub-figure (d)).**\n\n\n\nOther methods, including ColBERT (full retrieval), directly retrieve\nthe top-1000 results from the entire collection.\nAs the figure shows, BERT considerably improves search precision, raising MRR@10 by almost 7% against the best previous meth\n- ds; simultaneously, it increases latency by up to tens of thousands\n\n- f milliseconds even with a high-end GPU. Tis poses a challenging\ntradeoff since raising query response times by as litle as 100ms is\nknown to impact user experience and even measurably diminish\nrevenue [17]. To tackle this problem, recent work has started exploring using Natural Language Understanding (NLU) techniques\nto augment traditional retrieval models like BM25 [32]. For example, Nogueira _et al._ [26, 28] expand documents with NLU-generated\nqueries before indexing with BM25 scores and Dai & Callan [2] replace BM25\u2019s term frequency with NLU-estimated term importance.\nDespite successfully reducing latency, these approaches generally\nreduce precision substantially relative to BERT.\nTo reconcile efficiency and contextualization in IR, we propose\n**ColBERT**, a ranking model based on **co** ntextualized **l** ate interaction over **BERT** . As the name suggests, ColBERT proposes a novel\n_late interaction_ paradigm for estimating relevance between a query\n_q_ and a document _d_ . Under late interaction, _q_ and _d_ are separately\nencoded into two sets of contextual embeddings, and relevance is\nevaluated using cheap and _pruning-friendly_ computations between\nboth sets\u2014that is, fast computations that enable ranking without\nexhaustively evaluating every possible candidate.\nFigure 2 contrasts our proposed late interaction approach with\nexisting neural matching paradigms. On the lef, Figure 2 (a) illustrates _representation-focused_ rankers, which independently compute\nan embedding for _q_ and another for _d_ and estimate relevance", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_2250", "chunk_text": " late interaction approach with\nexisting neural matching paradigms. On the lef, Figure 2 (a) illustrates _representation-focused_ rankers, which independently compute\nan embedding for _q_ and another for _d_ and estimate relevance as\na single similarity score between two vectors [12, 41]. Moving to\nthe right, Figure 2 (b) visualizes typical _interaction-focused_ rankers.\nInstead of summarizing _q_ and _d_ into individual embeddings, these\nrankers model word- and phrase-level relationships across _q_ and _d_\nand match them using a deep neural network (e.g., with CNNs/MLPs\n\n[22] or kernels [36]). In the simplest case, they feed the neural network an _interaction matrix_ that reflects the similiarity between\nevery pair of words across _q_ and _d_ . Further right, Figure 2 (c) illustrates a more powerful interaction-based paradigm, which models\nthe interactions between words _within_ as well as _across q_ and _d_ at\nthe same time, as in BERT\u2019s transformer architecture [25].\n\n\n\nTese increasingly expressive architectures are in tension. While\ninteraction-based models (i.e., Figure 2 (b) and (c)) tend to be superior for IR tasks [8, 21], a representation-focused model\u2014by isolating the computations among _q_ and _d_ - makes it possible to precompute document representations offline [41], greatly reducing\nthe computational load per query. In this work, we observe that\nthe fine-grained matching of interaction-based models and the precomputation of document representations of representation-based\nmodels can be combined by retaining yet judiciously _delaying_ the\nquery\u2013document interaction. Figure 2 (d) illustrates an architecture that precisely does so. As illustrated, every query embedding\ninteracts with all document embeddings via a MaxSim operator,\nwhich computes maximum similarity (e.g., cosine similarity), and\nthe scalar outputs of these operators are summed across query\nterms. Tis paradigm allows ColBERT to exploit deep LM-based\nrepresentations while shifing the cost of encoding documents offline and amortizing the cost of encoding the query once across\nall ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., [1, 15]) to retrieve the\ntop-", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_2700", "chunk_text": " of encoding documents offline and amortizing the cost of encoding the query once across\nall ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., [1, 15]) to retrieve the\ntop- _k_ results directly from a large document collection, substantially improving _recall_ - ver models that only re-rank the output of\nterm-based retrieval.\n\nAs Figure 1 illustrates, ColBERT can serve queries in tens or\nfew hundreds of milliseconds. For instance, when used for reranking as in \u201cColBERT (re-rank)\u201d, it delivers over 170\u00d7 speedup\n(and requires 14,000\u00d7 fewer FLOPs) relative to existing BERT-based\nmodels, while being more effective than every non-BERT baseline\n(\u00a74.2 & 4.3). ColBERT\u2019s indexing\u2014the only time it needs to feed\ndocuments through BERT\u2014is also practical: it can index the MS\nMARCO collection of 9M passages in about 3 hours using a single\nserver with four GPUs (\u00a74.5), retaining its effectiveness with a space\nfootprint of as litle as few tens of GiBs. Our extensive ablation\nstudy (\u00a74.4) shows that late interaction, its implementation via\nMaxSim operations, and crucial design choices within our BERTbased encoders are all essential to ColBERT\u2019s effectiveness.\nOur main contributions are as follows.\n\n\n(1) We propose _late interaction_ (\u00a73.1) as a paradigm for efficient\nand effective neural ranking.\n(2) We present ColBERT (\u00a73.2 & 3.3), a highly-effective model\nthat employs novel BERT-based query and document encoders within the late interaction paradigm.\n\n\n(3) We show how to leverage ColBERT both for re-ranking on\ntop of a term-based retrieval model (\u00a73.5) and for searching\na full collection using vector similarity indexes (\u00a73.6).\n(4) We evaluate ColBERT on MS MARCO and TREC CAR, two\nrecent passage search collections.\n\n\n**2** **RELATED WORK**\n\n\n**Neural Matching Models.** Over the past few years, IR researchers\nhave introduced numerous neural architectures for ranking. In\nthis work, we compare against KNRM [4, 36], Duet [20, 22], ConvKNRM [4], and fastText+Conv", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_3150", "chunk_text": " past few years, IR researchers\nhave introduced numerous neural architectures for ranking. In\nthis work, we compare against KNRM [4, 36], Duet [20, 22], ConvKNRM [4], and fastText+ConvKNRM [10]. KNRM proposes a\ndifferentiable kernel-pooling technique for extracting matching\nsignals from an interaction matrix, while Duet combines signals\nfrom exact-match-based as well as embedding-based similarities\nfor ranking. Introduced in 2018, ConvKNRM learns to match _n_ grams in the query and the document. Lastly, fastText+ConvKNRM\n(abbreviated fT+ConvKNRM) tackles the absence of rare words\nfrom typical word embeddings lists by adopting sub-word token\nembeddings.\nIn 2018, Zamani _et al._ [41] introduced SNRM, a representationfocused IR model that encodes each query and each document as\na single, sparse high-dimensional vector of \u201clatent terms\u201d. By producing a sparse-vector representation for each document, SNRM\nis able to use a traditional IR inverted index for representing documents, allowing fast end-to-end retrieval. Despite highly promising\nresults and insights, SNRM\u2019s effectiveness is substantially outperformed by the state of the art on the datasets with which it was\nevaluated (e.g., see [18, 38]). While SNRM employs sparsity to allow using inverted indexes, we relax this assumption and compare\na (dense) BERT-based representation-focused model against our\nlate-interaction ColBERT in our ablation experiments in \u00a74.4. For a\ndetailed overview of existing neural ranking models, we refer the\nreaders to two recent surveys of the literature [8, 21].\n**Language Model Pretraining for IR.** Recent work in NLU\nemphasizes the importance pre-training language representation\nmodels in an unsupervised fashion before subsequently fine-tuning\nthem on downstream tasks. A notable example is BERT [5], a bidirectional transformer-based language model whose fine-tuning\nadvanced the state of the art on various NLU benchmarks. Nogueira _et_\n_al._ [25], MacAvaney _et al._ [18], and Dai & Callan [3] investigate\nincorporating such LMs (mainly BERT, but also ELMo [29]) on different ranking datasets. As illustrated in Figure 2 (", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_3600", "chunk_text": "aney _et al._ [18], and Dai & Callan [3] investigate\nincorporating such LMs (mainly BERT, but also ELMo [29]) on different ranking datasets. As illustrated in Figure 2 (c), the common\napproach (and the one adopted by Nogueira _et al._ - n MS MARCO\nand TREC CAR) is to feed the query\u2013document pair through BERT\nand use an MLP on top of BERT\u2019s [CLS] output token to produce a\nrelevance score. Subsequent work by Nogueira _et al._ [27] introduced\nduoBERT, which fine-tunes BERT to compare the relevance of a\n_pair_ - f documents given a query. Relative to their single-document\nBERT, this gives duoBERT a 1% MRR@10 advantage on MS MARCO\nwhile increasing the cost by at least 1.4\u00d7.\n**BERT Optimizations.** As discussed in \u00a71, these LM-based\nrankers can be highly expensive in practice. While ongoing efforts in the NLU literature for distilling [14, 33], compressing [40],\nand pruning [19] BERT can be instrumental in narrowing this gap,\n\n\n\n|score<br>MaxSim MaxSim|Col2|\n|---|---|\n|**MaxSim**|**MaxSim**|\n\n\n**Figure 3: Te general architecture of ColBERT given a query**\n_q_ **and a document** _d_ **.**\n\n\nthey generally achieve significantly smaller speedups than our redesigned architecture for IR, due to their generic nature, and more\naggressive optimizations ofen come at the cost of lower quality.\n**Efficient NLU-based Models.** Recently, a direction emerged\nthat employs expensive NLU computation offline. Tis includes\ndoc2query [28] and DeepCT [2]. Te doc2query model expands\neach document with a pre-defined number of synthetic queries\nqueries generated by a seq2seq transformer model that is trained to\ngenerate _queries_ given a document. It then relies on a BM25 index\nfor retrieval from the (expanded) documents. DeepCT uses BERT\nto produce the _term frequency_ component of BM25 in a contextaware manner, essentially representing a feasible realization of the\nterm-independence assumption with neural networks [23]. Lastly,\ndocTTTTTquery [26", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_4050", "chunk_text": " DeepCT uses BERT\nto produce the _term frequency_ component of BM25 in a contextaware manner, essentially representing a feasible realization of the\nterm-independence assumption with neural networks [23]. Lastly,\ndocTTTTTquery [26] is identical to doc2query except that it finetunes a pre-trained model (namely, T5 [31]) for generating the\npredicted queries.\nConcurrently with our drafing of this paper, Hofstater\u00a8 _et al._ [11]\npublished their Transformer-Kernel (TK) model. At a high level, TK\nimproves the KNRM architecture described earlier: while KNRM\nemploys kernel pooling on top of word-embedding-based interaction, TK uses a Transformer [34] component for contextually\nencoding queries and documents before kernel pooling. TK establishes a new state-of-the-art for non-BERT models on MS MARCO\n\n(Dev); however, the best non-ensemble MRR@10 it achieves is 31%\nwhile ColBERT reaches up to 36%. Moreover, due to indexing document representations offline and employing a MaxSim-based late\ninteraction mechanism, ColBERT is much more scalable, enabling\nend-to-end retrieval which is not supported by TK.\n\n\n**3** **COLBERT**\n\n\nColBERT prescribes a simple framework for balancing the quality\nand cost of neural IR, particularly deep language models like BERT.\nAs introduced earlier, delaying the query\u2013document interaction can\nfacilitate cheap neural re-ranking (i.e., through pre-computation)\nand even support practical end-to-end neural retrieval (i.e., through\npruning via vector-similarity search). ColBERT addresses how to\ndo so while still preserving the effectiveness of state-of-the-art\nmodels, which condition the bulk of their computations on the\njoint query\u2013document pair.\n\n\nEven though ColBERT\u2019s late-interaction framework can be applied to a wide variety of architectures (e.g., CNNs, RNNs, transformers, etc.), we choose to focus this work on bi-directional transformerbased encoders (i.e., BERT) owing to their state-of-the-art effectiveness yet very high computational cost.\n\n\n**3.1** **Architecture**\n\n\nFigure 3 depicts the general architecture of ColBERT, which comprises: (a) a query encoder _fQ_, (b) a document encoder _fD_, and (c)\nthe late interaction mechanism. Given a", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_4500", "chunk_text": "** **Architecture**\n\n\nFigure 3 depicts the general architecture of ColBERT, which comprises: (a) a query encoder _fQ_, (b) a document encoder _fD_, and (c)\nthe late interaction mechanism. Given a query _q_ and document _d_,\n_fQ_ encodes _q_ into a bag of fixed-size embeddings _Eq_ while _fD_ encodes _d_ into another bag _Ed_ . Crucially, each embeddings in _Eq_ and\n_Ed_ is _contextualized_ based on the other terms in _q_ - r _d_, respectively.\nWe describe our BERT-based encoders in \u00a73.2.\nUsing _Eq_ and _Ed_, ColBERT computes the relevance score between _q_ and _d_ via late interaction, which we define as a summation\n\n- f maximum similarity (MaxSim) operators. In particular, we find\nthe maximum cosine similarity of each _v_ \u2208 _Eq_ with vectors in _Ed_,\nand combine the outputs via summation. Besides cosine, we also\nevaluate squared L2 distance as a measure of vector similarity. Intuitively, this interaction mechanism sofly _searches_ for each query\nterm _tq_ - in a manner that reflects its context in the query\u2014against\nthe document\u2019s embeddings, quantifying the strength of the \u201cmatch\u201d\nvia the largest similarity score between _tq_ and a document term _td_ .\nGiven these term scores, it then estimates the document relevance\nby summing the matching evidence across all query terms.\nWhile more sophisticated matching is possible with other choices\nsuch as deep convolution and atention layers (i.e., as in typical\ninteraction-focused models), a summation of maximum similarity\ncomputations has two distinctive characteristics. First, it stands\n\n- ut as a particularly cheap interaction mechanism, as we examine\nits FLOPs in \u00a74.2. Second, and more importantly, it is amenable\nto highly-efficient pruning for top- _k_ retrieval, as we evaluate in\n\u00a74.3. Tis enables using vector-similarity algorithms for skipping\ndocuments without materializing the full interaction matrix or even\nconsidering each document in isolation. Other cheap choices (e.g.,\na summation of _average_ similarity scores, instead of maximum) are\npossible; however, many are less amenable to pruning.", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_4950", "chunk_text": "izing the full interaction matrix or even\nconsidering each document in isolation. Other cheap choices (e.g.,\na summation of _average_ similarity scores, instead of maximum) are\npossible; however, many are less amenable to pruning. In \u00a74.4, we\nconduct an extensive ablation study that empirically verifies the advantage of our MaxSim-based late interaction against alternatives.\n\n\n**3.2** **Qery & Document Encoders**\n\n\nPrior to late interaction, ColBERT encodes each query or document\ninto a bag of embeddings, employing BERT-based encoders. We\nshare a single BERT model among our query and document encoders but distinguish input sequences that correspond to queries\nand documents by prepending a special token [Q] to queries and\nanother token [D] to documents.\n**Qery Encoder.** Given a textual query _q_, we tokenize it into its\nBERT-based WordPiece [35] tokens _q_ 1 _q_ 2... _ql_ . We prepend the token\n\n[Q] to the query. We place this token right afer BERT\u2019s sequencestart token [CLS]. If the query has fewer than a pre-defined number\n\n- f tokens _Nq_, we pad it with BERT\u2019s special [mask] tokens up\nto length _Nq_ (otherwise, we truncate it to the first _Nq_ tokens).\nTis padded sequence of input tokens is then passed into BERT\u2019s\n\n\n\ndeep transformer architecture, which computes a contextualized\nrepresentation of each token.\nWe denote the padding with masked tokens as **query augmen-**\n**tation**, a step that allows BERT to produce query-based embeddings\nat the positions corresponding to these masks. Qery augmentation\nis intended to serve as a sof, differentiable mechanism for learning\nto expand queries with new terms or to re-weigh existing terms\nbased on their importance for matching the query. As we show in\n\u00a74.4, this operation is essential for ColBERT\u2019s effectiveness.\nGiven BERT\u2019s representation of each token, our encoder passes\nthe contextualized output representations through a linear layer\nwith no activations. Tis layer serves to control the dimension\n\n- f ColBERT\u2019s embeddings, producing _m_ - dimensional embeddings\nfor the layer\u2019s output size _m_ . As we discuss later in more detail,\nwe typically fix _m_ to be much smaller than", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_5400", "chunk_text": " serves to control the dimension\n\n- f ColBERT\u2019s embeddings, producing _m_ - dimensional embeddings\nfor the layer\u2019s output size _m_ . As we discuss later in more detail,\nwe typically fix _m_ to be much smaller than BERT\u2019s fixed hidden\ndimension.\n\nWhile ColBERT\u2019s embedding dimension has limited impact on\nthe efficiency of query encoding, this step is crucial for controlling\nthe space footprint of documents, as we show in \u00a74.5. In addition, it\ncan have a significant impact on query execution time, particularly\nthe time taken for transferring the document representations onto\nthe GPU from system memory (where they reside before processing\na query). In fact, as we show in \u00a74.2, gathering, stacking, and\ntransferring the embeddings from CPU to GPU can be the most\nexpensive step in re-ranking with ColBERT. Finally, the output\nembeddings are normalized so each has L2 norm equal to one.\nTe result is that the dot-product of any two embeddings becomes\nequivalent to their cosine similarity, falling in the [\u22121, 1] range.\n**Document Encoder.** Our document encoder has a very similar\narchitecture. We first segment a document _d_ into its constituent tokens _d_ 1 _d_ 2... _dm_, to which we prepend BERT\u2019s start token [CLS] followed by our special token [D] that indicates a document sequence.\nUnlike queries, we do not append [mask] tokens to documents. After passing this input sequence through BERT and the subsequent\nlinear layer, the document encoder filters out the embeddings corresponding to punctuation symbols, determined via a pre-defined list.\nTis filtering is meant to reduce the number of embeddings per document, as we hypothesize that (even contextualized) embeddings\n\n- f punctuation are unnecessary for effectiveness.\nIn summary, given _q_ = _q_ 0 _q_ 1... _ql_ and _d_ = _d_ 0 _d_ 1... _dn_, we compute\nthe bags of embeddings _Eq_ and _Ed_ in the following manner, where\n# refers to the [mask] tokens:\n\n\n\u201c\n_Eq_ := Normalize( CNN( BERT( [ _Q_ ] _q_ 0 _q_ 1... _ql_ ##...#\u201d) ) ) (1)\n\n\n_Ed_ := Filter( Normalize", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_5850", "chunk_text": " tokens:\n\n\n\u201c\n_Eq_ := Normalize( CNN( BERT( [ _Q_ ] _q_ 0 _q_ 1... _ql_ ##...#\u201d) ) ) (1)\n\n\n_Ed_ := Filter( Normalize( CNN( BERT(\u201c[ _D_ ] _d_ 0 _d_ 1... _dn_ \u201d) ) ) ) (2)\n\n\n**3.3** **Late Interaction**\n\n\nGiven the representation of a query _q_ and a document _d_, the relevance score of _d_ to _q_, denoted as _Sq_, _d_, is estimated via late interaction between their bags of contextualized embeddings. As\nmentioned before, this is conducted as a sum of maximum similarity computations, namely cosine similarity (implemented as\ndot-products due to the embedding normalization) or squared L2\ndistance.\n\n\n_Sq_, _d_ := - _j_ \u2208[|max _Ed_ |] _[E][q][i]_ [ \u00b7] _[ E][T]_ _dj_ (3)\n\n_i_ \u2208[| _Eq_ |]\n\n\n\n(unlike our approach in \u00a73.6). To begin with, our query serving subsystem loads the indexed documents representations into memory,\nrepresenting each document as a matrix of embeddings.\nGiven a query _q_, we compute its bag of contextualized embeddings _Eq_ (Equation 1) and, concurrently, gather the document representations into a 3-dimensional tensor _D_ consisting of _k_ document\nmatrices. We pad the _k_ documents to their maximum length to\nfacilitate batched operations, and move the tensor _D_ to the GPU\u2019s\nmemory. On the GPU, we compute a batch dot-product of _Eq_ and\n_D_, possibly over multiple mini-batches. Te output materializes a\n3-dimensional tensor that is a collection of cross-match matrices\n\nbetween _q_ and each document. To compute the score of each document, we reduce its matrix across document terms via a max-pool\n(i.e., representing an exhaustive implementation of our MaxSim\ncomputation) and reduce across query terms via a summation. Finally, we sort the _k_ documents by their total scores.\nRelative to existing neural rankers (especially, but not exclusively, BERT-based ones), this computation is very cheap that, in\nfact, its cost is dominated by", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_6300", "chunk_text": "ation. Finally, we sort the _k_ documents by their total scores.\nRelative to existing neural rankers (especially, but not exclusively, BERT-based ones), this computation is very cheap that, in\nfact, its cost is dominated by the cost of gathering and transferring\nthe pre-computed embeddings. To illustrate, ranking _k_ documents\nvia typical BERT rankers requires feeding BERT _k_ different inputs\neach of length _l_ = | _q_ | + | _di_ | for query _q_ and documents _di_, where\natention has quadratic cost in the length of the sequence. In contrast, ColBERT feeds BERT only a single, much shorter sequence of\nlength _l_ = | _q_ |. Consequently, ColBERT is not only cheaper, it also\nscales much beter with _k_ as we examine in \u00a74.2.\n\n\n**3.6** **End-to-end Top-** _k_ **Retrieval with ColBERT**\n\nAs mentioned before, ColBERT\u2019s late-interaction operator is specifically designed to enable end-to-end retrieval from a large collection,\nlargely to improve recall relative to term-based retrieval approaches.\nTis section is concerned with cases where the number of docu\nments to be ranked is too large for exhaustive evaluation of each\npossible candidate document, particularly when we are only interested in the highest scoring ones. Concretely, we focus here on\nretrieving the top- _k_ results directly from a large document collection with _N_ (e.g., _N_ = 10, 000, 000) documents, where _k_ \u226a _N_ .\nTo do so, we leverage the pruning-friendly nature of the MaxSim\n\n- perations at the backbone of late interaction. Instead of applying MaxSim between one of the query embeddings and all of one\ndocument\u2019s embeddings, we can use fast vector-similarity data\nstructures to efficiently conduct this search between the query\nembedding and _all_ document embeddings across the full collection. For this, we employ an off-the-shelf library for large-scale\nvector-similarity search, namely faiss [15] from Facebook. [4] In particular, at the end of offline indexing (\u00a73.4), we maintain a mapping\nfrom each embedding to its document of origin and then index all\ndocument embeddings into faiss.\nSubsequently", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_6750", "chunk_text": " [15] from Facebook. [4] In particular, at the end of offline indexing (\u00a73.4), we maintain a mapping\nfrom each embedding to its document of origin and then index all\ndocument embeddings into faiss.\nSubsequently, when serving queries, we use a two-stage procedure to retrieve the top- _k_ documents from the entire collection.\nBoth stages rely on ColBERT\u2019s scoring: the first is an approximate\nstage aimed at filtering while the second is a refinement stage. For\nthe first stage, we concurrently issue _Nq_ vector-similarity queries\n(corresponding to each of the embeddings in _Eq_ ) onto our faiss index. Tis retrieves the top- _k_ [\u2032] (e.g., _k_ [\u2032] = _k_ /2) matches for that vector\n\n\n4htps://github.com/facebookresearch/faiss\n\n\n\nColBERT is differentiable end-to-end. We fine-tune the BERT\nencoders and train from scratch the additional parameters (i.e., the\nlinear layer and the [Q] and [D] markers\u2019 embeddings) using the\nAdam [16] optimizer. Notice that our interaction mechanism has\nno trainable parameters. Given a triple \u27e8 _q_, _d_ [+], _d_ [\u2212] \u27e9 with query _q_,\npositive document _d_ [+] and negative document _d_ [\u2212], ColBERT is used\nto produce a score for each document individually and is optimized\nvia pairwise sofmax cross-entropy loss over the computed scores\n\n- f _d_ [+] and _d_ [\u2212] .\n\n\n**3.4** **Offline Indexing: Computing & Storing**\n**Document Embeddings**\n\n\nBy design, ColBERT isolates almost all of the computations between\nqueries and documents, largely to enable pre-computing document\nrepresentations offline. At a high level, our indexing procedure is\nstraight-forward: we proceed over the documents in the collection\nin batches, running our document encoder _fD_ - n each batch and\nstoring the output embeddings per document. Although indexing\na set of documents is an offline process, we incorporate a few\nsimple optimizations for enhancing the throughput of indexing. As\nwe show in \u00a74.5, these optimizations can considerably reduce the\n\n- ffline cost of indexing.\nTo begin with, we exploit multiple GPUs, if available,", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_7200", "chunk_text": " we incorporate a few\nsimple optimizations for enhancing the throughput of indexing. As\nwe show in \u00a74.5, these optimizations can considerably reduce the\n\n- ffline cost of indexing.\nTo begin with, we exploit multiple GPUs, if available, for faster\nencoding of batches of documents in parallel. When batching, we\npad all documents to the maximum length of a document _within_\nthe batch. [3] To make capping the sequence length on a per-batch\nbasis more effective, our indexer proceeds through documents in\ngroups of _B_ (e.g., _B_ = 100,000) documents. It sorts these documents\nby length and then feeds batches of _b_ (e.g., _b_ = 128) documents of\ncomparable length through our encoder. Tis length-based bucketing is sometimes refered to as a BucketIterator in some libraries\n(e.g., allenNLP). Lastly, while most computations occur on the GPU,\nwe found that a non-trivial portion of the indexing time is spent on\npre-processing the text sequences, primarily BERT\u2019s WordPiece tokenization. Exploiting that these operations are independent across\ndocuments in a batch, we parallelize the pre-processing across the\navailable CPU cores.\n\nOnce the document representations are produced, they are saved\nto disk using 32-bit or 16-bit values to represent each dimension.\nAs we describe in \u00a73.5 and 3.6, these representations are either\nsimply loaded from disk for ranking or are subsequently indexed\nfor vector-similarity search, respectively.\n\n\n**3.5** **Top-** _k_ **Re-ranking with ColBERT**\n\n\nRecall that ColBERT can be used for re-ranking the output of an\n- ther retrieval model, typically a term-based model, or directly\nfor end-to-end retrieval from a document collection. In this sec\ntion, we discuss how we use ColBERT for ranking a small set of\n_k_ (e.g., _k_ = 1000) documents given a query _q_ . Since _k_ is small, we\nrely on batch computations to exhaustively score each document\n\n\n3Te public BERT implementations we saw simply pad to a pre-defined length.\n\n\n- ver all document embeddings. We map each of those to its document of origin, producing _Nq_ \u00d7 _k_ [\u2032] document IDs, only _", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_7650", "chunk_text": "3Te public BERT implementations we saw simply pad to a pre-defined length.\n\n\n- ver all document embeddings. We map each of those to its document of origin, producing _Nq_ \u00d7 _k_ [\u2032] document IDs, only _K_ \u2264 _Nq_ \u00d7 _k_ [\u2032]\n\n- f which are unique. Tese _K_ documents likely contain one or more\nembeddings that are highly similar to the query embeddings. For\nthe second stage, we refine this set by exhaustively re-ranking _only_\nthose _K_ documents in the usual manner described in \u00a73.5.\nIn our faiss-based implementation, we use an IVFPQ index (\u201cinverted file with product quantization\u201d). Tis index partitions the\nembedding space into _P_ (e.g., _P_ = 1000) cells based on _k_ - means clustering and then assigns each document embedding to its nearest cell\nbased on the selected vector-similarity metric. For serving queries,\nwhen searching for the top- _k_ [\u2032] matches for a single query embedding, only the nearest _p_ (e.g., _p_ = 10) partitions are searched. To\nimprove memory efficiency, every embedding is divided into _s_ (e.g.,\n_s_ = 16) sub-vectors, each represented using one byte. Moreover,\nthe index conducts the similarity computations in this compressed\ndomain, leading to cheaper computations and thus faster search.\n\n\n**4** **EXPERIMENTAL EVALUATION**\n\n\nWe now turn our atention to empirically testing ColBERT, addressing the following research questions.\n**RQ** 1: In a typical re-ranking setup, how well can ColBERT bridge\nthe existing gap (highlighted in \u00a71) between highly-efficient and\nhighly-effective neural models? (\u00a74.2)\n**RQ** 2: Beyond re-ranking, can ColBERT effectively support endto-end retrieval directly from a large collection? (\u00a74.3)\n**RQ** 3: What does each component of ColBERT (e.g., late interaction, query augmentation) contribute to its quality? (\u00a74.4)\n**RQ** 4: What are ColBERT\u2019s indexing-related costs in terms of\n\n- ffline computation and memory overhead? (\u00a74.5)\n\n\n**4.1** **Methodology**\n\n\n_4.1", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_8100", "chunk_text": ".4)\n**RQ** 4: What are ColBERT\u2019s indexing-related costs in terms of\n\n- ffline computation and memory overhead? (\u00a74.5)\n\n\n**4.1** **Methodology**\n\n\n_4.1.1_ _Datasets & Metrics._ Similar to related work [2, 27, 28],\nwe conduct our experiments on the MS MARCO Ranking [24]\n(henceforth, MS MARCO) and TREC Complex Answer Retrieval\n(TREC-CAR) [6] datasets. Both of these recent datasets provide\nlarge training data of the scale that facilitates training and evaluating deep neural networks. We describe both in detail below.\n**MS MARCO.** MS MARCO is a dataset (and a corresponding\ncompetition) introduced by Microsof in 2016 for reading comprehension and adapted in 2018 for retrieval. It is a collection of 8.8M\npassages from Web pages, which were gathered from Bing\u2019s results\nto 1M real-world queries. Each query is associated with _sparse_\nrelevance judgements of one (or very few) documents marked as\nrelevant and no documents explicitly indicated as irrelevant. Per\nthe official evaluation, we use MRR@10 to measure effectiveness.\nWe use three sets of queries for evaluation. Te official devel\n- pment and evaluation sets contain roughly 7k queries. However,\nthe relevance judgements of the evaluation set are held-out by Microsof and effectiveness results can only be obtained by submiting\nto the competition\u2019s organizers. We submited our main re-ranking\nColBERT model for the results in \u00a74.2. In addition, the collection\nincludes roughly 55k queries (with labels) that are provided as additional validation data. We re-purpose a random sample of 5k\nqueries among those (i.e., ones not in our development or training\n\n\n\nsets) as a \u201clocal\u201d evaluation set. Along with the official development set, we use this held-out set for testing our models as well as\nbaselines in \u00a74.3. We do so to avoid submiting multiple variants\n\n- f the same model at once, as the organizers discourage too many\nsubmissions by the same team.\n**TREC CAR.** Introduced by Dietz [6] _et al._ in 2017, TREC CAR\nis a synthetic dataset based on Wikipedia that consists of about", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_8550", "chunk_text": " the organizers discourage too many\nsubmissions by the same team.\n**TREC CAR.** Introduced by Dietz [6] _et al._ in 2017, TREC CAR\nis a synthetic dataset based on Wikipedia that consists of about\n29M passages. Similar to related work [25], we use the first four of\nfive pre-defined folds for training and the fifh for validation. Tis\namounts to roughly 3M queries generated by concatenating the\ntitle of a Wikipedia page with the heading of one of its sections.\nTat section\u2019s passages are marked as relevant to the corresponding\nquery. Our evaluation is conducted on the test set used in TREC\n2017 CAR, which contains 2,254 queries.\n\n\n_4.1.2_ _Implementation._ Our ColBERT models are implemented\nusing Python 3 and PyTorch 1. We use the popular transformers [5]\n\nlibrary for the pre-trained BERT model. Similar to [25], we fine-tune\nall ColBERT models with learning rate 3 \u00d7 10 [\u2212][6] with a batch size\n32. We fix the number of embeddings per query at _Nq_ = 32. We set\n\n- ur ColBERT embedding dimension _m_ to be 128; \u00a74.5 demonstrates\nColBERT\u2019s robustness to a wide range of embedding dimensions.\nFor MS MARCO, we initialize the BERT components of the ColBERT query and document encoders using Google\u2019s official pretrained BERTbase model. Further, we train all models for 200k iterations. For TREC CAR, we follow related work [2, 25] and use a different pre-trained model to the official ones. To explain, the official\nBERT models were pre-trained on Wikipedia, which is the source\n\n- f TREC CAR\u2019s training and test sets. To avoid leaking test data\ninto train, Nogueira and Cho\u2019s [25] pre-train a randomly-initialized\nBERT model on the Wiki pages corresponding to training subset of\nTREC CAR. Tey release their BERTlarge pre-trained model, which\nwe fine-tune for ColBERT\u2019s experiments on TREC CAR. Since finetuning this model is significantly slower than BERTbase, we train\n\n- n TREC CAR for only 125k iterations.\nIn our re-ranking results, unless stated otherwise, we use 4 bytes\nper dimension in", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_9000", "chunk_text": " CAR. Since finetuning this model is significantly slower than BERTbase, we train\n\n- n TREC CAR for only 125k iterations.\nIn our re-ranking results, unless stated otherwise, we use 4 bytes\nper dimension in our embeddings and employ cosine as our vectorsimilarity function. For end-to-end ranking, we use (squared) L2\ndistance, as we found our faiss index was faster at L2-based retrieval. For our faiss index, we set the number of partitions to\n_P_ =2,000, and search the nearest _p_ = 10 to each query embedding to\nretrieve _k_ [\u2032] = _k_ = 1000 document vectors per query embedding. We\ndivide each embedding into _s_ = 16 sub-vectors, each encoded using\n\n- ne byte. To represent the index used for the second stage of our\nend-to-end retrieval procedure, we use 16-bit values per dimension.\n\n\n_4.1.3_ _Hardware & Time Measurements._ To evaluate the latency\n\n- f neural re-ranking models in \u00a74.2, we use a single Tesla V100 GPU\nthat has 32 GiBs of memory on a server with two Intel Xeon Gold\n6132 CPUs, each with 14 physical cores (24 hyperthreads), and 469\nGiBs of RAM. For the mostly CPU-based retrieval experiments in\n\u00a74.3 and the indexing experiments in \u00a74.5, we use another server\nwith the same CPU and system memory specifications but which\nhas four Titan V GPUs atached, each with 12 GiBs of memory.\nAcross all experiments, only one GPU is dedicated per query for\n\n\n5htps://github.com/huggingface/transformers\n\n\n**Method** **MRR@10 (Dev)** **MRR@10 (Eval)** **Re-ranking Latency** **(ms)** **FLOPs/query**\n\n\nBM25 (official) 16.7 16.5 - \n\nKNRM 19.8 19.8 3 592M (0.085\u00d7)\nDuet 24.3 24.5 22 159B (23\u00d7)\nfastText+ConvKNRM 29.0 27.7 28 78B (11\u00d7)\nBERTbase [25] 34.7 - 10,700 97T (13,", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_9450", "chunk_text": " 159B (23\u00d7)\nfastText+ConvKNRM 29.0 27.7 28 78B (11\u00d7)\nBERTbase [25] 34.7 - 10,700 97T (13,900\u00d7)\nBERTbase (our training) 36.0 - 10,700 97T (13,900\u00d7)\nBERTlarge [25] 36.5 35.9 32,900 340T (48,600\u00d7)\n\n\nColBERT (over BERTbase) 34.9 34.9 61 7B (1\u00d7)\n\n\n**Table 1: \u201cRe-ranking\u201d results on MS MARCO. Each neural model re-ranks the official top-1000 results produced by BM25.**\n**Latency is reported for re-ranking only. To obtain the end-to-end latency in Figure 1, we add the BM25 latency from Table 2.**\n\n\n**Method** **MRR@10 (Dev)** **MRR@10 (Local Eval)** **Latency** **(ms)** **Recall@50** **Recall@200** **Recall@1000**\n\n\nBM25 (official) 16.7 - - - - 81.4\nBM25 (Anserini) 18.7 19.5 62 59.2 73.8 85.7\ndoc2query 21.5 22.8 85 64.4 77.9 89.1\nDeepCT 24.3 - 62 _(est.)_ 69 [2] 82 [2] 91 [2]\ndocTTTTTquery 27.7 28.4 87 75.6 86.9 94.7\n\n\nColBERTL2 (re-rank) 34.8 36.4 - 75.3 80.5 81.4\nColBERTL2 (end-to-end) 36.0 36.7 458 82.9 92.3 96.8\n\n\n**Table 2: End-to-end retrieval results on MS MARCO. Each model retrieves the top-1000 documents per query** _**directly**_ **from the**\n**entire 8.8M document collection.**\n\n\n\nretrieval (i.e., for methods with neural computations", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_9900", "chunk_text": " MARCO. Each model retrieves the top-1000 documents per query** _**directly**_ **from the**\n**entire 8.8M document collection.**\n\n\n\nretrieval (i.e., for methods with neural computations) but we use\nup to all four GPUs during indexing.\n\n\n**4.2** **Qality\u2013Cost Tradeoff: Top-** _k_ **Re-ranking**\n\nIn this section, we examine ColBERT\u2019s efficiency and effectiveness\nat re-ranking the top- _k_ results extracted by a bag-of-words retrieval\nmodel, which is the most typical seting for testing and deploying\nneural ranking models. We begin with the MS MARCO dataset. We\ncompare against KNRM, Duet, and fastText+ConvKNRM, a representative set of neural matching models that have been previously\ntested on MS MARCO. In addition, we compare against the natural adaptation of BERT for ranking by Nogueira and Cho [25],\nin particular, BERTbase and its deeper counterpart BERTlarge. We\nalso report results for \u201cBERTbase (our training)\u201d, which is based on\nNogueira and Cho\u2019s base model (including hyperparameters) but\nis trained with the same loss function as ColBERT (\u00a73.3) for 200k\niterations, allowing for a more direct comparison of the results.\nWe report the competition\u2019s official metric, namely MRR@10,\n\n- n the validation set (Dev) and the evaluation set (Eval). We also\nreport the re-ranking latency, which we measure using a single\nTesla V100 GPU, and the FLOPs per query for each neural ranking\nmodel. For ColBERT, our reported latency subsumes the entire\ncomputation from gathering the document representations, moving\nthem to the GPU, tokenizing then encoding the query, and applying\nlate interaction to compute document scores. For the baselines,\nwe measure the scoring computations on the GPU and exclude\nthe CPU-based text preprocessing (similar to [9]). In principle,\nthe baselines can pre-compute the majority of this preprocessing\n(e.g., document tokenization) offline and parallelize the rest across\n\n\n\ndocuments online, leaving only a negligible cost. We estimate the\nFLOPs per query of each model using the torchprofile [6] library.\nWe now proceed to study the results, which are reported in Table 1. To", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_10350", "chunk_text": " rest across\n\n\n\ndocuments online, leaving only a negligible cost. We estimate the\nFLOPs per query of each model using the torchprofile [6] library.\nWe now proceed to study the results, which are reported in Table 1. To begin with, we notice the fast progress from KNRM in\n2017 to the BERT-based models in 2019, manifesting itself in over\n16% increase in MRR@10. As described in \u00a71, the simultaneous\nincrease in computational cost is difficult to miss. Judging by their\nrather monotonic patern of increasingly larger cost and higher effectiveness, these results appear to paint a picture where expensive\nmodels are necessary for high-quality ranking.\nIn contrast with this trend, ColBERT (which employs late interaction over BERTbase) performs no worse than the original adaptation of BERTbase for ranking by Nogueira and Cho [25, 27] and\nis only marginally less effective than BERTlarge and our training\n\n- f BERTbase (described above). While highly competitive in effectiveness, ColBERT is orders of magnitude cheaper than BERTbase,\nin particular, by over 170\u00d7 in latency and 13,900\u00d7 in FLOPs. Tis\nhighlights the expressiveness of our proposed late interaction mechanism, particularly when coupled with a powerful pre-trained LM\nlike BERT. While ColBERT\u2019s re-ranking latency is slightly higher\nthan the non-BERT re-ranking models shown (i.e., by 10s of milliseconds), this difference is explained by the time it takes to gather,\nstack, and transfer the document embeddings to the GPU. In particular, the query encoding and interaction in ColBERT consume only\n13 milliseconds of its total execution time. We note that ColBERT\u2019s\n\nlatency and FLOPs can be considerably reduced by padding queries\nto a shorter length, using smaller vector dimensions (the MRR@10\n\n- f which is tested in \u00a74.5), employing quantization of the document\n\n\n6htps://github.com/mit-han-lab/torchprofile\n\n\nvectors, and storing the embeddings on GPU if sufficient memory\nexists. We leave these directions for future work.\n\n\n\n10 [9]\n\n10 [8]\n\n10 [7]\n\n10 [6]\n\n10 [5]\n\n10 [4]\n\n10 [3]\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Col2|BERTbase|(ourtr|aining)|Col6|Col", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_10800", "chunk_text": ".\n\n\n\n10 [9]\n\n10 [8]\n\n10 [7]\n\n10 [6]\n\n10 [5]\n\n10 [4]\n\n10 [3]\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Col2|BERTbase|(ourtr|aining)|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|||**ColBER**|**T**|||~~**500**~~<br>**200**|**0**|\n|||||~~**50**~~|**100**<br>|~~**200**~~<br> **10**|**0**|\n|||**k=10**|**20**|||||\n|||||||||\n|~~**k**~~|~~**k**~~|~~**=10**~~|~~**20**~~|~~**50**~~|~~**100**~~<br>~~**500**~~<br>|**2000**||\n||||||**200**|**1000**||\n\n\n\n0.27 0.29 0.31 0.33 0.35 0.37\n\n\nMRR@10\n\n\n**Figure 4: FLOPs (in millions) and MRR@10 as functions**\n\n**of the re-ranking depth** _k_ **. Since the official BM25 ranking**\n**is not ordered, the initial top-** _k_ **retrieval is conducted with**\n**Anserini\u2019s BM25.**\n\n\nDiving deeper into the quality\u2013cost tradeoff between BERT and\nColBERT, Figure 4 demonstrates the relationships between FLOPs\nand effectiveness (MRR@10) as a function of the re-ranking depth\n_k_ when re-ranking the top- _k_ results by BM25, comparing ColBERT\nand BERTbase (our training). We conduct this experiment on MS\nMARCO (Dev). We note here that as the official top-1000 ranking\ndoes not provide the BM25 order (and also lacks documents beyond\nthe top-1000 per query), the models in this experiment re-rank the\nAnserini [37] toolkit\u2019s BM25 output. Consequently, both MRR@10\nvalues at _k_ = 1000 are slightly higher from those reported in Table 1.\nStudying the results in Figure 4, we notice that not only is ColBERT much", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_11250", "chunk_text": "25 output. Consequently, both MRR@10\nvalues at _k_ = 1000 are slightly higher from those reported in Table 1.\nStudying the results in Figure 4, we notice that not only is ColBERT much cheaper than BERT for the same model size (i.e., 12layer \u201cbase\u201d transformer encoder), it also scales beter with the\nnumber of ranked documents. In part, this is because ColBERT\n\n- nly needs to process the query once, irrespective of the number of\ndocuments evaluated. For instance, at _k_ = 10, BERT requires nearly\n180\u00d7 more FLOPs than ColBERT; at _k_ = 1000, BERT\u2019s overhead\njumps to 13,900\u00d7. It then reaches 23,000\u00d7 at _k_ = 2000. In fact, our\ninformal experimentation shows that this orders-of-magnitude gap\nin FLOPs makes it practical to run ColBERT entirely on the CPU,\nalthough CPU-based re-ranking lies outside our scope.\n\n\nMethod MAP MRR@10\n\n\nBM25 (Anserini) 15.3     doc2query 18.1     DeepCT 24.6 33.2\n\n\nBM25 + BERTbase 31.0     BM25 + BERTlarge 33.5     \n\nBM25 + ColBERT 31.3 44.3\n\n\n**Table 3: Results on TREC CAR.**\n\n\nHaving studied our results on MS MARCO, we now consider\nTREC CAR, whose official metric is MAP. Results are summarized\nin Table 3, which includes a number of important baselines (BM25,\ndoc2query, and DeepCT) in addition to re-ranking baselines that\n\n\n\nhave been tested on this dataset. Tese results directly mirror those\nwith MS MARCO.\n\n\n**4.3** **End-to-end Top-** _k_ **Retrieval**\n\n\nBeyond cheap re-ranking, ColBERT is amenable to top- _k_ retrieval directly from a full collection. Table 2 considers full retrieval, wherein\neach model retrieves the top-1000 documents directly from MS\nMARCO\u2019s 8.8M documents per query. In addition to MRR@10 and\nlatency in milliseconds, the table reports Recall@50, Recall@200,\nand Recall@1000, important metrics", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_11700", "chunk_text": " documents directly from MS\nMARCO\u2019s 8.8M documents per query. In addition to MRR@10 and\nlatency in milliseconds, the table reports Recall@50, Recall@200,\nand Recall@1000, important metrics for a full-retrieval model that\nessentially filters down a large collection on a per-query basis.\nWe compare against BM25, in particular MS MARCO\u2019s official\nBM25 ranking as well as a well-tuned baseline based on the Anserini\ntoolkit. [7] While many other traditional models exist, we are not\naware of any that substantially outperform Anserini\u2019s BM25 implementation (e.g., see RM3 in [28], LMDir in [2], or Microsof\u2019s\nproprietary feature-based RankSVM on the leaderboard).\nWe also compare against doc2query, DeepCT, and docTTTTTquery. All three rely on a traditional bag-of-words model (primarily BM25) for retrieval. Crucially, however, they re-weigh the\nfrequency of terms per document and/or expand the set of terms\nin each document before building the BM25 index. In particular,\ndoc2query expands each document with a pre-defined number\n\n- f synthetic queries generated by a seq2seq transformer model\n(which docTTTTquery replaced with a pre-trained language model,\nT5 [31]). In contrast, DeepCT uses BERT to produce the term frequency component of BM25 in a context-aware manner.\nFor the latency of Anserini\u2019s BM25, doc2query, and docTTTTquery, we use the authors\u2019 [26, 28] Anserini-based implementation.\nWhile this implementation supports multi-threading, it only utilizes\nparallelism across different queries. We thus report single-threaded\nlatency for these models, noting that simply parallelizing their\ncomputation over _shards_ - f the index can substantially decrease\ntheir already-low latency. For DeepCT, we only estimate its latency\nusing that of BM25 (as denoted by _(est.)_ in the table), since DeepCT\nre-weighs BM25\u2019s term frequency without modifying the index\n\n- therwise. [8] As discussed in \u00a74.1, we use ColBERTL2 for end-toend retrieval, which employs negative squared L2 distance as its\nvector-similarity function. For its latency,", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_12150", "chunk_text": " the index\n\n- therwise. [8] As discussed in \u00a74.1, we use ColBERTL2 for end-toend retrieval, which employs negative squared L2 distance as its\nvector-similarity function. For its latency, we measure the time for\nfaiss-based candidate filtering and the subsequent re-ranking. In\nthis experiment, faiss uses all available CPU cores.\nLooking at Table 2, we first see Anserini\u2019s BM25 baseline at 18.7\nMRR@10, noticing its very low latency as implemented in Anserini\n(which extends the well-known Lucene system), owing to both\nvery cheap operations and decades of bag-of-words top- _k_ retrieval\n\n- ptimizations. Te three subsequent baselines, namely doc2query,\nDeepCT, and docTTTTquery, each brings a decisive enhancement\nto effectiveness. Tese improvements come at negligible overheads\nin latency, since these baselines ultimately rely on BM25-based\nretrieval. Te most effective among these three, docTTTTquery,\ndemonstrates a massive 9% gain over vanilla BM25 by fine-tuning\nthe recent language model T5.\n\n\n7htp://anserini.io/\n8In practice, a myriad of reasons could still cause DeepCT\u2019s latency to differ\nslightly from BM25\u2019s. For instance, the top- _k_ pruning strategy employed, if any, could\ninteract differently with a changed distribution of scores.\n\n\nShifing our atention to ColBERT\u2019s end-to-end retrieval effectiveness, we see its major gains in MRR@10 over all of these end-toend models. In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due\nto the improved recall. Moving beyond MRR@10, we also see large\ngains in Recall@ _k_ for _k_ equals to 50, 200, and 1000. For instance,\nits Recall@50 actually exceeds the official BM25\u2019s Recall@1000 and\neven all but docTTTTTquery\u2019s Recall@200, emphasizing the value\n\n- f end-to-end retrieval (instead of just re-ranking) with ColBERT.\n\n|BERT [CLS]-based dot-product (5-layer) [A]|Col2|Col3|Col4|Col5|Col6", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_12600", "chunk_text": " emphasizing the value\n\n- f end-to-end retrieval (instead of just re-ranking) with ColBERT.\n\n|BERT [CLS]-based dot-product (5-layer) [A]|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|**ColBERT via average similarity (5-layer) [B]**||||||||||\n|**ColBERT without query augmentation (5-layer) [C]**||||||||||\n|**ColBERT (5-layer) [D]**||||||||||\n|**ColBERT (12-layer) [E]**||||||||||\n|**ColBERT + e2e retrieval (12-layer) [F]**||||||||||\n\n\n\n0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36\n\n\nMRR@10\n\n\n**Figure 5: Ablation results on MS MARCO (Dev). Between**\n**brackets is the number of BERT layers used in each model.**\n\n\nTe results from \u00a74.2 indicate that ColBERT is highly effective\ndespite the low cost and simplicity of its late interaction mechanism.\nTo beter understand the source of this effectiveness, we examine a\nnumber of important details in ColBERT\u2019s interaction and encoder\narchitecture. For this ablation, we report MRR@10 on the validation\nset of MS MARCO in Figure 5, which shows our main _re-ranking_\nColBERT model [E], with MRR@10 of 34.9%.\nDue to the cost of training all models, we train a copy of our\nmain model that retains only the first 5 layers of BERT out of 12\n(i.e., model [D]) and similarly train all our ablation models for 200k\niterations with five BERT layers. To begin with, we ask if the finegranular _interaction_ in late interaction is necessary. Model [A]\ntackles this question: it uses BERT to produce a single embedding\nvector for the query and another for the document, extracted from\nBERT\u2019s [CLS] contextualized embedding and expanded through a\nlinear layer to dimension 4096 (which equals _Nq_ \u00d7 128 = 32 \u00d7 128).\nRelevance is estimated", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_13050", "chunk_text": " and another for the document, extracted from\nBERT\u2019s [CLS] contextualized embedding and expanded through a\nlinear layer to dimension 4096 (which equals _Nq_ \u00d7 128 = 32 \u00d7 128).\nRelevance is estimated as the inner product of the query\u2019s and the\ndocument\u2019s embeddings, which we found to perform beter than\ncosine similarity for single-vector re-ranking. As the results show,\nthis model is considerably less effective than ColBERT, reinforcing\nthe importance of late interaction.\nSubsequently, we ask if our MaxSim-based late interaction is better than other simple alternatives. We test a model [B] that replaces\nColBERT\u2019s maximum similarity with _average_ similarity. Te results\nsuggest the importance of individual terms in the query paying\nspecial atention to particular terms in the document. Similarly,\nthe figure emphasizes the importance of our query augmentation\nmechanism: without query augmentation [C], ColBERT has a noticeably lower MRR@10. Lastly, we see the impact of end-to-end\nretrieval not only on recall but also on MRR@10. By retrieving\ndirectly from the full collection, ColBERT is able to retrieve to the\ntop-10 documents missed entirely from BM25\u2019s top-1000.\n\n\n\n|Basic ColBERT Indexing<br>+multi-GPU document processing<br>+per-batch maximum sequence length<br>+length-based bucketing<br>+multi-core pre-processing|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_13500", "chunk_text": "**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n\n\n0 10000 20000 30000 40000 50000\n\n\nThroughput (documents/minute)\n\n\n**Figure 6: Effect of ColBERT\u2019s indexing optimizations on the**\n\n**offline indexing throughput.**\n\n\n**4.5** **Indexing Troughput & Footprint**\n\n\nLastly, we examine the indexing throughput and space footprint\n\n- f ColBERT. Figure 6 reports indexing throughput on MS MARCO\ndocuments with ColBERT and four other ablation setings, which\nindividually enable optimizations described in \u00a73.4 on top of basic\nbatched indexing. Based on these throughputs, ColBERT can index\nMS MARCO in about three hours. Note that any BERT-based model\nmust incur the computational cost of processing each document\nat least once. While ColBERT encodes each document with BERT\n\nexactly once, existing BERT-based rankers would repeat similar\ncomputations on possibly hundreds of documents for each query.\n\n\nSeting Dimension( _m_ ) Bytes/Dim Space(GiBs) MRR@10\n\n\nRe-rank Cosine 128 4 286 34.9\n\nEnd-to-end L2 128 2 154 36.0\n\nRe-rank L2 128 2 143 34.8\n\nRe-rank Cosine 48 4 54 34.4\n\nRe-rank Cosine 24 2 27 33.9\n\n\n**Table 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.**\n\n\nTable 4 reports the space footprint of ColBERT under various\nsetings as we reduce the embeddings dimension", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_13950", "chunk_text": "27 33.9\n\n\n**Table 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.**\n\n\nTable 4 reports the space footprint of ColBERT under various\nsetings as we reduce the embeddings dimension and/or the bytes\nper dimension. Interestingly, the most space-efficient seting, that\nis, re-ranking with cosine similarity with 24-dimensional vectors\nstored as 2-byte floats, is only 1% worse in MRR@10 than the most\nspace-consuming one, while the former requires only 27 GiBs to\nrepresent the MS MARCO collection.\n\n\n**5** **CONCLUSIONS**\n\n\nIn this paper, we introduced ColBERT, a novel ranking model that\nemploys _contextualized late interaction_ - ver deep LMs (in particular,\nBERT) for efficient retrieval. By independently encoding queries\nand documents into fine-grained representations that interact via\ncheap and pruning-friendly computations, ColBERT can leverage\nthe expressiveness of deep LMs while greatly speeding up query\nprocessing. In addition, doing so allows using ColBERT for end-toend neural retrieval directly from a large document collection. Our\nresults show that ColBERT is more than 170\u00d7 faster and requires\n14,000\u00d7 fewer FLOPs/query than existing BERT-based models, all\nwhile only minimally impacting quality and while outperforming\nevery non-BERT baseline.\n**Acknowledgments.** OK was supported by the Eltoukhy Family\nGraduate Fellowship at the Stanford School of Engineering. Tis\nresearch was supported in part by affiliate members and other\nsupporters of the Stanford DAWN project\u2014Ant Financial, Facebook,\nGoogle, Infosys, NEC, and VMware\u2014as well as Cisco, SAP, and the\n\n\nNSF under CAREER grant CNS-1651570. Any opinions, findings,\nand conclusions or recommendations expressed in this material are\nthose of the authors and do not necessarily reflect the views of the\nNational Science Foundation.\n\n\n**REFERENCES**\n\n\n[1] Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\nto Index: Optimizing Exact Maximum Inner Product Search. In _2019 IEEE 35th_\n_International Conference on Data Engineering (ICDE)_ . IEEE, 1250\u20131261.\n\n[2] Zhuyun Dai and Jamie Callan. ", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_14400", "chunk_text": " Optimizing Exact Maximum Inner Product Search. In _2019 IEEE 35th_\n_International Conference on Data Engineering (ICDE)_ . IEEE, 1250\u20131261.\n\n[2] Zhuyun Dai and Jamie Callan. 2019. Context-Aware Sentence/Passage Term\nImportance Estimation For First Stage Retrieval. _arXiv preprint arXiv:1910.10687_\n(2019).\n\n[3] Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with\nContextual Neural Language Modeling. _arXiv preprint arXiv:1905.09217_ (2019).\n\n[4] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional\nneural networks for sof-matching n-grams in ad-hoc search. In _Proceedings of the_\n_eleventh ACM international conference on web search and data mining_ . 126\u2013134.\n\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.\n_arXiv preprint arXiv:1810.04805_ (2018).\n\n[6] Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TREC\nComplex Answer Retrieval Overview.. In _TREC_ .\n\n[7] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Crof. 2016. A deep relevance\nmatching model for ad-hoc retrieval. In _Proceedings of the 25th ACM International_\n\n_on Conference on Information and Knowledge Management_ . ACM, 55\u201364.\n\n[8] Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani,\nChen Wu, W Bruce Crof, and Xueqi Cheng. 2019. A deep look into neural\n\n[9] Sebastian Hofstranking models for information retrieval.ater and Allan Hanbury. 2019. Let\u2019s measure run time! Extending\u00a8 _arXiv preprint arXiv:1903.06902_ (2019).\nthe IR replicability infrastructure to", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_14850", "chunk_text": " models for information retrieval.ater and Allan Hanbury. 2019. Let\u2019s measure run time! Extending\u00a8 _arXiv preprint arXiv:1903.06902_ (2019).\nthe IR replicability infrastructure to include performance aspects. _arXiv preprint_\n\n[10] Sebastian Hofst _arXiv:1907.04614_ ater, Navid Rekabsaz, Carsten Eickhoff, and Allan Hanbury.\u00a8 (2019).\n2019. On the effect of low-frequency terms on neural-IR models. In _Proceedings_\n\n_of the 42nd International ACM SIGIR Conference on Research and Development in_\n\n[11] Sebastian Hofst _Information Retrieval_ ater, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@\u00a8 . 1137\u20131140.\nTREC Deep Learning\u201919\u2013Simple Contextualization for Re-ranking. _arXiv preprint_\n_arXiv:1912.01385_ (2019).\n\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\nHeck. 2013. Learning deep structured semantic models for web search using\nclickthrough data. In _Proceedings of the 22nd ACM international conference on_\n_Information & Knowledge Management_ . 2333\u20132338.\n\n[13] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural\nRanking with Locality Sensitive Hashing. In _Te World Wide Web Conference_ .\nACM, 2858\u20132864.\n\n[14] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qn Liu. 2019. Tinybert: Distilling bert for natural language understanding.\n\n[15] Jeff Johnson, Mathijs Douze, and Herv _arXiv preprint arXiv:1909.10351_ (2019). e J\u00b4 egou. 2017. Billion-scale similarity\u00b4\nsearch with GPUs. _arXiv preprint arXiv:1702.08734_ (2017).\n\n[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_15300", "chunk_text": "\nsearch with GPUs. _arXiv preprint arXiv:1702.08734_ (2017).\n\n[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_ (2014).\n\n[17] Ron Kohavi, Alex Deng, Brian Frasca, Toby Walker, Ya Xu, and Nils Pohlmann.\n2013. Online controlled experiments at large scale. In _SIGKDD_ .\n\n[18] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. Cedr:\nContextualized embeddings for document ranking. In _Proceedings of the 42nd_\n_International ACM SIGIR Conference on Research and Development in Information_\n_Retrieval_ . ACM, 1101\u20131104.\n\n[19] Paul Michel, Omer Levy, and Graham Neubig. 2019. Are Sixteen Heads Really\nBeter than One?. In _Advances in Neural Information Processing Systems_ . 14014\u2013\n14024.\n\n[20] Bhaskar Mitra and Nick Craswell. 2019. An Updated Duet Model for Passage\nRe-ranking. _arXiv preprint arXiv:1903.07666_ (2019).\n\n[21] Bhaskar Mitra, Nick Craswell, et al. 2018. An introduction to neural information\nretrieval. _Foundations and Trends\u00ae in Information Retrieval_ 13, 1 (2018), 1\u2013126.\n\n[22] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using\nlocal and distributed representations of text for web search. In _Proceedings of_\n_the 26th International Conference on World Wide Web_ . International World Wide\nWeb Conferences Steering Commitee, 1291\u20131299.\n\n[23] Bhaskar Mitra, Corby Rosset, David Hawking, Nick Craswell, Fernando Diaz,\nand Emine Yilmaz. 2019. Incorporating query term independence assumption\nfor efficient retrieval and ranking using deep neural networks. _arXiv preprint_\n_arXiv:1907.03693_ (2019).\n\n[24] Tri Nguyen,", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2004.12832_colbert_khattab:chunk_16200", "chunk_text": ",\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the\nlimits of transfer learning with a unified text-to-text transformer. _arXiv preprint_\n_arXiv:1910.10683_ (2019).\n\n[32] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\nMike Gatford, et al. 1995. Okapi at TREC-3. _NIST Special Publication_ (1995).\n\n[33] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\n2019. Distilling task-specific knowledge from BERT into simple neural networks.\n_arXiv preprint arXiv:1903.12136_ (2019).\n\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Atention is all\nyou need. In _Advances in neural information processing systems_ . 5998\u20136008.\n\n[35] Yonghui Wu, Mike Schuster, Zhifeng Chen, Qoc V Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google\u2019s neural machine translation system: Bridging the gap between\nhuman and machine translation. _arXiv preprint arXiv:1609.08144_ (2016).\n\n[36] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power.\n2017. End-to-end neural ad-hoc ranking with kernel pooling. In _Proceedings_\n\n_of the 40th International ACM SIGIR conference on research and development in_\n_information retrieval_ . 55\u201364.\n\n[37] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking\nbaselines using Lucene. _Journal of Data and Information Qality (JDIQ)_ 10, 4\n(2018), 1\u201320.\n\n[38] Wei Yang, Kuang Lu, Peilin Yang, and", "token_count": 500, "metadata": {"arxiv_id": "2004.12832", "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "authors": ["Omar Khattab", "Matei Zaharia"], "year": 2020, "url": "https://arxiv.org/pdf/2004.12832v2"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_0", "chunk_text": "## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_450", "chunk_text": ", our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar of text retrieval using learned dense representations, which has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using sparse bag-of-words\nrepresentations (Zhan et al., 2020; Xiong et al.,\n2020; Hofst\u00a8atter et al., 2020; Lin et al., 2020).\nOur experiments largely verify the claims of\nKarpukhin et al. (2020) regarding the effectiveness of their proposed techniques. However, we\narrived at two important additional findings, one\n\n- f which is inconsistent with the original work, the\n\n- ther of which presents an enhancement:\n\n\n1. Focusing on retrieval, we found that the effectiveness of the sparse retrieval (BM25)\nbaseline is higher than values reported by\nKarpukhin et al. (2020). Whereas they reported\nthat dense\u2013sparse hybrid results do not meaningfully improve over dense retrieval alone,\nwe arrived at the opposite conclusion, where\nhybrid techniques yield statistically significant\ngains. We are able to achieve on average\na three-point improvement in top-20 accuracy\n\n  - ver the best DPR results across five standard\nQA test collections.\n\n\n2. Focusing on end-to-end QA effectiveness, we\nexplored different techniques for evidence com\n\n[1Artifact Review and Badging](https://www.acm.org/publications/policies/artifact-review-and-badging-current)\n\n\nbination to extract the final answer span.\nWhereas the original DPR paper only used\nscores from the reader to identify the final\nanswer span, we investigated combining retriever scores and further experimented with\nthe answer span selection technique described\nby Mao et al. (2020). In our best condition,\nwe were able to achieve statistically significant\nimprovements of around three points on exact\nmatch scores over the original DPR implementation, using the same exact models.\n\n\nThe main contribution of this work is the replication of DPR, where our experimental results\nadd a number of important refinements to the\n\n- riginal work. Code associated with our retrieval experiments is packaged in our Pyserini IR\ntoolkit [2] (Lin et al., 2021) and code associated with\n\n-", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_900", "chunk_text": "add a number of important refinements to the\n\n- riginal work. Code associated with our retrieval experiments is packaged in our Pyserini IR\ntoolkit [2] (Lin et al., 2021) and code associated with\n\n- ur end-to-end QA experiments is part of our PyGaggle toolkit [3] for neural text ranking.\n\n\n2 Methods\n\n\nDPR (Karpukhin et al., 2020) adopts the retriever\u2013\nreader design proposed by Chen et al. (2017) for\nthe open-domain QA task. Both the task formulation and the pipeline architecture for tackling the\ntask date from the late 1990s (Voorhees and Tice,\n1999), so this general approach has a long history\nthat predates neural networks. The open-source\ncode associated with the paper is available on\nGitHub (which we refer to as \u201cthe DPR repo\u201d), [4]\n\nbut it does not appear to contain code and models\nnecessary to reproduce all results reported in the\npaper (more detailed discussions below).\n\n\n2.1 Retriever\n\n\nIn the retrieval stage, given a corpus C =\n{D1, D2, ..., Dm}, the task is to return for each\nquery q a list of k most relevant documents (i.e.,\nmost likely to contain the answer) from C, where\nk << |C|. In the original DPR paper and also our\nreplication study, the corpus refers to a version of\nEnglish Wikipedia (dump from 2018-12-20), and\nthe \u201cdocuments\u201d are non-overlapping 100-word\nsplits from the articles.\nTo be clear, in most text ranking applications,\nthe \u201cunit of indexing\u201d (and also retrieval) is usually referred to as a \u201cdocument\u201d Dj, although\nin this case it is a passage (i.e., a split) from\n\n\n[2http://pyserini.io/](http://pyserini.io/)\n[3http://pygaggle.ai/](http://pygaggle.ai/)\n[4https://github.com/facebookresearch/DPR](https://github.com/facebookresearch/DPR)\n\n\n\nWikipedia. For consistency with this parlance,\nwe use \u201cdocument\u201d and \u201cpassage\u201d interchangeably throughout this paper. To add to the potential\nconfusion, results of the retriever are also referred\n\nto as \u201ccontexts\u201d that are fed to the reader", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_1350", "chunk_text": " with this parlance,\nwe use \u201cdocument\u201d and \u201cpassage\u201d interchangeably throughout this paper. To add to the potential\nconfusion, results of the retriever are also referred\n\nto as \u201ccontexts\u201d that are fed to the reader.\n\nDense retrieval with DPR uses a query encoder\nand a passage encoder, which are both based on\nBERT. Queries and passages are encoded as dense\nrepresentation vectors as follows:\n\n\nq [\u2217] = BERTq(q), Dj [\u2217] [=][ BERT][D][(][D][j][)]\n\n\nwhere q [\u2217] and Dj [\u2217] [are low dimensional vectors]\n(768). The relevance score of a passage to a query\nis computed by dot product:\n\n\nSim(q, Dj) = \u27e8q [\u2217], Dj [\u2217][\u27e9]\n\n\nThus, the top k retrieval problem can be recast\nas a nearest neighbor search problem in vector\nspace. Operationally, this is accomplished via\nFacebook\u2019s Faiss library (Johnson et al., 2017).\n\nKarpukhin et al. (2020) also investigated hybrid\nretrieval, combining results from dense retrieval\n(DPR) and sparse retrieval (BM25) by computing\nthe linear combination of their respective scores to\nrerank the union of the two initial retrieved sets:\n\n\n\u03bb \u00b7 Sim(q, Dj) + BM25(q, Dj),\n\n\nwhere \u03bb = 1.1, an empirical value tuned on the\ndevelopment set. BM25 retrieval was performed\nusing Lucene with parameters b = 0.4 and k1 =\n0.9. However, the DPR repo does not appear to\ncontain code for reproducing the BM25 and hybrid\nfusion results.\n\nWe attempted to replicate the retriever results\nreported in Karpukhin et al. (2020) with Pyserini,\nan IR toolkit that our group has been developing\nsince 2019 (Lin et al., 2021). The toolkit supports\nsparse retrieval (i.e., BM25) via integration with\nanother toolkit called Anserini (Yang et al., 2017),\nwhich is built on Lucene. Like in the original DPR\nwork, Pyserini supports dense retrieval via integration with Facebook\u2019s Faiss library. Combining\ndense and sparse retrieval, our toolkit supports hybrid retrieval as well.\n\nTo be clear, we started with model checkpoint\nreleases in the", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_1800", "chunk_text": " original DPR\nwork, Pyserini supports dense retrieval via integration with Facebook\u2019s Faiss library. Combining\ndense and sparse retrieval, our toolkit supports hybrid retrieval as well.\n\nTo be clear, we started with model checkpoint\nreleases in the DPR repo and did not retrain the\nquery and passage encoders from scratch. Otherwise, our implementation does not share any code\nwith the DPR repo, other than evaluation scripts to\nensure that results are comparable.\n\n\nSimilar to the original work, we calculated\nhybrid retrieval scores by linear combination of\ndense and sparse scores, as follows:\n\n\nSim(q, Dj) + \u03b1 \u00b7 BM25(q, Dj).\n\n\nNote that, contrary to the original work, we placed\nthe \u03b1 weight on the BM25 score because this\nyields a more natural way to answer the pertinent\nresearch question: Given dense retrieval as a starting point, does adding BM25 as an additional relevance signal provide any value? This question is\nanswered by a setting of \u03b1 = 0, which is equivalent to discarding BM25 results.\nFinally, there are a few more details of exactly\nhow to combine BM25 and DPR scores worth ex\nploring. As a baseline, we tried using the raw\nscores directly in the linear combination (exactly\nas above). However, we noticed that the range of\nscores from DPR and BM25 can be quite different. To potentially address this issue, we tried the\nfollowing normalization technique: If a document\nfrom sparse retrieval is not in the dense retrieval\nresults, we assign to it the the minimum dense retrieval score among the retrieved documents as its\ndense retrieval score, and vice versa for the sparse\nretrieval score.\n\nTo arrive at a final top-k ranking, the original\nDPR paper generated top k [\u2032] results from DPR and\ntop k [\u2032] results from BM25 (where k [\u2032] - k), before considering the union of the two result sets\nand combining the scores to arrive at the final top\nk. Karpukhin et al. (2020) set k [\u2032] = 2000, but after some preliminary experimentation, we decided\nto fix k [\u2032] = 1000 in our experiments since it is a\nmore common setting in information retrieval experiments (for example, k = 1000 is the default in\nmost TREC evaluations).\n\n\n2.2 Reader\n\n\nAs is standard in a retriever", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_2250", "chunk_text": " = 1000 in our experiments since it is a\nmore common setting in information retrieval experiments (for example, k = 1000 is the default in\nmost TREC evaluations).\n\n\n2.2 Reader\n\n\nAs is standard in a retriever\u2013reader design, the retriever in Karpukhin et al. (2020) returns k candidate passages (i.e., splits from Wikipedia) for each\nquery q. The reader extracts the final answer span\nfrom these candidate contexts, where each context\nCi is comprised of the Wikipedia article title Ci [title]\nand the content text Ci [text] .\nThe reader in DPR uses BERT-base and takes\n\nas input each candidate context Ci concatenated to\nthe question q. Answer extraction is treated as a labeling task, and the reader identifies the answer by\npredicting the start and end tokens of the answer\n\n\n\nspan in the contexts. To do so, the DPR reader\nadds a linear layer on top of BERT to predict the\nstart logit and end logit for each token from the final hidden layer representations. The score of an\nanswer span is calculated by adding the start logit\n\n- f the first token and the end logit of the last token. The reader returns the m highest scoring answer spans. In addition, the reader uses the learned\nrepresentation of [CLS] to predict the overall relevance of the context to the question.\n\nIn more detail, the reader operates as follows:\n\n\nri, S = Reader([CLS] q [SEP] Ci [title] [SEP] Ci [text] )\n\n\nwhere ri is the overall relevance score for context\nCi, and S comprises m potential (answer span,\nspan score) pairs extracted from context Ci:\n\n\n{(Si,1, si,1), (Si,2, si,2), . . . (Si,m, si,m)}.\n\n\nIn the original paper, the final answer span is the\ncandidate with the maximum span score from the\ncontext with the highest relevance score.\n\nWe attempted to replicate exactly the DPR implementation of answer extraction using our opensource PyGaggle neural reranking library, which\nholds the code to many of our other search-related\nprojects. Once again, we began with reader checkpoints released in the DPR repo, but otherwise our\nimplementation is completely independent (other\nthan, again, the evaluation code).\n\nIn addition to the answer extraction algorithm above, we also implemented the normalized answer span scoring technique described\n", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_2700", "chunk_text": " we began with reader checkpoints released in the DPR repo, but otherwise our\nimplementation is completely independent (other\nthan, again, the evaluation code).\n\nIn addition to the answer extraction algorithm above, we also implemented the normalized answer span scoring technique described\nby Mao et al. (2020). Each answer span in each\ncandidate context Ci is rescored by:\n\n\ns [\u2032] i,j [=][ softmax][(][\u20d7r][)][i] [\u00b7][ softmax][(][\u20d7s][i][)][j]\n\n\nwhere \u20d7r = {r1, \u00b7 \u00b7 \u00b7, rk} is the set of relevance scores of all candidate contexts and \u20d7si =\n{si,1, \u00b7 \u00b7 \u00b7, si,m} is the set of all span scores within\ncontext Ci. Duplicate answer spans across all contexts are scored by accumulating their individual\nscores. The answer span with the maximum final\nscore is selected as the final prediction.\n\nIn summary, we compared two answer span\nscoring techniques in the reader: the \u201coriginal\u201d answer span scoring technique described\nby Karpukhin et al. (2020), and the span scoring\ntechnique described by Mao et al. (2020).\n\n\n2.3 Final Evidence Fusion\n\n\nIn the original DPR paper, the final answer span is\n\n- nly selected based on scores from the reader. In\n\n- ur replication attempt, we additionally tried to exploit scores from the retriever to improve answer\nspan selection. Our intuition is that predictions\nfrom both the retriever and the reader should con\ntribute to the final answer. Concretely, instead of\njust using the relevance score ri from the reader to\nscore contexts, we fuse ri with the retriever score\nRi, calculated by:\n\n\n\u03b2 \u00b7 ri + \u03b3 \u00b7 Ri\n\n\nDepending on the retrieval method, Ri can be the\nsparse retrieval score, the dense retrieval score,\n\n- r the score after hybrid fusion. This final fused\nscore replaces ri as the relevance score for each\ncontext in the answer span scoring step. For example, with fusion, the answer span scoring technique of Mao et al. (2020) becomes softmax(\u03b2 - \u20d7r+\n\u03b3 \u00b7 R [\u20d7] )i \u00b7 softmax(\u20d7si)j.\nThus, to summarize, we explored four settings\nin our end-to-end QA replication: the original\nDPR span scoring technique, with", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_3150", "chunk_text": "r+\n\u03b3 \u00b7 R [\u20d7] )i \u00b7 softmax(\u20d7si)j.\nThus, to summarize, we explored four settings\nin our end-to-end QA replication: the original\nDPR span scoring technique, with and without retriever score fusion, and the answer span scoring\ntechnique of Mao et al. (2020), with and without\nretriever score fusion.\n\n\n3 Experimental Setup\n\n\nModels Our replication efforts began with\nmodel checkpoints provided in the DPR repo. Unfortunately, Karpukhin et al. (2020) did not appear\nto make available all models used in their experiments, and thus, to be precise, our experiments\nused the following models:\n\n\n- RetrieverNQ: DPR encoders trained using just\nthe NQ dataset (for the retriever).\n\n\n- RetrieverMulti: DPR encoders trained using a\ncombination of datasets (for the retriever).\n\n\n- ReaderNQ-Single: the DPR reader trained on NQ\nwith negative passages from retrieval results by\nRetrieverNQ.\n\n\n- ReaderTQA-Multi: the DPR reader trained on\nTriviaQA with negative passages from retrieval\nresults by RetrieverMulti.\n\n\n\nDatasets We evaluated retrieval effectiveness\n\n- n five standard benchmark QA datasets (NQ,\nTriviaQA, WQ, CuratedTREC, SQuAD), exactly\nthe same as Karpukhin et al. (2020). We used the\nRetrieverMulti model, which can be applied to all\nfive datasets. For end-to-end QA, we evaluated\n\n- n NQ and TriviaQA with the available models.\nMore precisely, we used the ReaderNQ-Single model\nto process the retrieved contexts from RetrieverNQ\nfor NQ and used the ReaderTQA-Multi model to process the retrieved contexts from RetrieverMulti for\nTriviaQA.\n\n\nMetrics For retrieval, we measured effectiveness in terms of top-k retrieval accuracy, defined\nas the fraction of questions that have a correct answer span in the top-k retrieved contexts at least\n\n- nce. End-to-end QA effectiveness is measured\nin terms of the exact match (EM) metric, defined\nas the fraction of questions that have an extracted\nanswer span exactly matching the ground truth an\nswer.\n\nMissing from the original DPR paper, we performed significance testing to assess the statistical\nsignificance of metric differences. In all cases,", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_3600", "chunk_text": ", defined\nas the fraction of questions that have an extracted\nanswer span exactly matching the ground truth an\nswer.\n\nMissing from the original DPR paper, we performed significance testing to assess the statistical\nsignificance of metric differences. In all cases, we\napplied paired t-tests at p < 0.01; the Bonferroni\ncorrection was applied to correct for multiple hypothesis testing as appropriate.\n\n\nHyperparameters In the hybrid retrieval technique described in the DPR paper, the \u03bb weight\nfor combining dense and sparse retrieval scores\nis fixed to 1.1. However, our implementation replaces \u03bb with \u03b1 (see Section 2.1). We tuned the\n\u03b1 values on different datasets by optimizing top20 retrieval accuracy: For datasets where we can\n\n- btain exactly same train/dev/test splits as the original DPR paper (NQ and TriviaQA), we tuned the\nweight on the development set. For the remaining\ndatasets, where splits are not available or the original DPR paper does not provide specific guidance,\nwe tuned the weight on a subset of the training\ndata. We obtained the optimal weight by performing grid search in the range [0, 2] with step size\n0.05.\n\nSimilarly, for final evidence fusion, we tuned\n\u03b2 (i.e., the weight for the relevance score) and\n\u03b3 (i.e., the weight for retriever score) on the development set of NQ and TriviaQA using grid\nsearch. For greater computational efficiency, we\nperformed tuning in multiple passes, first with a\ncoarser step size and then with a finer step size.\n\n\nFor the original DPR answer span scoring technique, we fixed \u03b2 to one and performed a two-step\ngrid search on \u03b3. We started with step size 0.05\nand found the optimal \u03b31. Then, we used step size\n0.01 in the range [\u03b31 \u2212 0.04, \u03b31+0.04] to find the\n\n- ptimal \u03b3.\nFor the answer span scoring technique\n\n- f Mao et al. (2020), we defined \u03b4 = \u03b2\u03b3 [and]\nperformed a three-step grid search on \u03b2 and \u03b4\n(i.e., the weight for the retriever score becomes\n\u03b3 = \u03b2 \u00b7 \u03b4). We started with step size 0.2 for\nboth \u03b2 and \u03b4 and found the optimal pair of values\n\u03b21, \u03b41. We then repeated this process", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_4050", "chunk_text": "., the weight for the retriever score becomes\n\u03b3 = \u03b2 \u00b7 \u03b4). We started with step size 0.2 for\nboth \u03b2 and \u03b4 and found the optimal pair of values\n\u03b21, \u03b41. We then repeated this process with step\nsize 0.05 and then 0.01 in a smaller range around\nthe optimal \u03b2i and \u03b4i from the previous pass.\nFor final evidence fusion, we tuned the weight\nparameters together with the number of retrieval\nresults (k) up to 500 with a step size of 20. Optimal parameters were selected based on the exact\nhighest match score.\n\n\n4 Results\n\n\n4.1 Retrieval\n\n\nTable 1 reports top-k = {20, 100} retrieval accuracy from our replication attempt, compared to\nfigures copied directly from the original DPR paper; here we focus on results from RetrieverMulti.\nThe hybrid retrieval results reported in the original\nDPR paper is denoted Hybridorig, which is not directly comparable to either of our two techniques:\nHybridnorm (with minimum score normalization)\n\n- r Hybrid (without such normalization). We make\nthe following observations:\nFirst, our dense retrieval results are very close\nto those reported in Karpukhin et al. (2020). We\nconsider this a successful replication attempt and\n\n- ur efforts add veracity to the effectiveness of the\nDPR technique. Yay!\nSecond, our Pyserini BM25 implementation\n\n- utperforms the BM25 results reported in the original paper across all datasets. Furthermore, the gap\nis larger for k = 20. On average, our results represent a nearly seven-point improvement in top-20\naccuracy and a nearly five-point improvement for\ntop-100 accuracy. Since Karpukhin et al. (2020)\nhave not made available their code for generating\nthe BM25 results, we are unable to further diagnose these differences.\n\nNevertheless, the results do support the finding that dense retrieval using DPR is (generally)\nmore effective than sparse retrieval. We confirmed\n\n\n\nTop-20 Top-100\nCondition  - rig repl  - rig repl\n\n\nNQ\n\nDPR 79.4 79.5 86.0 86.1\nBM25 59.1 62.9 [\u2020] 73.7 78.3 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 78.0", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_4500", "chunk_text": ".5 86.0 86.1\nBM25 59.1 62.9 [\u2020] 73.7 78.3 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 78.0  - 83.9  Hybridnorm (\u03b1 = 1.30)  - 82.6 [\u2021]  - 88.6 [\u2021]\n\nHybrid (\u03b1 = 0.55)  - 82.7 [\u2021]  - 88.1 [\u2021]\n\n\nTriviaQA\n\nDPR 78.8 78.9 84.7 84.8\nBM25 66.9 76.4 [\u2020] 76.7 83.2 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 79.9  - 84.4  Hybridnorm (\u03b1 = 0.95)  - 82.6 [\u2021]  - 86.5 [\u2021]\n\nHybrid (\u03b1 = 0.55)  - 82.3 [\u2021]  - 86.1 [\u2021]\n\n\nWQ\n\nDPR 75.0 75.0 82.9 83.0\nBM25 55.0 62.4 [\u2020] 71.1 75.5 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 74.7  - 82.3  Hybridnorm (\u03b1 = 0.95)  - 77.1 [\u2021]  - 84.4 [\u2021]\n\nHybrid (\u03b1 = 0.3)  - 77.5 [\u2021]  - 84.0 [\u2021]\n\n\nCuratedTREC\n\nDPR 89.1 88.8 93.9 93.4\nBM25 70.9 80.7 [\u2020] 84.1 89.9 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 88.5  - 94.1  Hybridnorm (\u03b1 = 1.05)  - 90.1  - 95.0 [\u2021]\n\nHybrid (\u03b1 = 0.7)  - 89.6  - 94.6 [\u2021]\n\n\nSQuAD\n\nD", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_4950", "chunk_text": ".05)  - 90.1  - 95.0 [\u2021]\n\nHybrid (\u03b1 = 0.7)  - 89.6  - 94.6 [\u2021]\n\n\nSQuAD\n\nDPR 51.6 52.0 67.6 67.7\nBM25 68.8 71.1 [\u2020] 80.0 81.8 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 66.2  - 78.6  Hybridnorm (\u03b1 = 2.00)  - 75.1 [\u2021]  - 84.4 [\u2021]\n\nHybrid (\u03b1 = 28)  - 75.0 [\u2021]  - 84.0 [\u2021]\n\n\nTable 1: Retrieval effectiveness comparing results from\nthe original DPR paper (\u201corig\u201d) and our replication attempt (\u201crepl\u201d). The symbol [\u2020] - n a BM25 result indicates effectiveness that is significantly different from\nDPR. The symbol [\u2021] indicates that the hybrid technique\nis significantly better than BM25 (for SQuAD) or DPR\n(for all remaining collections).\n\n\nthat the effectiveness differences between DPR\n\nand BM25 in our replication results are statistically significant. In all datasets except for SQuAD,\nDPR outperforms BM25; this is consistent with\nthe original paper. We further confirmed that for\nSQuAD, DPR is significantly worse than BM25.\nAs Karpukhin et al. (2020) noted, RetrieverMulti\nwas trained by combining training data from all\ndatasets but excluding SQuAD; these poor results\nare expected, since SQuAD draws from a very\nsmall set of Wikipedia articles.\nThird, the effectiveness of hybrid dense\u2013sparse\nfusion appears to be understated in the original\nDPR paper. Karpukhin et al. (2020) found that\n\n\nCondition k = 20 100 500 1000\n\n\nNQ 6.1 5.2 4.4 4.2\nTriviaQA 9.2 6.6 5.0 4.6\nWQ 5.9 5.9 5.8 5.7\n\nCuratedTrec 6.9 7.2 6.3 5.9\n\n", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_5400", "chunk_text": "6 5.0 4.6\nWQ 5.9 5.9 5.8 5.7\n\nCuratedTrec 6.9 7.2 6.3 5.9\n\nSQuAD 4.5 4.1 4.0 4.0\n\n\nTable 2: The Jaccard overlap between sparse retrieval\nresults and dense retrieval results.\n\n\nhybrid retrieval is less effective than dense retrieval in most settings, which is inconsistent with\n\n- ur experimental results. Instead, we found that\ndense\u2013sparse retrieval consistently beats sparse retrieval across all settings. The gains from both\nhybrid scoring techniques are statistically significant, with the exception of top-20 for CuratedTREC. Our results might be due to better BM25\neffectiveness, but we are unable to further diagnose these differences because, once again, the hybrid retrieval code is not provided in the DPR repo.\nFurther testing also found that the differences between the two hybrid techniques are not significant. Thus, there does not appear to be a strong\nbasis to prefer one hybrid technique over the other.\nIn Table 2, we report overlap when taking different top-k results from dense retrieval and sparse\nretrieval. Overlap is measured in terms of Jaccard overlap, which is computed by the intersection over the union. It is apparent that the overlap between dense and sparse results is quite small,\nwhich suggests that they are effective in very different ways. This provides an explanation of why\nhybrid retrieval is effective, i.e., they are exploiting very different signals. These results also justify the DPR design choice of retrieving k [\u2032] - k\nresults from dense and sparse retrieval and then\nrescoring the union to arrive at the final top-k.\n\n\n4.2 End-to-End QA\n\n\nTable 3 presents results for our end-to-end question answering replication experiments on the NQ\nand TriviaQA datasets in terms of the exact match\nscore. The original results are shown in the\n\u201corig\u201d column. The \u201crepl\u201d column reports our attempt to replicate exactly the span scoring technique described in the original paper, whereas the\n\u201cGAR\u201d column shows results from using the technique proposed by Mao et al. (2020). The version of each technique that incorporates retriever\nscores (see Section 2.3) is denoted with a * symbol, i.e., \u201crepl", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_5850", "chunk_text": " column shows results from using the technique proposed by Mao et al. (2020). The version of each technique that incorporates retriever\nscores (see Section 2.3) is denoted with a * symbol, i.e., \u201crepl*\u201d and \u201cGAR*\u201d. For NQ, we used\n\n\n\nCondition   - rig repl repl* GAR GAR*\n\n\nNQ\nDPR 41.5 41.2 42.5 [\u2020] 41.5 43.5 [\u2020\u2021]\n\nBM25 32.6 36.3 37.0 37.3 [\u2020] 38.4 [\u2020\u2021]\n\nHybrid 39.0 41.2 43.2 [\u2020] 41.9 [\u2020] 44.0 [\u2020\u2021]\n\n\nTriviaQA\nDPR 56.8 57.5 58.3 [\u2020] 58.9 [\u2020] 59.5 [\u2020\u2021]\n\nBM25 52.4 58.8 59.2 61.1 [\u2020] 61.6 [\u2020\u2021]\n\nHybrid 57.9 59.1 60.0 [\u2020] 61.0 [\u2020] 61.7 [\u2020\u2021]\n\n\nTable 3: End-to-end QA effectiveness in terms of the\nexact match score, comparing different answer span\nscoring techniques. The \u201corig\u201d and \u201crepl\u201d columns are\nthe original and replicated results; \u201cGAR\u201d refers to the\ntechnique of Mao et al. (2020); \u2018*\u201d represents fusion of\nretriever scores. The symbol [\u2020] - n a \u201crepl*\u201d result indicates stat sig. improvement over \u201crepl\u201d; on \u201cGAR\u201d,\n\n- ver \u201crepl\u201d; on \u201cGAR*\u201d, over \u201cGAR\u201d. The symbol [\u2021]\n\n- n \u201cGAR*\u201d indicates sig. improvement over \u201crepl\u201d.\n\n\nRetrieverNQ and ReaderNQ-Single; for TriviaQA, we\nused RetrieverMulti and ReaderTQA-Multi.\nWith retrieval using DPR only, the \u201corig\u201d and\n\u201crepl\u201d scores on both datasets are close (within a\npoint), which suggests that we have successfully\nreplicated the results reported in Karpukhin et al.\n(2020).", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_6300", "chunk_text": " using DPR only, the \u201corig\u201d and\n\u201crepl\u201d scores on both datasets are close (within a\npoint), which suggests that we have successfully\nreplicated the results reported in Karpukhin et al.\n(2020). Again, yay!\nWith retrieval using BM25 only, our replicated\nresults are quite a bit higher than the original DPR\nresults; this is not a surprise given that our BM25\nresults are also better. When combining DPR and\nBM25 results at the retriever stage, the end-to-end\neffectiveness remains unchanged for NQ, but we\n\n- bserve a modest gain for TriviaQA. The gain for\nTriviaQA is statistically significant. So, it is not\nthe case that better top-k retrieval leads to improvements in end-to-end effectiveness.\n\nComparing the \u201crepl\u201d and \u201crepl*\u201d columns, we\n\n- bserve that combining scores from the retriever\nyields modest gains across all conditions. These\ngains are significant for four out of the six conditions, which suggests that retriever scores contribute to improving effectiveness. Comparing the\n\u201cGAR\u201d and \u201crepl\u201d columns, we also observe modest gains when adopting the answer span selection\ntechnique of Mao et al. (2020). These gains are\nsignificant for all except one condition. Comparing the \u201cGAR\u201d and \u201cGAR*\u201d columns, we find that\nin all cases, incorporating retriever scores significantly increases effectiveness.\nFinally, putting everything together\u2014using\nboth the answer span scoring technique\n\n- f Mao et al. (2020) and incorporating re\n\n45\n\n\n42\n\n\n39\n\n\n36\n\n\n33\n\n\n\n\n\n\n\n63\n\n\n61\n\n\n59\n\n\n57\n\n\n55\n\n\n\n30\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n||||||BM25-repl<br>BM25-GAR<br>BM25-GAR*<br>l|\n||||||BM25-repl<br>BM25-GAR<br>BM25-GAR*<br>l|\n|||||||\n|||||||\n|||||||\n||||||DPR-rep<br>DPR-GAR<br>DPR-GAR*<br>Hybrid-GAR*|\n|||||||\n|||||||\n\n0 100 200 300 400 500\n\n\nnumber of retrieval results (k", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_6750", "chunk_text": "||||||DPR-rep<br>DPR-GAR<br>DPR-GAR*<br>Hybrid-GAR*|\n|||||||\n|||||||\n\n0 100 200 300 400 500\n\n\nnumber of retrieval results (k)\n\n\n\n53\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n||||||BM25-repl<br>BM25-GAR<br>BM25-GAR*<br>l|\n||||||BM25-repl<br>BM25-GAR<br>BM25-GAR*<br>l|\n|||||||\n|||||||\n|||||||\n||||||DPR-rep<br>DPR-GAR<br>DPR-GAR*<br>Hybrid-GAR*|\n|||||||\n|||||||\n\n0 100 200 300 400 500\n\n\nnumber of retrieval results (k)\n\n\n\nFigure 1: End-to-end question answering effectiveness (exact match score) varying the number of retrieval results\n(k) for NQ (left) and TriviaQA (right).\n\n\n\ntriever scores\u2014we observe statistically significant\ngains across all retrieval conditions, as can be\nseen in the \u201cGAR*\u201d vs. \u201crepl\u201d columns across all\nrows. Compared to the best replicated results, we\n\n- btained an improvement of approximately three\npoints in end-to-end QA effectiveness compared\nto the best answer extraction approach described\nin Karpukhin et al. (2020). Note that we were able\nto obtain these improvements using exactly the\nmodel checkpoints provided in the DPR repo\u2014we\nhave simply added two relatively simple tricks to\nimprove scoring and evidence combination.\nIn Figure 1, we plot exact match scores as a\nfunction of varying k retrieval results for NQ (left)\nand TriviaQA (right). That is, we show how endto-end QA effectiveness changes as the reader is\nprovided more contexts from the retriever to consider. There are two factors here at play: On the\n\n- ne hand, top-k accuracy increases monotonically,\ni.e., as k increases, so does the likelihood that the\nanswer appears in the contexts fed to the reader.\nOn the other hand, the reader is asked to con\nsider more contexts, and thus needs to discriminate the correct answer from a larger pool of candidate contexts, some of which", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_7200", "chunk_text": " likelihood that the\nanswer appears in the contexts fed to the reader.\nOn the other hand, the reader is asked to con\nsider more contexts, and thus needs to discriminate the correct answer from a larger pool of candidate contexts, some of which might be low quality and thus serve as \u201cdistractors\u201d from the correct answer. How do these factors balance out?\n\nSimilar analyses in previous work with BM25 retrieval have shown that end-to-end QA effectiveness increases with increasing k (Yang et al., 2019;\nXie et al., 2020); that is, the reader does not appear to be \u201cconfused\u201d by the non-relevant material. Indeed, in our BM25 results we also observe\n\nthe same trend.\n\nInterestingly, however, when we switch from\nBM25 results to DPR results, the behavior ap\n\n\npears to change. For TriviaQA, the effectiveness\ncurve behaves as expected, but for NQ, the exact\nmatch score trends up and then decreases after a\npeak. This means that while the likelihood of the\nreader seeing a correct answer in the candidate\ncontexts increases with k, it is more likely to be\nnegatively affected by increasing amounts of nonrelevant contexts as well. This general behavior is\nalso seen for the hybrid scoring techniques: as k\nincreases, so does the exact match score, but only\nup to a certain point. Beyond this point, feeding\nthe reader more candidate contexts leads to slight\ndecreases in end-to-end effectiveness.\n\n\n5 Conclusion\n\n\nThe breakneck pace at which NLP and IR are\nadvancing, we argue, makes reproducibility and\nreplicability critical to advancing science\u2014to ensure that we are building on a firm foundation.\nOur study adds to the veracity of the claims made\nby Karpukhin et al. (2020), and our work does indeed confirm that DPR is an effective dense retrieval technique. However, we arrived at two important additional findings, one of which is inconsistent with the original work, the other of which\npresents an enhancement. Together, they enrich\n\n- ur understanding of DPR.\n\n\n6 Acknowledgments\n\n\nThis research was supported in part by the Canada\nFirst Research Excellence Fund and the Natu\nral Sciences and Engineering Research Council\n(NSERC) of Canada. Computational resources\nwere provided by Compute Ontario and Compute\nCanada.\n\n\nReferences\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2104.05740_hybrid_retrieval_ma:chunk_7650", "chunk_text": "First Research Excellence Fund and the Natu\nral Sciences and Engineering Research Council\n(NSERC) of Canada. Computational resources\nwere provided by Compute Ontario and Compute\nCanada.\n\n\nReferences\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (ACL 2017), pages 1870\u20131879, Vancouver, British Columbia, Canada.\n\n\nSebastian Hofst\u00a8atter, Sophia Althammer, Michael\nSchr\u00a8oder, Mete Sertkan, and Allan Hanbury.\n2020. Improving efficient neural ranking models with cross-architecture knowledge distillation.\narXiv:2010.02666.\n\n\nJeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou.\n2017. Billion-scale similarity search with GPUs.\narXiv:1702.08734.\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\n\n  - pen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769\u2013\n6781.\n\n\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021. Pyserini: An easy-to-use Python toolkit to\nsupport replicable IR research with sparse and dense\nrepresentations. arXiv:2102.10073.\n\n\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n2020. Distilling dense representations for ranking\nusing tightly-coupled teachers. arXiv:2010.11386.\n\n\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2020. Generation-augmented retrieval for opendomain question answering. arXiv:2009.08553.\n\n\nEllen M. Voorhees and Dawn M. Tice. 1999. The\nTREC-8 question answering track evaluation. In\n", "token_count": 500, "metadata": {"arxiv_id": "2104.05740", "title": "A Replication Study of Dense Passage Retriever", "authors": ["Xueguang Ma", "Kai Sun", "Ronak Pradeep", "Jimmy Lin"], "year": 2021, "url": "https://arxiv.org/pdf/2104.05740v1"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_0", "chunk_text": "## **Active Retrieval Augmented Generation**\n\n**Zhengbao Jiang** [1] _[\u2217]_ **Frank F. Xu** [1] _[\u2217]_ **Luyu Gao** [1] _[\u2217]_ **Zhiqing Sun** [1] _[\u2217]_ **Qian Liu** [2]\n\n**Jane Dwivedi-Yu** [3] **Yiming Yang** [1] **Jamie Callan** [1] **Graham Neubig** [1]\n\n1Language Technologies Institute, Carnegie Mellon University\n2Sea AI Lab 3FAIR, Meta\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\n\n\n\n**Abstract**\n\n\nDespite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one\npromising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate\nsetup that only retrieves information once based\n\n  - n the input. This is limiting, however, in\nmore general scenarios involving generation\n\n  - f long texts, where continually gathering information throughout generation is essential. In\nthis work, we provide a generalized view of _ac-_\n_tive retrieval augmented generation_, methods\nthat actively decide when and what to retrieve\nacross the course of the generation. We propose\n**F**  - rward- **L**  - oking **A** ctive **RE** trieval augmented\ngeneration ( **FLARE** ), a generic method which\niteratively uses a prediction of the upcoming\nsentence to anticipate future content, which is\nthen utilized as a query to retrieve relevant documents to regenerate the sentence if it contains\nlow-confidence tokens. We test FLARE along\nwith baselines comprehensively over 4 longform knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating\nthe effectiveness of our method. [1]\n\n\n**1** **Introduction**\n\n\nGenerative language models (LMs) (Brown et al.,\n2020; Ouyang et al., 2022; OpenAI, 2023; Chowdhery et al., 2022; Zhang et al., 2022; Touvron et al", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_450", "chunk_text": "LMs) (Brown et al.,\n2020; Ouyang et al., 2022; OpenAI, 2023; Chowdhery et al., 2022; Zhang et al., 2022; Touvron et al.,\n2023; Zhao et al., 2023) have become a foundational component in natural language processing\n(NLP) systems with their remarkable abilities. Although LMs have memorized some world knowledge during training (Petroni et al., 2019; Roberts\net al., 2020; Jiang et al., 2020), they still tend to\n\n\n_\u2217_ Lead contributors.\n[1Code and datasets are available at https://github.com/](https://github.com/jzbjyb/FLARE)\n[jzbjyb/FLARE.](https://github.com/jzbjyb/FLARE)\n\n\n\nhallucinate and create imaginary content (Maynez\net al., 2020; Zhou et al., 2021). Augmenting LMs\nwith retrieval components that look up relevant information from external knowledge resources is a\npromising direction to address hallucination (Khandelwal et al., 2020; Izacard et al., 2022).\n\nRetrieval augmented LMs commonly use a\nretrieve-and-generate setup where they retrieve documents based on the user\u2019s input, and then generate\na complete answer conditioning on the retrieved\ndocuments (Chen et al., 2017; Guu et al., 2020;\nLewis et al., 2020; Izacard and Grave, 2021; Sachan\net al., 2021; Lee et al., 2021; Jiang et al., 2022;\nIzacard et al., 2022; Nakano et al., 2021; Qian\net al., 2023; Lazaridou et al., 2022; Shi et al., 2023).\nThese single-time retrieval augmented LMs outperform purely parametric LMs, particularly for shortform knowledge-intensive generation tasks such\nas factoid question answering (QA) (Kwiatkowski\net al., 2019; Joshi et al., 2017), where _the informa-_\n_tion needs are clear in the user\u2019s input, and it is_\n_sufficient to retrieve relevant knowledge once solely_\n_based on the input_ .\n\nIncreasingly powerful large", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_900", "chunk_text": " Joshi et al., 2017), where _the informa-_\n_tion needs are clear in the user\u2019s input, and it is_\n_sufficient to retrieve relevant knowledge once solely_\n_based on the input_ .\n\nIncreasingly powerful large LMs have also\ndemonstrated abilities in more complex tasks that\ninvolve generating long-form output, such as longform QA (Fan et al., 2019; Stelmakh et al., 2022),\n\n- pen-domain summarization (Cohen et al., 2021;\nHayashi et al., 2021; Giorgi et al., 2022), and\n(chain-of-thought; CoT) reasoning (Wei et al.,\n2022; Ho et al., 2020; Geva et al., 2021; Hendrycks\net al., 2020). In contrast to short-form generation,\nlong-form generation presents complex information needs that are _not always evident from the in-_\n_put alone_ . Similar to how humans gradually gather\ninformation as we create content such as papers,\nessays, or books, long-form generation with LMs\nwould _require gathering multiple pieces of knowl-_\n_edge throughout the generation process_ . For example, to generate a summary about a particular\ntopic, the initial retrieval based on the topic name\n\n\nFigure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user\ninput _**x**_ and initial retrieval results _D_ _**x**_, FLARE iteratively generates a temporary next sentence (shown in _gray_\n_italic_ ) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the\nsystem retrieves relevant documents and regenerates the sentence.\n\n\n\n(e.g., Joe Biden) may not cover all aspects and details. It is crucial to retrieve extra information as\n\nneeded during generation, such as when generating a certain aspect (e.g., Joe Biden\u2019s education\nhistory) or a specific detail (e.g., the date of Joe\nBiden\u2019s presidential campaign announcement).\n\n\nSeveral attempts have been made to retrieve multiple times throughout generation. These attempts\ninclude methods that passively use the past context\nto retrieve additional information at a fixed interval\n(Khandelwal et al., 2020; Borgeaud et al., 2022;\nRam et al., 202", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_1350", "chunk_text": ". These attempts\ninclude methods that passively use the past context\nto retrieve additional information at a fixed interval\n(Khandelwal et al., 2020; Borgeaud et al., 2022;\nRam et al., 2023; Trivedi et al., 2022) which might\nnot accurately reflect what LMs intend to generate in the future or retrieve at inappropriate points.\nSome works in multihop QA decompose the full\nquestion into sub-questions, each of which is used\nto retrieve extra information (Press et al., 2022; Yao\net al., 2022; Khot et al., 2022; Khattab et al., 2022).\n\n\nWe ask the following question: can we create a\nsimple and generic retrieval augmented LM that _ac-_\n_tively decides when and what to retrieve_ throughout\nthe generation process, and are applicable to a variety of long-form generation tasks? We provide a\ngeneralized view of active retrieval augmented generation. Our hypothesis regarding _when to retrieve_\nis that LMs should retrieve information only when\n\n\n\nthey lack the required knowledge to avoid unnecessary or inappropriate retrieval that occurs in passive\nretrieval augmented LMs (Khandelwal et al., 2020;\nBorgeaud et al., 2022; Ram et al., 2023; Trivedi\net al., 2022). Given the observation that large LMs\ntend to be well-calibrated and low probability/confidence often indicates a lack of knowledge (Kadavath et al., 2022), we adopt an active retrieval\nstrategy that only retrieves when LMs generate lowprobability tokens. When deciding _what to retrieve_,\nit is important to consider what LMs intend to generate in the future, as the goal of active retrieval is to\nbenefit future generations. Therefore, we propose\nanticipating the future by generating a temporary\nnext sentence, using it as a query to retrieve relevant documents, and then regenerating the next\nsentence conditioning on the retrieved documents.\nCombining the two aspects, we propose **F** - rward**L** - oking **A** ctive **RE** trieval augmented generation\n( **FLARE** ), as illustrated in Figure 1. FLARE iteratively generates _a temporary next sentence_, use\nit as the query to retrieve relevant documents _if it_\n_contains low-prob", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_1800", "chunk_text": " **RE** trieval augmented generation\n( **FLARE** ), as illustrated in Figure 1. FLARE iteratively generates _a temporary next sentence_, use\nit as the query to retrieve relevant documents _if it_\n_contains low-probability tokens_ and regenerate the\nnext sentence until reaches the end.\n\n\nFLARE is applicable to any existing LMs at\ninference time without additional training. Con\n\nsidering the impressive performance achieved by\nGPT-3.5 (Ouyang et al., 2022) on a variety of\ntasks, we examine the effectiveness of our meth\n- ds on text-davinci-003. We evaluate FLARE\n\n- n 4 diverse tasks/datasets involving generating\nlong outputs, including multihop QA (2WikiMultihopQA), commonsense reasoning (StrategyQA),\nlong-form QA (ASQA), and open-domain summarization (WikiAsp) (Ho et al., 2020; Geva et al.,\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\nOver all tasks, FLARE achieves superior or competitive performance compared to single-time and\nmulti-time retrieval baselines, demonstrating the\neffectiveness and generalizability of our method.\n\n\n**2** **Retrieval Augmented Generation**\n\n\nWe formally define single-time retrieval augmented\ngeneration and propose the framework of active\nretrieval augmented generation.\n\n\n**2.1** **Notations and Definitions**\n\n\nGiven a user input _**x**_ and a document corpus _D_ =\n_{_ _**d**_ _i}_ _[|D|]_ _i_ =1 [(such as all Wikipedia articles), the goal of]\nretrieval augmented LMs is to generate the answer\n_**y**_ = [ _**s**_ 1 _,_ _**s**_ 2 _, ...,_ _**s**_ _m_ ] = [ _w_ 1 _, w_ 2 _, ..., wn_ ] containing\n_m_ sentences or _n_ tokens leveraging information\nretrieved from the corpus.\nIn retrieval augmented LM, the LM typically\npairs with a retriever that can retrieve a list of\ndocuments _D_ _**q**_ = ret( _**q**_ ) for a query _**q**_ ; the LM\nconditions on both the user input", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_2250", "chunk_text": " typically\npairs with a retriever that can retrieve a list of\ndocuments _D_ _**q**_ = ret( _**q**_ ) for a query _**q**_ ; the LM\nconditions on both the user input _**x**_ and retrieved\ndocuments _D_ _**q**_ to generate the answer. Since we\nfocus on examining various methods of determining when and what to retrieve, we follow existing methods (Ram et al., 2023; Trivedi et al.,\n2022) to prepend the retrieved documents before\nthe user input to aid future generation for both\nbaselines and our method for fair comparisons:\n_**y**_ = LM([ _D_ _**q**_ _,_ _**x**_ ]), where [ _\u00b7, \u00b7_ ] is concatenation following the specified order.\n\n\n**2.2** **Single-time Retrieval Augmented**\n**Generation**\n\n\nThe most common choice is to directly use the user\ninput as the query for retrieval and generate the\ncomplete answer at once _**y**_ = LM([ _D_ _**x**_ _,_ _**x**_ ]).\n\n\n**2.3** **Active Retrieval Augmented Generation**\n\n\nTo aid long-form generation with retrieval, we propose active retrieval augmented generation. It is a\ngeneric framework that actively decides when and\nwhat to retrieve through the generation process,\n\n\n\nresulting in the interleaving of retrieval and generation. Formally, at step _t_ ( _t \u2265_ 1), the retrieval query\n_**q**_ _t_ is formulated based on both the user input _**x**_ and\npreviously generated output _**y**_ _<t_ = [ _**y**_ 0 _, ...,_ _**y**_ _t\u2212_ 1]:\n\n\n_**q**_ _t_ = qry( _**x**_ _,_ _**y**_ _<t_ ) _,_\n\n\nwhere qry( _\u00b7_ ) is the query formulation function. At\nthe beginning ( _t_ = 1), the previous generation is\nempty ( _**y**_ _<_ 1 = _\u2205_ ), and the user input is used as the\ninitial query ( _**q**_ 1 = _**x**_ ). Given retrieved documents\n_D_ _**q**_ _", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_2700", "chunk_text": "**_ _<_ 1 = _\u2205_ ), and the user input is used as the\ninitial query ( _**q**_ 1 = _**x**_ ). Given retrieved documents\n_D_ _**q**_ _t_, LMs continually generate the answer until the\nnext retrieval is triggered or reaches the end:\n\n\n_**y**_ _t_ = LM([ _D_ _**q**_ _t,_ _**x**_ _,_ _**y**_ _<t_ ]) _,_\n\n\nwhere _**y**_ _t_ represents the generated tokens at the current step _t_, and the input to LMs is the concatenation of the retrieved documents _D_ _**q**_ _t_, the user input\n_**x**_, and the previous generation _**y**_ _<t_ . We discard\npreviously retrieved documents _\u222at\u2032<tD_ _**q**_ _t\u2032_ and only\nuse the retrieved documents from the current step\nto condition the next generation to prevent reaching\nthe input length limit of LMs.\n\n\n**3** **FLARE: Forward-Looking Active**\n**REtrieval Augmented Generation**\n\n\nOur intuition is that (1) LMs should only retrieve\ninformation when they do not have the necessary\nknowledge to avoid unnecessary or inappropriate\nretrieval, and (2) the retrieval queries should reflect\nthe intents of future generations. We propose two\nforward-looking active retrieval augmented generation (FLARE) methods to implement the active\nretrieval augmented generation framework. The\nfirst method prompts the LM to generate retrieval\nqueries when necessary while generating the answer using retrieval-encouraging instructions, denoted as FLAREinstruct. The second method directly\nuses the LM\u2019s generation as search queries, denoted\nas FLAREdirect, which iteratively generates the next\nsentence to gain insight into the future topic, and\nif uncertain tokens are present, retrieves relevant\ndocuments to regenerate the next sentence.\n\n**3.1** **FLARE with Retrieval Instructions**\n\nInspired by Toolformer (Schick et al., 2023), a\nstraightforward way of expressing information\nneeds for retrieval is to generate \u201c[Search(query)]\u201d\nwhen additional information is needed (Schick\net al., 2023), e.g., \u201cThe colors on the flag of\nGhana have", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_3150", "chunk_text": " a\nstraightforward way of expressing information\nneeds for retrieval is to generate \u201c[Search(query)]\u201d\nwhen additional information is needed (Schick\net al., 2023), e.g., \u201cThe colors on the flag of\nGhana have the following meanings. Red is for\n\n[Search(Ghana flag red meaning)] the blood of martyrs, ...\u201d When working with GPT-3.5 models that\n\n\nSearch results:  !!\n\n[2]: \u2026\n\n\n\nretrieval instructions might not be reliable. Therefore, we propose a more direct way of forwardlooking active retrieval that uses the next sentence\nto decide when and what to retrieve.\n\n\n**3.2.1** **Confidence-based Active Retrieval**\n\n\n\n\n\n_Retriever_\n\n\n\n\n\n\n\n$\n\n\n&#\n\n\n%$\n\n\n&$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%\n\n\n&%\n\n\n\n\n\n\n\nFigure 2: An illustration of forward-looking active retrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in _gray italic_ ) to retrieve relevant information to\naid future generations.\n\n\n- ffer only API access, we elicit such behavior by\nfew-shot prompting (Brown et al., 2020).\nSpecifically, for a downstream task, we place\nthe search-related instruction and exemplars at the\nbeginning as skill 1, followed by the instruction and\nexemplars of the downstream task as skill 2. Given\na test case, we ask LMs to combine skills 1 and 2 to\ngenerate search queries while performing the task.\nThe structure of the prompt is shown in Prompt 3.1,\nand full details can be found in Prompt D.3.\n\n\n\nAs shown in Figure 1, at step _t_, we first generate a\ntemporary next sentence \u02c6 _**s**_ _t_ = LM([ _**x**_ _,_ _**y**_ _<t_ ]) with\n- ut conditioning on retrieved documents. Then we\ndecide whether to trigger retrieval and formulate\nqueries based on \u02c6 _**s**_ _t_ . If the LM is confident about \u02c6 _**s**_ _t_,\nwe accept it without retrieving additional information; if not, we use \u02c6 _**s**_ _t_ to formulate search queries\n_**q**_ _t_ to retrieve relevant documents, and then regenerate the next sentence _**s**_", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_3600", "chunk_text": " without retrieving additional information; if not, we use \u02c6 _**s**_ _t_ to formulate search queries\n_**q**_ _t_ to retrieve relevant documents, and then regenerate the next sentence _**s**_ _t_ . The reason we utilize\n\nsentences as the basis of our iteration is due to their\n\nsignificance as semantic units that are neither too\nshort nor too lengthy like phrases and paragraphs.\nHowever, our approach can also utilize phrases or\nparagraphs as the basis.\n\nSince LMs tend to be well-calibrated that low\n\nprobability/confidence often indicates a lack of\nknowledge (Jiang et al., 2021; Kadavath et al.,\n2022; Varshney et al., 2022), we actively trigger\nretrieval if any token of \u02c6 _**s**_ _t_ has a probability lower\nthan a threshold _\u03b8 \u2208_ [0 _,_ 1]. _\u03b8_ = 0 means retrieval\nis never triggered, while _\u03b8_ = 1 triggers retrieval\n\nevery sentence.\n\n\n\n_**s**_ \u02c6 _t_ if all tokens of \u02c6 _**s**_ _t_ have probs _\u2265_ _\u03b8_\n\n- _**s**_ _t_ = LM([ _D_ _**q**_ _t,_ _**x**_ _,_ _**y**_ _<t_ ]) - therwise\n\n\n\n\n\n_**y**_ _t_ =\n\n\n\nAs shown in Figure 2, when the LM generates\n\u201c[Search(query)]\u201d (shown in _gray italic_ ), we stop\nthe generation and use the query terms to retrieve\nrelevant documents, which are prepended before\nthe user input to aid future generation until the\nnext search query is generated or reaches the end.\nAdditional implementation details are included in\nAppendix A.\n\n**3.2** **Direct FLARE**\n\nSince we cannot fine-tune black-box LMs, we\nfound queries generated by FLAREinstruct through\n\n\n\nwhere the query _**q**_ _t_ is formulated based on \u02c6 _**s**_ _t_ .\n\n\n**3.2.2** **Confidence-based Query Formulation**\n\n\nOne way to perform retrieval is to directly use the\nnext sentence \u02c6 _**s**_ _t_ as the query _**q", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_4050", "chunk_text": "t_ .\n\n\n**3.2.2** **Confidence-based Query Formulation**\n\n\nOne way to perform retrieval is to directly use the\nnext sentence \u02c6 _**s**_ _t_ as the query _**q**_ _t_ . This shares a similar spirit with methods that use generated hypothetical titles or paragraphs from LMs as retrieval\nqueries or evidences (Gao et al., 2022; Sun et al.,\n2022; Yu et al., 2022; Mao et al., 2021). We generalize such techniques to long-form generation\nwhere active information access is essential.\n\nWe found retrieving with the next sentence\nachieves significantly better results than with the\nprevious context, as shown later in subsection 6.2.\nHowever, it has a risk of perpetuating errors contained in it. For example, if the LM produces the\nsentence \u201cJoe Biden attended the University of\nPennsylvania\u201d instead of the correct fact that he\nattended the University of Delaware, using this erroneous sentence as a query might retrieve mislead\n\nJoe Biden attended the University of Pennsylvania,\nwhere he earned a law degree.\n\n\n\nqueries _**q**_ _t_ are formulated based on \u02c6 _**s**_ _t_ as follows:\n\n\n\n\n\n_explicit query by_\n_question generation_\n\n\n\n_**q**_ _t_ =\n\n\n\n_\u2205_ if all tokens of \u02c6 _**s**_ _t_ have probs _\u2265_ _\u03b8_\n\n- mask(\u02c6 _**s**_ _t_ ) or qgen(\u02c6 _**s**_ _t_ ) - therwise\n\n\n\n\n\nAsk a question to which the answer is \u201cthe University of Pennsylvania\u201d\nAsk a question to which the answer is \u201ca law degree\u201d\n\n\nLM such as ChatGPT\n\n\n\n\n\nFigure 3: Implicit and explicit query formulation. Tokens with low probabilities are marked with underlines.\n\n\ning information. We propose two simple methods\nto overcome this issue as illustrated in Figure 3.\n\n\n**Masked sentences as implicit queries.** The first\nmethod masks out low-confidence tokens in \u02c6 _**s**_ _t_ with\nprobabilities below a threshold _\u03b2 \u2208_ [0 _,_ 1], where a\nhigher _\u03b2_ results in more aggressive masking. This\nremoves potential distractions from the sentence to\nimprove retrieval accuracy.\n\n\n**Generated questions as explicit queries", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_4500", "chunk_text": "abilities below a threshold _\u03b2 \u2208_ [0 _,_ 1], where a\nhigher _\u03b2_ results in more aggressive masking. This\nremoves potential distractions from the sentence to\nimprove retrieval accuracy.\n\n\n**Generated questions as explicit queries.** An\n- ther method is to generate explicit questions that\ntarget the low-confident span in \u02c6 _**s**_ _t_ . For example, if\nthe LM is uncertain about \u201cthe University of Pennsylvania\u201d, a question like \u201cWhich university did\nJoe Biden attend?\u201d can help retrieve relevant information. Self-ask (Press et al., 2022) achieved\nthis by manually inserting follow-up questions\ninto downstream task exemplars as shown later\nin Prompt D.2, which requires task-specific annotation efforts. Instead, we developed a universal approach that generates questions for low-confidence\nspans without additional annotation. Specifically,\nWe first extract all spans from \u02c6 _**s**_ _t_ with probabilities\nbelow _\u03b2_ . For each extracted span _**z**_, we prompt\ngpt-3.5-turbo to generate a question _**q**_ _t,_ _**z**_ that\ncan be answered with the span:\n\n\n\n\n\nWe retrieve using each generated question and\ninterleave the returned documents into a single\nranking list to aid future generations. In summary,\n\n\n\n**3.3** **Implementation Details**\n\n**Base LM** We validate our method on one of the\n\nmost advanced GPT-3.5 LMs text-davinci-003\nby iteratively querying their API. [2]\n\n\n**Document corpus and retrievers.** Since we focus on the integration of retrieval and generation,\nwe use off-the-shelf retrievers that take queries\nas inputs and return a list of relevant documents.\nFor datasets that mainly rely on knowledge from\nWikipedia, we use the Wikipedia dump from\n\nKarpukhin et al. (2020) and employ BM25 (Robertson and Zaragoza, 2009) as the retriever. For\ndatasets that rely on knowledge from the open web,\nwe use the Bing search engine as our retriever. [3]\n\n\n**Retrieved document formatting.** Multiple retrieved documents are linearized according to their\nranking and then added to the beginning of the user\ninput using Prompt D.1.\nOther implementation details such as sentence tokenization and efficiency are included Appendix A.\n\n\n**", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_4950", "chunk_text": "Retrieved document formatting.** Multiple retrieved documents are linearized according to their\nranking and then added to the beginning of the user\ninput using Prompt D.1.\nOther implementation details such as sentence tokenization and efficiency are included Appendix A.\n\n\n**4** **Multi-time Retrieval Baselines**\n\n\nExisting passive multi-time retrieval augmented\nLMs can also be formulated using our framework\n(subsection 2.3). In this section, we formally introduce three baseline categories based on when\nand what to retrieve. These baselines are not exact\n\nreproductions of the corresponding paper because\nmany design choices differ which makes direct\ncomparisons impossible. We implemented them\nusing the same settings, with the only variation\nbeing when and what to retrieve.\n\n\n**Previous-window** approaches trigger retrieval\nevery _l_ tokens, where _l_ represents the window size.\nGenerated tokens from the previous window are\nused as the query:\n\n\n_**q**_ _t_ = _**y**_ _t\u2212_ 1 ( _t \u2265_ 2) _,_\n\n_**y**_ _t_ = [ _w_ ( _t\u2212_ 1) _l_ +1 _, ..., wtl_ ] _._\n\n\nSome existing methods in this category are RETRO\n(Borgeaud et al., 2022), IC-RALM (Ram et al.,\n\n\n2https://api.openai.com/v1/completions April 23.\n[3https://www.microsoft.com/en-us/bing/apis/](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api)\n[bing-web-search-api](https://www.microsoft.com/en-us/bing/apis/bing-web-search-api)\n\n\n2023), which retrieve every few tokens, and KNNLM (Khandelwal et al., 2020), which retrieves every token. [4] We follow Ram et al. (2023) to use a\nwindow size of _l_ = 16.\n\n\n**Previous-sentence** approaches trigger retrieval\nevery sentence and use the previous sentence as the\nquery, and IRCoT (Trivedi et al., 2022) belongs to\nthis category:\n\n\n_**q**_ _t_ = _**y**_ _t\u2212_ 1 ( _t \u2265_ 2) _,_\n\n\n_**y**_ _t_ = _**s**_ _t._", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_5400", "chunk_text": " category:\n\n\n_**q**_ _t_ = _**y**_ _t\u2212_ 1 ( _t \u2265_ 2) _,_\n\n\n_**y**_ _t_ = _**s**_ _t._\n\n\n**Question decomposition** approaches manually\nannotated task-specific exemplars to guide LMs\nto generate decomposed sub-questions while producing outputs. For example, self-ask (Press et al.,\n2022), a method in this category, manually inserts\nsub-questions in exemplars using Prompt D.2. For\nthe test case, retrieval is triggered dynamically\nwhenever the model generates a sub-question.\nThe aforementioned approaches can retrieve additional information while generating. However,\nthey have notable drawbacks: (1) Using previously\ngenerated tokens as queries might not reflect what\nLMs intend to generate in the future. (2) Retrieving information at a fixed interval can be inefficient\nbecause it might occur at inappropriate points. (3)\nQuestion decomposition approaches require taskspecific prompt engineering, which restricts their\ngeneralizability in new tasks.\n\n\n**5** **Experimental Setup**\n\n\nWe evaluate the effectiveness of FLARE on 4 di\nverse knowledge-intensive tasks using few-shot incontext learning (Radford et al., 2019; Brown et al.,\n2020; Liu et al., 2023). We follow previous works\n(Trivedi et al., 2022) to sub-sample at most 500\nexamples from each dataset due to the cost of running experiments. Datasets, metrics, and settings\nare summarized in Table 7 of Appendix B. The\nhyperparameters of FLARE are selected based on\nthe development set and listed in Table 9. FLARE\nrefers to FLAREdirect if not specifically stated.\n\n\n**Multihop QA** The goal of multihop QA is to\nanswer complex questions through information retrieval and reasoning. We use 2WikiMultihopQA\n(Ho et al., 2020) which contains 2-hop complex\n\n\n4Since KNN-LM uses the contextualized representation\ncorresponding to the current decoding position to retrieve relevant information which encodes all previous tokens. Strictly\nspeaking, _**q**_ _t_ should be _**y**_ _<t_ .\n\n\n\nquestions sourced from Wikipedia articles that require composition, comparison, or inference, e.g.,\n\u201cWhy did the founder of Versus", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_5850", "chunk_text": "ly\nspeaking, _**q**_ _t_ should be _**y**_ _<t_ .\n\n\n\nquestions sourced from Wikipedia articles that require composition, comparison, or inference, e.g.,\n\u201cWhy did the founder of Versus die?\u201d We follow\n\nWang et al. (2022) to generate both the chain-ofthought and the final answer. Experimental setting\ndetails are included in Appendix B.\nWe use regular expressions to extract the final\nanswer from the output and compare it with the reference answer using exact match (EM), and tokenlevel F1, precision, and recall.\n\n\n**Commonsense reasoning** Commonsense reasoning requires world and commonsense knowledge\nto generate answers. We use StrategyQA (Geva\net al., 2021) which is a collection of crowdsourced\nyes/no questions, e.g., \u201cWould a pear sink in water?\u201d We follow Wei et al. (2022) to generate both\nthe chain-of-thought and the final yes/no answer.\nDetails are included in Appendix B.\nWe extract the final answer and match it against\nthe gold answer using exact match.\n\n\n**Long-form QA** Long-form QA aims to generate\ncomprehensive answers to questions seeking complex information (Fan et al., 2019; Stelmakh et al.,\n2022). We use ASQA (Stelmakh et al., 2022) as our\ntestbed where inputs are ambiguous questions with\nmultiple interpretations, and outputs should cover\nall of them. For example, \u201cWhere do the Philadelphia Eagles play their home games?\u201d could be\nasking about the city, sports complex, or stadium.\nWe found in many cases it is challenging even for\nhumans to identify which aspect of the question\nis ambiguous. Therefore, we created another setting (ASQA-hint) where we provide a brief hint\nto guide LMs to stay on track when generating answers. The hint for the above case is \u201cThis question\nis ambiguous in terms of which specific location or\nvenue is being referred to.\u201d Experimental setting\ndetails are included in Appendix B.\nWe use metrics from Stelmakh et al. (2022), including EM, RoBERTa-based QA score (DisambigF1), ROUGE (Lin, 2004), and an overall score combining Disambig-F1 and ROUGE (DR).\n\n\n**Open-domain summarization** The goal of opendomain summarization is to generate a", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_6300", "chunk_text": " (DisambigF1), ROUGE (Lin, 2004), and an overall score combining Disambig-F1 and ROUGE (DR).\n\n\n**Open-domain summarization** The goal of opendomain summarization is to generate a comprehensive summary about a topic by gathering information from open web (Giorgi et al., 2022). We use\nWikiAsp (Hayashi et al., 2021) which aims to generate aspect-based summaries about entities from\n20 domains in Wikipedia, e.g., \u201cGenerate a summary about Echo School (Oregon) including the\n\n\nNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp\n\n\nFigure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\n\n\n\nfollowing aspects: academics, history.\u201d Experimental setting details are included in Appendix B.\nMetrics include ROUGE, named entity-based F1,\nand UniEval (Zhong et al., 2022) which measures\nfactual consistency.\n\n\n**6** **Experimental Results**\n\n\nWe first report overall results across 4 tasks/datasets\nand compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\n\nrun ablation experiments to study the efficacy of\nvarious design choices of our method.\n\n**6.1** **Comparison with Baselines**\n\n**Overall results.** The overall performance of\nFLARE and baseline across all tasks/datasets are\n\nreported in Figure 4. FLARE outperforms all baseline on all tasks/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks, multihop QA shows the\nmost significant improvement. This is largely due\nto the task\u2019s clear definition and specific objective\n\n- f producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to\ngenerate on-topic output. In contrast, ASQA and\nWikiAsp are more open-ended, which increases the\ndifficulty of both generation and evaluation. The\n", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_6750", "chunk_text": " through a 2-hop reasoning process, which makes it easier for LMs to\ngenerate on-topic output. In contrast, ASQA and\nWikiAsp are more open-ended, which increases the\ndifficulty of both generation and evaluation. The\nimprovement on ASQA-hint is larger than that of\nASQA because identifying ambiguous aspects is\nchallenging even for humans in many cases, and\nproviding a generic hint helps LMs to stay on topic.\n\n\n**Thorough comparisons with baselines.** The performance of all baselines on 2WikiMultihopQA\nare reported in Table 1. FLARE outperforms all\nbaselines by a large margin, which confirms that\nforward-looking active retrieval is highly effective.\nMost multi-time retrieval augmented approaches\n\n- utperform single-time retrieval but with different\n\n\n\n**Methods** **EM** **F** 1 **Prec.** **Rec.**\n\n\nNo retrieval 28.2 36.8 36.5 38.6\nSingle-time retrieval 39.4 48.8 48.6 51.5\n\n\n_Multi-time retrieval_\n\nPrevious-window 43.2 52.3 51.7 54.5\n\nPrevious-sentence 39.0 49.2 48.9 51.8\nQuestion decomposition 47.8 56.4 56.1 58.6\nFLAREinstruct (ours) 42.4 49.8 49.1 52.5\nFLAREdirect (ours) **51.0** **59.7** **59.1** **62.6**\n\n\nTable 1: FLARE and baselines on 2WikiMultihopQA.\nPrevious-window (Borgeaud et al., 2022; Ram et al.,\n2023), previous-sentence (Trivedi et al., 2022), and question decomposition (Press et al., 2022; Yao et al., 2022)\nmethods are reimplemented for fair comparisons.\n\n\nmargins. The improvement of retrieving using the\nprevious sentence is relatively small which we hypothesize is mainly because the previous sentence\n\n- ften describes entities or relations different from\n\nthose in the next sentence in 2WikiMultihopQA.\nWhile the previous-window approach might use\nthe first half of a sentence to retrieve information\npotentially helpful for generating the second", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_7200", "chunk_text": "\n\n- ften describes entities or relations different from\n\nthose in the next sentence in 2WikiMultihopQA.\nWhile the previous-window approach might use\nthe first half of a sentence to retrieve information\npotentially helpful for generating the second half.\nAmong all baselines, the question decomposition\napproach (Press et al., 2022) achieves the best performance. which is not surprising since the incontext exemplars manually annotated with decomposed sub-questions (Prompt D.2) guide LMs to\ngenerate sub-questions that align with the topic/intent of future generations. FLARE outperforms\nthis baseline, indicating that manual exemplar annotation is not necessary for effective future-aware\nretrieval. The gap between FLAREinstruct and question decomposition is large, indicating that teaching\nLMs to generate search queries using task-generic\nretrieval instructions and exemplars is challenging.\n\nWe report all metrics for the other datasets in\nTable 2. FLARE outperforms baselines with respect to all metrics. Retrieval using the previ\n\n**Datasets** **StrategyQA** **ASQA** **ASQA-hint** **WikiAsp**\n**Metrics** **EM** **EM D-F** 1 **R-L DR** **EM D-F** 1 **R-L DR** **UniEval E-F** 1 **R-L**\n\n\nNo retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4\n\nSingle-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9\n\n\n_Multi-time retrieval_\n\nPrevious-window 71.2 39.9 27.0 **34.3** 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3\n\nPrevious-sentence 71.0 39.9 27.9 **34.3** 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\n\nFLARE", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_7650", "chunk_text": ".0 39.9 27.9 **34.3** 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\n\nFLARE (ours) **77.3** **41.3 28.2 34.3 31.1** **46.2 36.7 37.7 37.2** **53.4** **18.9 27.6**\n\n\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is\nDisambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\n\n\n\n**2WikiMultihopQA** **ASQA-hint**\n**EM** **F** 1 **Prec. Rec.** **EM D-F** 1 **R-L DR**\n\n\nPrevious 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\n\nNext 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\n\n\nTable 3: A head-to-head comparison between using the\nprevious sentence and the next sentence for retrieval.\n\n\n**#Tokens** **EM** **F** 1 **Prec.** **Rec.**\n\n\n16 43.2 52.3 51.7 54.5\n\n32 43.6 52.4 52.0 55.0\n\n48 40.0 49.3 49.0 52.0\n\nAll 39.0 48.5 48.2 51.1\n\n\nTable 4: Previous-window approaches using different\nnumbers of tokens as queries.\n\n\n- us window underperforms single-time retrieval\n\n- n ASQA, which we hypothesize is because the\nprevious window does not accurately reflect future\nintent. Since we focus on evaluating factuality, metrics with an emphasis on factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n\n\n**6.2** **Ablation Study**\n\n\n**Importance of forward-looking", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_8100", "chunk_text": " factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n\n\n**6.2** **Ablation Study**\n\n\n**Importance of forward-looking retrieval.** We\nfirst validate that forward-looking retrieval is more\neffective than past-context-based retrieval. We run\nablation experiments on 2WikiMultihopQA and\nASQA-hint comparing retrieval using the previ\n- us versus the next sentence. Specifically, both\nmethods retrieve every sentence and directly use\nthe complete previous/next sentence as queries. As\nshown in Table 3, using the next sentence to retrieve\nis clearly better than using the previous sentence,\nconfirming our hypothesis.\nWe also run previous-window approaches using\ndifferent numbers of past tokens as queries. As\nshown in Table 4, using too many tokens ( _>_ 32) in\n\n\n\nFigure 5: Performance (EM) of FLARE with respect\nto the percentage of steps/sentences with retrieval on\n2WikiMultihopQA and StrategyQA.\n\n\nthe past hurts the performance, further confirming\n\n- ur hypothesis that previous context might not be\nrelevant to intent of future generations.\n\n\n**Importance of active retrieval.** Next, we investigate how active retrieval threshold _\u03b8_ affects performance. To alter our method from not retrieving\nto retrieving every sentence, we adjust the confidence threshold _\u03b8_ that determines when to trigger\nretrieval from 0 to 1. We then calculate the proportion of steps/sentences where retrieval is activated, and present the performance based on it. As\nshown in Figure 5, on 2WikiMultihopQA, the performance plateaus when the retrieval percentage\nexceeds 60%, indicating that retrieval when LMs\nare confident is not necessary. On StrategyQA, the\nperformance drops when the retrieval percentage\nexceeds 50%, indicating that unnecessary retrieval\ncan introduce noise and impede the original generation process. We found triggering retrieval for\n40%-80% of sentences usually leads to a good performance across tasks/datasets.\n\n\n**Effectiveness of different query formulation**\n**methods** We study implicit query formation by\nmasking and explicit query formulation through\nquestion generation. In Table 5, we compare the\nperformance of FLARE with different masking\n\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_8550", "chunk_text": " study implicit query formation by\nmasking and explicit query formulation through\nquestion generation. In Table 5, we compare the\nperformance of FLARE with different masking\n\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n\n\n2WikiMultihopQA StrategyQA\n\n\n0.0 25.0 50.0 75.0 100.0\n\n\n%steps/sentences with retrieval\n\n\n_\u03b2_ **EM** **F** 1 **Prec.** **Rec.**\n\n\n0.0 0.488 0.576 0.571 0.605\n\n0.2 0.498 0.588 0.582 0.616\n\n0.4 0.510 0.597 0.591 0.627\n\n0.6 0.506 0.593 0.586 0.622\n\n\nTable 5: Performance of FLARE with respect to the\nmasking threshold _\u03b2_ - n 2WikiMultihopQA.\n\n\n**ASQA-hint** **WikiAsp**\n**EM D-F** 1 **R-L DR** **UniEval E-F** 1 **R-L**\n\n\nImplicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7\nExplicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6\n\n\nTable 6: A comparison between implicit and explicit\nquery formulation methods in FLARE.\n\n\nthresholds _\u03b2_ . Retrieving directly with the complete\nsentence ( _\u03b2_ = 0) is worse than masking tokens\nwith low probabilities, confirming our hypothesis\nthat low-confidence erroneous tokens can distract\nretrievers. We compare implicit and explicit query\nformulation methods in Table 6. Performances of\n\nboth methods are similar, indicating that both meth\n- ds can effectively reflect information needs.\n\n\n**7** **Related Work**\n\n\nWe refer to subsection 2.2 and section 4 for ex\ntensively discussion on single-time and multi-time\nretrieval augmented LMs, which is the most relevant area to this paper.\n\n\n**Iterative and adaptive retrieval** Iterative retrieval and refinement has been studied in both\ntext and code generation tasks (Peng et al., 202", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_9000", "chunk_text": "-time\nretrieval augmented LMs, which is the most relevant area to this paper.\n\n\n**Iterative and adaptive retrieval** Iterative retrieval and refinement has been studied in both\ntext and code generation tasks (Peng et al., 2023;\nZhang et al., 2023; Zemlyanskiy et al., 2022; Yu\net al., 2023). FLARE differs from these methods in\nthe granularity of generation and retrieval strategies.\nAdaptive retrieval has been studied in single-time\nretrieval scenarios based on either question popularity or generation probabilities (Mallen et al.,\n2022; Li et al., 2023), while we focus on long-form\ngeneration requiring active information access.\n\n\n**Browser-enhanced LMs** WebGPT (Nakano\net al., 2021) and WebCPM (Qin et al., 2023) train\nLMs to interact with browser to enhance factuality\nusing reinforcement learning or supervised training where multiple queries can be triggered before\ngeneration. FLARE is built on text-based retrievers\nbut can be combined with a browser to potentially\nimprove retrieval quality.\n\n\n\n**8** **Conclusion**\n\n\nTo aid long-form generation with retrieval augmentation, we propose an active retrieval augmented generation framework that decides when\nand what to retrieve during generation. We implement this framework with forward-looking active\nretrieval that iteratively uses the upcoming sentence\nto retrieve relevant information if it contains low\nconfidence tokens and regenerates the next sentence. Experimental results on 4 tasks/datasets\ndemonstrate the effectiveness of our methods. Fu\nture directions include better strategies for active\nretrieval and developing efficient LM architectures\nfor active information integration.\n\n\n**9** **Limitations**\n\n\nWe also conduct experiments on Wizard of\nWikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,\n\n2019), and found that FLARE did not provide significant gains. Wizard of Wikipedia is a knowledgeintensive dialogue generation dataset where the out\n_\u223c_\nput is relatively short ( 20 tokens on average) so\nretrieving multiple disparate pieces of information\nmight not be necessary. ELI5 (Fan et al., 2019)\nis a long-form QA dataset requiring in-depth answers to open-ended questions. Due to issues mentioned in Krishna et al. (2021) such as difficulties\n\n-", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_9450", "chunk_text": "might not be necessary. ELI5 (Fan et al., 2019)\nis a long-form QA dataset requiring in-depth answers to open-ended questions. Due to issues mentioned in Krishna et al. (2021) such as difficulties\n\n- f grounding generation in retrieval and evaluation, both single-time retrieval and FLARE did not\nprovide significant gains over not using retrieval.\nFrom an engineering perspective, interleaving generation and retrieval with a naive implementation\nincreases both overheads and the cost of generation.\nLMs need to be activated multiple times (once for\neach retrieval) and a caching-free implementation\nalso requires recomputing the previous activation\neach time after retrieval. This issue can be potentially alleviated with special architectural designs\nthat encode the retrieved documents _D_ _**q**_ _t_ and the\ninput/generation ( _**x**_ / _**y**_ _<t_ ) independently.\n\n\n**Acknowledgements**\n\n\nThis work was supported in part by a grant from\nthe Singapore Defence Science and Technology\nAgency and the IBM PhD Fellowship. We thank\nChunting Zhou, Amanda Bertsch, Uri Alon, Hiroaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo\nSchick, Kaixin Ma, Shuyan Zhou, and Songwei Ge\nfor their insightful discussions and help with the\nexperiments.\n\n\n**References**\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n[2022. Improving language models by retrieving from](https://proceedings.mlr.press/v162/borgeaud22a.html)\n[trillions of tokens. In](https://proceedings.mlr.press/v162/borgeaud22a.html) _International Conference on_\n_Machine Learning, ICML ", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_10350", "chunk_text": "istics, ACL 2017, Vancouver, Canada, July 30 -_\n_August 4, Volume 1: Long Papers_, pages 1870\u20131879.\nAssociation for Computational Linguistics.\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin\n - dkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\n[and Noah Fiedel. 2022. Palm: Scaling language mod-](https://doi.org/10.48550/arXiv.2204.02311)\n[eling with pathways.](https://doi.org/10.48550/arXiv.2204.02311) _CoRR_, abs/2204.02311.\n\n\n\nNachshon Cohen, Oren Kalinsky, Yftah Ziser, and\n[Alessandro Moschitti. 2021. Wikisum: Coherent](https://doi.org/10.18653/v1/2021", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_10800", "chunk_text": "Nachshon Cohen, Oren Kalinsky, Yftah Ziser, and\n[Alessandro Moschitti. 2021. Wikisum: Coherent](https://doi.org/10.18653/v1/2021.acl-short.28)\n[summarization dataset for efficient human-evaluation.](https://doi.org/10.18653/v1/2021.acl-short.28)\nIn _Proceedings of the 59th Annual Meeting of the As-_\n_sociation for Computational Linguistics and the 11th_\n_International Joint Conference on Natural Language_\n_Processing, ACL/IJCNLP 2021, (Volume 2: Short_\n_Papers), Virtual Event, August 1-6, 2021_, pages 212\u2013\n219. Association for Computational Linguistics.\n\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\n[Fan, Michael Auli, and Jason Weston. 2019. Wizard](https://openreview.net/forum?id=r1l73iRqKm)\n\n[of wikipedia: Knowledge-powered conversational](https://openreview.net/forum?id=r1l73iRqKm)\n[agents. In](https://openreview.net/forum?id=r1l73iRqKm) _7th International Conference on Learning_\n_Representations, ICLR 2019, New Orleans, LA, USA,_\n_May 6-9, 2019_ . OpenReview.net.\n\n\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang[ier, Jason Weston, and Michael Auli. 2019. ELI5:](https://doi.org/10.18653/v1/p19-1346)\n[long form question answering. In](https://doi.org/10.18653/v1/p19-1346) _Proceedings of_\n_the 57th Conference of the Association for Compu-_\n_tational Linguistics, ACL 2019, Florence, Italy, July_\n_28- August 2, 2019, Volume 1: Long Papers_, pages\n3558\u20133567. Association for Computational Linguistics.\n\n\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n[2022. Precise zero-shot dense retrieval without rele-](https://doi.org/10.48550/arXiv", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_11250", "chunk_text": " Computational Linguistics.\n\n\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n[2022. Precise zero-shot dense retrieval without rele-](https://doi.org/10.48550/arXiv.2212.10496)\n[vance labels.](https://doi.org/10.48550/arXiv.2212.10496) _CoRR_, abs/2212.10496.\n\n\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. _Transactions of the_\n_Association for Computational Linguistics_, 9:346\u2013\n361.\n\n\nJohn M. Giorgi, Luca Soldaini, Bo Wang, Gary D.\nBader, Kyle Lo, Lucy Lu Wang, and Arman Cohan. 2022. [Exploring the challenges of open](https://doi.org/10.48550/arXiv.2212.10526)\n[domain multi-document summarization.](https://doi.org/10.48550/arXiv.2212.10526) _CoRR_,\nabs/2212.10526.\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu[pat, and Ming-Wei Chang. 2020. REALM: retrieval-](http://arxiv.org/abs/2002.08909)\n[augmented language model pre-training.](http://arxiv.org/abs/2002.08909) _CoRR_,\nabs/2002.08909.\n\n\nHiroaki Hayashi, Prashant Budania, Peng Wang, Chris\nAckerson, Raj Neervannan, and Graham Neubig.\n[2021. Wikiasp: A dataset for multi-domain aspect-](https://doi.org/10.1162/tacl_a_00362)\n[based summarization.](https://doi.org/10.1162/tacl_a_00362) _Trans. Assoc. Comput. Lin-_\n_guistics_, 9:211\u2013225.\n\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n[2020. Measuring", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_11700", "chunk_text": " Lin-_\n_guistics_, 9:211\u2013225.\n\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n[2020. Measuring massive multitask language under-](http://arxiv.org/abs/2009.03300)\n[standing.](http://arxiv.org/abs/2009.03300) _CoRR_, abs/2009.03300.\n\n\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\n[and Akiko Aizawa. 2020. Constructing A multi-hop](https://doi.org/10.18653/v1/2020.coling-main.580)\n[QA dataset for comprehensive evaluation of reason-](https://doi.org/10.18653/v1/2020.coling-main.580)\n[ing steps. In](https://doi.org/10.18653/v1/2020.coling-main.580) _Proceedings of the 28th International_\n_Conference on Computational Linguistics, COLING_\n_2020, Barcelona, Spain (Online), December 8-13,_\n\n\n_2020_, pages 6609\u20136625. International Committee on\nComputational Linguistics.\n\n\n[Gautier Izacard and Edouard Grave. 2021. Leveraging](https://doi.org/10.18653/v1/2021.eacl-main.74)\n[passage retrieval with generative models for open do-](https://doi.org/10.18653/v1/2021.eacl-main.74)\n[main question answering. In](https://doi.org/10.18653/v1/2021.eacl-main.74) _Proceedings of the 16th_\n_Conference of the European Chapter of the Associ-_\n_ation for Computational Linguistics: Main Volume,_\n_EACL 2021, Online, April 19 - 23, 2021_, pages 874\u2013\n880. Association for Computational Linguistics.\n\n\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. [", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_12600", "chunk_text": " Luke\n[Zettlemoyer. 2017. Triviaqa: A large scale distantly](https://doi.org/10.18653/v1/P17-1147)\n[supervised challenge dataset for reading comprehen-](https://doi.org/10.18653/v1/P17-1147)\n[sion. In](https://doi.org/10.18653/v1/P17-1147) _Proceedings of the 55th Annual Meeting of_\n_the Association for Computational Linguistics, ACL_\n_2017, Vancouver, Canada, July 30 - August 4, Volume_\n_1: Long Papers_, pages 1601\u20131611. Association for\nComputational Linguistics.\n\n\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, Scott Johnston, Sheer El Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYuntao Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom Brown, Jack Clark, Nicholas Joseph,\nBen Mann, Sam McCandlish, Chris Olah, and Jared\n[Kaplan. 2022. Language models (mostly) know what](https://doi.org/10.48550/arXiv.2207.05221)\n[they know.](https://doi.org/10.48550/arXiv.2207.05221) _CoRR_, abs/2207.05221.\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\n[and Wen-tau Yih. 2020. Dense passage retrieval for](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n\n[open-domain question answering. In](https://doi.org/10.18653/v1/2020.emnlp-main.550) _Proceedings of_\n_the 2020 Conference on Emp", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_13050", "chunk_text": "2020.emnlp-main.550)\n\n[open-domain question answering. In](https://doi.org/10.18653/v1/2020.emnlp-main.550) _Proceedings of_\n_the 2020 Conference on Empirical Methods in Nat-_\n_ural Language Processing, EMNLP 2020, Online,_\n_November 16-20, 2020_, pages 6769\u20136781. Association for Computational Linguistics.\n\n\n\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n[Zettlemoyer, and Mike Lewis. 2020. Generalization](https://openreview.net/forum?id=HklBjCEKvH)\n[through memorization: Nearest neighbor language](https://openreview.net/forum?id=HklBjCEKvH)\n[models. In](https://openreview.net/forum?id=HklBjCEKvH) _8th International Conference on Learning_\n_Representations, ICLR 2020, Addis Ababa, Ethiopia,_\n_April 26-30, 2020_ . OpenReview.net.\n\n\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\nDavid Hall, Percy Liang, Christopher Potts, and\n[Matei Zaharia. 2022. Demonstrate-search-predict:](https://doi.org/10.48550/arXiv.2212.14024)\n[Composing retrieval and language models for](https://doi.org/10.48550/arXiv.2212.14024)\n[knowledge-intensive NLP.](https://doi.org/10.48550/arXiv.2212.14024) _CoRR_, abs/2212.14024.\n\n\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\n[2022. Decomposed prompting: A modular approach](https://doi.org/10.48550/arXiv.2210.02406)\n[for solving complex tasks.](https://doi.org/10.48550/arXiv.2210.02406) _CoRR_, abs/2210.02406.\n\n\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_13950", "chunk_text": "/abs/2112.07381) _CoRR_, abs/2112.07381.\n\n\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih,\nTim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. 2020. [Retrieval-augmented generation for](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html)\n[knowledge-intensive NLP tasks. In](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html) _Advances in Neu-_\n_ral Information Processing Systems 33: Annual Con-_\n_ference on Neural Information Processing Systems_\n_2020, NeurIPS 2020, December 6-12, 2020, virtual_ .\n\n\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,\n[Jian-Yun Nie, and Ji-Rong Wen. 2023. The web can](https://doi.org/10.48550/arXiv.2305.10998)\n[be your oyster for improving large language models.](https://doi.org/10.48550/arXiv.2305.10998)\n_CoRR_, abs/2305.10998.\n\n\n[Chin-Yew Lin. 2004. ROUGE: A package for auto-](https://aclanthology.org/W04-1013)\n[matic evaluation of summaries. In](https://aclanthology.org/W04-1013) _Text Summariza-_\n_tion Branches Out_, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\n\n\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\n[Hiroaki Hayashi, and Graham Neubig. 2023. Pre-](https://doi.org/10.1145/3560815)\n[train, prompt, and predict: A systematic survey of](https://doi.org/10.1145/3560815)\n[prompting", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_14400", "chunk_text": " 2023. Pre-](https://doi.org/10.1145/3560815)\n[train, prompt, and predict: A systematic survey of](https://doi.org/10.1145/3560815)\n[prompting methods in natural language processing.](https://doi.org/10.1145/3560815)\n_ACM Comput. Surv._, 55(9):195:1\u2013195:35.\n\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nHannaneh Hajishirzi, and Daniel Khashabi. 2022.\n[When not to trust language models: Investigating](https://doi.org/10.48550/arXiv.2212.10511)\n[effectiveness and limitations of parametric and non-](https://doi.org/10.48550/arXiv.2212.10511)\n[parametric memories.](https://doi.org/10.48550/arXiv.2212.10511) _CoRR_, abs/2212.10511.\n\n\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2021. [Generation-augmented retrieval for open-](https://doi.org/10.18653/v1/2021.acl-long.316)\n[domain question answering. In](https://doi.org/10.18653/v1/2021.acl-long.316) _Proceedings of the_\n_59th Annual Meeting of the Association for Compu-_\n_tational Linguistics and the 11th International Joint_\n_Conference on Natural Language Processing, ACL/I-_\n_JCNLP 2021, (Volume 1: Long Papers), Virtual Event,_\n_August 1-6, 2021_, pages 4089\u20134100. Association for\nComputational Linguistics.\n\n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\n[Ryan McDonald. 2020. On faithfulness and factu-](https://doi.org/10.18653/v1/2020.acl-main.173)\n[ality in abstractive summarization. In](https://doi.org/10.18653/v1/2020.acl-main.173) _Proceedings_\n\n_of the", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_15300", "chunk_text": "ujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\n[Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check](https://doi.org/10.48550/arXiv.2302.12813)\n[your facts and try again: Improving large language](https://doi.org/10.48550/arXiv.2302.12813)\n[models with external knowledge and automated feed-](https://doi.org/10.48550/arXiv.2302.12813)\n[back.](https://doi.org/10.48550/arXiv.2302.12813) _CoRR_, abs/2302.12813.\n\n\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. [Language mod-](https://doi.org/10.18653/v1/D19-1250)\n[els as knowledge bases?](https://doi.org/10.18653/v1/D19-1250) In _Proceedings of the_\n_2019 Conference on Empirical Methods in Natu-_\n_ral Language Processing and the 9th International_\n_Joint Conference on Natural Language Processing,_\n\n\n\n_EMNLP-IJCNLP 2019, Hong Kong, China, Novem-_\n_ber 3-7, 2019_, pages 2463\u20132473. Association for\nComputational Linguistics.\n\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. _arXiv preprint arXiv:2210.03350_ .\n\n\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,\nXinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,\n[Jian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:](https://doi.org/10.48550/arXiv.2304.04358)\n[Learning to generate factually correct articles for](https://doi.org/10.48550/arXiv.2304.04358)\n[queries by grounding", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_15750", "chunk_text": "doi.org/10.48550/arXiv.2304.04358)\n[Learning to generate factually correct articles for](https://doi.org/10.48550/arXiv.2304.04358)\n[queries by grounding on large web corpus.](https://doi.org/10.48550/arXiv.2304.04358) _CoRR_,\nabs/2304.04358.\n\n\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao\nLiang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,\nHuadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan\n[Liu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-](https://doi.org/10.48550/arXiv.2305.06849)\n[teractive web search for chinese long-form question](https://doi.org/10.48550/arXiv.2305.06849)\n[answering.](https://doi.org/10.48550/arXiv.2305.06849) _CoRR_, abs/2305.06849.\n\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\n[Dario Amodei, and Ilya Sutskever. 2019. Language](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)\n[models are unsupervised multitask learners.](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) _OpenAI_\n_Blog_, 1(8).\n\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented language models. _arXiv preprint arXiv:2302.00083_ .\n\n\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\n\n[How much knowledge can you pack into the param-](https://doi.org/10.18653/v1/2020.emnlp-main.437)\n[eters of a language model? In](https://doi.org/", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_16200", "chunk_text": "2020.\n\n[How much knowledge can you pack into the param-](https://doi.org/10.18653/v1/2020.emnlp-main.437)\n[eters of a language model? In](https://doi.org/10.18653/v1/2020.emnlp-main.437) _Proceedings of the_\n_2020 Conference on Empirical Methods in Natural_\n_Language Processing, EMNLP 2020, Online, Novem-_\n_ber 16-20, 2020_, pages 5418\u20135426. Association for\nComputational Linguistics.\n\n\n[Stephen E. Robertson and Hugo Zaragoza. 2009. The](https://doi.org/10.1561/1500000019)\n[probabilistic relevance framework: BM25 and be-](https://doi.org/10.1561/1500000019)\n[yond.](https://doi.org/10.1561/1500000019) _Found. Trends Inf. Retr._, 3(4):333\u2013389.\n\n\nDevendra Singh Sachan, Siva Reddy, William L. Hamil[ton, Chris Dyer, and Dani Yogatama. 2021. End-to-](https://proceedings.neurips.cc/paper/2021/hash/da3fde159d754a2555eaa198d2d105b2-Abstract.html)\n[end training of multi-document reader and retriever](https://proceedings.neurips.cc/paper/2021/hash/da3fde159d754a2555eaa198d2d105b2-Abstract.html)\n[for open-domain question answering. In](https://proceedings.neurips.cc/paper/2021/hash/da3fde159d754a2555eaa198d2d105b2-Abstract.html) _Advances_\n_in Neural Information Processing Systems 34: An-_\n_nual Conference on Neural Information Processing_\n_Systems 2021, NeurIPS 2021, December 6-14, 2021,_\n_virtual_, pages 25968\u201325981.\n\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\n[Cancedda, and Thomas Scialom", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_16650", "chunk_text": "\u201325981.\n\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\n[Cancedda, and Thomas Scialom. 2023. Toolformer:](http://arxiv.org/abs/2302.04761)\n[Language models can teach themselves to use tools.](http://arxiv.org/abs/2302.04761)\n\n\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\n[Wen-tau Yih. 2023. REPLUG: retrieval-augmented](https://doi.org/10.48550/arXiv.2301.12652)\n[black-box language models.](https://doi.org/10.48550/arXiv.2301.12652) _CoRR_, abs/2301.12652.\n\n\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming[Wei Chang. 2022. ASQA: factoid questions meet](https://aclanthology.org/2022.emnlp-main.566)\n[long-form answers. In](https://aclanthology.org/2022.emnlp-main.566) _Proceedings of the 2022 Con-_\n_ference on Empirical Methods in Natural Language_\n_Processing, EMNLP 2022, Abu Dhabi, United Arab_\n_Emirates, December 7-11, 2022_, pages 8273\u20138288.\nAssociation for Computational Linguistics.\n\n\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\n[Denny Zhou. 2022. Recitation-augmented language](https://doi.org/10.48550/arXiv.2210.01296)\n[models.](https://doi.org/10.48550/arXiv.2210.01296) _CoRR_, abs/2210.01296.\n\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u00e9li", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_17100", "chunk_text": " Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard\n[Grave, and Guillaume Lample. 2023. Llama: Open](https://doi.org/10.48550/arXiv.2302.13971)\n[and efficient foundation language models.](https://doi.org/10.48550/arXiv.2302.13971) _CoRR_,\nabs/2302.13971.\n\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022. [Interleav-](https://doi.org/10.48550/arXiv.2212.10509)\n[ing retrieval with chain-of-thought reasoning for](https://doi.org/10.48550/arXiv.2212.10509)\n[knowledge-intensive multi-step questions.](https://doi.org/10.48550/arXiv.2212.10509) _CoRR_,\nabs/2212.10509.\n\n\n[Neeraj Varshney, Man Luo, and Chitta Baral. 2022. Can](https://doi.org/10.48550/arXiv.2211.12707)\n\n[open-domain QA reader utilize external knowledge](https://doi.org/10.48550/arXiv.2211.12707)\n[efficiently like humans?](https://doi.org/10.48550/arXiv.2211.12707) _CoRR_, abs/2211.12707.\n\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\nLe, Ed H. Chi, and Denny Zhou. 2022. [Self-](https://doi.org/10.48550/arXiv.2203.11171)\n[consistency improves chain of thought reasoning in](https://doi.org/10.48550/arXiv.2203.11171)\n[language models.](https://doi.org/10.48550/arXiv.2203.11171) _CoRR_, abs/2203.11171.\n\n\nJason Wei, Xuez", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_17550", "chunk_text": "Xiv.2203.11171)\n[language models.](https://doi.org/10.48550/arXiv.2203.11171) _CoRR_, abs/2203.11171.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\n[Chain of thought prompting elicits reasoning in large](http://arxiv.org/abs/2201.11903)\n[language models.](http://arxiv.org/abs/2201.11903) _CoRR_, abs/2201.11903.\n\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\n[React: Synergizing reasoning and acting in language](https://doi.org/10.48550/arXiv.2210.03629)\n[models.](https://doi.org/10.48550/arXiv.2210.03629) _CoRR_, abs/2210.03629.\n\n\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2022. [Generate](https://doi.org/10.48550/arXiv.2209.10063)\n[rather than retrieve: Large language models are](https://doi.org/10.48550/arXiv.2209.10063)\n[strong context generators.](https://doi.org/10.48550/arXiv.2209.10063) _CoRR_, abs/2209.10063.\n\n\nWenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng\n[Jiang, and Ashish Sabharwal. 2023. Improving lan-](https://doi.org/10.48550/arXiv.2305.14002)\n[guage models via plug-and-play retrieval feedback.](https://doi.org/10.48550/arXiv.2305.14002)\n_CoRR_, abs/2305.14002.\n\n\nYury Zemlyanskiy, Michiel de Jong, Joshua Ainslie,\n", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_18000", "chunk_text": "https://doi.org/10.48550/arXiv.2305.14002)\n_CoRR_, abs/2305.14002.\n\n\nYury Zemlyanskiy, Michiel de Jong, Joshua Ainslie,\nPanupong Pasupat, Peter Shaw, Linlu Qiu, Sumit\n[Sanghai, and Fei Sha. 2022. Generate-and-retrieve:](https://aclanthology.org/2022.coling-1.438)\n[Use your predictions to improve retrieval for seman-](https://aclanthology.org/2022.coling-1.438)\n[tic parsing. In](https://aclanthology.org/2022.coling-1.438) _Proceedings of the 29th International_\n_Conference on Computational Linguistics, COLING_\n_2022, Gyeongju, Republic of Korea, October 12-17,_\n_2022_, pages 4946\u20134951. International Committee on\nComputational Linguistics.\n\n\n\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang\nZan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\n[2023. Repocoder: Repository-level code completion](https://doi.org/10.48550/arXiv.2303.12570)\n[through iterative retrieval and generation.](https://doi.org/10.48550/arXiv.2303.12570) _CoRR_,\nabs/2303.12570.\n\n\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open\npre-trained transformer language models. _ArXiv_,\nabs/2205.01068.\n\n\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhip", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_18450", "chunk_text": "yi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n[2023. A survey of large language models.](https://doi.org/10.48550/arXiv.2303.18223) _CoRR_,\nabs/2303.18223.\n\n\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022. [Towards a unified multi-](https://aclanthology.org/2022.emnlp-main.131)\n[dimensional evaluator for text generation. In](https://aclanthology.org/2022.emnlp-main.131) _Pro-_\n_ceedings of the 2022 Conference on Empirical Meth-_\n\n_ods in Natural Language Processing, EMNLP 2022,_\n_Abu Dhabi, United Arab Emirates, December 7-11,_\n_2022_, pages 2023\u20132038. Association for Computational Linguistics.\n\n\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,\nFrancisco Guzm\u00e1n, Luke Zettlemoyer, and Marjan\n[Ghazvininejad. 2021. Detecting hallucinated content](https://doi.org/10.18653/v1/2021.findings-acl.120)\n[in conditional neural sequence generation. In](https://doi.org/10.18653/v1/2021.findings-acl.120) _Find-_\n_ings of the Association for Computational Linguis-_\n_tics: ACL-IJCNLP 2021_, pages 1393\u20131404, Online.\nAssociation for Computational Linguistics.\n\n\n**A** **FLARE Implementation Details**\n\n\n**FLAREinstruct** **implementation** **details** We\nfound that LMs can effectively combine retrieval\nand downstream task-related skills and generate\nmeaningful search queries while performing the\ntask. However", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_18900", "chunk_text": "** **FLARE Implementation Details**\n\n\n**FLAREinstruct** **implementation** **details** We\nfound that LMs can effectively combine retrieval\nand downstream task-related skills and generate\nmeaningful search queries while performing the\ntask. However, there are two issues: (1) LMs tend\nto generate fewer search queries than necessary.\n(2) Generating excessive search queries can\ndisrupt answer generation and adversely affect\nperformance. We address these issues using two\nmethods respectively. First, we increase the logit\n\n- f the token \u201c[\u201d by 2.0 to improve the chances\n\n- f LMs generating \u201c[Search(query)]\u201d. Second,\nwhenever LMs generate a search query, we use it\nto retrieve relevant information, promptly remove\nit from the generation, and generate the next few\ntokens while forbidding \u201c[\u201d by adding a large\nnegative value to the logit of \u201c[\u201d.\n\n\n**The initial query of FLARE.** FLARE starts\nwith the user input _**x**_ as the initial query to retrieve documents to generate the first sentence\n_**s**_ \u02c61 = LM([ _D_ _**x**_ _,_ _**x**_ ]) to bootstrap the iterative generation process. For the following steps, the temporary forward-looking sentence is generated without\nretrieved documents.\n\n\n**Sentence tokenization.** For each step _t_, we generate 64 tokens which are longer than most sentences, and use NLTK sentence tokenizer [5] to extract the first sentence and discard the rest.\n\n\n**Efficiency** As shown in subsection 6.2, on average retrieval is triggered for 30% _\u223c_ 60% of sentences depending on downstream tasks. In comparision, KNN-LM (Khandelwal et al., 2020) retrieves\nevery token, RETRO or IC-RALM (Borgeaud et al.,\n2022; Ram et al., 2023) retrievers every 4 _\u223c_ 32 tokens, and IRCoT (Trivedi et al., 2022) retrieves\nevery sentence. Compared to single-time retrieval,\nhowever, interleaving retrieval and generation with\na naive implementation indeed increases overheads,\nwhich we discuss in the limitation section (section 9).\n\n\n**B** **Datasets and Settings**\n\n\nDatasets, metrics, and experimental settings are\nsummarized in Table", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_19350", "chunk_text": " retrieval and generation with\na naive implementation indeed increases overheads,\nwhich we discuss in the limitation section (section 9).\n\n\n**B** **Datasets and Settings**\n\n\nDatasets, metrics, and experimental settings are\nsummarized in Table 7.\n\n\n[5https://www.nltk.org/api/nltk.tokenize.](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html)\n[PunktSentenceTokenizer.html](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html)\n\n\n\n**Multihop QA** For \u201cWhy did the founder of Versus die?\u201d, the output we aim to generate is \u201cThe\nfounder of Versus was Gianni Versace. Gianni Ver\nsace was shot and killed on the steps of his Miami\nBeach mansion on July 15, 1997. So the answer\nis shot.\u201d We use 8 exemplars from Trivedi et al.\n(2022) listed in Prompt D.4 for in-context learning, BM25 as the retriever, and Wikipedia articles\nas the retrieval corpus. Similar to the observation\nin Trivedi et al. (2022), we found incorporating\nretrieval results for exemplars improves the performance, we use the input _**x**_ - f each exemplar to\nretrieve several documents and then add them using\nthe format in Prompt D.1. We found increasing the\nnumber of retrieval documents often increases performance. Therefore, we use the maximum number\n\n- f documents that can fit within the input length\nlimit of text-davinci-003, which is 2 for 2WikiMultihopQA.\n\n\n**Commonsense Reasoning** For \u201cWould a pear\nsink in water?\u201d, the output we aim to generate is\n\u201cThe density of a pear is about 0.6g/cm [3], which is\nless than water. Objects less dense than water float.\nThus, a pear would float. So the final answer is no.\u201d\nWe use 6 exemplars from Wei et al. (2022) listed in\nPrompt D.5, BM25 on the Wikipedia corpus, and 3\nretrieved documents to run experiments.\n\n\n**Long-form QA** For \u201cWhere do the Philadelphia\nEagles play their home games?\u201d, the output we\naim to generate is \u201cWe need to consider the different possible locations or venues that could be\nconsidered the", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_19800", "chunk_text": " run experiments.\n\n\n**Long-form QA** For \u201cWhere do the Philadelphia\nEagles play their home games?\u201d, the output we\naim to generate is \u201cWe need to consider the different possible locations or venues that could be\nconsidered the home field of the Philadelphia Eagles. These include the city, the sports complex,\n\n- r the stadium. Therefore, this question has 3 interpretations and the answers are: (1) The city is\nPhiladelphia. (2) The sports complex is the South\nPhiladelphia Sports Complex. (3) The stadium is\nthe Lincoln Financial Field stadium.\u201d For both the\n\n- riginal setting (ASQA) and the setting with hints\n(ASQA-hint), we manually annotate 8 exemplars\n(Prompt D.6 and D.8), use BM25 on the Wikipedia\ncorpus, and 3 retrieved documents to run experi\nments.\n\n\n**Open-domain Summarization** The original\nWikiAsp dataset is designed for multi-document\nsummarization and provides a list of references to\nsystems. We converted it into the open-domain\nsetting by removing the associated references and\ninstead gathering information from the open web.\nFor \u201cGenerate a summary about Echo School (Ore\n\ngon) including the following aspects: academics,\nhistory.\u201d, the output we aim to generate is \u201c# Academics. In 2008, 91% of the school\u2019s seniors received their high school diploma... # History. The\nclass of 2008 was the 100th class in the school\u2019s\n\nhistory.\u201d where # is used to indicate aspects. We\nmanually annotate 4 exemplars (Prompt D.10), and\nuse the Bing search engine to retrieve 5 documents\nfrom the open web. To avoid leaking, we exclude\nseveral Wikipedia-related domains listed in Table 8\nfrom Bing\u2019s search results.\n\n\n**C** **Hyperparameters**\n\n\nHyperparameters of FLARE on different datasets\nare listed in Table 9.\n\n\n**D** **Prompts and Few-shot exemplars**\n\n\nThe prompt used to linearize multiple documents\nis shown in Prompt D.1. The prompt used in selfask (Press et al., 2022) is shown in Prompt D.2.\nPrompts and exemplars of different tasks/datasets\nare shown in Prompt D.3, D.4, D.5, D.6, D.8, and\nD.10", "token_count": 500, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2305.06983_active_rag_jiang:chunk_20250", "chunk_text": "2) is shown in Prompt D.2.\nPrompts and exemplars of different tasks/datasets\nare shown in Prompt D.3, D.4, D.5, D.6, D.8, and\nD.10, respectively.\n\n\n\n\n\n\n**Settings** **2WikiMultihopQA** **StrategyQA** **ASQA** **WikiAsp**\n(Ho et al., 2020) (Geva et al., 2021) (Stelmakh et al., 2022) (Hayashi et al., 2021)\n\n\n_Dataset statistics_\nTask multihop QA commonsense QA long-form QA - pen-domain summarization\n#Examples 500 229 500 500\n\n\n_Evaluation settings_\nMetrics EM, F1, Prec., Rec. EM EM, Disambig-F1, ROUGE, DR UniEval, entity-F1, ROUGE\n\n\n_Retrieval settings_\nCorpus Wikipedia Wikipedia Wikipedia - pen web\nRetriever BM25 BM25 BM25 Bing\nTop-k 2 3 3 5\n\n\n_Prompt format_\n#Exemplars 8 6 8 4\nRet. for exemplars \u2713 \u2717 \u2717 \u2717\n\n\nTable 7: Dataset statistics and experimental settings of different tasks.\n\n\nwikipedia.org, wikiwand.com, wiki2.org, wikimedia.org\n\n\nTable 8: Wikipedia-related domains excluded from Bing\u2019s search results.\n\n\n**Dataset** _\u03b8_ _\u03b2_ **Query formulation** **Combine single- & multi-time retrieval**\n\n\n2WikiMultihopQA 0.8 0.4 implicit \u2717\nStrategyQA 0.4 0.4 implicit \u2717\nASQA & ASQA-hint 0.8 0.4 explicit \u2713\nWikiAsp 0.8 0.4 explicit \u2713\n\n\nTable 9: Hyperparameters of FLARE on different datasets.\n\n\n", "token_count": 397, "metadata": {"arxiv_id": "2305.06983", "title": "Active Retrieval Augmented Generation", "authors": ["Zhengbao Jiang", "Frank F. Xu", "Luyu Gao", "Zhiqing Sun", "Qian Liu", "Jane Dwivedi-Yu", "Yiming Yang", "Jamie Callan", "Graham Neubig"], "year": 2023, "url": "https://arxiv.org/pdf/2305.06983v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_0", "chunk_text": "## **Leveraging Passage Retrieval with Generative Models** **for Open Domain Question Answering**\n\n**Gautier Izacard** [1] _[,]_ [2] _[,]_ [3] **Edouard Grave** [1]\n\n1 Facebook AI Research, Paris\n2 ENS, PSL University, Paris\n3 Inria, Paris\ngizacard|egrave@fb.com\n\n\n**Abstract**\n\n\n\nGenerative models for open domain question\nanswering have proven to be competitive, with\n  - ut resorting to external knowledge. While\npromising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we\ninvestigate how much these models can benefit from retrieving text passages, potentially\ncontaining evidence. We obtain state-of-theart results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that\nsequence-to-sequence models offers a flexible\nframework to efficiently aggregate and combine evidence from multiple passages.\n\n\n**1** **Introduction**\n\n\n\n\n\n\n\n\n\n\n\nRecently, several works have shown that factual\ninformation can be extracted from large scale\nlanguage models trained on vast quantities of\ndata (Radford et al., 2019; Petroni et al., 2019;\nJiang et al., 2019; Talmor et al., 2019). Building\n\n- n that observation and the advances in pretraining of natural language processing models, Roberts\net al. (2020) introduced a generative model for open\ndomain question answering. Without relying on\nexternal knowledge, this method obtained competitive results on several benchmarks. However, it\nrequires models containing billions of parameters,\nsince all the information needs to be stored in the\n\nweights. This makes models expensive to query\nand train. In this paper, we investigate how much\nthis method could benefit from having access to an\nexternal source of knowledge, such as Wikipedia.\nRetrieval based approaches were previously considered in the context of open domain question\nanswering with extractive models (Chen et al.,\n2017). In that case, systems start by retrieving\n\n\n\nFigure 1: A simple approach to open domain question\nanswering. First, it retrieves support text passages from\nan external source of knowledge such as Wikipedia.\nThen, a generative", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_450", "chunk_text": "7). In that case, systems start by retrieving\n\n\n\nFigure 1: A simple approach to open domain question\nanswering. First, it retrieves support text passages from\nan external source of knowledge such as Wikipedia.\nThen, a generative encoder-decoder model produces\nthe answer, conditioned on the question and the retrieved passages. This approach scales well with the\nnumber of retrieved passages, as the performance keeps\nimproving when retrieving up to one hundred passages.\n\n\nsupport documents, before extracting the answer\nfrom these documents. Different retrieval tech\nniques have been considered, either using sparse\nrepresentations based on TF/IDF or using dense\nembeddings (Guu et al., 2020; Karpukhin et al.,\n2020). The models which extract the answers are\n\n- ften based on contextualized word representations\nsuch as ELMo or BERT (Peters et al., 2018; Devlin et al., 2019), and predict a span as answer.\nAggregating and combining evidence from multiple passages is not straightforward when using\nextractive models, and multiple techniques have\nbeen proposed to address this limitation (Clark and\nGardner, 2018; Min et al., 2019a).\nIn this paper, we explore a simple approach having the best of both worlds, by building on the\nexciting developments in generative modeling and\nretrieval for open domain question answering. This\nmethod proceeds in two steps, by first retrieving\nsupporting passages using either sparse or dense\n\n\n```\nQuestion + Passage 1 encoder\n\nQuestion + Passage 2 encoder\n\nQuestion + Passage N encoder\n\n```\n\n```\nconcat \u2026 decoder Answer\n\n```\n\n\nFigure 2: Architecture of the Fusion-in-Decoder method.\n\n\n\nrepresentations. Then, a sequence-to-sequence\nmodel generates the answer, taking as input the retrieved passages in addition to the question. While\nconceptually simple, this method sets new state-ofthe-art results on the TriviaQA and NaturalQuestions benchmarks. In particular, we show that the\nperformance of our method significantly improves\nwhen the number of retrieved passages increases.\nWe believe that this is evidence that generative models are good at combining evidence from multiple\npassages, compared to extractive ones.\n\n\n**2** **Related work**\n\n\n**Open domain question answering** is the task\n\n- f answering general domain questions, in which\nthe evidence is not", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_900", "chunk_text": " models are good at combining evidence from multiple\npassages, compared to extractive ones.\n\n\n**2** **Related work**\n\n\n**Open domain question answering** is the task\n\n- f answering general domain questions, in which\nthe evidence is not given as input to the system.\nWhile being a longstanding problem in natural language processing (Voorhees et al., 1999), this task\nhas recently regained interest following the work\nby Chen et al. (2017). In that version of the problem, strong supervision is available to the learning\nsystem, in the form of spans corresponding to answers. Chen et al. (2017) proposed to solve the\nproblem by first retrieving support document from\nWikipedia, before extracting the answer from the\nretrieved document. Different methods were proposed to tackle the setting where no gold spans are\ngiven to the system, but only the correct answer.\nClark and Gardner (2018) proposed to use a global\nnormalization over all the span corresponding to\nthe answer, which was later applied to BERT based\nmodels (Wang et al., 2019). Min et al. (2019a)\nintroduced a method based on hard expectationmaximization to tackle noisy supervision from this\nsetting. Wang et al. (2018b) described a technique\nto aggregate answers from different paragraphs,\nusing confidence and coverage scores.\n\n\n**Passage retrieval** is an important step in open\ndomain question answering, and is an active area of\nresearch to improve QA systems. Initially, sparse\nrepresentations based on TF/IDF were used to\nretrieve support documents (Chen et al., 2017).\nLee et al. (2018) introduced a supervised learning\n\n\n\nmethod to rerank paragraphs based on BiLSTM,\nwhile Wang et al. (2018a) trained a ranking system\nwith reinforcement learning. A second approach\nto improve the retrieval step of QA systems is to\nused additional information such as the Wikipedia\n\n- r Wikidata graphs (Min et al., 2019b; Asai et al.,\n2020). Recently, multiple works show that retrieval\nsystems entirely based on dense representation\nand approximate nearest neighbors were competitive with traditional approaches. Such models can\nbe trained using weak supervision in the form of\nquestion-answer pairs (Karpukhin et al., 2020), or\npretrained using a cloze task and finetuned end-to", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_1350", "chunk_text": " were competitive with traditional approaches. Such models can\nbe trained using weak supervision in the form of\nquestion-answer pairs (Karpukhin et al., 2020), or\npretrained using a cloze task and finetuned end-toend (Guu et al., 2020; Lee et al., 2019).\n\n\n**Generative question answering** was mostly\nconsidered in previous work for datasets requiring\nto generate answers, such as NarrativeQA (Kocisk\u02c7 y`\net al., 2018), CoQA (Reddy et al., 2019) or\nELI5 (Fan et al., 2019). These datasets were generated in a way that answers do not correspond\nto spans in support documents, thus requiring abstractive models. Raffel et al. (2019) showed that\ngenerative models are competitive for reading comprehension tasks such as SQuAD (Rajpurkar et al.,\n2016), where answers are spans. Roberts et al.\n(2020) proposed to use large pretrained generative\nmodels, without using additional knowledge, for\n\n- pen domain question answering. Closest to our\nwork, Min et al. (2020) and Lewis et al. (2020) introduced retrieval augmented generative models for\n\n- pen domain question answering. Our approach\ndiffers from these works by how the generative\nmodel processes the retrieved passages. This allows to scale to large numbers of documents, and\nto benefit from this large amount of evidence.\n\n\n**3** **Method**\n\n\nIn this section, we describe our approach to open\ndomain question answering. It proceeds in two\nsteps, first retrieving support passages before processing them with a sequence to sequence model.\n\n\nModel NQ TriviaQA SQuAD Open\n\nEM EM EM EM F1\n\n\nDrQA (Chen et al., 2017)   -   -   - 29.8   Multi-Passage BERT (Wang et al., 2019)   -   -   - 53.0 60.9\nPath Retriever (Asai et al., 2020) 31.7    -    - **56.5** **63.8**\nGraph Retriever (Min et al., 2019b) 34.7 55.8   -   -   Hard EM (Min et al., 2019a)", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_1800", "chunk_text": " - **56.5** **63.8**\nGraph Retriever (Min et al., 2019b) 34.7 55.8   -   -   Hard EM (Min et al., 2019a) 28.8 50.9   -   -   ORQA (Lee et al., 2019) 31.3 45.1   - 20.2   REALM (Guu et al., 2020) 40.4   -   -   -   DPR (Karpukhin et al., 2020) 41.5 57.9   - 36.7   SpanSeqGen (Min et al., 2020) 42.5   -   -   -   RAG (Lewis et al., 2020) 44.5 56.1 68.0   -   \n\nT5 (Roberts et al., 2020) 36.6   - 60.5   -   GPT-3 few shot (Brown et al., 2020) 29.9   - 71.2   -   \n\nFusion-in-Decoder (base) 48.2 65.0 77.1 53.4 60.6\nFusion-in-Decoder (large) **51.4** **67.6** **80.1** **56.7** 63.2\n\n\nTable 1: Comparison to state-of-the-art. On TriviaQA, we report results on the open domain test set (left), and on\nthe hidden test set (right), competitions.codalab.org/competitions/17208#results).\n\n\n\n**Retrieval.** For the retrieval of support passages,\nwe consider two methods: BM25 (Robertson et al.,\n1995) and DPR (Karpukhin et al., 2020). In BM25,\npassages are represented as bag of words, and the\nranking function is based on term and inverse document frequencies. We use the implementation\nfrom Apache Lucene [1] with default parameters, and\ntokenize questions and passages with SpaCy. [2] In\nDPR, passages and questions are represented as\ndense vector representations, computed using two\nBERT networks. The ranking function is the dot\nproduct between the query and passage representations. Retrieval is performed using approximate", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_2250", "chunk_text": " SpaCy. [2] In\nDPR, passages and questions are represented as\ndense vector representations, computed using two\nBERT networks. The ranking function is the dot\nproduct between the query and passage representations. Retrieval is performed using approximate\nnearest neighbors with the FAISS library. [3]\n\n\n**Reading.** Our generative model for open domain\nQA is based on a sequence-to-sequence network,\npretrained on unsupervised data, such as T5 or\nBART (Raffel et al., 2019; Lewis et al., 2019). The\nmodel takes as input the question, as well as the\nsupport passages, and generates the answer. More\nprecisely, each retrieved passage and its title are\nconcatenated with the question, and processed independently from other passages by the encoder.\nWe add special tokens question:, title: and\ncontext: before the question, title and text of\neach passage. Finally, the decoder performs atten\n\n1lucene.apache.org\n2spacy.io\n3github.com/facebookresearch/faiss\n\n\n\ntion over the concatenation of the resulting representations of all the retrieved passages. The model\nthus performs evidence fusion in the decoder only,\nand we refer to it as _Fusion-in-Decoder_ .\n\nBy processing passages independently in the encoder, but jointly in the decoder, this method differs from Min et al. (2020) and Lewis et al. (2020).\nProcessing passages independently in the encoder\nallows to scale to large number of contexts, as it\n\n- nly performs self attention over one context at a\ntime. This means that the computation time of the\nmodel grows linearly with the number of passages,\ninstead of quadratically. On the other hand, processing passages jointly in the decoder allows to\nbetter aggregate evidence from multiple passages.\n\n\n**4** **Experiments**\n\n\nIn this section, we report empirical evaluations of\nFusion-in-Decoder for open domain QA.\n\n\n**Datasets.** We consider the following datasets,\nand use the same setting as Lee et al. (2019):\n\n\n  - NaturalQuestions (Kwiatkowski et al., 2019)\ncontains questions corresponding to Google\nsearch queries. The open-domain version of\nthis dataset is obtained by discarding answers\nwith more than 5 tokens.\n\n\n  - TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_2700", "chunk_text": " corresponding to Google\nsearch queries. The open-domain version of\nthis dataset is obtained by discarding answers\nwith more than 5 tokens.\n\n\n  - TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and quiz-league\n\n\nFigure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of retrieved passages.\n\n\n\nwebsites. The _unfiltered_ version of TriviaQA\nis used for open-domain question answering.\n\n\n  - SQuAD v1.1 (Rajpurkar et al., 2016) is a reading comprehension dataset. Given a paragraph\nextracted from Wikipedia, annotators were\nasked to write questions, for which the answer\nis a span from the corresponding paragraph.\n\n\nFollowing Lee et al. (2019) we use the validation as\ntest, and keep 10% of the training set for validation.\nWe use the Wikipedia dumps from Dec. 20, 2018\nfor NQ and TriviaQA and from Dec. 21, 2016 for\nSQuAD. We apply the same preprocessing as Chen\net al. (2017); Karpukhin et al. (2020), leading to\npassages of 100 words, which do not overlap.\n\n\n**Evaluation.** Predicted answers are evaluated\n\nwith the standard exact match metric (EM), as introduced by Rajpurkar et al. (2016). A generated\nanswer is considered correct if it matches any answer of the list of acceptable answers after normalization. This normalization step consists in lowercasing and removing articles, punctuation and\nduplicated whitespace.\n\n\n**Technical details.** We initialize our models with\n\nthe pretrained T5 models (Raffel et al., 2019), available in the HuggingFace Transformers library. [4] We\nconsider two model sizes, base and large, containing respectively 220M and 770M parameters. We\nfine-tune the models on each dataset independently,\nusing Adam (Kingma and Ba, 2014) with a constant learning rate of 10 _[\u2212]_ [4] and a dropout rate of\n10%. We train the model for 10k gradient steps,\nwith a batch size of 64, using 64 Tesla V100 32Gb.\nWe evaluate models every 500 steps and select the\nbest one on the validation set based on the Exact\n\n", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_3150", "chunk_text": " train the model for 10k gradient steps,\nwith a batch size of 64, using 64 Tesla V100 32Gb.\nWe evaluate models every 500 steps and select the\nbest one on the validation set based on the Exact\n\nMatch score. During training on NaturalQuestions\n\n\n4github.com/huggingface/transformers\n\n\n\nand SQuAD, we sample the target among the list\n\n- f answers, while for TriviaQA, we use the unique\nhuman-generated answer. For TriviaQA, answers\nin uppercase are normalized by converting all letters in lowercase except the first letter of each word,\nusing the title Python string method. For both\ntraining and testing, we retrieve 100 passages (unless said otherwise), and truncate them to 250 word\npieces. Following the results of Karpukhin et al.\n(2020), passages are retrieved with DPR for NQ\nand TriviaQA, and with BM25 for SQuAD. We\ngenerate answers by using greedy decoding.\n\n\n**Comparison to state-of-the-art.** In table 1, we\ncompare the results obtained by Fusion-in-Decoder\nwith existing approaches for open domain question answering. We observe that while conceptually simple, this method outperforms existing work\n\n- n the NaturalQuestion and TriviaQA benchmarks.\nIn particular, generative models seem to perform\nwell when evidence from multiple passages need to\nbe aggregated, compared to extractive approaches.\nOur method also performs better than other generative models, showing that scaling to large number\n\n- f passages and processing them jointly leads to\nimprovement in accuracy. Second, we observe that\nusing additional knowledge in generative models\nby using retrieval lead to important performance\ngains. On NaturalQuestions, the _closed book_ T5\nmodel obtains 36.6% accuracy with 11B parameters, while our approach obtains 44.1% with 770M\nparameters plus Wikipedia with BM25 retrieval.\nBoth methods use roughly the same amount of\nmemory to store information, indicating that text\nbased explicit memories are competitive for knowledge retrieval tasks.\n\n\n**Scaling with number of passages.** In Figure 3,\nwe report the performance with respect to the\n\n\nNaturalQuestions TriviaQA\nTraining Passages w/o finetuning w/ finetuning w/o finetuning w/ finetuning\n\n\n5 37.8 45.0 58.1 64.", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_3600", "chunk_text": " respect to the\n\n\nNaturalQuestions TriviaQA\nTraining Passages w/o finetuning w/ finetuning w/o finetuning w/ finetuning\n\n\n5 37.8 45.0 58.1 64.2\n\n10 42.3 45.3 61.1 63.6\n\n25 45.3 46.0 63.2 64.2\n\n50 45.7 46.0 64.2 64.3\n\n100 46.5        - 64.7        \n\nTable 2: Performance depending on the number of passages used during training. Exact Match scores are reported\n\n- n dev sets.\n\n\n\nnumber of retrieved passages. In particular, we\n\n- bserve that increasing the number of passages\nfrom 10 to 100 leads to 6% improvement on TriviaQA and 3.5% improvement on NaturalQuestions.\nOn the other hand, the performance of most extractive models seems to peak around 10 to 20\npassages (Wang et al., 2019; Yang et al., 2019).\nWe believe that this is evidence that sequence-tosequence models are good at combining informations from multiple passages.\n\n\n**Impact of the number of training passages.** In\nthe previous section, the model was trained and\nevaluated with the same number of passages. To\nreduce the training computational budget, a simple\nsolution consists in training the model with fewer\npassages. In Table 2, we report the performance\n\n- btained by training with different numbers of passages, while testing with 100 passages. We observe\nthat reducing the number of training passages leads\nto a decrease of accuracy. Further, we propose to\nfinetune the previous models using 100 passages\nfor 1000 steps. This allows to reduce the accuracy\ngap, while using significantly less computational\nresources: we can reach 46.0 EM on NaturalQuestions, using 147 GPU hours, compared to 425 GPU\nhours when training on 100 passages.\n\n\n**5** **Conclusion**\n\n\nIn this paper, we study a simple approach to open\ndomain question answering, which relies on retrieving support passages before processing them with a\ngenerative model. We show that while conceptually\nsimple, this approach is competitive with existing\nmethods, and that it scales well with the number\n\n- f retrieved passages. In future work", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_4050", "chunk_text": " relies on retrieving support passages before processing them with a\ngenerative model. We show that while conceptually\nsimple, this approach is competitive with existing\nmethods, and that it scales well with the number\n\n- f retrieved passages. In future work, we plan to\nmake this model more efficient, in particular when\nscaling to large number of support passages. We\nalso plan to integrate the retrieval in our model, and\nto learn the whole system end-to-end.\n\n\n\n**References**\n\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over wikipedia graph\nfor question answering. In _Proc. ICLR_ .\n\n\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. _arXiv preprint arXiv:2005.14165_ .\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer opendomain questions. In _Proc. ACL_ .\n\n\nChristopher Clark and Matt Gardner. 2018. Simple\nand effective multi-paragraph reading comprehension. In _Proc. ACL_ .\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In _Proc. NAACL_ .\n\n\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In _Proc. ACL_ .\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Realm: Retrievalaugmented language model pre-training. _arXiv_\n_preprint arXiv:2002.08909_ .\n\n\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2019. How can we know what language\nmodels know? _", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_4500", "chunk_text": "_\n_preprint arXiv:2002.08909_ .\n\n\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2019. How can we know what language\nmodels know? _arXiv preprint arXiv:1911.12543_ .\n\n\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. In _Proc. ACL_ .\n\n\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Ledell\nWu, Sergey Edunov, Danqi Chen, and Wentau Yih. 2020. Dense passage retrieval for\n\n - pen-domain question answering. _arXiv preprint_\n_arXiv:2004.04906_ .\n\n\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. _arXiv preprint_\n_arXiv:1412.6980_ .\n\n\nTom\u00b4a\u02c7s Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, G\u00b4abor Melis, and\nEdward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. _TACL_ .\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a benchmark for question answering\nresearch. _TACL_ .\n\n\nJinhyuk Lee, Seongjun Yun, Hyunjae Kim, Miyoung\nKo, and Jaewoo Kang. 2018. Ranking paragraphs\nfor improving answer recall in open-domain question answering. In _Proc. EMNLP_ .\n\n\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_4950", "chunk_text": " answer recall in open-domain question answering. In _Proc. EMNLP_ .\n\n\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In _Proc. ACL_ .\n\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer.\n2019. BART: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. _arXiv_ _preprint_\n_arXiv:1910.13461_ .\n\n\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim\nRockt\u00a8aschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. _arXiv_\n_preprint arXiv:2005.11401_ .\n\n\nSewon Min, Danqi Chen, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2019a. A discrete hard EM approach for weakly supervised question answering.\nIn _Proc. EMNLP-IJCNLP_ .\n\n\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. Knowledge guided text retrieval and reading for open domain question answering. _arXiv preprint arXiv:1911.03868_ .\n\n\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. Ambigqa: Answering\nambiguous open-domain questions. _arXiv preprint_\n_arXiv:2004.10645_ .\n\n\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word representations. In _Proc. NAACL_ .\n\n\nFabio Petroni, Tim Rockt\u00a8aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_5400", "chunk_text": ". 2018. Deep contextualized word representations. In _Proc. NAACL_ .\n\n\nFabio Petroni, Tim Rockt\u00a8aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowledge bases? In _Proc. EMNLP-IJCNLP_ .\n\n\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. _OpenAI_\n_Technical Report_ .\n\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\n\n  - f transfer learning with a unified text-to-text transformer. _arXiv preprint arXiv:1910.10683_ .\n\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In _Proc. EMNLP_ .\n\n\nSiva Reddy, Danqi Chen, and Christopher D Manning.\n2019. CoQA: A conversational question answering\nchallenge. _TACL_ .\n\n\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parameters of a language model? _arXiv preprint_\n_arXiv:2002.08910_ .\n\n\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at TREC-3. _NIST Special Publication_\n_Sp_ .\n\n\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019.  - LMpics\u2013on what language model pre-training captures. _arXiv preprint_\n_arXiv:1912.13283_ .\n\n\nEllen M Voorhees et al. 1999. The TREC-8 question\nanswering track report. In _TREC_ .\n\n\nShuoh", "token_count": 500, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2007.01282_fid_izacard:chunk_5850", "chunk_text": "print_\n_arXiv:1912.13283_ .\n\n\nEllen M Voorhees et al. 1999. The TREC-8 question\nanswering track report. In _TREC_ .\n\n\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang,\nTim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018a. R [3] :\nReinforced ranker-reader for open-domain question\nanswering. In _Proc. AAAI_ .\n\n\nShuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger,\nGerald Tesauro, and Murray Campbell. 2018b. Evidence aggregation for answer re-ranking in opendomain question answering. In _Proc. ICLR_ .\n\n\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. 2019. Multi-passage BERT: A\nglobally normalized BERT model for open-domain\nquestion answering. In _Proc. EMNLP-IJCNLP_ .\n\n\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nBERTserini. In _Proc. NAACL (Demonstrations)_ .\n\n\n", "token_count": 327, "metadata": {"arxiv_id": "2007.01282", "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "authors": ["Gautier Izacard", "Edouard Grave"], "year": 2020, "url": "https://arxiv.org/pdf/2007.01282v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_0", "chunk_text": "## **A Survey on Retrieval-Augmented Text Generation**\n\n**Huayang Li** _[\u2665][,][\u2217]_ **Yixuan Su** _[\u2660][,][\u2217]_ **Deng Cai** _[\u2666][,][\u2217]_ **Yan Wang** _[\u2663][,][\u2217]_ **Lemao Liu** _[\u2663][,][\u2217]_\n\n_\u2665_ Nara Institute of Science and Technology _\u2660_ University of Cambridge\n_\u2666_ The Chinese University of Hong Kong _\u2663_ Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\n\n\n\n**Abstract**\n\n\nRecently, retrieval-augmented text generation\nattracted increasing attention of the computational linguistics community. Compared\nwith conventional generation models, retrievalaugmented text generation has remarkable advantages and particularly has achieved state-ofthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrievalaugmented text generation. It firstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks including\ndialogue response generation, machine translation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval technology, has achieved state-of-the-art (SOTA) performance in many NLP tasks and attracted the attention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counterpart, this new paradigm has some remarkable advantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_450", "chunk_text": " from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (Weston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n\n\n_\u2217_ All authors contributed equally.\n\n\n\nfirstly present the generic paradigm of retrievalaugmented generation as well as three key components under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are organized with respect to different tasks. Specifically,\n\n- n the dialogue response generation task, exemplar/template retrieval as an intermediate step has\nbeen shown beneficial to informative response generation (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation exploring different forms of knowledge such as knowledge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summarize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we intensively highlight several popular methods to integrating translation memory to NMT models (Gu\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\nHe et al., 2021). We also review the applications\n\n- f retrieval-augmented generation in other generation tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b).", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_900", "chunk_text": " generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n\n\n**2** **Retrieval-Augmented Paradigm**\n\n\nIn this section, we first give a general formulation\n\n- f retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrievalaugmented generation paradigm, including the re\n\nOutput\n\n\n\n\n\nFigure 1: The overview of this survey.\n\n\n\ntrieval source, retrieval metric and integration meth\n- ds.\n\n\n**2.1** **Formulation**\n\n\nMost text generation tasks can be formulated as a\nmapping from input sequence _**x**_ to output sequence\n_**y**_ : _**y**_ = _f_ ( _**x**_ ). For instance, _**x**_ and _**y**_ could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\n\n\n_**y**_ = _f_ ( _**x**_ _,_ _**z**_ ) (1)\n\n\nwhere _**z**_ = _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that _**y**_ _[r]_\n\nmay benefit the response generation, if _**x**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_1350", "chunk_text": "**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r]_ = _\u2205_ when unsupervised retrieval\nsources are used. In general, the retrieval mem\n- ry can be retrieved from three kinds of sources:\nthe training corpus, external datasets in the same\nformat with the training corpus, and large-scale\nunsupervised corpus (\u00a72.2). Metrics that evaluate\nthe relevance between text are varied as well, in\n\u00a72.3 we divided them into three categories: sparsevector retrieval, dense-vector retrieval, and trainingbased retrieval. Finally, how to integrate the retrieval memory to the generation model is also significant, we also introduce some popular integration approaches in \u00a72.4.\n\n\n\n**2.2** **Retrieval Sources**\n\n\n**Training Corpus** Most previous studies search\nthe external memory from its _training corpus_ (Song\net al., 2016; Gu et al., 2018; Weston et al., 2018).\nIn the inference time, retrieved examples with high\nrelevant scores could be regarded as extra references and reduce model\u2019s uncertainty in generation.\nThe main motivation of those works is to to store\n\nknowledge not only in the model parameters but\nalso in an explicit and accessible form, making the\nmodel be able to re-access it during inference.\n\n\n**External Data** Some researchers also propose to\nretrieval relevant samples from _external datasets_\n(Su et al., 2021c; Xiao et al., 2021). In these studies, the retrieval pool is different with the training\ncorpus, which can further provide additional information that are not contained in the training corpus.\nThis is especially beneficial for applications such\nas domain adaptation and knowledge update. For\nexample, Khandelwal et al. (2020a); Zheng et al.\n(2021a) employ the in-domain dataset as the external memory to achieve fast domain adaptation for\nmachine translation.\n\n\n**Unsupervised Data** One limitation for previous\ntwo sources is that the datasets have to be supervised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) propose a cross-lingual retriever to directly retrieve target sentence from _unsupervised corpus_ (i.e", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_1800", "chunk_text": " datasets have to be supervised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) propose a cross-lingual retriever to directly retrieve target sentence from _unsupervised corpus_ (i.e., monolingual corpus in the target language). The main\nidea is aligning source-side sentences and the corresponding target-side translations in a dense vector\nspace, i.e., aligning _**x**_ and _**y**_ _[r]_ when _**x**_ _[r]_ is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side translations, enabling monolingual data in the target\n\n\nlanguage to be used alone as memories.\n\n\n**2.3** **Retrieval Metrics**\n\n\n**Sparse-vector Retrieval** Given an input sequence _**x**_ and a retrieval corpus, retrieval model\naims to retrieve a set of relevant examples _**z**_ =\n_{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ from the corpus. When a supervised\ncorpus is used, _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ is retrieved by measuring the similarity between _**x**_ and _**x**_ _[r]_ . For similarity measurement, _sparse-vector retrieval_ meth\n- ds such as TF-IDF and BM25 (Robertson and\nZaragoza, 2009) are widely used. They match keywords efficiently with an inverted index.\n\n\n**Dense-vector Retrieval** However, these meth\n- ds prefer examples with similar surfaces, and may\nfail to retrieve examples that are only semantically\nrelevant. To alleviate above problem, some studies (Cao and Xiong, 2018) attempt to retrieve in\n_dense-vector space_ instead of the lexical overlap.\nRecent work (Lee et al., 2019) makes use of pretrained language models, which encodes the text to\nlow-dimensional dense vectors via BERT-based en\ncoders. The retrieval score are computed via inner\nproducts between vectors.\n\n\n**Task-specific** **Retrieval** Similarity-based retrieval is based on a simple heuristic. That is, the\nmore _**x**_", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_2250", "chunk_text": "ERT-based en\ncoders. The retrieval score are computed via inner\nproducts between vectors.\n\n\n**Task-specific** **Retrieval** Similarity-based retrieval is based on a simple heuristic. That is, the\nmore _**x**_ _[r]_ resembles with _**x**_, the more likely _**x**_ _[r]_\n\nand _**y**_ _[r]_ will help the generation. However, the\nmost similar one by universal textual similarity\ndoes not necessarily serve the best for downstream\nmodels. Ideally, the retrieval metric would be\nlearned from the data in a task-dependent way: we\nwish to consider a memory only if it can indeed\nboost the quality of final generation. To this end,\nCai et al. (2021) propose to unify the memory\nretriever and its downstream generation model\ninto a learnable whole. Such memory retrieval is\nend-to-end optimized for _task-specific_ - bjectives.\n\n\n**2.4** **Integration**\n\n\n**Data Augmentation** There are several ways to\nintegrate the retrieved external memory in generation. One straightforward way is _data augmen-_\n_tation_, which constructs some augmented inputs\nby concatenating spans from _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ with the\n\n- riginal input _**x**_ . By training on the augmented\ninputs, a generation model implicitly leans how\nto integrate the retrieved information. Despite the\nsimplicity, this kind of methods works efficiently\nin lots of tasks (Song et al., 2016; Weston et al.,\n2018; Bulte and Tezcan, 2019).\n\n\n\n**Attention** **Mechanisms** Another integration\nmethod is based - n _attention_ _mechanisms_\n\n(Bahdanau et al., 2014). The main idea of this\nfashion is adopting additional encoders (in various\narchitectures) to encode retrieved target sentences,\nand integrate them through attention (Cao and\nXiong, 2018; Gu et al., 2018; Bapna and Firat,\n2019). Since the attention mechanism is becoming\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\nkey module in lots of", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_2700", "chunk_text": "., 2018; Bapna and Firat,\n2019). Since the attention mechanism is becoming\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\nkey module in lots of NLP models, integrating\nretrieved memory through attention becomes a\nvery nature and efficient way.\n\n\n**Skeleton Extraction** In the previous two meth\n- ds, the downstream generation model learns how\nto filter out irrelevant or even harmful information from the retrieved examples implicitly. There\nalso exist some works that try to explicitly extract\nuseful information, i.e., _skeleton extraction_, from\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\n2019; Cai et al., 2019b). For example, one skeleton\nshould be a part of a whole utterance with irrelevant\ncontent masked, and the generation model only integrate this skeleton in the generation process.\n\n\n**3** **Dialogue Response Generation**\n\n\n**Background** Dialogue systems can be grouped\ninto two categories: chit-chat systems and task\n- riented systems. While task-oriented dialogue\nsystems are designed to accomplish specific user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and fluent response for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity\n\n- f possible responses to a single dialogue history\n(i.e., the _one-to-many_ problem). The dialogue history alone cannot decide a meaningful and specific\nresponse. Also, external knowledge that is not\npresent in the dialogue history are often necessary\nfor avoiding safe but boring responses. We focus\n\n- n recent efforts tackling the challenges to develop\nchit-chat dialogue systems.\nMost modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrievalbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from curated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa\n\ntions and possibly post-edited by a human. However, such systems perform poorly when a given\ndialogue history is substantially different from\nthose in the retrieval pool. On the", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_3150", "chunk_text": " grammatical\nas they are collected from real-world conversa\n\ntions and possibly post-edited by a human. However, such systems perform poorly when a given\ndialogue history is substantially different from\nthose in the retrieval pool. On the other hand,\nthe generation-based models (Shang et al., 2015;\nVinyals and Le, 2015; Li et al., 2016a) generate\na new utterance from scratch. Those generationbased models have better generalization capacity\nwhen handling unseen dialogue contexts. Nevertheless, the generated utterances are inclined to be\ndull and non-informative (e.g., \u201cI don\u2019t know\u201d, \u201cI\nthink so\u201d, \u201cMe too\u201d etc.) (Li et al., 2016a).\n\n\n**Shallow Integration** As discussed, retrievalbased models may give informative but inappropriate responses while generation-based models\n\n- ften do the opposite. It is desirable to combine the\nbest of both worlds. Early work (Qiu et al., 2017)\nattempts to re-rank the output from both models.\nFor a deep integration, Song et al. (2016) and Yang\net al. (2019) extend the standard SEQ2SEQ encoderdecoder model (Bahdanau et al., 2014) with an extra encoder for encoding the retrieval result. The\n\n- utput of the extra encoder, along with the output\nfrom the original encoder for dialogue history, is\nused to feed the decoder. Weston et al. (2018) use\na single encoder that takes the concatenation of\nthe original dialogue history and the retrieved as\ninput. Wu et al. (2019) note that the retrieved information should be used in awareness of the context\n\ndifference, and further proposed to construct an\nedit vector by explicitly encoding the lexical differences between the input dialogue history and the\nretrieved dialogue history. Pandey et al. (2018) further propose to weight different training instances\nby context similarity.\n\n\n**Deep Integration** To prevent the inflow of erroneous information, Cai et al. (2019a) propose\na general framework that first extracts a skeleton\nfrom the retrieved response and then generates the\nresponse based on the extracted skeleton. This\nframework is also adopted for stylistic response\ngeneration (Su et al., 2021c). Gupta et al. (2021)\nsuggest to use the semantic", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_3600", "chunk_text": " the retrieved response and then generates the\nresponse based on the extracted skeleton. This\nframework is also adopted for stylistic response\ngeneration (Su et al., 2021c). Gupta et al. (2021)\nsuggest to use the semantic structure of an exemplar response, instead of the tokens of the exemplar response, to guide generation. Despite their\ndifferences, a common issue is that the generation model easily learns to ignore the retrieved response entirely and collapses to a vanilla seq2seq\nmodel. This happens with improper training instances. Due to the one-to-many nature, it hap\n\n\npens frequently that a retrieved response (extracted\nskeleton) is suitable for responding to the query,\nbut inconsistent with the current target response.\n\nEarlier studies (Weston et al., 2018; Wu et al.,\n2019; Cai et al., 2019a) alleviate the above problems by putting hard constraints on the data (e.g.,\ndiscarding data with low similarity of the retrieved\nresponse and the target response), which, however,\ngreatly reduces the amount of usable data. Cai\net al. (2019b) employ a random mechanism for\ngenerating the skeletons used for training, which\nextract skeletons from the corresponding responses\nwith some deliberate disturbance. Paranjape et al.\n(2021) propose to model the retriever after the posterior distribution of retrieval given the input and\nthe target output and train it jointly with the standard retriever and the generator by maximizing the\nevidence lower bound (ELBo) in expectation over\nretrieval.\n\n\n**Knowledge-Enhanced Generation** The afore\nmentioned work demonstrates that retrieval-based\n\ndialogue systems can be used for building better generation-based models. In general, this is\ndone by conditioning the generation on some retrieved responses. More traditionally, to infuse\nthe response with external knowledge, the retrieval\npool is not necessarily a dialogue corpus. In fact,\nknowledge-grounded dialogue response generation\nexploring different forms of knowledge such as\nknowledge bases and external documents (Dinan\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\n\n\n**Limitations", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_4050", "chunk_text": " al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\n\n\n**Limitations** We note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneficial to combine multiple retrieval responses.\nHowever, this can be difficult due to the one-tomany nature of dialogue response generation. Sec\n- nd, current methods use universal relevance score\n\nfor retrieval. It can be more effective if we can\n\nuse more customized retrieval metric especially\nfor controlled dialogue response generation (e.g.,\npersona, emotion, etc). Third, the retrieval pool\n\n- f existing methods is limited to dialogue corpora\n(context-response pairs) or documents. It might\nbe useful to enlarge the retrieval pool by including\nmore corpora in other domains or in other modali\n\nties. As discussed, there leaves plenty of possible\ndirections to explore in the future.\n\n\n**4** **Machine Translation**\n\n\nRetrieval augmented translation originates from human translation scenarios (Somers, 2003). When\ntranslating \u02c6 _**y**_ from an input source sentence _**x**_, a human translator typically involves a search engine to\nretrieve similar sentences _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ from a bilingual database. Such a technique called **translation**\n**memory** is helpful to improve the translation quality and efficiency for human translators (Dillon\nand Fraser, 2006). As the development of machine translation techniques, there is a surge of\ninterests in improving machine translation models\nwith translation memory. In the rest of this section,\nwe will review translation memory for both statistical machine translation (SMT) and neural machine\ntranslation (NMT).\n\n\n**4.1** **Translation Memory in SMT**\n\n\nGenerally, SMT includes three key components in\na pipeline manner such as phrase table extraction,\nparameter tuning and decoding (Koehn et al., 2003;\nChiang, 2007). As a result, many efforts have been\nmade to make use of translation memory (TM) on\ntop of each component.\n\n\n**Constrained", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_4500", "chunk_text": " and decoding (Koehn et al., 2003;\nChiang, 2007). As a result, many efforts have been\nmade to make use of translation memory (TM) on\ntop of each component.\n\n\n**Constrained Decoding with TM** Constrained\ndecoding is the most straightforward way to integrating TM into SMT (Smith and Clark, 2009;\nKoehn and Senellart, 2010; Zhechev and Van Genabith, 2010; Ma et al., 2011). Its basic idea is\nto reuse the useful segments in _**y**_ _[r]_ while translate other segments by SMT. Specifically, the approach consists of three steps: 1) identify the unmatched segments in both _**x**_ _[r]_ and _**x**_ through the\nedit-distance algorithm; 2) identify the unmatched\nsegments in _**y**_ _[r]_, each of which is aligned to one\nunmatched segment in _**x**_ _[r]_ by a word alignment\nalgorithm; 3) decode each unmatched segment in\n_**x**_ by SMT and then use the result to replace its\ncorresponding unmatched segment in _**y**_ _[r]_ . Li et al.\n(2016b) further extend this approach from sentence\nlevel to phrase level. The advantage in constrained\ndecoding is that it does not require to change the\ntranslation model (including phrase table and parameters) and can be applied in a plug-and-play\nway. This approach is successful when _**x**_ is highly\nsimilar to _**x**_ _[r]_ ; otherwise its performance is degraded largely, because it explicitly isolates TM\n\n\n\nmatching and SMT decoding and reuses the results\nin _**x**_ _[r]_ - r not in a deterministic way.\n\n\n**Phrase Table Aggregation with TM** There are\nalso notable efforts to augment the phrase table\nfor SMT by extracting translation rules from the\nretrieved bilingual sentences _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ . Then\nthey re-tune the parameters for the SMT model\nwhich makes use of translation knowledge from\n_{\u27e8_ _**x", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_4950", "chunk_text": " _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ . Then\nthey re-tune the parameters for the SMT model\nwhich makes use of translation knowledge from\n_{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ in a implicit way when translating _**x**_ .\nFor example, Bi\u00e7ici and Dymetman (2008); Simard\nand Isabelle (2009) directly combine the extracted\ntranslation rules into the phrase table in a shallow\ncombination way. They introduce an additional feature to indicate that whether translation rule is from\n_{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ - r not and then train all feature weights\nwith MERT (Och, 2003). One characteristic of\n\nthese work is that a translation rule extracted from\n_{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ which can not exactly match any segments in _**x**_ is useless even if it may contain some\nuseful words in its target side. To remedy this observation, Wang et al. (2013, 2014) resort to a deep\ncombination way to using the extracted translation\nrules. For each rule in the phrase table, it designs\na generative model to reward the rules which are\nsimilar to those extracted from _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ . Then\nthis generative model is used as a feature in the loglinear based SMT model whose weight is tuned\ntogether with other features by MERT. In addition,\nLi et al. (2014) employ a similar way to reward\nthe rules but it relies on a discriminative model\n\nwhich is easy to integrate potential features from\n_{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ .\n\n\n**Parameter Tuning with TM** Unlike the above\ntwo research lines, Liu et al. (2012, 2014) make use\n\n- f translation memory only in tuning parameters.\nTo be specific, when", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_5400", "chunk_text": "_ _\u27e9}_ .\n\n\n**Parameter Tuning with TM** Unlike the above\ntwo research lines, Liu et al. (2012, 2014) make use\n\n- f translation memory only in tuning parameters.\nTo be specific, when translating an input sentence\n_**x**_, they firstly retrieve many similar bilingual sentences _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_, and then tune the parameters on\ntop of the retrieved sentences as well as a given development dataset in a sentence-wise manner, i.e.,\nit performs an independent tuning for each input\nsentence. To improve the efficiency of each tuning\nstep, it propose a local update on top of _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_\nfrom a baseline model.\n\nDespite the successes of translation memory in\nSMT, there are still some limitations for the above\nthree kinds of methods. Firstly, all these methods\nemploy fuzzy score for retrieval which is highly dependent on word matching and thus can not recall\nsuch examples which are similar in word seman\n\ntics but different in surface form. Secondly, these\nmethods integrate the retrieved examples into a\nmodule of SMT in the ways which can not make\nfull use of the knowledge in retrieved examples.\nFor example, the integration ways in the first two\nkinds (constrained decoding and phrase table aggregation) are heuristic and not optimized towards\ntranslation quality; the parameter tuning method\nfine-tunes few parameters for log-linear based SMT\nwhich are not enough to preserve sufficient knowledge from retrieved examples. Thirdly, since SMT\nperforms in a pipeline manner, it is intractable to\njointly optimize retrieval metrics as well as SMT\nmodels. Consequently, all these methods adopt an\n\n- ff-the-shelf metric for retrieval, leading to sub\n- ptimal performance.\n\n\n**4.2** **Translation Memory in NMT**\n\n\nTranslation memory has been widely explored in\nNeural Machine Translation (NMT). Depending\n\n- n when retrieval is involved, we can categorize\nprevious works into two classes: 1) an NMT model\nleans how to cooperate with the retrieval model in\nthe training phase; 2) an NMT model is only aware\n\n- f the retrieved data in the", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_5850", "chunk_text": " can categorize\nprevious works into two classes: 1) an NMT model\nleans how to cooperate with the retrieval model in\nthe training phase; 2) an NMT model is only aware\n\n- f the retrieved data in the inference phase.\n\n\n**Inference Phase** The key point of literature in\nthis line is to reward some target words based on\nwords in _**y**_ _[r]_ in the inference process. Thus, a decision can be made based on both the distribution\n\n- f generation model and the additional reward of\nretrieval model. Some previous works propose to\nreward target words based on the sentence-level\nsimilarity between _**x**_ and _**x**_ _[r]_, and the word alignment between _**x**_ _[r]_ and _**y**_ _[r]_ . Given the input sentence\n_**x**_, Zhang et al. (2018) try to assign target words\nin \u02c6 _**y**_ with higher rewards, when they appear in _**y**_ _[r]_\n\nand the aligned source words are in both _**x**_ _[r]_ and\n_**x**_ . He et al. (2019) follow a similar framework\nand consider the position information of those target words when rewarding. Those works reward\nthe target words in an explicit way, however, the\n\n- ne-sentence-one-model approach (Li et al., 2016c;\nTurchi et al., 2017) propose to reward target word\nimplicitly. For each testing input _**x**_, their approach\nwill first finetune the translation model on retrieved\nmemory _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ and then translate _**x**_ .\nOthers try to reward target words based on tokenlevel similarity score. Most works in this line are\nbased on the dense retriever (Khandelwal et al.,\n2020a), e.g., faiss. Khandelwal et al. (2020a) build\na key-value datastore, where key _h_ ( _**x**_ _[r]_ _,_ _**y**_ _[r]_ _<t_ [)][ is the]\n\n\n\nhidden state", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_6300", "chunk_text": "2020a) build\na key-value datastore, where key _h_ ( _**x**_ _[r]_ _,_ _**y**_ _[r]_ _<t_ [)][ is the]\n\n\n\nhidden state at each time step when translating _**y**_ _[r]_\n\nfrom _**x**_ _[r]_, and value is its golden-truth target word\n_**y**_ _[r]_ _t_ [. Therefore, in the inference time, they can use]\nthe _h_ ( _**x**_ _,_ \u02c6 _**y**_ _<t_ ) as query and reward target words\nwith similar hidden representations in the datastore. Although this method achieves significant\nperformance gain, one drawback of it is the high latency. To address this issue, Meng et al. (2021) use\nsome heuristics, e.g., pre-filtering, to avoid searching on the entire datastore. The reward score of\nprevious works is got from some non-parametric\napproaches, however, Zheng et al. (2021a) propose\na light-weight network to learn the reward score.\nSince dense retrieval has the potential of crosslingual retrieval, Zheng et al. (2021b) use a similar\napproach to achieve unsupervised domain adaptation, where a main change is to create the datastore\nbased on synthetic sources sentence and the real\n\ntarget sentences.\n\n\n**Training Phase** Different from those modelagnostic approaches, previous works in this line\naim to train the generation model to learn how\nto cooperate with the retrieval model. It is also\nworth noting that most works in this line adopt\nthe sentence-level retrieval, when integrating the\nretrieval information in the training process. To\nachieve its goal, Bulte and Tezcan (2019) and\nHossain et al. (2020) propose a data augmentation method to integrate the retrieved information,\nwhere _**x**_ is concatenated with _**y**_ _[r]_ before feeding\ninto the model . Following the data augmentation\napproach, Xu et al. (2020) propose more matching\nmethods to determine including which retrieved\nexample in the source is better.\nThere also exist some works that propose new\narchitectures to integrate the retrieval information.\nUnder the RNN-based framework, Cao and Xion", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_6750", "chunk_text": "0) propose more matching\nmethods to determine including which retrieved\nexample in the source is better.\nThere also exist some works that propose new\narchitectures to integrate the retrieval information.\nUnder the RNN-based framework, Cao and Xiong\n(2018) and Gu et al. (2018) use the gating and attention mechanism to incorporate the retrieved target sentences. When Transformer (Vaswani et al.,\n2017) becomes the backbone of NMT, some works\n\nalso use additional transformer encoders to en\ncode retrieved target sentences, and integrate them\nthrough attention mechanism (Bapna and Firat,\n2019; Cao et al., 2019). Xia et al. (2019) represent the retrieved target sentences in a different\ndata structure, i.e., a graph structure, and integrate\nit through attention mechanism. He et al. (2021)\npropose a light-weight method to encode the retrieved target sentences and leverage the alignment\ninformation to filter out irrelevant information. Dif\n\nferent from previous works that rely on bilingual\nmemories, Cai et al. (2021) propose a framework\nthat can retrieve the most similar target sentence in\na monolingual dataset, using a source sentence as\n\nquery.\n\n\n**Limitations** In the section of SMT, we have\nshowed some limitations of the retrieval augmented\napproaches. There also exist some limitations in\nthe line of NMT. First, the information used for\nderiving reward scores is limited. The similarity\nbetween an input and retrieved examples is the\nprimary feature to derive reward scores. However, some information, e.g., frequencies of words\nand context, may also be beneficial for integrating\nthe translation memory. Second, it remains to be\nan open question that when should we use the retrieved information and when not. In the inference\n\nphase, approaches tend to integrate the translation\nmemory excessively, e.g., at each time step, which\nnot only reduces the translation efficiency but may\nalso dampen the fluency of generated results.\n\n\n**5** **Other Tasks**\n\n\nIn addition to dialogue system and machine translation, retrieval-augmented generation techniques\nhave shown to be beneficial in many other tasks. In\nthe following, we highlight several key tasks that\napply retrieval-augmented generation approaches. [1]\n\n\n**Language Modelling** It has been shown that\nproperly leveraging information from", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_7200", "chunk_text": "\nhave shown to be beneficial in many other tasks. In\nthe following, we highlight several key tasks that\napply retrieval-augmented generation approaches. [1]\n\n\n**Language Modelling** It has been shown that\nproperly leveraging information from retrieval\nmemory could improve the performance of large\npre-trained language model. To build a more accurate language model, Khandelwal et al. (2020b) propose to incorporate a soft memory module into the\nsystem. Specifically, an index is built by caching\nthe hidden states of the training corpus. Then, the\nlanguage model accesses the index via k-NN search\nand displays a greatly improved performance. As\nanother example, Guu et al. (2020) propose a new\nparadigm that applies retrieval-augmented technique into the pre-training of generative language\nmodel. During learning, they train a neural selector that dynamically samples a relevant text to\nguide the reconstruction of a corrupted input sequence. In this way, the pre-trained model delivers better results by explicitly grounding on the\nretrieval memory. Lewis et al. (2020a) combine\nlanguage model pre-training with a paraphrasing\n\n\n1Here, we focus on tasks other than question answering.\nWe refer readers interested in QA to Chen and Yih (2020).\n\n\n\napproach. During learning, an input sequence to\nthe model is first corrupted. In the meantime, a set\n\n- f multi-lingual texts are retrieved based on which\nthe model learns to reconstruct the original input\nsequence. Recently, Borgeaud et al. (2021) propose RETRO, a large pre-trained language model\nenhanced with retrieved documents, and obtained\ncomparable performances with GPT-3 using 25 _\u00d7_\nfewer parameters.\n\n\n**Summarization** Text summarization is another\n\nresearch area that benefits from retrievalaugmented text generation. Peng et al. (2019)\npropose an adaptive decoding framework which\nfirst retrieves an exemplar document given the\nsource document. Then, the summarization of the\nsource document is derived through an adaptive\ngeneration process based on the retrieved template.\nDifferent from Peng et al. (2019), Cao et al.\n(2018) and Hossain et al. (2020) introduce an\nintermediate re-ranking stage into the generation\npipeline. Specifically, before generating the\ndocument summary, the retrieval documents are\nfirst", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_7650", "chunk_text": "), Cao et al.\n(2018) and Hossain et al. (2020) introduce an\nintermediate re-ranking stage into the generation\npipeline. Specifically, before generating the\ndocument summary, the retrieval documents are\nfirst re-ranked based on their similarity scores\nwith respect to the source document. Then, the\ndocument summarization is produced by re-writing\nthe selected templates.\n\n\n**Paraphrase Generation** To address the lack of\nquality as well as diversity in the generation of paraphrases, Kazemnejad et al. (2020) propose a generation framework which first retrieves a sentence\nthat is similar to input sentence. Then, based on\nthe retrieved sentence, a neural editor produces the\nresulting paraphrased sentence. Chen et al. (2019)\ninvestigate a different aspect of paraphrasing, i.e.\nhow to control the linguistic syntax displayed in\nthe generated text. To achieve this goal, Chen et al.\n(2019) propose to first extract a sentential exemplar that serves as the syntax template. A neural\nmodel then generates the paraphrase with desired\nlinguistic syntax following the retrieved exemplar.\n\n\n**Text Style Transfer** To improve the quality of\ngenerated text, Li et al. (2018) propose a retrievalaugmented framework which first retrieves texts\nthat are similar to the input based on lexical-level\nsimilarity. Then, the retrieved tokens that are irrelevant to the source are deleted, and the output is\nderived from the edited template. Xiao et al. (2021)\nalso adopte this framework by incorporating retrieval information from two sources (i.e. sparse\nand dense memories) and obtained an improved\n\n\nmodel performance.\n\n\n**Data-to-Text Generation** Recently, retrievalaugmented generation has been adapted to the task\n\n- f data-to-text generation. To bridge the gap between the structured data and natural language\ntext, Su et al. (2021a) propose a novel retrievalaugmented framework. Specifically, given the\nsource data, a set of candidate texts are first retrieved from a large unlabelled corpus. Then, a\nneural selector is applied to measure the similarities between the source data and candidate texts,\nand extract a set of more fine-grained prototypes\nfrom the candidates. Lastly, a generation model\ntakes the prototypes as input to produce the text\nthat describes the given structured data.\n\nWhile retrieval-augmented generation has been\n", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_8100", "chunk_text": " candidate texts,\nand extract a set of more fine-grained prototypes\nfrom the candidates. Lastly, a generation model\ntakes the prototypes as input to produce the text\nthat describes the given structured data.\n\nWhile retrieval-augmented generation has been\nwidely explored in the NLP community, we suggest that future research could extend this approach\nto tasks that involve data from multiple modalities. For instance, with recent advancements in\nimage-text retrieval (Jia et al., 2021; Radford et al.,\n2021), the structural gap between images and texts\nis largely bridged. Some early studies (Zhang et al.,\n2020) have shown that information retrieved from\nimages could improve the performance of neural\nmachine translation model. Naturally, such meth\n- ds could be extended to other multi-modal tasks,\nsuch as image captioning (Karpathy and Li, 2015).\nA similar idea could also be applied to tasks beyond images, such as speech-to-text transcription\n(Gales and Young, 2007).\n\n\n**6** **Future Directions**\n\n\nDespite the current success of retrieval augmented\ntext generation, there is still a long way to go as\ndiscussed in previous sections. We highlight some\ndirections to facilitate the future research as fol\nlows:\n\n\n**Retrieval Sensitivity** The performance of retrieval augmented text generation is very sensitive\nto the retrieval quality, i.e., the similarity between\nthe query and the retrieved examples. Currently, retrieval augmented text generation models perform\nwell when the retrieved examples are very similar to the query. However, they are even worse\nthan the generation models without retrieval when\nthe retrieval examples are less similar. Therefore,\nit would be important to exploit new methods to\naddress such an issue on similarity.\n\n\n\n**Retrieval Efficiency** Generally, if one enlarges\nthe retrieval memory to some extent, it would be\npossible to retrieve an example which is very similar to the query.Unfortunately, the downside is that\nthe overall inference for the retrieval augmented\ngeneration models is less efficient due the considerable retrieval overhead. In this sense, it is urgent\nto consider some methods to trade off the retrieval\n\nmemory size and retrieval efficiency, for example,\ndata compression for the retrieval memory.\n\n\n**Local vs. Global Optimization** Theoretically, it\nseems promising to jointly learn retrieval metrics\nand generation models. However, in practice, there\nis an essential gap about the retrieval", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_8550", "chunk_text": " for example,\ndata compression for the retrieval memory.\n\n\n**Local vs. Global Optimization** Theoretically, it\nseems promising to jointly learn retrieval metrics\nand generation models. However, in practice, there\nis an essential gap about the retrieval metric between the training and inference phrases. In the\ntraining phase, the loss is locally back-propagated\nto only a few retrieved examples while in the inference phase the metric is globally conducted among\nall examples in the memory. It would be interesting\nto narrow such a gap when learning a better metric\nfor generation tasks.\n\n\n**Multi-Modalities** With recent advancement in\n\nimage-text retrieval, directly associating images\nwith relevant text becomes possible. This urges\nresearchers to investigate the possibility of retrievalbased text generation in tasks that involve data from\ndifferent modalities. One typical task is image\ncaptioning. Beyond images, other tasks like speechto-text transcription could potentially benefit from\nretrieval-based generation methods as well.\n\n\n**Diverse & Controllable Retrieval** Most of the\n\nexisting approaches adopt a universal metric for\nretrieval, such as lexical similarities of sentences.\nFuture work should explore how to use customized\nmetrics for retrieval. This can be beneficial for\nmore controlled text generation. For example, instances with emotions and styles may be more desirable in the personalized dialogue generation, parallel data that contains specific terminologies is\nmore helpful in machine translation, and so on. On\nthe other hand, using a universal metric for retrieval\nmay lead to the lack of diversity of the retrieval results. Collecting a diverse set of retrieval results\ncan improve the coverage of useful information.\nThus, considering multiple different metrics for retrieval may lead to generation with higher quality\nin the future.\n\n\n**7** **Conclusion**\n\n\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different components of retrieval-augmented text generation including retrieval metrics, retrieval sources, and integration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other generation tasks. We also pointed out some future directions for retrieval-augmented text generation.\n\n\n**References**\n\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly\nlearning to align and translate. _", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_9000", "chunk_text": " directions for retrieval-augmented text generation.\n\n\n**References**\n\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly\nlearning to align and translate. _arXiv preprint_\n_arXiv:1409.0473_ .\n\n\nAnkur Bapna and Orhan Firat. 2019. Non-parametric\nadaptation for neural machine translation. In _Pro-_\n_ceedings of the 2019 Conference of the North Amer-_\n_ican Chapter of the Association for Computational_\n_Linguistics: Human Language Technologies, Vol-_\n_ume 1 (Long and Short Papers)_, pages 1921\u20131931.\n\n\nErgun Bi\u00e7ici and Marc Dymetman. 2008. Dynamic\ntranslation memory: Using statistical machine translation to improve translation memory fuzzy matches.\nIn _International Conference on Intelligent Text Pro-_\n_cessing and Computational Linguistics_, pages 454\u2013\n465. Springer.\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, Diego de Las Casas,\nAurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones,\nAlbin Cassirer, Andy Brock, Michela Paganini, Ge\n  - ffrey Irving, Oriol Vinyals, Simon Osindero, Karen\nSimonyan, Jack W. Rae, Erich Elsen, and Laurent\n[Sifre. 2021. Improving language models by retriev-](http://arxiv.org/abs/2112.04426)\n[ing from trillions of tokens.](http://arxiv.org/abs/2112.04426) _CoRR_, abs/2112.04426.\n\n\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy repair: Integrating fuzzy matches into neural machine\ntranslation. In _Proceedings of the 57th Annual Meet-_\n_ing of the Association for Computational Linguistics_,\npages 1800\u20131809.\n\n\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu,", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_9900", "chunk_text": " Encoding gated\ntranslation memory into neural machine translation.\nIn _Proceedings of the 2018 Conference on Empiri-_\n_cal Methods in Natural Language Processing_, pages\n3042\u20133047.\n\n\nZiqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.\n[2018. Retrieve, rerank and rewrite: Soft template](https://doi.org/10.18653/v1/P18-1015)\n[based neural summarization. In](https://doi.org/10.18653/v1/P18-1015) _Proceedings of the_\n_56th Annual Meeting of the Association for Com-_\n_putational Linguistics, ACL 2018, Melbourne, Aus-_\n_tralia, July 15-20, 2018, Volume 1: Long Papers_,\npages 152\u2013161. Association for Computational Linguistics.\n\n\nDanqi Chen and Wen-tau Yih. 2020. [Open-domain](https://doi.org/10.18653/v1/2020.acl-tutorials.8)\n[question answering. In](https://doi.org/10.18653/v1/2020.acl-tutorials.8) _Proceedings of the 58th An-_\n_nual Meeting of the Association for Computational_\n_Linguistics: Tutorial Abstracts_, pages 34\u201337, Online. Association for Computational Linguistics.\n\n\nMingda Chen, Qingming Tang, Sam Wiseman, and\n[Kevin Gimpel. 2019. Controllable paraphrase gen-](https://doi.org/10.18653/v1/p19-1599)\n[eration with a syntactic exemplar. In](https://doi.org/10.18653/v1/p19-1599) _Proceedings of_\n_the 57th Conference of the Association for Compu-_\n_tational Linguistics, ACL 2019, Florence, Italy, July_\n_28- August 2, 2019, Volume 1: Long Papers_, pages\n5972\u20135984. Association for Computational Linguistics.\n\n\nDavid Chiang. 2007. Hierarchical phrase-based translation. _computational linguistics_, 33(2):201\u2013228.\n\n\nSarah Dillon and Janet Fraser. 2006. Translators and\ntm: An investigation of translators\u2019 perceptions of\ntranslation memory adoption. _Machine Translation_,\n20(2):67\u2013", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_10350", "chunk_text": "istics_, 33(2):201\u2013228.\n\n\nSarah Dillon and Janet Fraser. 2006. Translators and\ntm: An investigation of translators\u2019 perceptions of\ntranslation memory adoption. _Machine Translation_,\n20(2):67\u201379.\n\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\n\n - f wikipedia: Knowledge-powered conversational\nagents. _arXiv preprint arXiv:1811.01241_ .\n\n\n[Mark J. F. Gales and Steve J. Young. 2007. The applica-](https://doi.org/10.1561/2000000004)\n[tion of hidden markov models in speech recognition.](https://doi.org/10.1561/2000000004)\n_Found. Trends Signal Process._, 1(3):195\u2013304.\n\n\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 2018. Search engine guided neural machine translation. In _Proceedings of the AAAI Con-_\n_ference on Artificial Intelligence_, volume 32.\n\n\nPrakhar Gupta, Jeffrey Bigham, Yulia Tsvetkov, and\n[Amy Pavel. 2021. Controlling dialogue generation](https://doi.org/10.18653/v1/2021.naacl-main.240)\n[with semantic exemplars.](https://doi.org/10.18653/v1/2021.naacl-main.240) In _Proceedings of the_\n_2021 Conference of the North American Chapter of_\n_the Association for Computational Linguistics: Hu-_\n_man Language Technologies_, pages 3018\u20133029, Online. Association for Computational Linguistics.\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu[pat, and Ming-Wei Chang. 2020. REALM: retrieval-](http://arxiv.org/abs/2002.08909)\n[augmented language model pre-training.](http://arxiv.org/abs/2002.08909) _CoRR_,\nabs/2002.08909.\n\n\nTatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,\nand Percy S Liang. 2018. A retrieve-and-edit framework for predicting structured outputs. In _", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_10800", "chunk_text": "RR_,\nabs/2002.08909.\n\n\nTatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,\nand Percy S Liang. 2018. A retrieve-and-edit framework for predicting structured outputs. In _Advances_\n_in Neural Information Processing Systems_, pages\n10052\u201310062.\n\n\nQiuxiang He, Guoping Huang, Qu Cui, Li Li, and\nLemao Liu. 2021. Fast and accurate neural machine\ntranslation with translation memory. In _Proceed-_\n_ings of the 59th Annual Meeting of the Association_\n_for Computational Linguistics and the 11th Interna-_\n_tional Joint Conference on Natural Language Pro-_\n_cessing (Volume 1: Long Papers)_, pages 3170\u20133180.\n\n\nQiuxiang He, Guoping Huang, Lemao Liu, and Li Li.\n2019. Word position aware translation memory for\nneural machine translation. In _CCF International_\n_Conference on Natural Language Processing and_\n_Chinese Computing_, pages 367\u2013379. Springer.\n\n\nNabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. 2020. Simple and effective retrieve-editrerank text generation. In _Proceedings of the 58th_\n_Annual Meeting of the Association for Computa-_\n_tional Linguistics_, pages 2532\u20132538.\n\n\nBaotian Hu, Zhengdong Lu, Hang Li, and Qingcai\nChen. 2014. Convolutional neural network architectures for matching natural language sentences. In\n_NIPS_, pages 2042\u20132050.\n\n\nZongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An\ninformation retrieval approach to short text conversation. _arXiv preprint arXiv:1408.6988_ .\n\n\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,\n[Zhen Li, and Tom Duerig. 2021. Scaling up visual](http://proceedings.mlr.press/v139/jia21b.html)\n[and vision-language representation learning with](http://proceedings.mlr.press/v139/jia21b.html)\n[noisy text supervision. In](http://proceedings", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_11250", "chunk_text": "ceedings.mlr.press/v139/jia21b.html)\n[and vision-language representation learning with](http://proceedings.mlr.press/v139/jia21b.html)\n[noisy text supervision. In](http://proceedings.mlr.press/v139/jia21b.html) _Proceedings of the 38th In-_\n_ternational Conference on Machine Learning, ICML_\n_2021, 18-24 July 2021, Virtual Event_, volume 139 of\n_Proceedings of Machine Learning Research_, pages\n4904\u20134916. PMLR.\n\n\n\n[Andrej Karpathy and Fei-Fei Li. 2015. Deep visual-](https://doi.org/10.1109/CVPR.2015.7298932)\n[semantic alignments for generating image descrip-](https://doi.org/10.1109/CVPR.2015.7298932)\n[tions. In](https://doi.org/10.1109/CVPR.2015.7298932) _IEEE Conference on Computer Vision and_\n_Pattern Recognition, CVPR 2015, Boston, MA, USA,_\n_June 7-12, 2015_, pages 3128\u20133137. IEEE Computer\nSociety.\n\n\nAmirhossein Kazemnejad, Mohammadreza Salehi, and\nMahdieh Soleymani Baghshah. 2020. [Paraphrase](https://doi.org/10.18653/v1/2020.acl-main.535)\n[generation by learning how to edit from samples. In](https://doi.org/10.18653/v1/2020.acl-main.535)\n_Proceedings of the 58th Annual Meeting of the Asso-_\n_ciation for Computational Linguistics_, pages 6010\u2013\n6021, Online. Association for Computational Linguistics.\n\n\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020a. Nearest neighbor machine translation. _arXiv preprint_\n_arXiv:2010.00710_ .\n\n\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n[Zettlemoyer, and Mike Lewis. 2020b. Generaliza-](https://openreview.net/forum", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_11700", "chunk_text": "0.00710_ .\n\n\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\n[Zettlemoyer, and Mike Lewis. 2020b. Generaliza-](https://openreview.net/forum?id=HklBjCEKvH)\n[tion through memorization: Nearest neighbor lan-](https://openreview.net/forum?id=HklBjCEKvH)\n[guage models. In](https://openreview.net/forum?id=HklBjCEKvH) _8th International Conference on_\n_Learning Representations, ICLR 2020, Addis Ababa,_\n_Ethiopia, April 26-30, 2020_ . OpenReview.net.\n\n\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\n\n[Statistical phrase-based translation. In](https://aclanthology.org/N03-1017) _Proceedings_\n\n_of the 2003 Human Language Technology Confer-_\n_ence of the North American Chapter of the Associa-_\n_tion for Computational Linguistics_, pages 127\u2013133.\n\n\nPhilipp Koehn and Jean Senellart. 2010. Convergence\n\n  - f translation memory and statistical machine translation. In _Proceedings of AMTA Workshop on MT_\n_Research and the Translation Industry_, pages 21\u201331.\n\n\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\n2021. Internet-augmented dialogue generation.\n_arXiv preprint arXiv:2107.07566_ .\n\n\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised\n\n - pen domain question answering. _arXiv preprint_\n_arXiv:1906.00300_ .\n\n\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Armen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n[2020a. Pre-training via paraphrasing. In](https://proceedings.neurips.cc/paper/2020/hash/d6f1dd034aabde7657e6680444ceff62-Abstract.html) _Advances_\n_in Neural Information Processing Systems 33: An-_\n_nual Conference on Neural Information Processing_\n_Systems 2020", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_12150", "chunk_text": "/hash/d6f1dd034aabde7657e6680444ceff62-Abstract.html) _Advances_\n_in Neural Information Processing Systems 33: An-_\n_nual Conference on Neural Information Processing_\n_Systems 2020, NeurIPS 2020, December 6-12, 2020,_\n_virtual_ .\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020b. Retrieval-augmented generation for knowledge-intensive nlp tasks. _arXiv_\n_preprint arXiv:2005.11401_ .\n\n\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016a. A diversity-promoting objective function for neural conversation models. In\n_NAACL_, pages 110\u2013119.\n\n\nJuncen Li, Robin Jia, He He, and Percy Liang. 2018.\n\n[Delete, retrieve, generate: a simple approach to sen-](https://doi.org/10.18653/v1/n18-1169)\n[timent and style transfer. In](https://doi.org/10.18653/v1/n18-1169) _Proceedings of the 2018_\n_Conference of the North American Chapter of the_\n_Association for Computational Linguistics: Human_\n_Language Technologies, NAACL-HLT 2018, New_\n_Orleans, Louisiana, USA, June 1-6, 2018, Volume_\n_1 (Long Papers)_, pages 1865\u20131874. Association for\nComputational Linguistics.\n\n\nLiangyou Li, Andy Way, and Qun Liu. 2014. A\ndiscriminative framework of integrating translation\nmemory features into smt. In _Proceedings of the_\n_11th Conference of the Association for Machine_\n_Translation in the Americas_, volume 1, pages 249\u2013\n260.\n\n\nLiangyou Li, Andy Way, and Qun Liu. 2016b. Phraselevel combination of smt and tm using constrained\nword lattice. Association for Computational Linguistics (ACL).\n\n\nXiaoqing Li, Jiajun Zhang, and Cheng", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_12600", "chunk_text": "you Li, Andy Way, and Qun Liu. 2016b. Phraselevel combination of smt and tm using constrained\nword lattice. Association for Computational Linguistics (ACL).\n\n\nXiaoqing Li, Jiajun Zhang, and Chengqing Zong.\n2016c. One sentence one model for neural machine\ntranslation. _arXiv preprint arXiv:1609.06490_ .\n\n\nZekang Li, Cheng Niu, Fandong Meng, Yang Feng,\nQian Li, and Jie Zhou. 2019. Incremental transformer with deliberation decoder for document\ngrounded conversations. In _Proceedings of the 57th_\n_Annual Meeting of the Association for Computa-_\n_tional Linguistics_, pages 12\u201321.\n\n\nRongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,\nand Hua Wu. 2019. Learning to select knowledge\nfor response generation in dialog systems. _arXiv_\n_preprint arXiv:1902.04911_ .\n\n\nLemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao,\nMo Yu, and Conghui Zhu. 2012. Locally training\nthe log-linear model for smt. In _Proceedings of the_\n_2012 Joint Conference on Empirical Methods in Nat-_\n_ural Language Processing and Computational Natu-_\n_ral Language Learning_, pages 402\u2013411.\n\n\nLemao Liu, Tiejun Zhao, Taro Watanabe, Hailong Cao,\nand Conghui Zhu. 2014. Discriminative training for\nlog-linear based smt: Global or local methods. _ACM_\n_Transactions on Asian Language Information Pro-_\n_cessing (TALIP)_, 13(4):1\u201325.\n\n\nYanjun Ma, Yifan He, Andy Way, and Josef van Genabith. 2011. Consistent translation using discriminative learning-a translation memory-inspired approach. In _Proceedings of the 49th Annual Meet-_\n_ing of the Association for Computational Linguistics:_\n_Human Language Technologies_, pages 1239\u20131248.\n\n\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tianwei Zhang, and Jiwei Li. 2021.\nFast", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_13050", "chunk_text": "_Human Language Technologies_, pages 1239\u20131248.\n\n\nYuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xiaofei Sun, Tianwei Zhang, and Jiwei Li. 2021.\nFast nearest neighbor machine translation. _arXiv_\n_preprint arXiv:2105.14528_ .\n\n\n[Franz Josef Och. 2003. Minimum error rate training in](https://doi.org/10.3115/1075096.1075117)\n[statistical machine translation. In](https://doi.org/10.3115/1075096.1075117) _Proceedings of the_\n\n\n\n_41st Annual Meeting of the Association for Compu-_\n_tational Linguistics_, pages 160\u2013167, Sapporo, Japan.\nAssociation for Computational Linguistics.\n\n\nGaurav Pandey, Danish Contractor, Vineet Kumar, and\nSachindra Joshi. 2018. Exemplar encoder-decoder\nfor neural conversation generation. In _ACL_, pages\n1329\u20131338.\n\n\nAshwin Paranjape, Omar Khattab, Christopher Potts,\nMatei Zaharia, and Christopher D Manning. 2021.\nHindsight: Posterior-guided training of retrievers for\nimproved open-ended generation. _arXiv preprint_\n_arXiv:2110.07752_ .\n\n\nHao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan\nDhingra, and Das Dipanjan. 2019. Text generation\nwith exemplar-based adaptive decoding. In _Proceed-_\n_ings of the Conference of the North American Chap-_\n_ter of the Association for Computational Linguistics:_\n_Human Language Technologies_ .\n\n\nLianhui Qin, Michel Galley, Chris Brockett, Xiaodong\nLiu, Xiang Gao, William B Dolan, Yejin Choi, and\nJianfeng Gao. 2019. Conversing by reading: Contentful neural conversation with on-demand machine\nreading. In _Proceedings of the 57th Annual Meet-_\n_ing of the Association for Computational Linguistics_,\npages 5427\u20135436.\n\n\nMinghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan\nChen, Weipeng Zhao,", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_14400", "chunk_text": "3087948)\n[TO-STYLE: dialogue generation with style-aware](https://doi.org/10.1109/TASLP.2021.3087948)\n[editing on retrieval memory.](https://doi.org/10.1109/TASLP.2021.3087948) _IEEE ACM Trans. Au-_\n_dio Speech Lang. Process._, 29:2152\u20132161.\n\n\nMarco Turchi, Matteo Negri, M Farajian, and Marcello\nFederico. 2017. Continuous learning from human\npost-edits for neural machine translation.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In _Advances in neural information pro-_\n_cessing systems_, pages 5998\u20136008.\n\n\nOriol Vinyals and Quoc Le. 2015. A neural conversational model. In _ICML (Deep Learning Workshop)_ .\n\n\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2013.\nIntegrating translation memory into phrase-based\nmachine translation during decoding. In _Proceed-_\n_ings of the 51st Annual Meeting of the Association_\n_for Computational Linguistics (Volume 1: Long Pa-_\n_pers)_, pages 11\u201321.\n\n\nKun Wang, Chengqing Zong, and Keh-Yih Su. 2014.\nDynamically integrating cross-domain translation\nmemory into phrase-based machine translation during decoding. In _Proceedings of COLING 2014,_\n_the 25th International Conference on Computational_\n_Linguistics: Technical Papers_, pages 398\u2013408.\n\n\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and refine: Improved sequence generation models for dialogue. In _Proceedings of the_\n_2018 EMNLP Workshop SCAI: The 2nd Interna-_\n_tional Workshop on Search-Oriented Conversational_\n_AI_, pages 87\u201392.\n\n\n\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In _Proceedings_\n\n", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_14850", "chunk_text": "_\n_AI_, pages 87\u201392.\n\n\n\nYu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. 2019. Response generation\nby context-aware prototype editing. In _Proceedings_\n\n_of the AAAI Conference on Artificial Intelligence_,\nvolume 33, pages 7281\u20137288.\n\n\nZeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,\nXiang Gao, Chris Quirk, Rik Koncel-Kedziorski,\nJianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,\net al. 2021. A controllable model of grounded response generation. In _Proceedings of the AAAI Con-_\n_ference on Artificial Intelligence_, volume 35, pages\n14085\u201314093.\n\n\nMengzhou Xia, Guoping Huang, Lemao Liu, and\nShuming Shi. 2019. Graph based translation mem\n  - ry for neural machine translation. In _Proceedings_\n\n_of the AAAI Conference on Artificial Intelligence_,\nvolume 33, pages 7297\u20137304.\n\n\nFei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei\n[Shen, and Xueqi Cheng. 2021. Transductive learn-](https://aclanthology.org/2021.emnlp-main.195)\n[ing for unsupervised text style transfer. In](https://aclanthology.org/2021.emnlp-main.195) _Proceed-_\n_ings of the 2021 Conference on Empirical Methods_\n_in Natural Language Processing, EMNLP 2021, Vir-_\n_tual Event / Punta Cana, Dominican Republic, 7-11_\n_November, 2021_, pages 2510\u20132521. Association for\nComputational Linguistics.\n\n\nJitao Xu, Josep M Crego, and Jean Senellart. 2020.\nBoosting neural machine translation with similar\ntranslations. In _Proceedings of the 58th Annual_\n_Meeting of the Association for Computational Lin-_\n_guistics_, pages 1580\u20131590.\n\n\nLiu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jianfeng Gao, W Bruce Croft, Xiaodong Liu, Yelong\nShen, and Jingjing Liu. ", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2202.01110_rag_survey_li:chunk_15300", "chunk_text": "0\u20131590.\n\n\nLiu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jianfeng Gao, W Bruce Croft, Xiaodong Liu, Yelong\nShen, and Jingjing Liu. 2019. A hybrid retrievalgeneration neural conversation model. In _Proceed-_\n_ings of the 28th ACM international conference on in-_\n_formation and knowledge management_, pages 1341\u2013\n1350.\n\n\nJingyi Zhang, Masao Utiyama, Eiichiro Sumita, Graham Neubig, and Satoshi Nakamura. 2018. Guiding\nneural machine translation with retrieved translation\npieces. In _Proceedings of the 2018 Conference of the_\n_North American Chapter of the Association for Com-_\n_putational Linguistics: Human Language Technolo-_\n_gies, Volume 1 (Long Papers)_, pages 1325\u20131335.\n\n\nYizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris\nBrockett, Michel Galley, Jianfeng Gao, and Bill\nDolan. 2021. Joint retrieval and generation training for grounded text generation. _arXiv preprint_\n_arXiv:2105.06597_ .\n\n\nZhuosheng Zhang, Kehai Chen, Rui Wang, Masao\nUtiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.\n2020. [Neural machine translation with universal](https://openreview.net/forum?id=Byl8hhNYPS)\n[visual representation. In](https://openreview.net/forum?id=Byl8hhNYPS) _8th International Confer-_\n_ence on Learning Representations, ICLR 2020, Ad-_\n_dis Ababa, Ethiopia, April 26-30, 2020_ . OpenReview.net.\n\n\nVentsislav Zhechev and Josef Van Genabith. 2010.\nSeeding statistical machine translation with translation memory output through tree-based structural\nalignment. In _Proceedings of the 4th Workshop_\n\n_on Syntax and Structure in Statistical Translation_,\npages 43\u201351.\n\n\nXin Zheng, Zhirui Zhang, Junliang Guo, Shujian\nHuang, Boxing Chen, Weihua Luo, and Jiajun Chen.\n2021a. Adaptive nearest", "token_count": 500, "metadata": {"arxiv_id": "2202.01110", "title": "A Survey on Retrieval-Augmented Text Generation", "authors": ["Huayang Li", "Yixuan Su", "Deng Cai", "Yan Wang", "Lemao Liu"], "year": 2022, "url": "https://arxiv.org/pdf/2202.01110v2"}}
{"chunk_id": "2107.05720_splade_formal:chunk_0", "chunk_text": "## **SPLADE: Sparse Lexical and Expansion Model** **for First Stage Ranking**\n\n\n\nThibault Formal\n\nNaver Labs Europe\nMeylan, France\nSorbonne Universit\u00e9, LIP6\n\nParis, France\nthibault.formal@naverlabs.com\n\n\n**ABSTRACT**\n\n\n\nBenjamin Piwowarski\nSorbonne Universit\u00e9, CNRS, LIP6\n\nParis, France\nbenjamin.piwowarski@lip6.fr\n\n\n\nSt\u00e9phane Clinchant\nNaver Labs Europe\nMeylan, France\nstephane.clinchant@naverlabs.com\n\n\n\nIn neural Information Retrieval, ongoing research is directed towards improving the first retriever in ranking pipelines. Learning\ndense embeddings to conduct retrieval using efficient approximate\nnearest neighbors methods has proven to work well. Meanwhile,\nthere has been a growing interest in learning _sparse_ representations\nfor documents and queries, that could inherit from the desirable\nproperties of bag-of-words models such as the exact matching of\nterms and the efficiency of inverted indexes. In this work, we present\na new first-stage ranker based on explicit sparsity regularization\nand a log-saturation effect on term weights, leading to highly sparse\nrepresentations and competitive results with respect to state-ofthe-art dense and sparse methods. Our approach is simple, trained\nend-to-end in a single stage. We also explore the trade-off between\neffectiveness and efficiency, by controlling the contribution of the\nsparsity regularization.\n\n\n**KEYWORDS**\n\n\nneural networks, indexing, sparse representations, regularization\n\n\n**ACM Reference Format:**\n\nThibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021.\nSPLADE: Sparse Lexical and Expansion Model for First Stage Ranking.\nIn _Proceedings of ACM Conference (Conference\u201917)._ ACM, New York, NY,\n[USA, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)\n\n\n**1** **INTRODUCTION**\n\n\nThe release of large pre-trained language models like BERT [7]\nhas shaken-up Natural Language Processing and Information Retrieval. These models have shown a strong ability to adapt to various\ntasks by simple fine-tuning. At the beginning of 2019, _", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_450", "chunk_text": " large pre-trained language models like BERT [7]\nhas shaken-up Natural Language Processing and Information Retrieval. These models have shown a strong ability to adapt to various\ntasks by simple fine-tuning. At the beginning of 2019, _Nogueira_\n_and Cho_ [17] achieved state-of-the-art results \u2013 by a large margin\n\n- on the MS MARCO passage re-ranking task, paving the way for\nLM-based neural ranking models. Because of strict efficiency requirements, these models have initially been used as re-rankers\nin a two-stage ranking pipeline, where first-stage retrieval \u2013 or\ncandidate generation \u2013 is conducted with bag-of-words models (e.g.\n\n\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\n\n- n the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\n_Conference\u201917, July 2017, Washington, DC, USA_\n\u00a9 2021 Association for Computing Machinery.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\n[https://doi.org/10.1145/nnnnnnn.nnnnnnn](https://doi.org/10.1145/nnnnnnn.nnnnnnn)\n\n\n\nBM25) that rely on inverted indexes. While BOW models remain\nstrong baselines [27], they suffer from the long standing vocabulary\nmismatch problem, where relevant documents might not contain\nterms that appear in the query. Thus, there have been attempts to\nsubstitute standard BOW approaches by learned (neural) rankers.\nDesigning such models poses several challenges regarding efficiency and scalability: therefore there is a need for methods where\nmost of the computation can be done offline and online inference\nis fast. Dense retrieval with approximate nearest neighbors search\nhas shown impressive results [8, 15, 26], but is still combined with\nBOW models because of its inability to explicitly model term matching. Hence, there has recently been a growing interest in learning\n", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_900", "chunk_text": " with approximate nearest neighbors search\nhas shown impressive results [8, 15, 26], but is still combined with\nBOW models because of its inability to explicitly model term matching. Hence, there has recently been a growing interest in learning\n_sparse representations_ for queries and documents [1, 4, 19, 28, 29].\nBy doing so, models can inherit from the desirable properties of\nBOW models like exact-match of (possibly latent) terms, efficiency\n\n- f inverted indexes and interpretability. Additionally, by modeling\nimplicit or explicit (latent, contextualized) _expansion_ mechanisms \u2013\nsimilarly to standard expansion models in IR \u2013 these models can\nreduce the vocabulary mismatch.\nThe contributions of this paper are threefold: (1) we build upon\nSparTerm [1], and show that a mild tuning of hyperparameters\nbrings improvements that largely outperform the results reported\nin the original paper; (2) we propose the SParse Lexical AnD Expansion (SPLADE) model, based on a logarithmic activation and\nsparse regularization. SPLADE performs an efficient document expansion [1, 16], with competitive results with respect to complex\ntraining pipelines for dense models like ANCE [26]; (3) finally, we\nshow how the sparsity regularization can be controlled to influence the trade-off between efficiency (in terms of the number of\nfloating-point operations) and effectiveness.\n\n\n**2** **RELATED WORKS**\n\n\nDense retrieval based on BERT Siamese models [22] has become\nthe standard approach for candidate generation in Question Answering and IR [8, 10, 12, 15, 25]. While the backbone of these models remains the same, recent works highlight the critical aspects\n\n- f the training strategy to obtain state-of-the-art results, ranging\nfrom improved negative sampling [8, 25] to distillation [11, 15].\nColBERT [13] pushes things further: the postponed token-level\ninteractions allow to efficiently apply the model for first-stage retrieval, benefiting of the effectiveness of modeling fine-grained\ninteractions, at the cost of storing embeddings for each (sub)term\n\n- raising concerns about the actual scalability of the approach for\nlarge collections. To the best of our knowledge, very few studies\nhave discussed the impact of using _approximate_ nearest neighbors\n\n\n(ANN) search on IR metrics [", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_1350", "chunk_text": ")term\n\n- raising concerns about the actual scalability of the approach for\nlarge collections. To the best of our knowledge, very few studies\nhave discussed the impact of using _approximate_ nearest neighbors\n\n\n(ANN) search on IR metrics [2, 23]. Due to the moderate size of the\nMS MARCO collection, results are usually reported with an _exact_,\nbrute-force search, therefore giving no indication on the effective\ncomputing cost.\nAn alternative to dense indexes is term-based ones. Building on\nstandard BOW models, _Zamani et al._ first introduced SNRM [28]: the\nmodel embeds documents and queries in a sparse high-dimensional\nlatent space by means of _\u2113_ 1 regularization on representations. However, SNRM effectiveness remains limited and its efficiency has\nbeen questioned [20]. More recently, there have been attempts to\ntransfer the knowledge from pre-trained LM to sparse approaches.\nBased on BERT, DeepCT [4\u20136] focused on learning contextualized\nterm weights in the full vocabulary space \u2013 akin to BOW term\nweights. However, as the vocabulary associated with a document\nremains the same, this type of approach does not solve the vocabulary mismatch, as acknowledged by the use of query expansion for\nretrieval [4]. A first solution to this problem consists in expanding\ndocuments using generative approaches such as doc2query [19] and\ndocTTTTTquery [18] to predict expansion words for documents.\nThe document expansion adds new terms to documents \u2013 hence\nfighting the vocabulary mismatch \u2013 as well as repeats existing\nterms, implicitly performing re-weighting by boosting important\nterms. These methods are however limited by the way they are\ntrained (predicting queries), which is indirect in nature and limit\ntheir progress. A second solution to this problem, that has been\nchosen by recent works such as [1, 16, 29], is to estimate the importance of each term of the vocabulary _implied by_ each term of\nthe document, i.e. to compute an interaction matrix between the\ndocument or query tokens and all the tokens from the vocabulary.\nThis is followed by an aggregation mechanism (roughly sum for\nSparTerm [1], max for EPIC [16] and SPARTA [29]), that allows to\ncompute an importance weight for each term of the vocabulary, for\nthe full document or query.", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_1800", "chunk_text": " mechanism (roughly sum for\nSparTerm [1], max for EPIC [16] and SPARTA [29]), that allows to\ncompute an importance weight for each term of the vocabulary, for\nthe full document or query. However, EPIC and SPARTA (document)\nrepresentations are not sparse enough by construction \u2013 unless\nresorting on top- _\ud835\udc58_ pooling \u2013 contrary to SparTerm, for which fast\nretrieval is thus possible. Furthermore, the latter does not include\n(like SNRM) an _explicit_ sparsity regularization, which hinders its\nperformance. Our SPLADE model relies on such regularization, as\nwell as other key changes, that boost both the efficiency and the\neffectiveness of this type of models.\n\n\n**3** **SPARSE LEXICAL REPRESENTATIONS FOR**\n\n**FIRST-STAGE RANKING**\n\n\nIn this section, we first describe in details the SparTerm model [1],\nbefore presenting our model named SPLADE.\n\n\n**3.1** **SparTerm**\n\n\nSparTerm predicts term importance \u2013 in BERT WordPiece vocabulary (| _\ud835\udc49_ | = 30522) \u2013 based on the logits of the Masked Language Model (MLM) layer. More precisely, let us consider an input\nquery or document sequence (after WordPiece tokenization) _\ud835\udc61_ =\n( _\ud835\udc61_ 1 _,\ud835\udc61_ 2 _, ...,\ud835\udc61\ud835\udc41_ ), and its corresponding BERT embeddings ( _\u210e_ 1 _,\u210e_ 2 _, ...,\u210e\ud835\udc41_ ).\nWe consider the importance _\ud835\udc64\ud835\udc56\ud835\udc57_ - f the token _\ud835\udc57_ (vocabulary) for a\ntoken _\ud835\udc56_ (of the input sequence):\n\n\n_\ud835\udc64\ud835\udc56\ud835\udc57_ = transform( _\u210e\ud835\udc56_ ) _[\ud835\udc47]_ _\ud835\udc38_ _\ud835\udc57_ + _\ud835\udc4f_ _\ud835\udc57_ _\ud835\udc57_ \u2208{1 _, ...,_ | _\ud835\udc49_ |} (1)\n\n\n\nwhere _\ud835\udc38_ _\ud835\udc57_ denotes the BERT input embedding for token _\ud835\udc57_, _\ud835\udc4f_ _\ud835\udc57_ is a\ntoken-level bias, and transform(", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_2250", "chunk_text": "_ |} (1)\n\n\n\nwhere _\ud835\udc38_ _\ud835\udc57_ denotes the BERT input embedding for token _\ud835\udc57_, _\ud835\udc4f_ _\ud835\udc57_ is a\ntoken-level bias, and transform( _._ ) is a linear layer with GeLU activation and LayerNorm. Note that Eq. 1 is equivalent to the MLM\nprediction, thus it can be also be initialized from a pre-trained MLM\nmodel. The final representation is then obtained by summing importance predictors over the input sequence tokens, after applying\nReLU to ensure the positivity of term weights:\n\n\n_\ud835\udc64_ _\ud835\udc57_ = _\ud835\udc54\ud835\udc57_ \u00d7 \u2211\ufe01 ReLU( _\ud835\udc64\ud835\udc56\ud835\udc57_ ) (2)\n\n_\ud835\udc56_ \u2208 _\ud835\udc61_\n\n\nwhere _\ud835\udc54\ud835\udc57_ is a binary mask (gating) described latter. The above\nequation can be seen as a form of query/document _expansion_, as\n\n- bserved in [1, 16], since for each token of the vocabulary the\nmodel predicts a new weight _\ud835\udc64_ _\ud835\udc57_ . SparTerm [1] introduces two\nsparsification schemes that turn off a large amount of dimensions\nin query and document representations, allowing to efficiently\nretrieve from an inverted index:\n\n**lexical-only** is a BOW masking, i.e. _\ud835\udc54\ud835\udc57_ = 1 if token _\ud835\udc57_ appears\nin _\ud835\udc61_, and 0 otherwise;\n**expansion-aware** is a lexical/expansion-aware binary gating\nmechanism, where _\ud835\udc54\ud835\udc57_ is _learned_ . To preserve the original input, it\nis forced to 1 if the token _\ud835\udc57_ appears in _\ud835\udc61_ .\nLet _\ud835\udc60_ ( _\ud835\udc5e,\ud835\udc51_ ) denote the ranking score obtained via dot product between _\ud835\udc5e_ and _\ud835\udc51_ representations from Eq. (2). Given a query _\ud835\udc5e\ud835\udc56_, a\npositive document _\ud835\udc51\ud835\udc56_ [+] [and a negative document] _[ \ud835\udc51]_ _\ud835\udc56_ [\u2212][, SparTerm is]\ntrained by minimzing the following loss:\n\n\n_\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_2700", "chunk_text": " a\npositive document _\ud835\udc51\ud835\udc56_ [+] [and a negative document] _[ \ud835\udc51]_ _\ud835\udc56_ [\u2212][, SparTerm is]\ntrained by minimzing the following loss:\n\n\n_\ud835\udc52_ _[\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc56_ [+][)]\nL _\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58_ = \u2212 log (3)\n\n_\ud835\udc52_ _[\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc56_ [+][)] + _\ud835\udc52_ _[\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc56_ [\u2212][)]\n\n\n**Limitations.** SparTerm expansion-aware gating is somewhat\nintricate, and the model cannot be trained end-to-end: the gating\nmechanism is learned beforehand, and _fixed_ while fine-tuning the\nmatching model with L _\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58_, therefore preventing the model to\nlearn the optimal sparsification strategy for the ranking task. More\n- ver, the two lexical and expansion-aware strategies do perform\nalmost equally well, questioning the actual benefits of expansion.\n\n\n**3.2** **SPLADE: SParse Lexical AnD Expansion**\n**model**\n\n\nIn the following, we propose slight, but essential changes to the\nSparTerm model that dramatically improve its performance.\n\n\n**Model.** We introduce a minor change in the importance estimation from Eq. 2, by introducing a log-saturation effect which\nprevents some terms to dominate and naturally ensures sparsity in\nrepresentations:\n\n\n_\ud835\udc64_ _\ud835\udc57_ = \u2211\ufe01 log [\ufffd] 1 + ReLU( _\ud835\udc64\ud835\udc56\ud835\udc57_ ) [\ufffd] (4)\n\n_\ud835\udc56_ \u2208 _\ud835\udc61_\n\n\nWhile it is intuitive that using a log-saturation prevents some terms\nfrom dominating \u2013 drawing a parallel with axiomatic approaches\nin IR and log(tf) models [9] \u2013 the implied sparsity can seem surprising at first, but, according to our experiments, it obtains better\nexperimental", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_3150", "chunk_text": "uration prevents some terms\nfrom dominating \u2013 drawing a parallel with axiomatic approaches\nin IR and log(tf) models [9] \u2013 the implied sparsity can seem surprising at first, but, according to our experiments, it obtains better\nexperimental results and allows already to obtain sparse solutions\n_without any regularization_ .\n\n\n**Ranking loss.** Given a query _\ud835\udc5e\ud835\udc56_ in a batch, a positive document _\ud835\udc51_ [+]\n_\ud835\udc56_ [, a (hard) negative document] _[ \ud835\udc51]_ _\ud835\udc56_ [\u2212] [(e.g. coming from BM25]\nsampling), and a set of negative documents in the batch (positive\ndocuments from other queries) { _\ud835\udc51\ud835\udc56,\ud835\udc57_ [\u2212] [}] _[\ud835\udc57]_ [, we consider the ranking loss]\nfrom [8], which can be interpreted as the maximization of the probability of the document _\ud835\udc51\ud835\udc56_ [+] [being relevant among the documents]\n_\ud835\udc51_ [+]\n_\ud835\udc56_ _[,\ud835\udc51]_ _\ud835\udc56_ [\u2212] [and][ {] _[\ud835\udc51]_ _\ud835\udc56,\ud835\udc57_ [\u2212] [}][:]\n\n\n_\ud835\udc52_ _[\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc56_ [+][)]\nL _\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58_  - _\ud835\udc3c\ud835\udc35\ud835\udc41_ = \u2212 log (5)\n\n_\ud835\udc52_ _[\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc56_ [+][)] + _\ud835\udc52_ _[\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc56_ [\u2212][)] + ~~[\ufffd]~~ _\ud835\udc56,\ud835\udc57_ [)]\n_\ud835\udc57_ _[\ud835\udc52][\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ [\u2212]\n\n\nThe _in-batch negatives_ (IBN) sampling strategy is widely used\nfor training image retrieval models", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_3600", "chunk_text": "\ufffd][\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ [\u2212]\n\n\nThe _in-batch negatives_ (IBN) sampling strategy is widely used\nfor training image retrieval models, and has shown to be effective\nin learning first-stage rankers [8, 12, 15].\n\n\n**Learning sparse representations.** The idea of learning sparse\nrepresentations for first-stage retrieval dates back to SNRM [28],\nvia _\u2113_ 1 regularization. Later, [20] pointed-out that minimizing the _\u2113_ 1\nnorm of representations does not result in the most efficient index,\nas nothing ensures that posting lists are evenly distributed. Note\nthat this is even more true for standard indexes due to the Zipfian\nnature of the term frequency distribution. To obtain a well-balanced\nindex, _Paria et al._ [20] introduce the FLOPS regularizer, a smooth\nrelaxation of the average number of floating-point operations necessary to compute the score of a document, and hence directly related\nto the retrieval time. It is defined using _\ud835\udc4e_ _\ud835\udc57_ as a continuous relaxation\n\n- f the activation (i.e. the term has a non zero weight) probability\n_\ud835\udc5d_ _\ud835\udc57_ for token _\ud835\udc57_, and estimated for documents _\ud835\udc51_ in a batch of size _\ud835\udc41_\n\nby \u00af _\ud835\udc4e_ _\ud835\udc57_ = _\ud835\udc41_ [1] - _\ud835\udc56\ud835\udc41_ =1 _[\ud835\udc64]_ _\ud835\udc57_ [(] _[\ud835\udc51][\ud835\udc56]_ [)] . This gives the following regularization loss\n\n\n\n\n- 2\n\n\n\n_\u2113_ FLOPS =\n\u2211\ufe01\n\n\n\n_\ud835\udc4e_ \u00af [2]\n\n\u2211\ufe01 _\ud835\udc57_ [=] \u2211\ufe01\n\n_\ud835\udc57_ \u2208 _\ud835\udc49_ _\ud835\udc57_ \u2208 _\ud835\udc49_\n\n\n\n_\ud835\udc57_ \u2208 _\ud835\udc49_\n\n\n\n1\n\n_\ud835\udc41_\n\n\n\n\n_\ud835\udc41_\n\u2211\ufe01 _\ud835\udc64_ _\ud835\udc57_ [(] _[\ud835\udc51][\ud835\udc56]_ [)]\n\n_\ud835\udc56_ =1", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_4050", "chunk_text": "\n\n\n\n1\n\n_\ud835\udc41_\n\n\n\n\n_\ud835\udc41_\n\u2211\ufe01 _\ud835\udc64_ _\ud835\udc57_ [(] _[\ud835\udc51][\ud835\udc56]_ [)]\n\n_\ud835\udc56_ =1\n\n\n\nThis differs from the _\u2113_ 1 regularization used in SNRM [28] where\nthe \u00af _\ud835\udc4e_ _\ud835\udc57_ are not squared: using _\u2113_ FLOPS thus pushes down high average\nterm weight values, giving rise to a more balanced index.\n\n\n**Overall loss.** We propose to combine the best of both worlds\nfor end-to-end training of sparse, expansion-aware representations\n\n- f documents and queries. Thus, we discard the binary gating in\nSparTerm, and instead learn our log-saturated model (Eq. 4) by\njointly optimizing ranking and regularization losses:\n\n\nL = L _\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58_      - _\ud835\udc3c\ud835\udc35\ud835\udc41_ + _\ud835\udf06\ud835\udc5e_ L _[\ud835\udc5e]_ reg [+] _[ \ud835\udf06]_ _\ud835\udc51_ [L] _[\ud835\udc51]_ reg (6)\n\n\nwhere Lreg is a sparse regularization ( _\u2113_ 1 or _\u2113_ FLOPS). We use two distinct regularization weights ( _\ud835\udf06\ud835\udc51_ and _\ud835\udf06\ud835\udc5e_ ) for queries and documents\n\n- allowing to put more pressure on the sparsity for queries, which\nis critical for fast retrieval.\n\n\n**4** **EXPERIMENTAL SETTING AND RESULTS**\n\n\nWe trained and evaluated our models on the MS MARCO passage\nranking dataset [1] in the full ranking setting. The dataset contains\napproximately 8 _._ 8M passages, and hundreds of thousands training\nqueries with shallow annotation (\u2248 1 _._ 1 relevant passages per query\nin average). The development set contains 6980 queries with similar\n\n\n[1https://github.com/microsoft/MSMARCO-Passage-Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking)\n\n\n\nlabels, while the TREC DL 2019 evaluation set provides fine-grained\nannotations from human assessors for a set of 43 queries [3].\n\n\n**Training, indexing and retrieval.** We initialized the", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_4500", "chunk_text": "CO-Passage-Ranking)\n\n\n\nlabels, while the TREC DL 2019 evaluation set provides fine-grained\nannotations from human assessors for a set of 43 queries [3].\n\n\n**Training, indexing and retrieval.** We initialized the models\nwith the BERT-base checkpoint. Models are trained with the ADAM\n\n- ptimizer, using a learning rate of 2 _\ud835\udc52_ [\u2212][5] with linear scheduling and\na warmup of 6000 steps, and a batch size of 124. We keep the best\ncheckpoint using MRR@10 on a validation set of 500 queries, after\ntraining for 150k iterations (note that this is not optimal, as we\nvalidate on a re-ranking task). We consider a maximum length of\n256 for input sequences. In order to mitigate the contribution of the\nregularizer at the early stages of training, we follow [20] and use a\nscheduler for _\ud835\udf06_, quadratically increasing _\ud835\udf06_ at each training iteration,\nuntil a given step (50k in our case), from which it remains constant.\nTypical values for _\ud835\udf06_ fall between 1 _\ud835\udc52_ [\u2212][1] and 1 _\ud835\udc52_ [\u2212][4] . For storing the\nindex, we use a custom implementation based on Python arrays,\nand we rely on Numba [14] to parallelize retrieval. Models [2] are\ntrained using PyTorch [21] and HuggingFace transformers [24], on\n4 Tesla _\ud835\udc49_ 100 GPUs with 32GB memory.\n\n\n**Evaluation.** We report Recall@1000 for both datasets, as well\nas the official metrics MRR@10 and NDCG@10 for MS MARCO dev\nset and TREC DL 2019 respectively. Since we are essentially interested in the first retrieval step, we do not consider re-rankers based\n\n- n BERT, and we compare our approach to first stage rankers only\n\n- results reported on the MS MARCO leaderboard are thus not comparable to the results presented here. We compare to the following\nsparse approaches (1) BM25 (2) DeepCT [4] (3) doc2query-T5 [18]\n(4) and SparTerm [1], as well as state-of-the-art dense approaches\nANCE [25] and T", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_4950", "chunk_text": " approaches (1) BM25 (2) DeepCT [4] (3) doc2query-T5 [18]\n(4) and SparTerm [1], as well as state-of-the-art dense approaches\nANCE [25] and TCT-ColBERT [15]. We report the results from the\n\n- riginal papers. We include a pure lexical SparTerm trained with\n\n- ur ranking pipeline (ST lexical-only). To illustrate the benefits of\nthe log-saturation, we add results for models trained using Eq. (2) instead of Eq. (4) (ST exp- _\u2113_ 1 and ST exp- _\u2113_ FLOPS). For sparse models, we\nindicate an estimation of the average number of floating-point operations between a query and a document in Table 1, when available,\nwhich is defined as the expectation E _\ud835\udc5e,\ud835\udc51_ - \ufffd _\ud835\udc57_ \u2208 _\ud835\udc49_ _[\ud835\udc5d]_ _\ud835\udc57_ [(] _[\ud835\udc5e]_ [)] _\ud835\udc5d_ _\ud835\udc57_ [(] _[\ud835\udc51]_ [)] - where\n_\ud835\udc5d_ _\ud835\udc57_ is the activation probability for token _\ud835\udc57_ in a document _\ud835\udc51_ - r a\nquery _\ud835\udc5e_ . It is empirically estimated from a set of approximately\n100k development queries, on the MS MARCO collection.\nResults are given in Table 1. Overall, we observe that: (1) _our mod-_\n_els outperform the other sparse retrieval methods by a large margin_\n_(except for recall@1000 on TREC DL);_ (2) _the results are competitive_\n_with state-of-the-art dense retrieval methods._\nMore specifically, our training method for ST lexical-only already\n\n- utperforms the results of DeepCT as well as the results reported in\nthe original SparTerm paper \u2013 including the model using expansion.\nThanks to the additional sparse expansion mechanism, we are able\nto obtain results on par with state-of-the-art dense approaches on\nMS MARCO dev set (e.g. Recall@1000 close to 0 _._ 96 for ST exp- _\u2113_ 1),\nbut with a much bigger average number of FLOPS.\nBy adding a log-saturation effect to the expansion", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_5400", "chunk_text": " (e.g. Recall@1000 close to 0 _._ 96 for ST exp- _\u2113_ 1),\nbut with a much bigger average number of FLOPS.\nBy adding a log-saturation effect to the expansion model, SPLADE\ngreatly increases sparsity \u2013 reducing the FLOPS to similar levels\nthan BOW approaches \u2013 at no cost on performance when compared\nto the best first-stage rankers. In addition, we observe the advantage\n\n\n[2We made the code public at https://github.com/naver/splade](https://github.com/naver/splade)\n\n\n**Table 1: Evaluation on MS MARCO passage retrieval (dev set)**\n**and TREC DL 2019**\n\n\nmodel MS MARCO dev TREC DL 2019 FLOPS\n\nMRR@10 R@1000 NDCG@10 R@1000\n\n\nDense retrieval\n\nSiamese (ours) 0.312 0.941 0.637 0.711  ANCE [25] 0.330 0.959 0.648  -  TCT-ColBERT [15] 0.335 0.964 0.670 0.720  \n\nSparse retrieval\n\nBM25 0.184 0.853 0.506 0.745 0.13\n\nDeepCT [4] 0.243 0.913 0.551 0.756  doc2query-T5 [18] 0.277 0.947 0.642 0.827 0.81\nST lexical-only [1] 0.275 0.912  -  -  ST expansion [1] 0.279 0.925  -  -  \n\nOur methods\n\nST lexical-only 0.290 0.923 0.595 0.774 1.84\nST exp- _\u2113_ 1 0.314 0.959 0.668 0.800 4.62\nST exp- _\u2113_ FLOPS 0.312 0.954 0.671 0.813 2.83\nSPLADE- _\u2113_ 1 0.322 0.954 0.667 0.792 0.88\n\nSPLADE- _\u2113_ F", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_5850", "chunk_text": "671 0.813 2.83\nSPLADE- _\u2113_ 1 0.322 0.954 0.667 0.792 0.88\n\nSPLADE- _\u2113_ FLOPS 0.322 0.955 0.665 0.813 0.73\n\n\n- f the FLOPS regularization over _\u2113_ 1 in order to decrease the computing cost. Note that in contrast to SparTerm, SPLADE is trained\nend-to end in a single step. It is also remarkably simple, compared\nto dense state-of-the-art baselines such as ANCE [25], and avoids\nresorting to approximate neighbors search, whose impact on IR\nmetrics has not been fully evaluated yet.\n\n\n**Effectiveness-efficiency trade-off.** Figure 1 illustrates the trade\n- ff between effectiveness (MRR@10) and efficiency (FLOPS), when\nwe vary _\ud835\udf06\ud835\udc5e_ and _\ud835\udf06\ud835\udc51_ (varying both implies that plots are not smooth).\nWe observe that ST exp- _\u2113_ FLOPS falls far behind BOW models and\n\n\n**Figure 1: Performance vs FLOPS for SPLADE models trained**\n**with different regularization strength** _\ud835\udf06_ **on MS MARCO**\n\n\n\n**Table 2: Document and expansion terms: between parenthesis**\n**is the weight associated with the term \u2013 omitted for the second oc-**\n**currence of the term in the document, and strike-through for zeros**\n\n\n**original document (doc ID: 7131647)**\n\n\nif (1.2) bow (2.56) legs (1.18) ~~is~~ caused (1.29) by (0.47) ~~the~~ bone (1.2) alignment\n(1.88) issue (0.87) ~~than you may be~~ able (0.29) ~~to~~ correct (1.37) through (0.43)\n_bow legs_ correction (1.05) ~~exercises. read more here..~~ _if bow legs is caused by_\n_the bone alignment issue than you may be able to correct through bow legs_\n\n_correction exercises._\n\n\n**expansion terms**\n\n\n(leg, 1.62) (arrow, 0.7) (exercise, 0.64) (", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_6300", "chunk_text": "_the bone alignment issue than you may be able to correct through bow legs_\n\n_correction exercises._\n\n\n**expansion terms**\n\n\n(leg, 1.62) (arrow, 0.7) (exercise, 0.64) (bones, 0.63) (problem, 0.41) (treatment,\n0.35) (happen, 0.29) (create, 0.22) (can, 0.14) (worse, 0.14) (effect, 0.08) (teeth,\n0.06) (remove, 0.03)\n\n\nSPLADE in terms of efficiency. In the meantime, SPLADE reaches efficiency levels equivalent to sparse BOW models, while outperforming doc2query-T5. Interestingly, strongly regularized models still\nshow competitive performance (e.g. FLOPS=0 _._ 05 _,_ MRR@10=0 _._ 296).\nFinally, the regularization effect brought by _\u2113_ FLOPS compared to _\u2113_ 1\nis clear: for the same level of efficiency, performance of the latter is\nalways lower.\n\n\n**The role of expansion.** Experiments show that the expansion\nbrings improvements w.r.t. to the purely lexical approach by increasing recall. Additionally, representations obtained from expansionregularized models are sparser: the models learn how to balance\nexpansion and compression, by both turning-off irrelevant dimensions and activating useful ones. On a set of 10k documents, the\nSPLADE- _\u2113_ FLOPS from Table 1 drops in average 20 terms per document, while adding 32 expansion terms. For one of our most efficient\nmodel (FLOPS=0 _._ 05), 34 terms are dropped in average, for only 5\nnew expansion terms. In this case, representations are extremely\nsparse: documents and queries contain in average 18 and 6 non-zero\nvalues respectively, and we need less that 1 _._ 4 GB to store the index\n\n- n disk. Table 2 shows an example where the model performs term\nre-weighting by emphasizing on _important_ terms and discarding\nmost of the terms without information content. Expansion allows\nto enrich documents, either by implicitly adding stemming effects\n(legs \u2192 leg) or by adding relevant topic words (e.g. treatment", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_6750", "chunk_text": "ing by emphasizing on _important_ terms and discarding\nmost of the terms without information content. Expansion allows\nto enrich documents, either by implicitly adding stemming effects\n(legs \u2192 leg) or by adding relevant topic words (e.g. treatment).\n\n\n**5** **CONCLUSION**\n\n\nRecently, dense retrieval based on BERT has demonstrated its superiority for first-stage retrieval, questioning the competitiveness of\ntraditional sparse models. In this work, we have proposed SPLADE,\na sparse model revisiting query/document expansion. Our approach\nrelies on in-batch negatives, logarithmic activation and FLOPS regularization to learn effective and efficient sparse representations.\nSPLADE is an appealing candidate for initial retrieval: it rivals the\nlatest state-of-the-art dense retrieval models, its training procedure\nis straightforward, its sparsity/FLOPS can be controlled explicitly\nthrough the regularization, and it can operate on inverted indexes.\nIn reason of its simplicity, SPLADE is a solid basis for further improvements in this line of research.\n\n\n\n33\n\n\n32\n\n\n31\n\n\n30\n\n\n29\n\n\n28\n\n\n27\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||a<br>a|a<br>a|vg q len <br>vg d len|_ \u2243_15<br>_ \u2243_58||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||y-T5<br>|y-T5<br>|\n||||||||||||doc2quer<br>|y-T5<br>|y-T5<br>|\n||||~~a~~<br>a|~~a~~<br>a|~~vg q len~~ <br>vg d len|~~_ \u2243_6~~<br>_ \u2243_18||<br> <br>|||~~SparTerm~~<br>ST exp-_\u2113F_<br>SPLADE-|~~ lexical~~<br>_LOPS_<br>_\u2113_~~1~~||\n||||~~a~~<br>a|~~a~~<br>a||||||||||\n||||||||||||SPLADE-|_\u2113FLOPS_|_\u2113FLO", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_8100", "chunk_text": ":2002.08909 [cs.CL]](https://arxiv.org/abs/2002.08909)\n\n[11] Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, and\nAllan Hanbury. 2020. Improving Efficient Neural Ranking Models with Cross[Architecture Knowledge Distillation. arXiv:2010.02666 [cs.IR]](https://arxiv.org/abs/2010.02666)\n\n[12] Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen tau Yih. 2020. Dense Passage Retrieval for Open[Domain Question Answering. arXiv:2004.04906 [cs.CL]](https://arxiv.org/abs/2004.04906)\n\n[13] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\nSearch via Contextualized Late Interaction over BERT. In _Proceedings of the 43rd_\n_International ACM SIGIR Conference on Research and Development in Information_\n_Retrieval_ (Virtual Event, China) _(SIGIR \u201920)_ . Association for Computing Machinery,\n[New York, NY, USA, 39\u201348. https://doi.org/10.1145/3397271.3401075](https://doi.org/10.1145/3397271.3401075)\n\n[14] Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. 2015. Numba: A llvm-based\npython jit compiler. In _Proceedings of the Second Workshop on the LLVM Compiler_\n_Infrastructure in HPC_ . 1\u20136.\n\n[15] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Repre[sentations for Ranking using Tightly-Coupled Teachers. arXiv:2010.11386 [cs.IR]](https://arxiv.org/abs/2010.11386)\n\n[16] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\nGoharian, and Ophir Frieder. 2020. Expansion via Prediction", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_8550", "chunk_text": "11386)\n\n[16] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\nGoharian, and Ophir Frieder. 2020. Expansion via Prediction of Importance with\n\n\n\nContextualization. _Proceedings of the 43rd International ACM SIGIR Conference on_\n_Research and Development in Information Retrieval_ [(Jul 2020). https://doi.org/10.](https://doi.org/10.1145/3397271.3401262)\n[1145/3397271.3401262](https://doi.org/10.1145/3397271.3401262)\n\n[17] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\n[arXiv:1901.04085 [cs.IR]](https://arxiv.org/abs/1901.04085)\n\n[18] Rodrigo Nogueira and Jimmy Lin. 2019. From doc2query to docTTTTTquery.\n\n[19] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\n[Expansion by Query Prediction. arXiv:1904.08375 [cs.IR]](https://arxiv.org/abs/1904.08375)\n\n[20] Biswajit Paria, Chih-Kuan Yeh, Ian E. H. Yen, Ning Xu, Pradeep Ravikumar, and\nBarnab\u00e1s P\u00f3czos. 2020. Minimizing FLOPs to Learn Efficient Sparse Representa[tions. arXiv:2004.05665 [cs.LG]](https://arxiv.org/abs/2004.05665)\n\n[21] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPyTorch: An Imperative Style, High-Performance Deep Learning Library.. In\n_NeurIPS_ .\n\n[22] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In _Proceed", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2107.05720_splade_formal:chunk_9000", "chunk_text": " Library.. In\n_NeurIPS_ .\n\n[22] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In _Proceedings of the 2019 Conference on Em-_\n_pirical Methods in Natural Language Processing_ . Association for Computational\n[Linguistics. http://arxiv.org/abs/1908.10084](http://arxiv.org/abs/1908.10084)\n\n[23] Zhengkai Tu, Wei Yang, Zihang Fu, Yuqing Xie, Luchen Tan, Kun Xiong, Ming\nLi, and Jimmy Lin. 2020. Approximate Nearest Neighbor Search and Lightweight\nDense Vector Reranking in Multi-Stage Retrieval Architectures. In _Proceedings of_\n_the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval_ .\n97\u2013100.\n\n[24] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe\nDavison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,\nCanwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,\nand Alexander M. Rush. 2020. HuggingFace\u2019s Transformers: State-of-the-art\n[Natural Language Processing. arXiv:1910.03771 [cs.CL]](https://arxiv.org/abs/1910.03771)\n\n[25] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\nJunaid Ahmed, and Arnold Overwijk. 2020. Approximate Nearest Neighbor\n[Negative Contrastive Learning for Dense Text Retrieval. arXiv:2007.00808 [cs.IR]](https://arxiv.org/abs/2007.00808)\n\n[26] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett,\nJunaid Ahmed, and Arnold Overwikj. 2021.", "token_count": 500, "metadata": {"arxiv_id": "2107.05720", "title": "SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking", "authors": ["Thibault Formal", "Benjamin Piwowarski", "St\u00e9phane Clinchant"], "year": 2021, "url": "https://arxiv.org/pdf/2107.05720v1"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_0", "chunk_text": "## **Interleaving Retrieval with Chain-of-Thought Reasoning** **for Knowledge-Intensive Multi-Step Questions**\n\n\n\n**Harsh Trivedi** _[\u2020]_ **Niranjan Balasubramanian** _[\u2020]_\n\n\n\n**Tushar Khot** _[\u2021]_ **Ashish Sabharwal** _[\u2021]_\n\n\n_\u2021_ Allen Institute for AI\n\nSeattle, U.S.A.\n\n\n{tushark,ashishs}@allenai.org\n\n\n\n\n_\u2020_ Stony Brook University\nStony Brook, U.S.A.\n\n\n{hjtrivedi,niranjan}@cs.stonybrook.edu\n\n\n**Abstract**\n\n\nPrompting-based large language models\n(LLMs) are surprisingly powerful at generating natural language reasoning steps or\nChains-of-Thoughts (CoT) for multi-step\nquestion answering (QA). They struggle,\nhowever, when the necessary knowledge is\neither unavailable to the LLM or not up-to-date\nwithin its parameters. While using the question\nto retrieve relevant text from an external\n\nknowledge source helps LLMs, we observe\nthat this one-step retrieve-and-read approach\nis insufficient for multi-step QA. Here, _what_\n_to retrieve_ depends on _what has already_\n_been derived_, which in turn may depend on\n_what was previously retrieved_ . To address\nthis, we propose IRCoT, a new approach\nfor multi-step QA that interleaves retrieval\nwith steps (sentences) in a CoT, guiding the\nretrieval with CoT and in turn using retrieved\nresults to improve CoT. Using IRCoT with\nGPT3 substantially improves retrieval (up to\n21 points) as well as downstream QA (up\nto 15 points) on four datasets: HotpotQA,\n2WikiMultihopQA, MuSiQue, and IIRC. We\n\n  - bserve similar substantial gains in out-ofdistribution (OOD) settings as well as with\nmuch smaller models such as Flan-T5-large\nwithout additional training. IRCoT reduces\nmodel hallucination, resulting in factually\nmore accurate CoT reasoning. [1] .\n\n\n**1** **Introduction**\n\n\nLarge language models are capable of answering complex questions by generating step-bystep natural language reasoning steps\u2014so called\nchains of thoughts (CoT)\u2014when prompted appropriately (Wei", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_450", "chunk_text": "T reasoning. [1] .\n\n\n**1** **Introduction**\n\n\nLarge language models are capable of answering complex questions by generating step-bystep natural language reasoning steps\u2014so called\nchains of thoughts (CoT)\u2014when prompted appropriately (Wei et al., 2022). This approach has been\nsuccessful when all information needed to answer\n\nthe question is either provided as context (e.g., algebra questions) or assumed to be present in the\nmodel\u2019s parameters (e.g., commonsense reasoning).\n\n\n[1Code, data, and prompts are available at https://](https://github.com/stonybrooknlp/ircot)\n[github.com/stonybrooknlp/ircot](https://github.com/stonybrooknlp/ircot)\n\n\n\nFigure 1: IRCoT interleaves chain-of-thought (CoT)\ngeneration and knowledge retrieval steps in order to\nguide the retrieval by CoT and vice-versa. This interleaving allows retrieving more relevant information for\nlater reasoning steps, compared to standard retrieval using solely the question as the query.\n\n\nHowever, for many open-domain questions, all required knowledge is not always available or up-todate in models\u2019 parameters and it\u2019s beneficial to\nretrieve knowledge from external sources (Lazaridou et al., 2022; Kasai et al., 2022).\n\n_How can we augment chain-of-thought prompt-_\n_ing for open-domain, knowledge-intensive tasks_\n_that require complex, multi-step reasoning?_\n\nWhile a _one-shot_ retrieval from a knowledge\nsource based solely on the question can successfully augment LMs with relevant knowledge for\nmany factoid-based tasks (Lewis et al., 2020; Guu\net al., 2020; Borgeaud et al., 2022; Izacard et al.,\n2022), this strategy has clear limitations for more\ncomplex multi-step reasoning questions. For such\nquestions, one often must retrieve partial knowledge, perform partial reasoning, retrieve additional\ninformation based on the outcome of the partial\n\n\n\n\n\n\n\n\n\n\n\n\nreasoning done so far, and iterate. As an example,\nconsider the question illustrated in Fig. 1, _\u201cIn what_\n_country was Lost Gravity manufactured?\u201d_ . The\nWikipedia document retrieved using the question\n(in particular, the roller coaster Lost Gravity) as the\nquery does not mention where Lost Gravity was\nmanufactured. Instead, one must first infer that\nit was manufactured by a company", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_900", "chunk_text": "_ . The\nWikipedia document retrieved using the question\n(in particular, the roller coaster Lost Gravity) as the\nquery does not mention where Lost Gravity was\nmanufactured. Instead, one must first infer that\nit was manufactured by a company called Mack\nRides, and then perform further retrieval, guided\nby the inferred company name, to obtain evidence\npointing to the manufacturing country.\nThus, the retrieval and reasoning steps must inform each other. Without retrieval, a model is likely\nto generate an incorrect reasoning step due to hallucination. Additionally, without generating the first\nreasoning step, the text supporting the second step\ncan\u2019t be identified easily given the lack of lexical or\neven semantic overlap with the question. In other\nwords, we need retrieved facts in order to generate\nfactually correct reasoning steps and the reasoning\nsteps to retrieve relevant facts.\nBased on this intuition, we propose an _interleav-_\n_ing approach_ to this problem, where the idea is to\nuse retrieval to guide the chain-of-thought (CoT)\nreasoning steps and use CoT reasoning to guide the\nretrieval. Fig. 1 shows an overview of our retrieval\nmethod, which we call IRCoT. [2] We begin by retrieving a base set of paragraphs using the question\nas a query. Subsequently, we alternate between the\nfollowing two steps: (i) _extend CoT_ : use the question, the paragraphs collected thus far, and the CoT\nsentences generated thus far to generate the next\nCoT sentence; (ii) _expand retrieved information_ :\nuse the last CoT sentence as a query to retrieve\nadditional paragraphs to add to the collected set.\nWe repeat these steps till the CoT reports an answer or we reach the maximum allowed number\n\n- f reasoning steps. Upon termination, all collected\nparagraphs are returned as the retrieval outcome.\nFinally, we use these as the context for answering\nthe question via direct QA prompting (Brown et al.,\n2020) or CoT prompting (Wei et al., 2022).\nWe evaluate the efficacy of our system\n\n- n 4 multi-step reasoning datasets under an\n\n- pen-domain setting: HotpotQA (Yang et al.,\n2018), 2WikiMultihopQA (Ho et al., 2020),\nMuSiQue (Trivedi et al., 2022), and IIRC (", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_1350", "chunk_text": "- pen-domain setting: HotpotQA (Yang et al.,\n2018), 2WikiMultihopQA (Ho et al., 2020),\nMuSiQue (Trivedi et al., 2022), and IIRC (Ferguson et al., 2020). Our experiments using OpenAI\nGPT3 (code-davinci-002) (Brown et al., 2020;\nOuyang et al., 2022; Chen et al., 2021) demon\n\n2Interleaved Retrieval guided by Chain-of-Thought.\n\n\n\nstrate that retrieval using IRCoT is substantially\nmore effective than the baseline, one-step, questionbased retrieval by 11-21 recall points under a fixedbudget optimal recall setup. [3] When IRCoT is used\nin conjunction with a prompting-based reader, it\nalso leads to substantial improvement (up to 15 F1\npoints) in downstream few-shot QA performance\nand reduces factual errors in generated CoT by\nup to 50%. Our approach also works on much\nsmaller Flan-T5 models (11B, 3B, and 0.7B) showing similar trends. In particular, we find QA using\nFlan-T5-XL (3B) with IRCoT even outperforms\nthe 58X larger GPT3 with a one-step questionbased retrieval. Furthermore, these improvements\nalso hold up in an out-of-distribution (OOD) setting\nwhere the demonstrations from one dataset are used\n\nwhen testing on another dataset. Lastly, we note\nthat our QA scores exceed those reported by recent\nworks on few-shot prompting for open-domain QA\n(ODQA) (Khot et al., 2023; Press et al., 2022; Yao\net al., 2022), although a fair apples-to-apples comparison with them isn\u2019t possible (cf. Appendix C).\nIn summary, our main **contribution** is a novel retrieval method, IRCoT, that leverages LMs\u2019 chain\n- f-thought generation capabilities to guide retrieval\nand uses retrieval in turn to improve CoT reasoning.\nWe demonstrate that IRCoT:\n\n\n1. improves both retrieval and few-shot QA performance on several multi-step open-domain\nQA datasets, in both IID and OOD settings;\n2. reduces factual errors in generated CoTs; and\n3. improves performance with both large-scale", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_1800", "chunk_text": "T:\n\n\n1. improves both retrieval and few-shot QA performance on several multi-step open-domain\nQA datasets, in both IID and OOD settings;\n2. reduces factual errors in generated CoTs; and\n3. improves performance with both large-scale\n(175B models) as well as smaller-scale models (Flan-T5-*, _\u2264_ 11B) without any training.\n\n\n**2** **Related Work**\n\n\n**Prompting for Open-Domain QA.** LLMs can\nlearn various tasks by simply using a few examples as prompts (Brown et al., 2020). They\u2019ve\nalso been shown to answer complex questions\nby producing step-by-step reasoning (chain-ofthoughts, or CoT) when prompted with a few or\nzero demonstrations (Wei et al., 2022; Kojima et al.,\n2022). Prompting has been applied to open-domain\nQA (Lazaridou et al., 2022; Sun et al., 2022; Yu\net al., 2023) but its value in improving retrieval and\nQA for multi-step open-domain questions remains\nrelatively underexplored.\n\n\n3We explain later (in the Metric section and Footnote 7)\nthe appropriateness of this metric in our setting as opposed to\nmore mainstream information recall metrics.\n\n\nRecently three approaches have been proposed\nfor multi-step open-domain QA. SelfAsk (Press\net al., 2022) prompts LLMs to decompose a question into subquestions and answers subquestions by\na call to Google Search API. DecomP (Khot et al.,\n2023) is a general framework that decomposes a\ntask and delegates sub-tasks to appropriate submodels. They also decompose questions but delegate retrieval to a BM25-based retriever. Both of\nthese approaches are not developed for CoT reasoning, do not focus on the retrieval problem, and require a single-hop QA model to answer the decomposed questions. Recently proposed ReAct (Yao\net al., 2022) system frames the problem as generating a sequence of reasoning and action steps. These\nsteps are much more complex, rely on much larger\nmodels (PaLM-540B), and require fine-tuning to\n\n- utperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be\neffective for smaller models without any training.\nWhile a direct comparison", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_2250", "chunk_text": "models (PaLM-540B), and require fine-tuning to\n\n- utperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be\neffective for smaller models without any training.\nWhile a direct comparison with these approaches is\nnot straightforward (difference in knowledge corpus, LLMs, examples), we find that our ODQA\nperformance is much higher than all their reported\nnumbers where available (\u00a75).\n\n\n**Supervised** **Multi-Step** **Open-Domain** **QA.**\nPrior work has explored iterative retrieval for\n\n- pen-domain QA in a fully supervised setting. Das\net al. (2019) proposes an iterative retrieval model\nthat retrieves using a neural query representation\nand then updates it based on a reading comprehension model\u2019s output. Feldman and El-Yaniv\n(2019) apply similar neural query reformulation\nidea for multihop open-domain QA. Xiong et al.\n(2021) extends the widely-used Dense Passage\nRetrieval (DPR) (Karpukhin et al., 2020) to\nmultihop setting, which has since been improved\nby Khattab et al. (2021). Asai et al. (2020)\nleverages the graph structure induced by the entity\nlinks present in Wikipedia paragraphs to perform\niterative multi-step retrieval. GoldEn (Gold Entity)\nretriever (Qi et al., 2019) iteratively generates\ntext queries based on paragraphs retrieved from\nan off-the-shelf retriever but requires training\ndata for this next query generator. Nakano et al.\n(2021) used GPT3 to answer long-form questions\nby interacting with the browser but relied on\nhuman annotations of these interactions. All of\n\nthese methods rely on supervised training on a\nlarge-scale dataset and can not be easily extended\nto a few-shot setting.\n\n\n\n**3** **Chain-of-Thought-Guided Retrieval**\n**and Open-Domain QA**\n\n\nOur goal is to answer a knowledge-intensive multistep reasoning question _Q_ in a few-shot setting\nby using a knowledge source containing a large\nnumber of documents. To do this we follow a\n\nretrieve-and-read paradigm (Zhu et al., 2021),\nwhere the retriever first retrieves documents from\nthe knowledge source and the QA model reads the\nretrieved documents and the question to", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_2700", "chunk_text": " documents. To do this we follow a\n\nretrieve-and-read paradigm (Zhu et al., 2021),\nwhere the retriever first retrieves documents from\nthe knowledge source and the QA model reads the\nretrieved documents and the question to generate\nthe final answer. Our contribution is mainly in the\nretrieve step (\u00a73.1), and we use standard prompting strategies for the read step (\u00a73.2).\nAs noted earlier, for multi-step reasoning, retrieval can help guide the next reasoning step,\nwhich in turn can inform what to retrieve next. This\n\nmotivates our interleaving strategy, discussed next.\n\n\n**3.1** **Interleaving Retrieval with**\n**Chain-of-Thought Reasoning**\n\n\nOur proposed retriever method, IRCoT, can be\ninstantiated from the following three ingredients:\n(i) a base retriever that can take a query and return a given number of paragraphs from a corpus\n\n- r knowledge source; (ii) a language model with\nzero/few-shot Chain-of-Thought (CoT) generation\ncapabilities; and (iii) a small number of annotated\nquestions with reasoning steps explaining how to\narrive at the answer in natural language (chain of\nthoughts) and a set of paragraphs from the knowledge source that collectively support the reasoning\nchain and the answer.\n\nThe overview of IRCoT is given in Fig. 2. We\nfirst gather a base set of paragraphs by retrieving _K_\nparagraphs using the question _Q_ as the query. Then,\nwe interleave two steps (reason and retrieve)\niteratively until the termination criterion is met.\nThe **retrieval-guided reasoning step (\u201cRea-**\n**son\u201d)** generates the next CoT sentence using the\nquestion, the paragraphs collected thus far, and\nthe CoT sentences generated thus far. The prompt\ntemplate for the task looks as follows:\n\n\nWikipedia Title: <Page Title>\n<Paragraph Text>\n\n...\n\nWikipedia Title: <Page Title>\n<Paragraph Text>\n\n\nQ: <Question>\n\nA: <CoT-Sent-1> ... <CoT-Sent-n>\n\n\nFor in-context demonstrations, we use the complete CoT in the above format. For a test instance,\n\n\nFigure 2: IRCoT interleaves chain-of-thought (CoT) generation and retrieval steps to guide the retrieval by CoT and\nvice-versa. We start by retrieving _", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_3150", "chunk_text": "T in the above format. For a test instance,\n\n\nFigure 2: IRCoT interleaves chain-of-thought (CoT) generation and retrieval steps to guide the retrieval by CoT and\nvice-versa. We start by retrieving _K_ documents using the question as they query and repeat two steps alternatingly\nuntil termination. (i) reason-step generates next CoT sentence based on the question, so far retrieved paragraphs,\nand CoT sentences. (ii) retrieve-step retrieves _K_ more paragraphs based on the last CoT sentence. The process\nterminates when the generated CoT has \u201canswer is\u201d or the number of steps exceeds a threshold. The collection of\nall paragraphs is returned as the retrieval result on the termination.\n\n\n\nwe show the model only the CoT sentences generated thus far and let it complete the rest. Even\nthough the model may output multiple sentences,\nfor each reason-step, we only take the first generated sentence and discard the rest.\n\nFor the paragraphs in the in-context demonstrations, we use ground-truth supporting paragraphs\nand _M_ randomly sampled paragraphs shuffled and\nconcatenated together in the above format. For a\ntest instance, we show all the paragraphs collected\nthus far across all the previous retrieve-steps.\nIf the generated CoT sentence has the \u201canswer\nis:\u201d string or the maximum number of steps [4] has\nbeen reached, we terminate the process and return\nall collected paragraphs as the retrieval result.\nThe **CoT-guided retrieval step (\u201cRetrieve\u201d)**\nuses the last generated CoT sentence as a query\nto retrieve more paragraphs and adds them to the\ncollected paragraphs. We cap the total number of\ncollected paragraphs [5] so as to fit in at least a few\ndemonstrations in the model\u2019s context limit.\n\n\n**3.2** **Question Answering Reader**\n\n\nThe QA reader answers the question using retrieved\nparagraphs taken from the retriever. We consider\n\n\n4set to 8 in our experiments.\n5set to 15 in our experiments.\n\n\n\ntwo versions of the QA reader implemented via two\nprompting strategies: CoT Prompting as proposed\nby Wei et al. (2022), Direct Prompting as proposed\nby Brown et al. (2020). For CoT prompting, we use\nthe same template as shown in \u00a73.2, but at test time\nwe ask the model to generate the full", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_3600", "chunk_text": "2022), Direct Prompting as proposed\nby Brown et al. (2020). For CoT prompting, we use\nthe same template as shown in \u00a73.2, but at test time\nwe ask the model to generate the full CoT from\nscratch. The final sentence of CoT is expected to\nbe of the form \u201canswer is: ...\u201d, so that the answer\ncan be extracted programmatically. If it\u2019s not in\nthat form, the full generation is returned as the\nanswer. For Direct Prompting, we use the same\ntemplate as CoT Prompting but the answer field\n(\u201cA: \u201d) contains only the final answer instead of\nCoT. See App. G for details.\n\n\n**4** **Experimental Setup**\n\n\nWe evaluate - ur method - n 4 multi-step\nQA datasets in the - pen-domain setting:\n**HotpotQA** (Yang et al., 2018), **2WikiMul-**\n**tihopQA** (Ho et al., 2020), answerable subset of\n**MuSiQue** (Trivedi et al., 2022), and answerable\nsubset of **IIRC** (Ferguson et al., 2020). For\nHotpotQA, we use the Wikipedia corpus that\ncomes with it for the open-domain setting. For\neach of the other three datasets, which originally\ncome in a reading comprehension or mixed setting,\nwe used the associated contexts to construct a\n\n\ncorpus for our open-domain setting (see App. A\nfor details). For each dataset, we use 100 randomly\nsampled questions from the original development\nset for tuning hyperparameters, and 500 other\nrandomly sampled questions as our test set.\n\n\n**4.1** **Models**\n\n\n**Retriever.** We use BM25 (Robertson et al., 2009)\nimplemented in Elasticsearch [6] as our base retriever.\nWe compare two retriever systems:\n\n(i) **One-step Retriever (OneR)** uses the question as a query to retrieve _K_ paragraphs. We select\n_K \u2208{_ 5 _,_ 7 _,_ 9 _,_ 11 _,_ 13 _,_ 15 _}_ that\u2019s best on the dev set.\n\n(ii) **IRCoT Retriever** is our method described in \u00a73. We use BM25 as its underlying retriever", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_4050", "chunk_text": "9 _,_ 11 _,_ 13 _,_ 15 _}_ that\u2019s best on the dev set.\n\n(ii) **IRCoT Retriever** is our method described in \u00a73. We use BM25 as its underlying retriever and experiment with OpenAI GPT3\n(code-davinci-002) (Brown et al., 2020; Ouyang\net al., 2022; Chen et al., 2021) and Flan-T5 (Chung\net al., 2022) of different sizes as its CoT generator.\n\nFor demonstrating in-context examples to these\nLMs, we wrote CoTs for 20 questions for all the\ndatasets (see App. \u00a7G). We then create 3 demonstration (\u201ctraining\u201d) sets by sampling 15 questions\neach for each dataset. For each experiment, we\nsearch for the best hyperparameters for the dev set\nusing the first demonstration set and evaluate each\ndemonstration set on the test set using the selected\nhyperparameters. We report the mean and standard\ndeviation of these 3 results for each experiment.\n\nAt test time, we pack as many demonstrations\nas possible within the model\u2019s context length limit.\nThe context limit for GPT3 (code-davinci-002)\nis 8K word pieces. Flan-T5-* doesn\u2019t have any\nhard limit as it uses relative position embeddings.\nBut we limit Flan-T5\u2019s context to 6K word pieces,\nwhich is the maximum we could fit in the memory\n\n- f our 80G A100 GPUs.\n\nIRCoT Retriever has one key hyperparameter:\n_K \u2208{_ 2 _,_ 4 _,_ 6 _,_ 8 _}_, the number of paragraphs to retrieve at each step. Additionally, when creating\n\u201ctraining\u201d demonstrations for IRCoT\u2019s Reasoner\nmodule, we use gold paragraphs and a smaller number _M \u2208{_ 1 _,_ 2 _,_ 3 _}_ - f distractor paragraphs (\u00a73.1).\n\n**Retrieval Metric:** We allow a maximum of 15\n\nparagraphs for all retriever systems and measure\nthe recall of the gold paragraphs among the retrieved set of paragraphs. We search for the hyperparameter _K_ (and _M_ for IRCoT) that maximizes\nthe recall on the dev set and use it on the test set.\n\n\n[6", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_4500", "chunk_text": " of the gold paragraphs among the retrieved set of paragraphs. We search for the hyperparameter _K_ (and _M_ for IRCoT) that maximizes\nthe recall on the dev set and use it on the test set.\n\n\n[6https://www.elastic.co/](https://www.elastic.co/)\n\n\n\nThe reported metric can thus be viewed as the _fixed-_\n_budget optimal recall_ for each system considered. [7]\n\n\n**QA Reader.** To implement the reader, we use\nthe same LMs as used in the reason-step of\nIRCoT Retriever. We found that QA readers implemented with Flan-T5-* perform better with the\nDirect Prompting strategy and GPT3 performs better with CoT Prompting strategy (see App. E).\nHence we use Direct prompting strategy for QA\nwith Flan-T5-* and CoT with GPT3 for the experiments. [8]\n\n\nThe QA reader has one hyperparameter _M_ : the\nnumber of distractor paragraphs in the in-context\ndemonstrations. We search for _M_ in _{_ 1 _,_ 2 _,_ 3 _}_ .\nWhen used in conjunction with IRCoT retriever\n_M_ is tied for the CoT generator and the reader.\n\n\n**Open-Domain QA (ODQA) Models.** Putting retrievers and readers together, we experiment with\nODQA models constructed from the various language models denoted as **OneR QA** and **IRCoT**\n**QA** . For IRCoT QA, the choice of LM for the CoT\ngenerator and the reader is kept the same. We also\nexperiment with retriever-less QA readers **NoR QA**\nto assess how well LMs can answer the question\nfrom their parametric knowledge alone. To select\nthe best hyperparameters for the ODQA model,\nwe search for the hyperparameters _K_ and _M_ that\nmaximize the answer F1 on the development set.\nIIRC is structured slightly differently from the\n\n- ther datasets, in that its questions are grounded\nin a main passage and other supporting paragraphs\ncome from the Wikipedia pages of entities mentioned in this passage. We slightly modify the retrievers and readers to account for this (see App. B).\n\n\n**5** **Results**\n\n\n**IRCoT retrieval is better than one-step.** Fig. 3\ncompares OneR with", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_4950", "chunk_text": " this passage. We slightly modify the retrievers and readers to account for this (see App. B).\n\n\n**5** **Results**\n\n\n**IRCoT retrieval is better than one-step.** Fig. 3\ncompares OneR with IRCoT retrievers made from\n\n\n7Note that our retrieved documents are not ranked, making standard information retrieval metrics such as MAP and\nDCG inapplicable. Further, we can only limit the number of\nretrieved paragraphs _per step_ to _K_ . Since the total number\n\n- f reasoning steps varies for questions, and in some cases,\nwe don\u2019t even obtain all _K_ paragraphs in a given step, the\ntotal number of retrieved paragraphs also varies (even though\ncapped at 15). This makes Recall@k, Precision@k, etc., also\nnot applicable as metrics for any given k.\n8IRCoT, by construction, produces a CoT as a part of its\nretrieval process. Thus, instead of having a separate post-hoc\nreader, one can also just extract the answer from the CoT\ngenerated during retrieval. However, we found this to be a\nsuboptimal choice, so we always use a separate reader (see\nApp. F).\n\n\nFigure 3: Retrieval recall for one-step retriever (OneR) and IRCoT instantiated from Flan-T5-XXL (left) and GPT3\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n\n\n\nFigure 4: Answer F1 for ODQA model made using (i) no retriever (NoR QA) (ii) one-step retriever", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_5400", "chunk_text": "|||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n\n\n\nFigure 4: Answer F1 for ODQA model made using (i) no retriever (NoR QA) (ii) one-step retriever (OneR QA) and\n(iii) IRCoT QA instantiated from Flan-T5-XXL (left) and GPT3 (right) models. IRCoT QA outperforms OneR QA\nand NoR QA for both models on all datasets, except for GPT3 on IIRC.\n\n\n\nFlan-T5-XXL and GPT3 LMs. For both models,\nIRCoT significantly outperforms one-step retrieval\nacross all datasets. For Flan-T5-XXL, IRCoT improves our recall metric relative to one-step retrieval, on HotpotQA by 7.9, on 2WikiMultihopQA\nby 14.3, on MuSiQue by 3.5, and on IIRC by 10.2\npoints. For GPT3, this improvement is by 11.3, 22.6,\n12.5, and 21.2 points, respectively.\n\n\n**IRCoT QA outperforms NoR and OneR QA.**\nFig. 4 compares ODQA performance using\nNoR, OneR and IRCoT retriever made from\n\nFlan-T5-XXL and GPT3 LMs. For Flan-T5-XXL,\nIRCoT QA outperforms OneR QA on HotpotQA\nby 9.4, on 2WikiMultihopQA by 15.3, on MuSiQue\nby 5.0 and IIRC by 2.5 F1 points. For GPT3, the\ncorresponding numbers (except for IIRC) are 7.1,\n13.2, and 7.1 F1 points. For GPT3, IRCoT doesn\u2019t\nimprove the QA score on IIRC, despite significantly improved retrieval (21 points as shown in\nFig. 3). This is likely because IIRC relevant knowledge may already be present in GPT3, as also evidenced by its NoR QA score being similar. For\n\n- ther datasets and model combinations, NoR QA is\n\n\n\nmuch worse than IRCoT QA, indicating the limits\n\n- f", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_5850", "chunk_text": " knowledge may already be present in GPT3, as also evidenced by its NoR QA score being similar. For\n\n- ther datasets and model combinations, NoR QA is\n\n\n\nmuch worse than IRCoT QA, indicating the limits\n\n- f the models\u2019 parametric knowledge.\n\n\n**IRCoT is effective in OOD setting.** Since CoT\nmay not always be easy to write for new datasets,\nwe evaluate NoR, OneR, and IRCoT on generalization to new datasets, i.e. OOD setting. To do so,\nwe use prompt demonstrations from one dataset to\nevaluate on another dataset. [9] For all pairs of the\ndatasets [10] and for both Flan-T5-XXL and GPT3, we\nfind the same trend as in the IID setting: IRCoT retrieval outperforms OneR (Fig. 5), and IRCoT QA\n\n- utperforms both OneR QA and NoR QA (Fig. 6).\n\n\n**IRCoT generates CoT with fewer factual errors.**\nTo assess whether our approach also improves the\nfactuality of generated CoTs, we manually annotated CoTs generated by NoR QA, OneR QA, and\nIRCoT QA using GPT3 for 40 randomly sampled\nquestions from each of the four datasets. We considered CoT to have a factual error if at least one\n\n\n9We use the evaluation dataset\u2019s corpus for retrieval.\n10We skip IIRC in this exploration as the task is structured\na bit differently and requires special handling (see App. B).\n\n\nFigure 5: Retrieval recall for OneR and IRCoT using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_6300", "chunk_text": "Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n\n(OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X _\u2192_ Y indicates prompt\ndemonstrations are from dataset X and evaluation is on dataset Y. IRCoT outperforms OneR in such an OOD setting.\n\n\nFigure 6: Answer F1 for NoR QA, OneR QA and IRCoT QA using Flan-T5-XXL (Left) and GPT3 (Right) in\n\n- ut-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X _\u2192_ Y\nindicates prompt demonstrations are from dataset X and evaluation is on dataset Y. IRCoT QA outperforms OneR\nQA and NoR QA in such OOD setting.\n\n\n\nFigure 7: Number of questions, out of 40, where CoT\ngenerated by GPT3 using different methods has at least\n1 factual error. Factual errors: IRCoT _<_ OneR _<_ NoR.\n\n\n- f the facts [11] is not true. [12] As Fig. 7 shows, NoR\nmakes the most factual errors, OneR makes fewer,\n\n\n11all sentences before the final \u201canswer is:\u201d sentence.\n12Note that factual error doesn\u2019t necessarily mean the predicted answer is incorrect and vice-versa. This is because the\nmodel can generate a wrong answer despite all correct facts,\nand vice-versa. We also account for the possibility of answer\nannotation errors in the original datasets.\n\n\n\nand IRCoT the least. In particular, IRCoT reduces\nthe factual errors over OneR by 50% on HotpotQA\nand 40% on 2WikiMultihopQA.\nTable 2 illustrates how the CoT predictions for\ndifferent methods vary qualitatively. Since NoR\nrelies completely on parametric knowledge, it often\nmakes a factual error in the first sentence, which\nderails the full CoT. OneR can retrieve relevant\n\ninformation closest to the", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_6750", "chunk_text": "different methods vary qualitatively. Since NoR\nrelies completely on parametric knowledge, it often\nmakes a factual error in the first sentence, which\nderails the full CoT. OneR can retrieve relevant\n\ninformation closest to the question and is less likely\nto make such errors early on, but it still makes\nerrors later in the CoT. IRCoT, on the other hand,\nis often able to prevent such errors in each step.\n\n\n**IRCoT is also effective for smaller models.** To\n\nsee how effective IRCoT is at different LM sizes,\nwe show the scaling plots in Fig. 8. [13] We compare the recall for OneR and IRCoT using Flan-T5\n{base (0.2B), large (0.7B), XL (3B), XXL (11B)},\nand GPT3 code-davinci-002 (175B). IRCoT\nwith even the smallest model (0.2B) is better than\n\n\n13We skip IIRC here as the smaller models are not good at\nidentifying Wikipedia titles from a paragraph and a question\nwhich is necessary for IIRC (see App. B).\n\n\nFigure 8: Retrieval recall for OneR (bottom) and IRCoT (top) for LMs of increasing sizes: Flan-T5 {base (0.2B),\nlarge (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA, MuSiQue. IRCoT\n\n- utperforms OneR for all model sizes, including the 0.3B model, and the difference roughly grows with model size.\nNote: OneR doesn\u2019t use LM in its retrieval and so has a fixed score.\n\n\nFigure 9: Answer F1 for ODQA models made using OneR (bottom) and IRCoT (top) for LMs of increasing sizes:\nFlan-T5 {base (0.2B), large (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA and\nMuSiQue. IRCoT QA outperforms OneR QA for all model sizes except for the smallest, 0.3B. IRCoT with ", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_7200", "chunk_text": "B) on HotpotQA, 2WikiMultihopQA and\nMuSiQue. IRCoT QA outperforms OneR QA for all model sizes except for the smallest, 0.3B. IRCoT with 3B\nmodel even outperforms OneR with 58X larger GPT3 model showing the value of improved retrieval.\n\n\n\nOneR, and the performance roughly improves with\nthe model size. This shows the CoT generation\ncapabilities of even small models can be leveraged\nfor improving retrieval. Furthermore, we show the\neffect of model size on the QA score in Fig. 9. For\nall sizes except the smallest (0.2B), we see IRCoT\nQA is better than OneR QA. Moreover, IRCoT\nwith a 3B model even outperforms OneR and NoR\nwith a 58X larger 175B GPT3 model in all datasets.\n\n\n**IRCoT is SOTA for few-shot multistep ODQA.** [14]\n\nWe compare IRCoT QA with five recent approaches to using LLMs for ODQA: InternetAugmented QA (Lazaridou et al., 2022), RECITE (Sun et al., 2022) ReAct (Yao et al., 2022),\nSelfAsk (Press et al., 2022), and DecomP (Khot\net al., 2022). Although these are not head-to-head\ncomparisons as different methods use different\nAPIs, knowledge sources, and even LLMs (see\nApp. C for details), it is still informative to explore, in a leaderboard-style fashion, how IRCoT\nperforms relative to the best numbers published for\nthese recent systems.\n\n\n14App. \u00a7C reports updated SOTA numbers, including contemporaneous and newer works.\n\n\n\nModel HpQA [Br] HpQA 2WikiMQA MQ [2H]\n\n\nInterAug _\u2212_ | _\u2212_ 30.3 | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\nRECITE _\u2212_ | _\u2212_ 37.1 | 48.4 _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nReAct _\u2212_ | _\u2212_ 35.1 | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nSelf", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_7650", "chunk_text": " 48.4 _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nReAct _\u2212_ | _\u2212_ 35.1 | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nSelfAsk _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ 40.1 | _\u2212_ 15.2 | _\u2212_\n\nDecomP _\u2212_ | 50.0 _\u2212_ | _\u2212_ _\u2212_ | 59.3 _\u2212_ | _\u2212_\n\n\nIRCoT QA **45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8**\n\n\nTable 1: Comparison with other LLM-based ODQA\nsystems on EM and F1 scores. \u2018 _\u2212_ \u2019: score is unavailable. HpQA [Br] : Bridge questions subset of HotpotQA.\nMQ [2H] : MuSiQue 2-hop questions. IRCoT QA with\nGPT3 (ours) outperforms other systems by a large margin. Note: Comparisons aren\u2019t head-to-head as discussed in the text. App. \u00a7C reports updated SOTA numbers, including contemporaneous and newer works.\n\n\nAs shown in Table 1, IRCoT QA significantly\n\n- utperforms all of these recent systems by a large\nmargin, setting a new state of the art in terms of\nwhat\u2019s achievable via retrieval-augmented LLMs\n(without supervised training).\n\n\n**6** **Conclusions**\n\n\nChain-of-thought prompting has significantly improved LLMs\u2019 ability to perform multi-step reason\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nThe tap dancing\nsoloist in My\nFeet Keep\nDancing also\nstarred in what\n\n1935 MGM\n\nmusical?\n\n\nWho is the\n\nfather-in-law\n\n- f Queen Hyojeong?\n\n\nWhat is the\n\nname - f the\n\ncastle in the\ncity where the\nperformer - f\nA Collection\n\n1984\u20131989 was\n\nborn?\n\n\n\n_\u21d2_ The tap dancing\nsoloist in My Feet\nKeep Dancing is Fayard\nNicholas.\n_\u21d2_ Fayard Nicholas\nstarred in the 1935\n\nM", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_8100", "chunk_text": "\u20131989 was\n\nborn?\n\n\n\n_\u21d2_ The tap dancing\nsoloist in My Feet\nKeep Dancing is Fayard\nNicholas.\n_\u21d2_ Fayard Nicholas\nstarred in the 1935\n\nMGM musical An\n\nAll-Colored Vaudeville\n\nShow.\n\n\n_\u21d2_ Queen Hyojeong\nis the wife of King\nHeonjong of Joseon.\n_\u21d2_ King Heonjong\n\n- f Joseon is the son\n\n- f Crown Prince Hy\n- myeong.\n\n\n_\u21d2_ A Collection\n1984\u20131989 was performed by Jane Siberry.\n_\u21d2_ Jane Siberry was\nborn in Toronto.\n\n_\u21d2_ The castle in Toronto\n\nis the Casa Loma.\n\n\n\nThe tap dancing soloist\nin My Feet Keep Dancing is Eleanor Powell. Eleanor Powell\n\nalso starred in the 1935\n\nMGM musical Broadway Melody of 1936.\nSo the answer is: Broadway Melody of 1936.\n\n\nQueen Hyojeong has a\nhusband named King\nGojong. King Gojong\nhas a father named\nHeungseon Daewongun.\nThus, Queen Hyojeong\nhas a father-in-law\nnamed Heungseon\nDaewongun. So the\nanswer is: Heungseon\nDaewongun.\n\n\nThe performer of A Collection 1984\u20131989 is\n\nThe The. The The was\n\nborn in London. The\n\nname of the castle in\n\nLondon is the Tower of\n\nLondon. So the answer\n\nis: the Tower of Lon\ndon.\n\n\n\nThe tap dancing soloist\nin My Feet Keep Dancing is Fayard Nicholas.\nFayard Nicholas also\nstarred in the 1935\nMGM musical Top Hat.\nSo the answer is: Top\nHat.\n\n\nQueen Hyojeong is the\nwife of King Heonjong\n\n- f Joseon. King Heonjong of Joseon is the\nson of King Sejo of\nJoseon. Thus, King Sejo\n\n- f Joseon is the fatherin-law of Queen Hyojeong. So the answer is:\nKing Sejo of Joseon.\n\n\nA Collection\n\n1984\u20131989 was\nperformed by Jane\nSiberry. Jane Siberry\nwas born in Toronto.\n\nThe castle in Toronto\nis", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_8550", "chunk_text": "jeong. So the answer is:\nKing Sejo of Joseon.\n\n\nA Collection\n\n1984\u20131989 was\nperformed by Jane\nSiberry. Jane Siberry\nwas born in Toronto.\n\nThe castle in Toronto\nis Peqin Castle. So the\nanswer is: Peqin Castle.\n\n\n\nThe tap dancing soloist\nin My Feet Keep Dancing is Fayard Nicholas.\nFayard Nicholas starred\nin the 1935 MGM mu\nsical An All-Colored\n\nVaudeville Show. So\n\nthe answer is: An\n\nAll-Colored Vaudeville\n\nShow.\n\n\nQueen Hyojeong is the\nwife of King Heonjong\n\n- f Joseon. King Heonjong of Joseon is the son\n\n- f Crown Prince Hy\n- myeong. Thus, Crown\nPrince Hyomyeong is\nthe father-in-law - f\nQueen Hyojeong. So\nthe answer is: Crown\nPrince Hyomyeong.\n\n\nA Collection\n\n1984\u20131989 was\nperformed by Jane\nSiberry. Jane Siberry\nwas born in Toronto.\n\nThe castle in Toronto is\n\nthe Casa Loma. So the\n\nanswer is: Casa Loma.\n\n\n\nTable 2: Example CoTs generated by GPT3 with different methods. Since NoR relies on parametric knowledge, it\n\n- ften makes a factual error in the first sentence derailing the full CoT. OneR can retrieve relevant information closest\nto the question and is less likely to make such errors early on, but it still makes errors later in the CoT. As IRCoT\nperforms retrieval after each step, it is often able to prevent such errors in each step. More examples are in App. D.\n\n\n\ning. We leveraged this ability to improve retrieval,\nand in turn, improve QA performance for complex knowledge-intensive open-domain tasks in a\nfew-shot setting. We argued that one-step questionbased retrieval is insufficient for such tasks, and\nintroduced IRCoT, which uses interleaved CoT reasoning and retrieval steps that guide each other\nstep-by-step. On four datasets, IRCoT significantly\nimproves both retrieval and QA performance when\ncompared to one-step retrieval, for both large and\nrelatively smaller-scale LMs. Additionally, CoTs\ngenerated by IRCoT contain fewer factual errors.\n\n\n**Limitations**\n\n\nIRCoT relies", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_9000", "chunk_text": " both retrieval and QA performance when\ncompared to one-step retrieval, for both large and\nrelatively smaller-scale LMs. Additionally, CoTs\ngenerated by IRCoT contain fewer factual errors.\n\n\n**Limitations**\n\n\nIRCoT relies on the base LM to have a zero or\n\nfew-shot CoT-generation ability. While this is commonly available in large LMs (over 100B), it\u2019s not\nas common for small LMs (under 20B), which to\nsome extent limits IRCoT adoptability. Given the\nrecent surge of interest (Tay et al., 2023; Magister et al., 2022; Ho et al., 2022), however, smaller\n\n\n\nLMs will likely increasingly acquire such ability,\nmaking IRCoT compatible with many more LMs.\nIRCoT also relies on the base LM to support\nlong inputs as multiple retrieved paragraphs need\nto fit in the LM\u2019s input, in addition to at least\na few demonstrations of QA or CoT with paragraphs. This was supported by the models we used\nas code-davinci-002 (GPT3) allows 8K tokens\nand Flan-T5-* uses relative position embeddings\nmaking it as extensible as the GPU memory constraints allow. Future work can explore strategies to\nrerank and select the retrieved paragraphs instead\n\n- f passing all of them to the LM to alleviate the\nneed for the LM to support long input.\nThe performance gain of IRCoT retriever and\nQA (over OneR and ZeroR baselines) come with\nan additional computational cost. This is because\nIRCoT makes a separate call to an (L)LM for each\nsentence of CoT. Future work can focus on, for\ninstance, dynamically deciding when to retrieve\nmore information and when to perform additional\nreasoning with the current information.\n\n\nLastly, a portion of our experiments was carried\n\n- ut using a commercial LLM API from OpenAI\n(code-davinci-002). This model was deprecated\nby OpenAI after our submission making the reproduction of these experiments challenging despite\n\n- ur best efforts, just like any other work using such\nAPIs. The trends discussed in the paper (IRCoT\n_>_ OneR _>_ NoR), we believe, would still hold.\nAdditionally, all our experiments using Flan-T5-*,\nwhich exhibit similar trends as that of GPT", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_9450", "chunk_text": "s. The trends discussed in the paper (IRCoT\n_>_ OneR _>_ NoR), we believe, would still hold.\nAdditionally, all our experiments using Flan-T5-*,\nwhich exhibit similar trends as that of GPT3, will\nremain reproducible, thanks to its publicly available model weights.\n\n\n**Ethical Considerations**\n\n\nLanguage models are known to hallucinate incorrect and potentially biased information. This is\nespecially problematic when the questions asked\nto it are of a sensitive nature. While retrieval\naugmented approaches such as ours are expected\nto alleviate this issue to some extent by grounding\ngeneration in external text, this by no means solves\nthe problem of generating biased or offensive statements. Appropriate care should thus be taken if\ndeploying such systems in user-facing applications.\nAll the datasets and models used in this work\n\nare publicly available with permissible licenses.\nHotpotQA has CC BY-SA 4.0 license [15], 2WikiMultihopQA has Apache-2.0 license [16], MuSiQUe\nand IIRC have CC BY 4.0 license [17], and Flan-T5-*\nmodels have Apache-2.0 license.\n\n\n**Acknowledgments**\n\n\nWe thank the reviewers for their valuable feedback\n\nand suggestions. We also thank OpenAI for providing access to the code-davinci-002 API. This material is based on research supported in part by the\nAir Force Research Laboratory (AFRL), DARPA,\nfor the KAIROS program under agreement number\nFA8750-19-2-1003, in part by the National Science\nFoundation under the award IIS #2007290, and in\npart by an award from the Stony Brook Trustees\nFaculty Awards Program.\n\n\n**References**\n\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\n[Richard Socher, and Caiming Xiong. 2020. Learning](https://openreview.net/forum?id=SJgVHkrYDH)\n[to retrieve reasoning paths over wikipedia graph for](https://openreview.net/forum?id=SJgVHkrYDH)\n\n\n[15https://creativecommons.org/licenses/by-sa/4.](https://openreview.net/forum?id=SJgVHkrYDH)\n[0/](https://creativecommons.org/licenses/by-sa/4.0/)\n\n[16https://www.apache.org", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_9900", "chunk_text": ".org/licenses/by-sa/4.](https://openreview.net/forum?id=SJgVHkrYDH)\n[0/](https://creativecommons.org/licenses/by-sa/4.0/)\n\n[16https://www.apache.org/licenses/LICENSE-2.0](https://openreview.net/forum?id=SJgVHkrYDH)\n[17https://creativecommons.org/licenses/by/4.0](https://openreview.net/forum?id=SJgVHkrYDH)\n\n\n\n[question answering. In](https://openreview.net/forum?id=SJgVHkrYDH) _International Conference on_\n_Learning Representations_ .\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\n[Elsen, and Laurent Sifre. 2022. Improving language](https://proceedings.mlr.press/v162/borgeaud22a.html)\n[models by retrieving from trillions of tokens. In](https://proceedings.mlr.press/v162/borgeaud22a.html)\n_Proceedings of the 39th International Conference_\n\n_on Machine Learning_, volume 162 of _Proceedings_\n\n_of Machine Learning Research_, pages 2206\u20132240.\nPMLR.\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. _Advances in neural information processing_\n_systems_, 33:1877\u20131901.\n\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda,", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_10350", "chunk_text": "s_, 33:1877\u20131901.\n\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. _arXiv preprint_\n_arXiv:2107.03374_ .\n\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\n_arXiv preprint arXiv:2210.11416_ .\n\n\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\n[and Andrew McCallum. 2019. Multi-step retriever-](https://openreview.net/forum?id=HkfPSh05K7)\n[reader interaction for scalable open-domain question](https://openreview.net/forum?id=HkfPSh05K7)\n[answering. In](https://openreview.net/forum?id=HkfPSh05K7) _International Conference on Learning_\n_Representations_ .\n\n\n[Yair Feldman and Ran El-Yaniv. 2019. Multi-hop para-](https://doi.org/10.18653/v1/P19-1222)\n[graph retrieval for open-domain question answering.](https://doi.org/10.18653/v1/P19-1222)\nIn _Proceedings of the 57th Annual Meeting of the As-_\n_sociation for Computational Linguistics_, pages 2296\u2013\n2309, Florence, Italy. Association for Computational\nLinguistics.\n\n\nJames Ferguson, Matt Gardner, Hannaneh Hajishirzi,\nTushar Khot, and Pradeep Dasigi. 2020. IIRC: A\ndataset of incomplete information reading comprehension questions. In _EMNLP_ .\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\n[and Mingwei Chang. 2020. Retrieval augmented](https://proceedings.mlr.press/v119/guu20a.html)\n[language", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_10800", "chunk_text": "u, Kenton Lee, Zora Tung, Panupong Pasupat,\n[and Mingwei Chang. 2020. Retrieval augmented](https://proceedings.mlr.press/v119/guu20a.html)\n[language model pre-training. In](https://proceedings.mlr.press/v119/guu20a.html) _Proceedings of the_\n_37th International Conference on Machine Learning_,\nvolume 119 of _Proceedings of Machine Learning_\n_Research_, pages 3929\u20133938. PMLR.\n\n\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2022.\nLarge language models are reasoning teachers. _arXiv_\n_preprint arXiv:2212.10071_ .\n\n\nXanh Ho, A. Nguyen, Saku Sugawara, and Akiko\nAizawa. 2020. Constructing a multi-hop qa dataset\nfor comprehensive evaluation of reasoning steps. In\n_COLING_ .\n\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval augmented language models. _arXiv preprint_\n_arXiv:2208.03299_ .\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n[Wen-tau Yih. 2020. Dense passage retrieval for open-](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n[domain question answering. In](https://doi.org/10.18653/v1/2020.emnlp-main.550) _Proceedings of the_\n_2020 Conference on Empirical Methods in Natural_\n_Language Processing (EMNLP)_, pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\n\n\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_11250", "chunk_text": "uke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. RealTime QA: What\u2019s the answer right now?\n_arXiv preprint arXiv:2207.13332_ .\n\n\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n[2021. Baleen: Robust multi-hop reasoning at scale](https://openreview.net/forum?id=Ghk0AJ8XtVx)\n[via condensed retrieval. In](https://openreview.net/forum?id=Ghk0AJ8XtVx) _Advances in Neural Infor-_\n_mation Processing Systems_ .\n\n\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\nDavid Hall, Percy Liang, Christopher Potts, and\n[Matei Zaharia. 2023. Demonstrate-search-predict:](http://arxiv.org/abs/2212.14024)\n[Composing retrieval and language models for](http://arxiv.org/abs/2212.14024)\n[knowledge-intensive NLP.](http://arxiv.org/abs/2212.14024)\n\n\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\n[2022. Decomposed prompting: A modular approach](https://openreview.net/references/pdf?id=Ql68_Tgnpf)\n[for solving complex tasks.](https://openreview.net/references/pdf?id=Ql68_Tgnpf)\n\n\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab[harwal. 2023. Decomposed prompting: A modular](https://openreview.net/forum?id=_nGgzQjzaRy)\n[approach for solving complex tasks. In](https://openreview.net/forum?id=_nGgzQjzaRy) _The Eleventh_\n_International Conference on Learning Representa-_\n_tions_ .\n\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu[taka Matsuo, and Yusuke Iwasawa. 202", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_11700", "chunk_text": "The Eleventh_\n_International Conference on Learning Representa-_\n_tions_ .\n\n\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu[taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-](https://openreview.net/forum?id=6p3AuaHAFiN)\n[guage models are zero-shot reasoners. In](https://openreview.net/forum?id=6p3AuaHAFiN) _ICML 2022_\n_Workshop on Knowledge Retrieval and Language_\n_Models_ .\n\n\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot\nprompting for open-domain question answering.\n_arXiv preprint arXiv:2203.05115_ .\n\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\n[Retrieval-augmented generation for knowledge-](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)\n[intensive nlp tasks. In](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf) _Advances in Neural Infor-_\n_mation Processing Systems_, volume 33, pages 9459\u2013\n9474. Curran Associates, Inc.\n\n\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. _arXiv_\n_preprint arXiv:2212.08410_ .\n\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Web", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_12600", "chunk_text": ", China. Association for Computational Linguistics.\n\n\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and beyond. _Foundations and Trends\u00ae in Information Re-_\n_trieval_, 3(4):333\u2013389.\n\n\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and\nDenny Zhou. 2022. Recitation-augmented language\nmodels. _arXiv preprint arXiv:2210.01296_ .\n\n\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia,\nJason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil\n[Houlsby, and Donald Metzler. 2023. UL2: Unifying](https://openreview.net/forum?id=6ruVLB727MC)\n[language learning paradigms. In](https://openreview.net/forum?id=6ruVLB727MC) _The Eleventh Inter-_\n_national Conference on Learning Representations_ .\n\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition.\n_TACL_, 10:539\u2013554.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\n[and Denny Zhou. 2022. Chain of thought prompt-](https://openreview.net/forum?id=_VjQlMeSB_J)\n[ing elicits reasoning in large language models. In](https://openreview.net/forum?id=_VjQlMeSB_J)\n_Advances in Neural Information Processing Systems_ .\n\n\nWenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick\nLewis, William Yang Wang, Yashar Mehdad, Scott\nYih, Sebastian Riedel, Douwe Kiela, and Barlas\n[Oguz. 2021. Answering complex open-domain ques-](https://openreview.net/forum?id=EMHoBG0avc1)\n[t", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_13050", "chunk_text": "ih, Sebastian Riedel, Douwe Kiela, and Barlas\n[Oguz. 2021. Answering complex open-domain ques-](https://openreview.net/forum?id=EMHoBG0avc1)\n[tions with multi-hop dense retrieval. In](https://openreview.net/forum?id=EMHoBG0avc1) _International_\n_Conference on Learning Representations_ .\n\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answering. In _EMNLP_ .\n\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReAct: Synergizing reasoning and acting in language\nmodels. _arXiv preprint arXiv:2210.03629_ .\n\n\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. [Generate](https://openreview.net/forum?id=fB0hRu9GZUS)\n[rather than retrieve: Large language models are](https://openreview.net/forum?id=fB0hRu9GZUS)\n[strong context generators. In](https://openreview.net/forum?id=fB0hRu9GZUS) _The Eleventh Inter-_\n_national Conference on Learning Representations_ .\n\n\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\nRetrieving and reading: A comprehensive survey on\n\n - pen-domain question answering. _arXiv preprint_\n_arXiv:2101.00774_ .\n\n\n**A** **Constructing Retrieval Corpora**\n\n\nHotpotQA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultih", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_13500", "chunk_text": "QA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20\nparagraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these\ndatasets into an open-domain setting, we make two\ncorpora, one for each dataset, by combining all\nsupporting and non-supporting paragraphs for all\nits questions in the train, development, and test\nsets. IIRC is originally a mix between reading\ncomprehension and an open-domain setting. Each\nquestion is grounded in one main paragraph, which\ncontains links to multiple Wikipedia pages with\nseveral paragraphs each. We create a corpus out\n\n- f all the paragraphs from all the Wikipedia pages\npresent in the dataset. [18] We do assume the availability of the main passage which doesn\u2019t need\nto be retrieved and is always present. We don\u2019t\nassume the availability of Wikipedia links in the\nmain passage, however, to keep the retrieval problem challenging. [19]\n\n\n**B** **Special Handling of Models for IIRC**\n\n\nIIRC is slightly different from the other datasets,\nin that the question is grounded in the main passage and other supporting paragraphs come from\nthe Wikipedia pages of entities mentioned in this\npassage. We modify the retrievers and readers to\naccount for this difference: (i) We always keep the\nmain passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all\nthe retrieval methods, we first prompt the model to\ngenerate a list of Wikipedia page titles using the\nmain passage and the question. We map these generated titles to the nearest Wikipedia page titles in\nthe corpus (found using BM25), and then the rest\n\n- f the paragraph retrieval queries are scoped within\n\n- nly those Wikipedia pages.\nTo prompt the model to generate Wikipedia page\ntitles using the main passage and the question for\n\n\n18Following are the corpus sizes for the datasets: HotpotQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_13950", "chunk_text": "QA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e., the majority of supporting paragraphs are always within the first few positions of\nthe Wikipedia page. To keep the retrieval problem challenging\nenough we shuffle the paragraphs before indexing the corpus,\ni.e., we don\u2019t use positional information in any way.\n\n\n\nIIRC, we use the following template.\n\n\nWikipedia Title: <Main Page Title>\n<Main Paragraph Text>\n\n\nQ: The question is: '<Question>'. Generate titles\n\n- f <N> Wikipedia pages that have relevant\ninformation to answer this question.\nA: [\"<Title-1>\", \"<Title-2>\", ...]\n\n\nFor \u201ctraining\u201d, i.e., for demonstrations, N ( _\u2264_ 3)\nis the number of supporting Wikipedia page titles\nfor the question. At test time, since the number\n\n- f supporting page titles is unknown, we use a\nfixed value of 3. We found this trick of prompting\nthe model to generate more titles at the test time\nimproves its recall over letting the model decide by\nitself how many titles to generate.\n\n\n**C** **Comparison with Previous Systems for**\n**ODQA with LLMs**\n\n\nWe showed a leaderboard-style comparison with\nprevious approaches to using large language models for open-domain QA in \u00a7 5. We noted though\nthat the comparison is not head-to-head given vari\n- us differences. We briefly describe each method\nand the differences in API, LLM, retrieval corpus,\nand other choices here.\n\nInternet-Augmented QA (Lazaridou et al., 2022)\ndoes (one-step) Google Search retrieval, performs\nadditional LLM-based filtering on it, and then\nprompts an LLM to answer the question using\nthe resulting context. It uses the Gopher 280B\nlanguage model. RECITE (Sun et al., 2022) bypasses the retrieval and instead prompts an LLM\nto first generate (recite) one or several relevant passages from its own memory, and generate the answer conditioned on this generation. They experiment with many LLMs, the highest performing\n\n- f which is code-davinci-002 which we report\nhere. ReAct (Yao et al., 2022", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_14400", "chunk_text": " memory, and generate the answer conditioned on this generation. They experiment with many LLMs, the highest performing\n\n- f which is code-davinci-002 which we report\nhere. ReAct (Yao et al., 2022) prompts LLMs to\nproduce reasoning and action traces where actions\nare calls to a Wikipedia API to return the summary for a given Wikipedia page title. It uses\nthe PALM 540B model. SelfAsk (Press et al.,\n2022) prompts LLMs to decompose a question\ninto subquestions and answers these subquestions\nby issuing separate calls to the Google Search\nAPI. It uses the GPT3 (text-davinci-002) model.\nFinally, DecomP (Khot et al., 2023) is a general framework that decomposes a task and delegates sub-tasks to appropriate sub-models. Similar to our system, it uses BM25 Search and the\nGPT3 (code-davinci-002) model. And lastly,\n\n\nModel HpQA [Br] HpQA 2WikiMQA MQ [2H] MQ\n\n\nInterAug (Lazaridou et al., 2022) _\u2212_ | _\u2212_ 30.3 | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\nRECITE (Sun et al., 2022) _\u2212_ | _\u2212_ 37.1 | 48.4 _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\nReAct (Yao et al., 2022) _\u2212_ | _\u2212_ 35.1 | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\nSelfAsk (Press et al., 2022) _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ 40.1 | _\u2212_ 15.2 | _\u2212_ _\u2212_ | _\u2212_\nDecomP (Khot et al., 2022) _\u2212_ | 50.0 _\u2212_ | _\u2212_ _\u2212_ | 59.3 _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\n\nDecomP (Khot et al., 2023) * _\u2212_ | _\u2212_ _", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_14850", "chunk_text": "_ | _\u2212_ _\u2212_ | 59.3 _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\n\nDecomP (Khot et al., 2023) * _\u2212_ | _\u2212_ _\u2212_ | 53.5 _\u2212_ | **70.8** _\u2212_ | _\u2212_ _\u2212_ | 30.9\nDSP (Khattab et al., 2023) * _\u2212_ | _\u2212_ **51.4 | 62.9** _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\n\nIRCoT QA (ours) **45.8 | 58.5** 49.3 | 60.7 57.7 | 68.0 **34.2 | 43.8** **26.5 | 36.5**\n\n\nTable 3: Extended comparison with published LLM-based ODQA systems (as of May 25, 2023) on EM and\n\n_\u2212_\nF1 scores (with new numbers marked with *). \u2018 \u2019: score is unavailable. HpQA [Br] : Bridge questions subset of\nHotpotQA. MQ [2H] : MuSiQue 2-hop questions. IRCoT remains SOTA for MuSiQue and is close to SOTA for\nHotpotQA and 2WikiMultihopQA. Note the comparisons here are not head-to-head as discussed in the text.\n\n\nFlan-T5-XXL GPT3\n\n\nModel HotpotQA 2WikiMQA MuSiQue IIRC HotpotQA 2WikiMQA MuSiQue IIRC\n\n\nDirect **25.3** _\u00b1_ **0.3** **32.7** _\u00b1_ **0.3** **13.7** _\u00b1_ **0.3** **28.9** _\u00b1_ **0.3** 41.0 _\u00b1_ 1.1 38.5 _\u00b1_ 1.1 19.0 _\u00b1_ 1.2 40.9 _\u00b1_ 0.7\nZeroR QA\nCoT 22.9 _\u00b1_ 0.1 31.7 _\u00b1_ 1.5 10.3 _\u00b1_ 0.5 24.4 _\u00b1_ 0.", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_15300", "chunk_text": "\nZeroR QA\nCoT 22.9 _\u00b1_ 0.1 31.7 _\u00b1_ 1.5 10.3 _\u00b1_ 0.5 24.4 _\u00b1_ 0.1 **47.5** _\u00b1_ **0.4** **41.2** _\u00b1_ **1.0** **25.2** _\u00b1_ **1.2** **52.1** _\u00b1_ **0.1**\n\n\nDirect **49.7** _\u00b1_ **0.5** **51.2** _\u00b1_ **0.3** **25.8** _\u00b1_ **0.6** **40.0** _\u00b1_ **1.3** 50.7 _\u00b1_ 0.1 46.4 _\u00b1_ 2.9 20.4 _\u00b1_ 0.3 40.1 _\u00b1_ 0.9\nOneR QA\nCoT 43.1 _\u00b1_ 0.7 47.8 _\u00b1_ 0.9 17.6 _\u00b1_ 0.2 34.5 _\u00b1_ 1.5 **53.6** _\u00b1_ **0.7** **54.8** _\u00b1_ **2.1** **29.4** _\u00b1_ **0.8** **49.8** _\u00b1_ **2.3**\n\n\nDirect **59.1** _\u00b1_ **0.9** **66.5** _\u00b1_ **1.4** **30.8** _\u00b1_ **0.2** **42.5** _\u00b1_ **2.1** 60.6 _\u00b1_ 1.0 63.5 _\u00b1_ 2.7 36.0 _\u00b1_ 0.5 47.9 _\u00b1_ 2.3\nIRCoT QA\nCoT 52.0 _\u00b1_ 0.6 55.1 _\u00b1_ 1.0 24.9 _\u00b1_ 1.0 36.5 _\u00b1_ 1.3 **60.7** _\u00b1_ **1.1** **68.0** _\u00b1_ **1.5** **36.5** _\u00b1_ **1.2** **49.9** _\u00b1", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_15750", "chunk_text": " 1.3 **60.7** _\u00b1_ **1.1** **68.0** _\u00b1_ **1.5** **36.5** _\u00b1_ **1.2** **49.9** _\u00b1_ **1.1**\n\n\nTable 4: Answer F1 for different ODQA models made from NoR, One and IRCoT retrievals, and Direct and\nCoT prompting readers. For Flan-T5-XXL, Direct prompting is a better choice for the reader, and for GPT3, CoT\nprompting is a better choice for the reader. Hence, we make different reader choices for Flan-T5 and GPT3 for the\nexperiments in the main paper. Note that IRCoT QA _>_ OneR QA _>_ ZeroR QA holds up regardless of this choice.\n\n\n\nDSP (Khattab et al., 2023) provides a way to programmatically define interactions between LLM\nand retrieval for ODQA (e.g., via question decomposition), bootstrap demonstrations for such a program, and use them to make the answer prediction.\nIt uses GPT3.5 LLM with ColBERT-based retrieval.\n\nSince most of these methods use different knowl\nedge sources or APIs and are built using different\nLLMs and retrieval models, it\u2019s difficult to make a\nfair scientific comparison across these systems. Additionally, the evaluations in the respective papers\nare on different random subsets (from the same\ndistribution) of test instances.\n\n\nDespite these differences, it is still informative to\nexplore, in a leaderboard-style fashion, how IRCoT\nperforms relative to the best numbers published\nfor these recent systems. Table 3 shows results\nfrom different systems, including contemporane\n- us and newer numbers. The two new systems in\nthis table (relative to Table 1) are DecomP (newer\nversion) and DSP. While IRCoT remains SOTA on\nMuSiQue, DSP outperforms it on HotpotQA by 2.0\npoints and the newer version of Decomp outperforms IRCoT on 2WikiMultihopQA by 2.8 points.\n\n\n\nWe speculate DecomP performs well on 2WikiMultihopQA because it has only a few easy-to-predict\ndecomposition patterns, which DecomP\u2019s question\ndecomposition can leverage. The lack of such patterns in Hotpot", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_16200", "chunk_text": " points.\n\n\n\nWe speculate DecomP performs well on 2WikiMultihopQA because it has only a few easy-to-predict\ndecomposition patterns, which DecomP\u2019s question\ndecomposition can leverage. The lack of such patterns in HotpotQA and MuSiQue causes it to underperform compared to IRCoT. Lastly, it will be\nuseful to assess whether DSP, which is hardcoded\nfor 2-hop questions like that of HotpotQA, will\nwork well for a dataset with a varied number of\n\nhops like that of MuSiQue. We leave this further\ninvestigation to future work.\n\n\n**D** **Additional CoT Generation Examples**\n\n\nTable 5 provides illustrations, in addition to the\n\n- nes provided in Table 2, for how the CoT generations for NoR QA, OneR QA, and IRCoT QA\nmethods vary. This gives an insight into how IRCoT improves QA performance. Since NoR relies completely on parametric knowledge, it often\nmakes a factual error in the first sentence, which derails the full reasoning chain. Some of this factual\ninformation can be fixed by OneR, especially information closest to the question (i.e., can be retrieved\nusing the question). This is insufficient for fixing\n\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nWhat was the\n\nisland, - n\nwhich Marinelli\n\nGlacier is located, formerly\nknown as?\n\n\nWho is the\n\nchild - f the\ndirector of film\nMukhyamantri\n(1996 Film)?\n\n\nWhen did the\nGovernorship\nend - f the\ncity where the\nauthor of Sac\nerdotii Nostri\n\nPrimordia died?\n\n\n\n_\u21d2_ Marinelli Glacier is\n\nlocated on the island of\nTierra del Fuego.\n_\u21d2_ The island of Tierra\ndel Fuego was formerly\nknown as Isla de Xativa.\n\n\n_\u21d2_ The director - f\nMukhyamantri (1996\nfilm) is Anjan Choudhury.\n_\u21d2_ Anjan Choudhury\nhas a child named\nSandip Choudhury.\n\n\n_\u21d2_ Sacerdotii Nostri Primordia was written by\nPope John XXIII.\n_\u21d2_ Pope", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_16650", "chunk_text": " Anjan Choudhury\nhas a child named\nSandip Choudhury.\n\n\n_\u21d2_ Sacerdotii Nostri Primordia was written by\nPope John XXIII.\n_\u21d2_ Pope John XXIII\ndied in Vatican City.\n_\u21d2_ The Governorship of\nVatican City ended in\n1952.\n\n\n\nMarinelli Glacier is\n\nlocated on the island of\n\nGraham Land. Graham\nLand was formerly\nknown as Graham\u2019s\n\nLand. So the answer is:\n\nGraham\u2019s Land.\n\n\nThe film Mukhyamantri\nwas directed by S. V. Rajendra Singh Babu. S.\nV. Rajendra Singh Babu\nhas a child named S.\nV. Rajendra Singh Babu.\nSo the answer is: S. V.\nRajendra Singh Babu.\n\n\nThe author of Sacer\ndotii Nostri Primordia is\nPope Pius IX. Pope Pius\nIX died in the city of\nRome. The Governorship of Rome ended in\n1870. So the answer is:\n\n1870.\n\n\n\nMarinelli Glacier is lo\ncated on Isla Grande\nde Tierra del Fuego.\nIsla Grande de Tierra\ndel Fuego was formerly\nknown as Tierra del\nFuego. So the answer\nis: Tierra del Fuego.\n\n\nMukhyamantri (1996\nfilm) was directed\nby Anjan Choudhury.\nAnjan Choudhury has a\nchild named Aniruddha\nChoudhury. So the\nanswer is: Aniruddha\nChoudhury.\n\n\nSacerdotii Nostri Primordia was written by\nPope John XXIII. Pope\nJohn XXIII died in the\ncity of Rome. The\nGovernorship of Rome\nended in 1870. So the\n\nanswer is: 1870.\n\n\n\nMarinelli Glacier is lo\ncated on the island of\nTierra del Fuego. The island of Tierra del Fuego\nwas formerly known as\nIsla de Xativa. So the\n\nanswer is: Isla de Xa\ntiva.\n\n\nThe director of Mukhyamantri (1996 film)\nis Anjan Choudhury.\nAnjan Choudhury has\na child named Sandip\nChoudhury. So the", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_17100", "chunk_text": " Isla de Xa\ntiva.\n\n\nThe director of Mukhyamantri (1996 film)\nis Anjan Choudhury.\nAnjan Choudhury has\na child named Sandip\nChoudhury. So the\nanswer is: Sandip\nChoudhury.\n\n\nSacerdotii Nostri Primordia was written by\nPope John XXIII. Pope\nJohn XXIII died in Vatican City. The Governorship of Vatican City\nended in 1952. So the\n\nanswer is: 1952.\n\n\n\nTable 5: Additional CoTs generated by GPT3 with different methods. ZeroR is most prone to factual errors. OneR\n\n- ften fixes some of the factual information which is closest to the question but doesn\u2019t always fix it all the way.\nSince IRCoT retrieves after each step, it can also fix the errors at each step. More examples are in Table 2.\n\n\n\nall the mistakes. Since IRCoT involves retrieval\n\nafter each step, it can fix errors at each step.\n\n\n**E** **Direct vs CoT Prompting Readers**\n\n\nTable 4 compares reader choice (Direct vs CoT\nPrompting) for Flan-T5-XXL and GPT3. We find\nthat Flan-T5-XXL works better with Direct Prompting as a reader and GPT3 works better with CoT\nPrompting as a reader. Therefore, for the experiments in the main paper, we go with this choice.\nNote though that the trends discussed in \u00a7 5 (IRCoT\nQA _>_ OneR QA _>_ ZeroR QA) hold regardless of\nthe choice of the reader.\n\n\n**F** **Separate Reader in IRCoT QA**\n\n\nIRCoT, by construction, produces a CoT as a part\n\n- f its retrieval process. So, instead of having a separate post-hoc reader, one can also just extract the\nanswer from the CoT generated during retrieval.\nAs Table 6 shows the effect of such an ablation.\n\nFor Flan-T5-XXL having a separate reader is significantly better. For GPT3, this is not always true,\nbut at least a model with a separate reader is always better or close to the one without. So overall\nwe go with the choice of using the reader for the\nexperiments in this paper.\n\n\n\nModel HotpotQA 2WikiMQA", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_17550", "chunk_text": ",\nbut at least a model with a separate reader is always better or close to the one without. So overall\nwe go with the choice of using the reader for the\nexperiments in this paper.\n\n\n\nModel HotpotQA 2WikiMQA MuSiQue IIRC\n\n\nIRCoT QA **59.1** _\u00b1_ **0.9** **66.5** _\u00b1_ **1.4** **30.8** _\u00b1_ **0.2 42.5** _\u00b1_ **2.1**\nw/o reader 52.6 _\u00b1_ 0.3 60.9 _\u00b1_ 0.6 24.9 _\u00b1_ 0.2 40.3 _\u00b1_ 0.2\n\n\nIRCoT QA 60.7 _\u00b1_ 1.1 68.0 _\u00b1_ 1.5 **36.5** _\u00b1_ **1.2 49.9** _\u00b1_ **1.1**\nw/o reader **61.0** _\u00b1_ **0.7** **70.4** _\u00b1_ **1.5** 31.5 _\u00b1_ 0.6 48.4 _\u00b1_ 1.0\n\n\nTable 6: Answer F1 of IRCoT QA with and without\na separate reader for Flan-T5-XXL (top two rows) and\nGPT3 (bottom two rows). When the reader is not used,\nthe answer is extracted from the CoT generated by\nIRCoT while doing the retrieval. Ablating the reader\nusually hurts the performance.\n\n\n**G** **Prompts**\n\n\nOur manually written chain-of-thought annotations\nfor HotpotQA, 2WikiMultihopQA, MuSiQue, and\nIIRC are given in Listing 1, 2, 3 and 4 respectively. Our prompts for GPT3 CoT Prompting are\nthe same as these, except they have Wikipipedia\nparagraphs on the top of the questions as shown\nin \u00a7 3.1 [20] . Our prompts for GPT3 Direct Prompting are the same as that of CoT prompting, except\nhave the answer after \"A:\" directly. Our prompts\nfor Flan-T5-* are slightly different from that of\nGPT3. For CoT Prompting, we prefix", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_18000", "chunk_text": "ing are the same as that of CoT prompting, except\nhave the answer after \"A:\" directly. Our prompts\nfor Flan-T5-* are slightly different from that of\nGPT3. For CoT Prompting, we prefix the question\n\n\n20We are not showing the paragraphs in the paper for brevity\nbut they can be obtained from the released code.\n\n\nline: \"Q: Answer the following question by reasoning step-by-step. <actual-question>\". For Direct\nPrompting, we prefix the question line: \"Q: Answer\nthe following question. <actual-question>\". We\ndid this to follow Flan-T5-*\u2019s training format and\nfound it to help its CoT generation.\n\n\nListing 1: Chain-of-Thought annotations for HotpotQA.\n\n\nQ: Jeremy Theobald and Christopher Nolan share what profession?\nA: Jeremy Theobald is an actor and producer. Christopher Nolan is a director, producer, and screenwriter. Therefore, they\n\nboth share the profession of being a producer. So the answer is: producer.\n\n\nQ: What film directed by Brian Patrick Butler was inspired by a film directed by F.W. Murnau?\nA: Brian Patrick Butler directed the film The Phantom Hour. The Phantom Hour was inspired by the films such as Nosferatu\n\nand The Cabinet of Dr. Caligari. Of these Nosferatu was directed by F.W. Murnau. So the answer is: The Phantom Hour.\n\n\nQ: How many episodes were in the South Korean television series in which Ryu Hye\u2212young played Bo\u2212ra?\nA: The South Korean television series in which Ryu Hye\u2212young played Bo\u2212ra is Reply 1988. The number of episodes Reply\n\n1988 has is 20. So the answer is: 20.\n\n\nQ: Vertical Limit stars which actor who also played astronaut Alan Shepard in \"The Right Stuff\"?\nA: The actor who played astronaut Alan Shepard in \"The Right Stuff\" is Scott Glenn. The movie Vertical Limit also starred\n\nScott Glenn. So the answer is: Scott Glenn.\n\n\nQ: What was the 2014 population of the city where Lake Wales Medical Center is located?\nA: Lake Wales Medical Center is located in the city of Polk County, Florida. The population of Polk County in 2014 was\n\n15,140. So the answer is: 15,140.\n\n\nQ: Who was born first? Jan", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_18450", "chunk_text": " Lake Wales Medical Center is located in the city of Polk County, Florida. The population of Polk County in 2014 was\n\n15,140. So the answer is: 15,140.\n\n\nQ: Who was born first? Jan de Bont or Raoul Walsh?\nA: Jan de Bont was born on 22 October 1943. Raoul Walsh was born on March 11, 1887. Thus, Raoul Walsh was born the\n\nfirst. So the answer is: Raoul Walsh.\n\n\nQ: In what country was Lost Gravity manufactured?\nA: The Lost Gravity (roller coaster) was manufactured by Mack Rides. Mack Rides is a German company. So the answer is:\n\nGermany.\n\n\nQ: Which of the following had a debut album entitled \"We Have an Emergency\": Hot Hot Heat or The Operation M.D.?\nA: The debut album of the band \"Hot Hot Heat\" was \"Make Up the Breakdown\". The debut album of the band \"The Operation\n\nM.D.\" was \"We Have an Emergency\". So the answer is: The Operation M.D..\n\n\nQ: In which country did this Australian who was detained in Guantanamo Bay detention camp and published \"Guantanamo:\n\nMy Journey\" receive para\u2212military training?\nA: The Australian who was detained in Guantanamo Bay detention camp and published \"Guantanamo: My Journey\" is David\n\nHicks. David Hicks received his para\u2212military training in Afghanistan. So the answer is: Afghanistan.\n\n\nQ: Does The Border Surrender or Unsane have more members?\nA: The Border Surrender band has following members: Keith Austin, Simon Shields, Johnny Manning and Mark Austin. That\n\nis, it has 4 members. Unsane is a trio of 3 members. Thus, The Border Surrender has more members. So the answer is:\nThe Border Surrender.\n\n\nQ: Which band formed first, Sponge Cola or Hurricane No. 1?\nA: Sponge Cola band was formed in 1998. Hurricane No. 1 was formed in 1996. Thus, Hurricane No. 1 band formed the first.\n\nSo the answer is: Hurricane No. 1.\n\n\nQ: James Paris Lee is best known for investing the Lee\u2212Metford rifle and another rifle often referred to by what acronymn?\nA: James Paris Lee is best known for investing the Lee\u2212Metford rifle and Lee\u00e2 A\u00b8SEnfield", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_18900", "chunk_text": " James Paris Lee is best known for investing the Lee\u2212Metford rifle and another rifle often referred to by what acronymn?\nA: James Paris Lee is best known for investing the Lee\u2212Metford rifle and Lee\u00e2 A\u00b8SEnfield series of rifles. Lee\u00e2 [\u02d8] A\u00b8SEnfield is [\u02d8]\n\n   - ften referred to by the acronym of SMLE. So the answer is: SMLE.\n\n\nQ: Who was born first, James D Grant, who uses the pen name of Lee Child, or Bernhard Schlink?\nA: James D Grant, who uses the pen name of Lee Child, was born in 1954. Bernhard Schlink was born in 1944. Thus,\n\nBernhard Schlink was born first. So the answer is: Bernhard Schlink.\n\n\nQ: Which American neo\u2212noir science fiction has Pierce Gagnon starred?\nA: Pierce Gagnon has starred in One Tree Hill, Looper, Wish I Was Here and Extant. Of these, Looper is an American neo\u2212\n\nnoir science fiction. So the answer is: Looper.\n\n\nQ: What year did Edburga of Minster\u2212in\u2212Thanet's father die?\nA: The father of Edburga of Minster\u2212in\u2212Thanet is King Centwine. Centwine died after 685. So the answer is: after 685.\n\n\nQ: Were Lonny and Allure both founded in the 1990s?\nA: Lonny (magazine) was founded in 2009. Allure (magazine) was founded in 1991. Thus, of the two, only Allure was\n\nfounded in 1990s. So the answer is: no.\n\n\nQ: The actor that stars as Joe Proctor on the series \"Power\" also played a character on \"Entourage\" that has what last name?\n\n\nA: The actor that stars as Joe Proctor on the series \"Power\" is Jerry Ferrara. Jerry Ferrara also played a character on Entourage\n\nnamed Turtle Assante. Thus, Turtle Assante's last name is Assante. So the answer is: Assante.\n\n\nQ: Nobody Loves You was written by John Lennon and released on what album that was issued by Apple Records, and was\n\nwritten, recorded, and released during his 18 month separation from Yoko Ono?\nA: The album issued", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_19350", "chunk_text": "ante.\n\n\nQ: Nobody Loves You was written by John Lennon and released on what album that was issued by Apple Records, and was\n\nwritten, recorded, and released during his 18 month separation from Yoko Ono?\nA: The album issued by Apple Records, and written, recorded, and released during John Lennon's 18 month separation from\n\nYoko Ono is Walls and Bridges. Nobody Loves You was written by John Lennon on Walls and Bridges album. So the\nanswer is: Walls and Bridges.\n\n\nQ: How many awards did the \"A Girl Like Me\" singer win at the American Music Awards of 2012?\nA: The singer of \"A Girl Like Me\" singer is Rihanna. In the American Music Awards of 2012, Rihana won one award. So the\n\nanswer is: one.\n\n\nQ: who is older Jeremy Horn or Renato Sobral ?\nA: Jeremy Horn was born on August 25, 1975. Renato Sobral was born on September 7, 1975. Thus, Jeremy Horn is older. So\n\nthe answer is: Jeremy Horn.\n\n\nListing 2: Chain-of-Thought annotations for 2WikiMultihopQA.\n\n\nQ: When did the director of film Hypocrite (Film) die?\nA: The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is: 19 June\n\n2013.\n\n\nQ: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same nationality?\nA: Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David\n\nDhawan's nationality is India. Karl Freund's nationality is Germany. Thus, they do not have the same nationality. So the\nanswer is: no.\n\n\nQ: Are both Kurram Garhi and Trojkrsti located in the same country?\nA: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia. Thus,\n\nthey are not in the same country. So the answer is: no.\n\n\nQ: Who was born first out of Martin Hodge and Ivania Martinich?\nA: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_19800", "chunk_text": " the same country. So the answer is: no.\n\n\nQ: Who was born first out of Martin Hodge and Ivania Martinich?\nA: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was born\n\nfirst. So the answer is: Martin Hodge.\n\n\nQ: Which film came out first, The Night Of Tricks or The Genealogy?\nA: The Night of Tricks was published in the year 1939. The Genealogy was published in the year 1979. Thus, The Night of\n\nTricks came out first. So the answer is: The Night Of Tricks.\n\n\nQ: When did the director of film Laughter In Hell die?\nA: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the answer is:\n\nAugust 25, 1963.\n\n\nQ: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?\nA: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by Frederick de\n\nCordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus, the person\nto die later from the two is Twenty Plus Two. So the answer is: Twenty Plus Two.\n\n\nQ: Who is Boraqchin (Wife Of \u00c3Ugedei)'s father\u2212in\u2212law? [\u02dd]\nA: Boraqchin is married to \u00c3Ugedei Khan. \u00c3 [\u02dd] Ugedei Khan's father is Genghis Khan. Thus, Boraqchin's father\u2212in\u2212law is [\u02dd]\n\nGenghis Khan. So the answer is: Genghis Khan.\n\n\nQ: What is the cause of death of Grand Duke Alexei Alexandrovich Of Russia's mother?\nA: The mother of Grand Duke Alexei Alexandrovich of Russia is Maria Alexandrovna. Maria Alexandrovna died from\n\n\ntuberculosis. So the answer is: tuberculosis.\n\n\nQ: Which film has the director died earlier, When The Mad Aunts Arrive or The Miracle Worker (1962 Film)?\nA: When The Mad Aunts Arrive was directed by Franz Josef Gottlieb.", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_20250", "chunk_text": " the answer is: tuberculosis.\n\n\nQ: Which film has the director died earlier, When The Mad Aunts Arrive or The Miracle Worker (1962 Film)?\nA: When The Mad Aunts Arrive was directed by Franz Josef Gottlieb. The Miracle Worker (1962 film) was directed by\n\nArthur Penn. Franz Josef Gottlieb died on 23 July 2006. Arthur Penn died on September 28, 2010. Thus, of the two, the\ndirector to die earlier is Franz Josef Gottlieb, who directed When The Mad Aunts Arrive. So the answer is: When The\nMad Aunts Arrive.\n\n\nQ: Which album was released earlier, What'S Inside or Cassandra'S Dream (Album)?\nA: What's Inside was released in the year 1995. Cassandra's Dream (album) was released in the year 2008. Thus, of the two,\n\nthe album to release earlier is What's Inside. So the answer is: What's Inside.\n\n\nQ: Are both mountains, Serre Mourene and Monte Galbiga, located in the same country?\nA: Serre Mourene is located in Spain. Monte Galbiga is located in Italy. Thus, the two countries are not located in the same\n\n\ncountry. So the answer is: no.\n\n\nQ: What is the date of birth of the director of film Best Friends (1982 Film)?\nA: The film Best Friends was directed by Norman Jewison. Norman Jewison was born on July 21, 1926. So the answer is:\n\nJuly 21, 1926.\n\n\nQ: Which film has the director born first, Two Weeks With Pay or Chhailla Babu?\nA: Two Weeks with Pay was directed by Maurice Campbell. Chhailla Babu was directed by Joy Mukherjee. Maurice\n\nCampbell was born on November 28, 1919. Joy Mukherjee was born on 24 February 1939. Thus, from the two directors,\n\nChhailla Babu was born first, who directed Two Weeks With Pay. So the answer is: Two Weeks With Pay.\n\n\nQ: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nA: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah has a\n\ngrandchild named Prithvipati Shah. So the answer is:", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_20700", "chunk_text": "alese Royal)?\nA: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah has a\n\ngrandchild named Prithvipati Shah. So the answer is: Prithvipati Shah.\n\n\nQ: When was the director of film P.S. Jerusalem born?\nA: P.S. Jerusalem was directed by Danae Elon. Danae Elon was born on December 23, 1970. So the answer is: December 23,\n\n1970.\n\n\nQ: Which album was released more recently, If I Have to Stand Alone or Answering Machine Music?\nA: If I Have to Stand Alone was published in the year 1991. Answering Machine Music was released in the year 1999. Thus,\n\n   - f the two, the album to release more recently is Answering Machine Music. So the answer is: Answering Machine\nMusic.\n\n\nQ: Where did the director of film Maddalena (1954 Film) die?\nA: The film Maddalena is directed by Augusto Genina. Augusto Genina died in Rome. So the answer is: Rome.\n\n\nQ: When did the director of film The Boy And The Fog die?\nA: The director of The Boy and the Fog is Roberto Gavald\u00c3\u00b8sn. Roberto Gavald\u00c3\u00b8sn died on September 4, 1986. So the answer\n\nis: September 4, 1986.\n\n\nQ: Are the directors of films The Sun of the Sleepless and Nevada (1927 film) both from the same country?\nA: The director of Sun of the Sleepless is Temur Babluani. The director of Nevada (1927 film) is John Waters. John Waters is\n\nfrom the country of America. Temur Babluani is from the country of Georgia. Thus, John Walters and Temur Babluani\nare not from the same country. So the answer is: no.\n\n\nListing 3: Chain-of-Thought annotations for MuSiQue.\n\n\nQ: When did the first large winter carnival take place in the city where CIMI\u2212FM is licensed to broadcast?\nA: CIMI\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So\n\nthe answer is: 1894.\n\n\nQ: When was Neville A. Stanton's employer founded?\nA:", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_21150", "chunk_text": "I\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So\n\nthe answer is: 1894.\n\n\nQ: When was Neville A. Stanton's employer founded?\nA: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862.\n\nSo the answer is: 1862.\n\n\nQ: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to?\nA: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located\n\nin the Prince County. So the answer is: Prince County.\n\n\nQ: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America\u2212\n\nLite: How Imperial Academia Dismantled Our Culture's author?\nA: The author of America\u2212Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was\n\neducated at the Yale University. The city in Connecticut that has the highest number of Zagat\u2212rated restaurants is New\nHaven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale\nHerald.\n\n\nQ: What is the headquarters for the organization who sets the standards for ISO 21500?\nA: The standards for ISO 21500 were set by International Organization for Standardization. The International Organization\n\nfor Standardization has headquarters in Geneva. So the answer is: Geneva.\n\n\nQ: What did the publisher of Banjo\u2212Tooie rely primarily on for its support?\nA: The publisher of Banjo\u2212Tooie is Nintendo. Nintendo relied primarily for its support on first\u2212party games. So the answer is:\n\nfirst\u2212party games.\n\n\nQ: In which county was the birthplace of the Smoke in tha City performer?\nA: The performer of Smoke in tha City is MC Eiht. MC Eiht's birthplace is Compton. Compton is located in the county of Los\n\nAngeles County. So the answer is: Los Angeles County.\n\n\nQ: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?\n\n\nA: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central\n\nJakarta.\n\n\nQ:", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_21600", "chunk_text": " where Guy Shepherdson was born, contains SMA Negeri 68?\n\n\nA: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central\n\nJakarta.\n\n\nQ: When did Britain withdraw from the country containing Hoora?\nA: Hoora is in the country of Bahrain. Britain withdrew from Bahrain in 1971. So the answer is: 1971.\n\n\nQ: Where does the Snake River start, in the state where Lima Mountain is located?\nA: Lima Mountain is located in the state of Minnesota. The snake river in Minnesota starts in southern Aitkin County. So the\n\nanswer is: southern Aitkin County.\n\n\nQ: What shares a border with Rivi\u00c3 [\u00b4] lre\u2212Verte in the province WRSU\u2212FM broadcasts in?\nA: WRSU\u2212FM was licensed to broadcast to New Brunswick. Rivi\u00c3 [\u00b4] lre\u2212Verte, New Brunswick shares border with\n\nEdmundston. So the answer is: Edmundston.\n\n\nQ: When was the state of emergency declared in the country where the Senate is located?\nA: The Senate is in the country of Kenya. The state of emergency was declared in Kenya on 20 October 1952. So the answer\n\nis: 20 October 1952.\n\n\nQ: How long is the US border with the country that borders the state where Finding Dory takes place?\nA: Finding Dory is supposed to take place in California. The country that shares a border with California is Mexico. The\n\nlength of the us border with Mexico is 1,989 mi. So the answer is: 1,989 mi.\n\n\nQ: What genre is the record label of the performer of So Long, See You Tomorrow associated with?\nA: The performer of So Long, See You Tomorrow is Bombay Bicycle Club. The record label of Bombay Bicycle Club is\n\nIsland Records. The genre of Island Records is jazz. So the answer is: jazz.\n\n\nQ: When did the first large winter carnival happen in Olivier Robitaille's place of birth?\nA: Olivier Robitaille was born in Quebec City. The first large winter carnival in Quebec City happened in the 1894. So the\n\nanswer is: 1894.\n\n\nQ: What is the genre of the record label of the band that performed on the Crush Tour?\nA: The Crush Tour is performed by", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_22050", "chunk_text": " winter carnival in Quebec City happened in the 1894. So the\n\nanswer is: 1894.\n\n\nQ: What is the genre of the record label of the band that performed on the Crush Tour?\nA: The Crush Tour is performed by the band Bon Jovi. The record label of Bon Jovi is Island Records. The genre of Island\n\nRecords is jazz. So the answer is: jazz.\n\n\nQ: When was the first railway line constructed between Kotri and the city where Marie Adelaide Leprosy Centre is located?\nA: Marie Adelaide Leprosy Centre is located in Karachi. The first railway line between Kotri and Karachi was constructed in\n\nApril 1858. So the answer is: April 1858.\n\n\nQ: Where is the crying stone found in the country in which Raphael Tuju holds citizenship?\nA: Raphael Tuju is a citizen of Kenya. The crying stone in Kenya is found along the highway towards Kisumu. So the answer\n\nis: along the highway towards Kisumu.\n\n\nQ: When did Britain withdraw from the country where the village of Wadyan is found?\nA: Wadyan is in the country of Bahrain. Britain withdraw from Bahrain in 1971. So the answer is: 1971.\n\n\nQ: How many countries in Pacific National University's continent are recognized by the organization that mediated the truce\n\nending the Iran\u2212Iraq war?\nA: Pacific National University is located in Khabarovsk, Russia Khabarovsk, Russian is in the continent of Asia. The entity\n\nthat mediated the truce which ended the Iran\u2212Iraq War is the UN. The number of member states that UN recognises in\nAsia is 53. So the answer is: 53.\n\n\nListing 4: Chain-of-Thought annotations for IIRC.\n\n\nQ: What is the age difference between the kicker and the quarterback for the Chargers?\nA: The kicker for the Chargers is Nate Kaeding. The quarterback (QB) for the Chargers is Philip Rivers. Nate Kaeding was\n\nborn in the year 1982. Philip Rivers was born in the year 1981. Thus, the age difference between them is of 1 year. So\nthe answer is: 1.\n\n\nQ: How many years was the ship that took the battalion from New South Wales to Ceylon in service?\nA: The ship that took the battalion from New South Wales to Ceylon is General Hewitt.", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_22500", "chunk_text": " answer is: 1.\n\n\nQ: How many years was the ship that took the battalion from New South Wales to Ceylon in service?\nA: The ship that took the battalion from New South Wales to Ceylon is General Hewitt. General Hewitt was launched in\n\nCalcutta in 1811. General Hewitt was sold for a hulk or to be broken up in 1864. So she served for a total of 1864 \u2212\n1811 = 53 years. So the answer is: 53.\n\n\nQ: What year was the theatre that held the 2016 NFL Draft built?\nA: The theatre that held the 2016 NFL Draft is Auditorium Theatre. The Auditorium Theatre was built in 1889. So the answer\n\n\nis: 1889.\n\n\nQ: How long had Milan been established by the year that Nava returned there as a reserve in the first team's defense?\nA: Nava returned to Milan as a reserve in the first team's defense in the year 1990. Milan had been established in the year\n\n\n1899. Thus, Milan had been established for 1990 \u22121899 = 91 years when Milan returned to Milan as a reserve in the\nfirst team's defense. So the answer is: 91.\n\n\nQ: When was the town Scott was born in founded?\nA: Scott was born in the town of Cooksville, Illinois. Cooksville was founded in the year 1882. So the answer is: 1882.\n\n\nQ: In what country did Wright leave the French privateers?\nA: Wright left the French privateers in Bluefield's river. Bluefields is the capital of the South Caribbean Autonomous Region (\n\nRAAS) in the country of Nicaragua. So the answer is: Nicaragua.\n\n\nQ: Who plays the A\u2212Team character that Dr. Hibbert fashioned his hair after?\nA: Dr. Hibbert fashioned his hair after Mr. T from The A\u2212Team. Mr T.'s birthname is Lawrence Tureaud. So the answer is:\n\n\nLawrence Tureaud.\n\n\nQ: How many people attended the conference held near Berlin in January 1942?\nA: The conference held near Berlin in January 1942 is Wannsee Conference. Wannsee Conference was attended by 15 people.\n\nSo the answer is: 15.\n\n\nQ: When did the country Ottwalt went into exile in founded?\nA:", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_22950", "chunk_text": " held near Berlin in January 1942 is Wannsee Conference. Wannsee Conference was attended by 15 people.\n\nSo the answer is: 15.\n\n\nQ: When did the country Ottwalt went into exile in founded?\nA: Ottwalt went into exile in the country of Denmark. Denmark has been inhabited since around 12,500 BC. So the answer is:\n\n12,500 BC.\n\n\nQ: When was the J2 club Uki played for in 2001 founded?\nA: The J2 club that Uki played for is Montedio Yamagata. Montedio Yamagata was founded in 1984. So the answer is: 1984.\n\n\nQ: When was the person who produced A Little Ain't Enough born?\nA: A Little Ain't Enough was produced by Bob Rock. Bob Rock was born on April 19, 1954. So the answer is: April 19, 1954.\n\n\nQ: Which of the schools Fiser is affiliated with was founded first?\nA: The schools that Fiser is affiliated with (1) Academy of Music, University of Zagreb (2) Mozarteum University of Salzburg\n\n(3) Croatian Music Institute orchestra. Academy of Music, University of Zagreb was founded in the year 1829.\nMozarteum University of Salzburg was founded in the year 1841. Croatian Music Institute was founded in the year 1827.\n\nThus, the school founded earliest of these is Croatian Music Institute. So the answer is: Croatian Music Institute.\n\n\nQ: How many casualties were there at the battle that Dearing fought at under Jubal Early?\nA: Under Jubal Early, Dearing fought the First Battle of Bull Run. First Battle of Bull Run has 460 union casualties and 387\n\nconfederate casualties. Thus, in total the First Battle of Bull Run had 460 + 387 = 847 casualties. So the answer is: 847.\n\n\nQ: Which of the two congregations which provided leadership to the Pilgrims was founded first?\nA: The congregations which provided leadership to the Pilgrims are Brownists and Separatist Puritans. Brownist was founded\n\nin 1581. The Separatist Puritans was founded in 1640. Thus, Brownist was founded first. So the answer is: Brownist.\n\n\nQ: How long had the Rock and Roll Hall of Fame been", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_23400", "chunk_text": " founded\n\nin 1581. The Separatist Puritans was founded in 1640. Thus, Brownist was founded first. So the answer is: Brownist.\n\n\nQ: How long had the Rock and Roll Hall of Fame been open when the band was inducted into it?\nA: The band was inducted into Rock and Roll Hall of Fame in the year 2017. Rock and Roll Hall of Fame was established in\n\nthe year of 1983. Thus, Rock and Roll Hall of Fame been open for 2018 \u22121983 = 34 years when the band was inducted\ninto it. So the answer is: 34.\n\n\nQ: Did the Lord Sewer who was appointed at the 1509 coronation live longer than his king?\nA: Lord Sewer who was appointed at the 1509 coronation was Robert Radcliffe, 1st Earl of Sussex. Lord Sever's king in 1509\n\nwas Henry VIII of England. Robert Radcliffe, 1st Earl of Sussex was born in the year 1483, and died in the year 1542.\nSo Robert lived for 1542 \u22121483 = 59 years. Henry VIII of England was born in the year 1491 and died in the year 1547.\n\nSo Henry VIII lived for 1547 \u22121491 = 56 years. Thus, Robert Radcliffe lived longer than Henry VIII. So the answer is:\n\nyes.\n\n\nQ: When was the place near where Manuchar was defeated by Qvarqvare established?\nA: Manuchar was defeated by Qvarqvare near Erzurum. Erzurum was founded during the Urartian period. So the answer is:\n\nUrartian period.\n\n\nQ: What year was the man who implemented the 46 calendar reform born?\nA: The man who implemented the 46 calendar reform is Julius Caesar. Julius Caesar was born in the year 100 BC. So the\n\nanswer is: 100 BC.\n\n\nQ: How many years after the first recorded Tommy John surgery did Scott Baker undergo his?\nA: The first recorded Tommy John surgery happened when it was invented in the year 1974. Scott Baker underwent Tommy\n\nJohn surgery in the year 2012. Thus, Scott Baker underwent Tommy John surgery 2012 \u22121974 = 38 years after it was\nfirst recorded. So the answer is: 38.\n\n\nQ: Which was", "token_count": 500, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2212.10509_cot_retrieval_trivedi:chunk_23850", "chunk_text": " Baker underwent Tommy\n\nJohn surgery in the year 2012. Thus, Scott Baker underwent Tommy John surgery 2012 \u22121974 = 38 years after it was\nfirst recorded. So the answer is: 38.\n\n\nQ: Which was the older of the two players who found the net in the Double\u2212Headed Eagle of the North in the sixth final for\n\nPAOK?\n\n\nA: The two players who found the net in the Double\u2212Headed Eagle of the North in the sixth final for PAOK are Koudas and\n\nMatzourakis. Koudas was born on 23 November 1946. Matzourakis was born on 6 June 1949. Thus, the older person\namong the two is Koudas. So the answer is: Koudas.\n\n\n", "token_count": 166, "metadata": {"arxiv_id": "2212.10509", "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions", "authors": ["Harsh Trivedi", "Niranjan Balasubramanian", "Tushar Khot", "Ashish Sabharwal"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10509v2"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_0", "chunk_text": "Published as a conference paper at ICLR 2025\n\n## SUFFICIENT CONTEXT: A NEW LENS ON RETRIEVAL AUGMENTED GENERATION SYSTEMS\n\n\n\n**Chun-Sung Ferng**\nGoogle\ncsferng@google.com\n\n\n**Cyrus Rashtchian**\nGoogle\ncyroid@google.com\n\n\n\n**Hailey Joren** _[\u2217]_\nUC San Diego\nhjoren@ucsd.edu\n\n\n**Da-Cheng Juan**\nGoogle\ndacheng@google.com\n\n\n\n**Jianyi Zhang** _[\u2020]_\nDuke University\njianyi.zhang@duke.edu\n\n\n**Ankur Taly**\nGoogle\nataly@google.com\n\n\n\nABSTRACT\n\n\nAugmenting LLMs with context leads to improved performance across many\napplications. Despite much research on Retrieval Augmented Generation (RAG)\nsystems, an open question is whether errors arise because LLMs fail to utilize the\ncontext from retrieval or the context itself is insufficient to answer the query. To\nshed light on this, we develop a new notion of sufficient context, along with a\nmethod to classify instances that have enough information to answer the query. We\nthen use sufficient context to analyze several models and datasets. By stratifying\nerrors based on context sufficiency, we find that larger models with higher baseline\nperformance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when\nthe context is sufficient, but often output incorrect answers instead of abstaining\nwhen the context is not. On the other hand, smaller models with lower baseline\nperformance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient\ncontext. We further categorize cases when the context is useful, and improves\naccuracy, even though it does not fully answer the query and the model errs without\nthe context. Building on our findings, we explore ways to reduce hallucinations in\nRAG systems, including a new selective generation method that leverages sufficient\ncontext information for guided abstention. Our method improves the fraction of\ncorrect answers among times where the model responds by 2\u201310% for Gemini,\nGPT, and Gemma. Key findings and the prompts used in our autorater analysis are\n[available on our github.](https://github.com/hljoren/sufficientcontext)\n\n\n1 INTRODUCTION\n\n\nProvid", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_450", "chunk_text": "2\u201310% for Gemini,\nGPT, and Gemma. Key findings and the prompts used in our autorater analysis are\n[available on our github.](https://github.com/hljoren/sufficientcontext)\n\n\n1 INTRODUCTION\n\n\nProviding Large Language Models (LLMs) with additional context, such as in Retrieval Augmented\nGeneration (RAG) systems, has led to major improvements in LLM factuality and verifiability when\nadapting to new domains (Lewis et al., 2020). In the case of open-domain question answering, a\nretrieval model provides context at inference time in the form of snippets or long-form text (Zhu\net al., 2021). Then, the model synthesizes the query along with this added context to generate the\nanswer. Unfortunately, current RAG-based LLMs exhibit many undesirable traits, such as confidently\npredicting the incorrect answer with retrieved evidence (Mishra et al., 2024; Niu et al., 2024; Ru\net al., 2024), being distracted by unrelated information (Cuconasu et al., 2024; Yoran et al., 2024),\nand failing to properly extract answers from long text snippets (Hsieh et al., 2024; Liu et al., 2024).\n\n\nThe ideal outcome is for the LLM to output the correct answer if the provided context contains\nenough information to answer the question when combined with the model\u2019s parametric knowledge.\nOtherwise, the model should abstain from answering and/or ask for more information. One core\nchallenge in achieving this ideal outcome is building models that can use the provided context only\nwhen it helps answer the question correctly. Several works have investigated this issue by evaluating\n\n\n_\u2217_ Work done during an internship at Google.\n\n_\u2020_ Work done during an internship at Google.\n\n\n1\n\n\nPublished as a conference paper at ICLR 2025\n\n\nmodels in the presence of irrelevant information in the context (discussed in Section 2). However,\n\u201crelevant information\u201d can range from directly containing the answer to simply being topically related\nto the question. Even \u201cgolden\u201d or oracle documents in datasets vary in how much information they\nprovide about the query, and whether they directly inform the ground truth answer or not. In other\nwords, while the goal seems to be to understand how LLMs behave when they do or do", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_900", "chunk_text": " documents in datasets vary in how much information they\nprovide about the query, and whether they directly inform the ground truth answer or not. In other\nwords, while the goal seems to be to understand how LLMs behave when they do or do not have\nsufficient information to answer the query, prior work fails to address this head-on.\n\n\nAs our first contribution, we put forth a new notion of sufficient context. We divide instances into two\ncategories based on whether the context provides enough information to construct an answer to the\nquery. The sufficient context designation is a function of an input pair consisting of one question and\nthe associated context. Crucially, it does not require a ground truth answer. Figure 1 shows examples\nand a breakdown of model responses after splitting the data based on sufficient vs. insufficient context.\nTo divide the dataset, we use an LLM-based autorater to classify instances as sufficient or not. Here,\nan _autorater_ is a model that evaluates instances based on a property, e.g., a sufficient context autorater.\n\n\nUsing our sufficient context autorater, we uncover new insights into LLM behavior and into existing\nbenchmark datasets. First, we find models generate incorrect answers on a non-trivial fraction of\ninstances that have sufficient context to answer the query. In other words, open-book QA cannot\nbe solved by improving retrieval alone. Second, when given instances without sufficient context,\nmodels tend to hallucinate more than they abstain, especially for multi-hop questions. This finding\ncomplements prior work, which shows that LLMs are not robust to noisy retrieval (Yoran et al.,\n2024; Wu et al., 2024). Third, models generate correct answers in many cases, even when the\nprovided context is insufficient. Surprisingly, this remains true after we filter out questions that the\nmodel answers correctly in a closed book (w/o RAG) setting. Together, our analysis deepens our\nunderstanding of RAG systems by revealing nuances in how models generate responses with retrieval.\n\n\nAs a final contribution, we explore ways to use sufficient context labels to reduce model hallucinations.\nWe implement a new selective generation framework that improves accuracy. We use a smaller,\nintervention model to determine when the model generates or abstains, providing a controllable\ntrade-off. Moreover, we can combine our method with any LLM, including proprietary models like\nGemini and GPT. Our main result is that using sufficient", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_1350", "chunk_text": "ervention model to determine when the model generates or abstains, providing a controllable\ntrade-off. Moreover, we can combine our method with any LLM, including proprietary models like\nGemini and GPT. Our main result is that using sufficient context as an additional signal leads to\nmuch higher accuracy over the fraction of answered queries, for most coverage levels and across\nmultiple models/datasets. We also find that fine-tuning open-source models with sufficient context\ninformation does not easily reduce the hallucination rate. Instead, for Mistral 3, fine-tuning can lead\nto a higher abstention rate at the cost of fewer correct answers. Key findings and the prompts used in\n\n[our autorater analysis are available on our github.](https://github.com/hljoren/sufficientcontext)\n\n\nTo summarize, our main contributions are\n\n\n1. We define the notion of sufficient context, unifying existing work on relevance for RAG systems.\nThen, we design a sufficient context autorater (achieving 93% accuracy), enabling us to label\ninstances scalably and to analyze model responses with or without sufficient context.\n\n\n2. Our analysis leads to several new findings about retrieval-augmented model performance. One\ntakeaway is that SOTA LLMs output correct responses 35\u201362% of the time with insufficient\ncontext. Hence, intervention strategies to increase accuracy should not solely rely on sufficiency.\n\n\n3. Building on our findings above, we develop an efficient and general method for selective generation,\nusing both confidence and sufficient context signals. Our method improves the fraction of correct\nanswers (among total model responses) by up to 2\u201310% for Gemini, GPT, and Gemma.\n\n\n2 RELATED WORK\n\n\nMany papers have shown that reaping the benefits of RAG (e.g., better factuality) will require a deeper\nunderstanding of how LLMs respond to variations in the queries and provided context (Asai et al.,\n2024; Fan et al., 2024; Ram et al., 2023; Rau et al., 2024). We review two main areas. First, much\nwork has evaluated RAG systems with poor retrieval, uncovering cases where LLMs are led astray by\nirrelevant context. Another line of study aims to reduce LLM hallucinations in RAG settings.\n\n\n**(Ir)relevant Context.** Prior studies uncover a lack of robustness to imperfect retrieval. However,\n", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_1800", "chunk_text": " LLMs are led astray by\nirrelevant context. Another line of study aims to reduce LLM hallucinations in RAG settings.\n\n\n**(Ir)relevant Context.** Prior studies uncover a lack of robustness to imperfect retrieval. However,\nthese studies vary in terms of how they evaluate retrieval quality, without anchoring to a precise\n\u201crelevance\u201d definition. Shi et al. (2023a) adds sentences to math questions (based on GSM8K) which\n\n\n2\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: **New insights into RAG systems by looking at whether instances have sufficient context.**\nOn the left, we show examples of sufficient context; on the right, a breakdown of model responses on\nthe Musique dataset. Adding RAG improves the percentage of correct answers. Unfortunately, with\nRAG, models hallucinate more than abstain, and the insufficiency of the context does not account for\nthis major issue. Also, standard datasets have many instances with insufficient context (here, 55.4%).\nWe include results for other datasets (FreshQA, HotPotQA) in Appendix B, showing similar trends.\n\n\nshould not impact the answer at all (and GSM8K is designed to have sufficient context by definition).\nXie et al. (2024) looks at having counter-memory context, by either replacing the entity name with\nan erroneous one or using an LLM to generate a synthetic context supporting the erroneous entity.\nRet-Robust (Yoran et al., 2024) trains a model to be robust to irrelevant context, with an NLI-based\nentailment to determine relevance, and only uses the relevance scores to influence the training mixture\n\n- f relevant vs. irrelevant documents. Wu et al. (2024) looks at questions where the LLM gets the\nanswer correct without retrieval and is non-robust to changes in the retrieval. Multiple methods use\na model to predict relevance scores (as part of a larger pipeline), without calibration to a formal\ndefinition (Wang et al., 2024a; Zhou et al., 2024), including for iterative retrieval (Jiang et al., 2024;\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\nrelevant documents, but simply uses \u201cdoes not contain the answer\u201d", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_2250", "chunk_text": " et al., 2024;\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\nrelevant documents, but simply uses \u201cdoes not contain the answer\u201d as a proxy for irrelevant context.\n\n\n**Reducing Hallucinations.** There have also been efforts to improve RAG factuality on open-book QA\ntasks (Asai et al., 2023; Mineiro, 2024; Simhi et al., 2024; Wang et al., 2024b; Zhang et al., 2024b).\nThe main theme is to improve both the generation and retrieval quality, often by fine-tuning one or\nmore components. Also, since RAG leads to very long contexts, another issue that arises is the \u201clost\nin the middle\u201d problem (Hsieh et al., 2024; Liu et al., 2024; Yu et al., 2024). These works start with\nthe premise that the provided query/context should be precisely answerable by the LLM, and hence,\n\n- nly analyze their findings in the sufficient context scenario. Independent of RAG, many papers\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang\net al., 2024; Kadavath et al., 2022; Yin et al., 2023; Zhang et al., 2024a) and performance across\ndisaggregated subsets of data (Paes et al., 2022; Joren et al., 2023).\n\n\n3 SUFFICIENT CONTEXT\n\n\nAt a high level, our aim is to classify input instances based on whether the context contains enough\ninformation to answer the query. We split possible contexts into two cases: (1) **Sufficient Context.**\nThe context is sufficient to answer the query if it contains all the necessary information to provide a\ndefinitive answer. (2) **Insufficient Context.** Otherwise, a context is insufficient. A context may also\nbe insufficient if the query requires specialized knowledge that is not provided in the context or if\nthe information in the context is incomplete, inconclusive, or contradictory. In this section, we more\nthoroughly discuss sufficient context. Then, we show how to accurately and scalably label instances.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\n3.1 DEFIN", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_2700", "chunk_text": "clusive, or contradictory. In this section, we more\nthoroughly discuss sufficient context. Then, we show how to accurately and scalably label instances.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\n3.1 DEFINITION OF SUFFICIENT CONTEXT\n\n\nWe first set some notation for a generic open-domain question-answering setting following Trivedi\net al. (2020). Consider a dataset _D_ with instances of the form _q_ = ( _Q, C_ ; _A_ ), where _Q_ is the query\nand _C_ is the context that consists of a set of facts. At inference time, we also consider instances\n_q_ _[\u2032]_ = ( _Q, C_ ) without the ground truth answer, where the goal is to predict an answer _A_ _[\u2032]_ from _Q_, _C_,\nand the model\u2019s parametric knowledge. To measure correctness, we compare _A_ _[\u2032]_ and _A_, where there\nare many options such as exact match, F1 score, or an LLM-based assessment of answer sameness\n(we use an LLM). Using this notation, we can now define our notion of sufficient context.\n\n\n**Definition (Sufficient Context).** An instance _q_ _[\u2032]_ = ( _Q, C_ ) has sufficient context if and only if there\nexists an answer _A_ _[\u2032]_ such that _A_ _[\u2032]_ is a plausible answer to the question _Q_ given the information in _C_ .\n\n\nTo understand this, we can build on the Attributable to Identified Sources (AIS) framework (Rashkin\net al., 2023). Entailment via AIS answers a slightly different question. Namely, given an instance\n_q_ = ( _Q, C_ ; _A_ ), the entailment objective is to determine the truth value of the proposition: The\nanswer to the question _Q_ is _A_ given the information in _C_ . The key difference between entailment\nand sufficient context is that for sufficient context we do not presuppose that we have the answer _A_ _[\u2032]_\nin advance, only that such an answer exists. Finally, we only consider \u201cplausible\u201d answers, where\nwe mean that _A_ _[\u2032]", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_3150", "chunk_text": " do not presuppose that we have the answer _A_ _[\u2032]_\nin advance, only that such an answer exists. Finally, we only consider \u201cplausible\u201d answers, where\nwe mean that _A_ _[\u2032]_ could be an answer to the question _Q_ . For example, if the question asks about a\nperson\u2019s birthplace, then _A_ _[\u2032]_ should be a location. We note that this allows for the possibility that the\ncontext contains an _incorrect_ answer to the question. This is a key requirement, because (i) we would\nlike to be able to use signal from sufficient context at inference time, where we do not have ground\ntruth answers (see Section 5.1) and (ii) we hope to elucidate findings that are robust to ground truth\nlabel noise.\n\n\n**Remark 1 (Multi-hop queries).** In most benchmark dataset (e.g., Musique, HotPotQA), models are\nexpected to be able to do multi-hop reasoning up to four hops, in which they must combining facts to\nform the answer. However, they should not infer connections that are not in the context. For example,\nif \u201cBob\u2019s mother was born in New York\u201d then this does not suffice to say Bob was born in New York.\nBut, if the context also says \u201cBob\u2019s mother is Alice...\u201d and \u201c... all of Alice\u2019s children were born in\nNew York\u201d then this instance has sufficient context.\n\n\n**Remark 2 (Ambiguous queries).** If the query is ambiguous, then the context is sufficient if and\n\n- nly if (i) the context can disambiguate the query and (ii) the context provides an answer to the\ndisambiguated query. For example, the question could be \u201cWhat sport does Mia play?\u201d and the\ncontext could contain both \u201cMia, from New York, plays basketball. . . \u201d and \u201c... Mia, from California,\nplays volleyball.\u201d This is sufficient because if the query is referring to either Mia from New York or\nBob from California, then the context can answer the question.\n\n\n**Remark 3 (Ambiguous contexts).** Assume the context contains multiple plausible answers to the\nquery. Then it is sufficient if and only if it also provides enough information to distinguish between\nqueries that would lead to each answer. For example, if the question is \u201cWhat country", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_3600", "chunk_text": " contexts).** Assume the context contains multiple plausible answers to the\nquery. Then it is sufficient if and only if it also provides enough information to distinguish between\nqueries that would lead to each answer. For example, if the question is \u201cWhat country does Ali live\nin?\u201d and the context is \u201cAli lives in Paris\u201d then this instance does not have sufficient context because\nit is not clear if Ali lives in Paris, France or Paris, Texas, USA. If the context further contains \u201cThis\nweekend, Ali took the train from Paris to Marseille.\u201d Then this becomes sufficient because it is almost\ncertain that Ali lives in France as one cannot take a train from Texas to France.\n\n\n3.2 SUFFICIENT CONTEXT AUTORATER\n\n\nNext, we consider automating the task of labeling whether instances have sufficient context or not.\nWe investigate two questions: (1) Can today\u2019s models achieve high accuracy on a challenging,\nhuman-annotated dataset? (2) How does an entailment model compare to general-purpose LLMs? To\nanswer these questions, we evaluate methods on human-labeled data. Table 1 shows that Gemini 1.5\nPro can serve as an accurate autorater to label instances in terms of sufficient context. It achieves\n93% accuracy, outperforms other methods, and operates without needing a ground truth answer.\n\n\n**Sufficient Context Labeled Dataset.** Using the above definition, we construct gold labels for\neach (query, context) pair. We did not use ground truth answers or model responses. For\nthe instances, we sample a total of 115 instances (queries, contexts, and answers) from standard\nbenchmarks (PopQA, FreshQA, Natural Questions, EntityQuestions). We design the dataset to\nbe very challenging, including single- and multi-hop questions, as well as adding highly related\n\n\n4\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 1: **Sufficient Context AutoRater.** Evaluating model ability to classify sufficient context on a\ngold-labeled dataset of 115 (query, context; answer) instances. Gemini 1.5 Pro (1-shot)\nperforms the best, while FLAMe can be a cheaper alternative. TRUE-NLI and Contains GT need\nground truth (GT) answers, while others only use (query, context). Best in column in bold.\n\n\n**Metrics:** F1 Score Accuracy Precision Recall No GT", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_4050", "chunk_text": " FLAMe can be a cheaper alternative. TRUE-NLI and Contains GT need\nground truth (GT) answers, while others only use (query, context). Best in column in bold.\n\n\n**Metrics:** F1 Score Accuracy Precision Recall No GT Answer\n**Methods**\n\n\nGemini 1.5 Pro (1-shot) **0.935** **0.930** 0.935 **0.935** \u2713\nGemini 1.5 Pro (0-shot) 0.878 0.870 0.885 0.871 \u2713\nFLAMe (fine-tune PaLM 24B) 0.892 0.878 0.853 **0.935** \u2713\nTRUE-NLI (fine-tune T5 11B) 0.818 0.826 **0.938** 0.726\nContains GT 0.810 0.809 0.870 0.758\n\n\ninformation in the context even if it is not sufficient (e.g., a named entity from the question often\nappears in the context). We evaluate methods\u2019 abilities to classify sufficient context (binary labels).\n\n\n**Methods: Operating on Query-Context pairs.** We use Gemini 1.5 Pro with either instructions\n(0-shot) or both instructions and a 1-shot example, held out from our dataset. FLAMe 24B is a\ngeneral autorater model (Vu et al., 2024), but it has a small context window. For FLAMe, we divide\nthe contexts into 1600 token chunks and ask whether each chunk is sufficient. If any chunk is labeled\nsufficient, we consider the instance to have sufficient context; otherwise, it\u2019s labeled as insufficient.\nWe design prompts (in Appendix C) for both models based on the sufficient context definition above.\n\n\n**Methods: When a Ground Truth (GT) Answer is Available.** For two baselines (TRUE-NLI,\nContains GT), we use answers as an additional input to classify sufficient context. TRUE-NLI is a\nfine-tuned entailment model (Honovich et al., 2022) that checks if the context entails one of the GT\nanswers. Contains GT checks if a GT answer appears in the context. Comparing to entailment is\nparticularly interesting because if a given answer _A_ is entailed by ( _Q, C_ ), then the context is", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_4500", "chunk_text": " of the GT\nanswers. Contains GT checks if a GT answer appears in the context. Comparing to entailment is\nparticularly interesting because if a given answer _A_ is entailed by ( _Q, C_ ), then the context is also\nsufficient. On the other hand, the reverse is not true, since the answer _A_ is only one possible choice\nfor _A_ _[\u2032]_ . As one consequence, if the ground truth answer _A_ is incorrect, then it may not be entailed by\nthe context. This happens when a named entity is ambiguous (e.g., two people with the same name),\nand the GT answer is based on one of the people while the context describes the other.\n\n\n**Results.** Table 1 shows that Gemini 1.5 Pro (1-shot) performs the best overall in terms of F1 score\nand accuracy. As expected, TRUE-NLI has higher precision and lower recall: it measures entailment,\nwhich implies sufficient context. FLAMe outperforms TRUE-NLI in F1 and accuracy, but lags behind\nGemini (1-shot), likely because it is a smaller model. The Contains GT method works surprisingly\nwell, indicating that the presence of a ground truth answer correlates with context sufficiency.\n\n\n**Discussion.** In real-world scenarios, we cannot expect candidate answers when evaluating model\nperformance. Hence, it is desirable to use a method that works using only the query and context.\nAmong these methods, Gemini 1.5 Pro (1-shot) has high accuracy and balanced precision and recall.\nTherefore, we use it in Section 4 as our main method for analyzing datasets and model responses.\nLater, in Section 5.1, we use FLAMe as a computationally efficient autorater to provide a signal for\nselective generation. Our comparison with TRUE-NLI and Contains GT confirms that classifying\nsufficient context is a different (and more complex) task than determining entailment.\n\n\n4 A NEW LENS ON RAG PERFORMANCE\n\n\nWe set out to understand RAG performance by looking at sufficient context. We first analyze datasets\n(Section 4.1), then we investigate model performance with/without sufficient context (Section 4.2). We\nqualitatively discuss cases where insufficient context leads to correct model responses (Section 4.3).\n\n\n4.1 DO BENCHMARK DATASETS HAVE HIGH SUFFICIENT CONTEXT?\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_4950", "chunk_text": " with/without sufficient context (Section 4.2). We\nqualitatively discuss cases where insufficient context leads to correct model responses (Section 4.3).\n\n\n4.1 DO BENCHMARK DATASETS HAVE HIGH SUFFICIENT CONTEXT?\n\n\nWe introduce the datasets that we use for our analysis. Then, we investigate the percentage of\ninstances in these datasets that have sufficient context (according to our autorater). For our study, we\ndo not aim to optimize the retrieval methods (which could increase the sufficient context percentage).\n\n\n5\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|100 xt Measuring Sufficiency by Context Length|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_5400", "chunk_text": "~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_5850", "chunk_text": "~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_6300", "chunk_text": "<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens||~~.~~<br>|~~.~~<br>63.7<br>2000 Tokens|~~.~~<br>63.7<br>2000 Tokens|~~.~~<br>63.7<br>2000 Tokens|~~.~~<br>63.7<br>2000 Tokens|~~.~~<br>63.7<br>2000 Tokens|~~.~~<br>63.7<br>2000 Tokens|2000 Tokens|2000 Tokens|2000 Tokens|2000 Tokens|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_6750", "chunk_text": "2000 Tokens|2000 Tokens|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens||||46.2<br>44.6<br>46.2<br>44.6<br>45.4|46.2<br>44.6<br>46.2<br>44.6<br>45.4|46.2<br>44.6<br>46.2<br>44.6<br>45.4|46.2<br>44.6<br>46.2<br>44.6<br>45.4|46.2<br>44.6<br>46.2<br>44.6<br>45.4|46.2<br>44.6<br>46.2<br>44.6<br>45.4|46.2<br>44.6<br>46.2<br>44.6<br>45.4|46.2<br>44.6<br>46.2<br>44.6<br>45.4|46.2<br>44.6<br>46.2<br>44.6<br>45.4|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|||||||||||33.4|33.4|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_7200", "chunk_text": "|\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens|||||||||||||\n|FreshQA<br>HotpotQA<br>Musique<br>0<br>20<br>40<br>60<br>80<br>100<br>% of Dataset with Sufficient Conte<br>~~77.4~~<br>46.2<br>44.6<br>~~77.4~~<br>46.2<br>44.6<br>63.7<br>45.4<br>33.4<br> <br>10000 Tokens<br>6000 Tokens<br>2000 Tokens||||||pot|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|QA<br>Musique|\n\n\nFigure 2: We compare the % of instances that our autorater labels as sufficient across datasets,\neither with the first 10k, 6k, or 2k tokens of the provided sources. FreshQA has hand-curated URLs\nthat support the answers and exhibits high sufficient context. HotPotQA and Musique have lower\nsufficient context (and even lower with 2000 tokens). We use 6000 token contexts in the remainder.\n\n\nThis is not the focus of our work, as we wish to understand how models perform with or without\nsufficient context. Having a mix of both is inevitable in generic RAG systems.\n\n\n**Datasets.** We consider **FreshQA**, **Musique-Ans**, and **HotpotQA** as a representative spread of open\nbook QA datasets. FreshQA (Vu et al., 2023) evaluates time-sensitive information and has up-to-date\nURLs that should support an answer to the queries, which we use to construct the context (see\nAppendix A.3 for details on the retrieval). We use the \u2018True Premise\u2019 setting (452 instances), skipping\n\u2018False Premise\u2019 questions that", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_7650", "chunk_text": " support an answer to the queries, which we use to construct the context (see\nAppendix A.3 for details on the retrieval). We use the \u2018True Premise\u2019 setting (452 instances), skipping\n\u2018False Premise\u2019 questions that mislead by design. Musique-Ans (Trivedi et al., 2022) is a multi-hop\nQA benchmark, created by composing two to four single-hop interconnected questions. Here, \u2018Ans\u2019\nis the standard \u2018answerable\u2019 subset. Musique instances have 20 supporting text snippets as sources,\nwhich we use as the context. HotPotQA (Yang et al., 2018) is a Wikipedia-based QA dataset, with\nsingle- and multi-hop questions. The corpus is a large set of snippets; we retrieve the top 5 as the\ncontext (via REPLUG (Shi et al., 2023b) from FlashRAG (Jin et al., 2024)). We randomly sample\n500 instances from the development sets of Musique-Ans and HotPotQA for evaluation.\n\n\n**Sufficient Context % of Datasets.** Figure 2 shows the fraction of instances that our autorater\nclassifies as having sufficient context. We explore three context lengths, ranging from a maximum\n\n- f 2000 to maximum of 10000 tokens. The motivation behind this is to assess if there is a large\nchange in sufficient context if we were to simply truncate the retrieval (e.g., for models that have\nsmall context windows). In general, we see a modest difference from 2000 to 6000 token limit, but\neffectively none from 6000 to 10000 tokens. FreshQA has the highest sufficient context percentage,\nwhich makes sense as the context comes from oracle supporting documents. The lower sufficient\ncontext in Musique is perhaps surprising, given that the retrieval is fixed as part of the dataset. From\nthe results in Figure 2, we truncate at 6000 tokens for all three datasets in the remainder of the paper.\n\n\n4.2 INITIAL FINDINGS BASED ON SUFFICIENT CONTEXT\n\n\nIn general, the ideal behavior for a language generation model is to answer questions correctly when\npossible and to otherwise abstain. RAG seeks to move models towards this desired behavior, such\nthat the provided context shifts hallucinations to correct answers, or to abstentions if needed. We\nanalyze several cases to assess how far we are", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_8100", "chunk_text": "\npossible and to otherwise abstain. RAG seeks to move models towards this desired behavior, such\nthat the provided context shifts hallucinations to correct answers, or to abstentions if needed. We\nanalyze several cases to assess how far we are from this ideal trade-off.\n\n\n**Experimental Set-up and LLMEval.** We employed a basic chain of thought (CoT) prompting\napproach, with the prompt structure and further information detailed in Appendix C.4. We then\nprocessed the outputted answers to identify matches between the response and any of the ground truth\nanswers. Responses where a clear correct match could not be determined were processed through\nthe LLMEval pipeline using a zero-shot approach, with the prompt based on Krishna et al. (2024)\n(see Appendix C.3). Then, for each example, we can rate it as \u201ccorrect\u201d or \u201cabstain\u201d or \u201challucinate\u201d\ndepending on the LLMEval output. We use an LLM for evaluation instead of checking for an exact\nmatch because it is more robust to syntactic variations. See Appendix B.3 for details and examples.\n\n\n**Models Abstain Less with RAG.** While overall performance improves with RAG, the introduction\n\n- f additional context paradoxically reduces the model\u2019s ability to abstain from answering when\n\n\n6\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n\n\n\n\n\n|Gemini 1.5 Pro GPT 4o Claude 3.5 Sonnet Gemma 27B Categorizing RAG Responses: Sufficient vs Insufficient Context FreshQA Musique HotpotQA|Col2|Col3|Col4|Col5|Col6|Gemma 27B HotpotQA|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Ab", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_8550", "chunk_text": "alluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_9000", "chunk_text": ">Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B||||||||\n|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Ab", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_9450", "chunk_text": "<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_9900", "chunk_text": ">Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B||||||||\n|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_10350", "chunk_text": "<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_10800", "chunk_text": "Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B||||||||\n|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_11250", "chunk_text": "<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B||||||||\n|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_11700", "chunk_text": "laude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B||||||||\n|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_12150", "chunk_text": "stain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B||||||||\n|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_12600", "chunk_text": ".<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_13050", "chunk_text": ".5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Abstain<br>Halluc.<br>Claude 3.5 Sonnet<br>Correct<br>Abstain<br>Halluc.<br>Gemma 27B||||||||\n||||||||||||||\n|Co|Co|rrect<br>Abs|rrect<br>Abs|rrect<br>Abs|tain<br>Ha|tain<br>Ha|ct<br>A|bsta|in<br>Hall|in<br>Hall|uc.|uc.|\n\n\n|Correct Abstain Halluc. GPT 4o|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||\n|||||||||||||\n|||||||||||||\n|||rrect<br>Ab|rrect<br>Ab|rrect<br>Ab|rrect<br>Ab|sta|in<br>Hal|in<br>Hal|rect<br>Abst|rect<br>Abst|ain<br>Hall|\n\n\n\nFigure 3: **Model Performance on Datasets Stratified by", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_13500", "chunk_text": "Ab|rrect<br>Ab|sta|in<br>Hal|in<br>Hal|rect<br>Abst|rect<br>Abst|ain<br>Hall|\n\n\n\nFigure 3: **Model Performance on Datasets Stratified by Sufficient Context.** Given sufficient\ncontext, models have a higher correct percentage on these challenging datasets. Performance drops,\nbut the models are still able to answer a large portion of questions correct without sufficient context.\nOne prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient\ncontext. The smallest model Gemma 27B struggles to avoid hallucinations given insufficient context.\n\n\nappropriate. Without RAG, Claude 3.5 Sonnet abstains on 84.1% questions, while with RAG, the\nfraction of abstentions drops to 52%. Similarly, GPT 4o\u2019s abstention fraction moves from 34.4%\nto 31.2% and Gemini 1.5 Pro\u2019s drops from 100% to 18.6%. This phenomenon may arise from the\nmodel\u2019s increased confidence in the presence of any contextual information, leading to a higher\npropensity for hallucination rather than abstention.\n\n\n**Models Hallucinate with Both Sufficient and Insufficient Context.** Considering Figure 3, models\ngenerally achieve higher accuracy with sufficient context (higher **green bars**, top row) than without\nsufficient context (lower **green bars**, bottom row). However, looking at each row separately, we\ndiscover several findings. First, in the sufficient context case (top row), we see that models hallucinate\nmore than they abstain ( **red bars** are higher than **blue bars**, usually). The trend holds across all\nthree datasets. Moving to insufficient context (bottom row), we find a different distribution of model\nresponses, with more abstentions and hallucinations. This tendency varies notably across different\nmodels. For instance, Claude abstains more (higher **blue bars** ) with insufficient context, but answers\nfewer questions correctly (lower **green bars** ) than Gemini and GPT. These differences underscore\nthe potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\nmuch more hallucinations (higher **red bars** ) than the other models, except for HotPotQA, where we\nattribute the higher accuracy to the smaller retrieved contexts.\n\n\n4.3 QUALITATIVELY ANALYZING", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_13950", "chunk_text": " Gemma has\nmuch more hallucinations (higher **red bars** ) than the other models, except for HotPotQA, where we\nattribute the higher accuracy to the smaller retrieved contexts.\n\n\n4.3 QUALITATIVELY ANALYZING RESPONSES WITH INSUFFICIENT CONTEXT\n\n\nOne curious observation in our analysis is the ability of models to sometimes provide correct answers\neven when presented with insufficient context. For example, from Figure 3, all three models are able\nto correctly answer upwards of 35% of instances with insufficient context on HotpotQA. A natural\nassumption is that the models already know the answer from pre-training, and they can generate a\ncorrect response from parametric memory. However, this only explains part of the story.\n\n\nLooking deeper, we provide a qualitative categorization in Table 2 of instance types where our\nautorater labels an instance as insufficient context, while the LLM evaluator marks the model answer\n\n\n7\n\n\nPublished as a conference paper at ICLR 2025\n\n\n**Instance type** **Why model may be correct** **Example**\n\n\nYes/No question 50% chance of correct **Q:** Is there a total eclipse in the United States this year?\n\n\nLimited choice Some chance of correct **Q:** Which band has more members, Chvrches or\nGoodbye Mr. Mackenzie?\n\n\n\nMulti-hop: fragment Use parametric inference\n\n\nMulti-hop: partial Use parametric knowledge\n\n\nToo many hops Execute complex reasoning\n\n\nAmbiguous query Guess right interpretation\n\n\n\n**Q:** Who did the original voice for the character\nwhose series Mickey\u2019s Safari in Letterland is from?\n_Context says Mickey\u2019s Safari is a video game_\n_and Walt Disney voices Mickey Mouse in cartoons._\n_Must infer the game is in the Mickey Mouse series._\n\n\n**Q:** Claudine\u2019s Return starred the actress who played\nwhich role on \u201cMarried...with Children\u201d?\n\n_Context lists actresses but not their roles in_\n_\u201cMarried...with Children\u201d. Must know extra facts._\n\n\n**Q:** How many cyclists have won all three of women\u2019s\ncycling Grand Tours equivalents in the same year?\n_Context requires cross-referencing lists of events_\n_and lists of winners while tracking winners by year._\n\n\n**Q:** Who is the spouse of a cast member from King\n\n- f the Mountain?\n\n_Context has many cast members and query/context do_\n_not specify which spouse to answer about._\n\n\n\nRater error Mislabel insuff. or correct", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_14400", "chunk_text": "._\n\n\n**Q:** Who is the spouse of a cast member from King\n\n- f the Mountain?\n\n_Context has many cast members and query/context do_\n_not specify which spouse to answer about._\n\n\n\nRater error Mislabel insuff. or correct  \n\nClosed-book correct Known from pre-training  \n\nTable 2: **Qualitative Analysis of Correct Answer & Insufficient Context.** Examining model\nresponses across datasets, we identify common cases where the model generates a correct answer\neven though our autorater labels the instance as insufficient. We categorize such instances into eight\ntypes, as well as provide examples. Given that models are also correct on many questions in the\nclosed-book setting, we believe this mostly explains the 35\u201362% correct rate with insufficient context.\n\n\nas correct. For example, one type accounts for when the provided context is not sufficient to answer the\nquery, but it bridges gaps in the model\u2019s knowledge. Another type is when the retrieved information\nclarifies ambiguities inherent in the question (without answering the question). Finally, we also have\nthe times where either the autorater or the evaluator model makes an error. We note that our analysis\nexpands on prior work by Yoran et al. (2024), who also find a large fraction of cases where the model\nis correct with RAG (but not without) even though the context does not contain the answer.\n\n\nWe additionally investigated cases where the autorater labels an instance as having sufficient context\nwhile the LLM evaluator marks the answer as incorrect. One source of these discrepancies occurs\nwhen the ground truth answer conflicts with the answer provided in the source. This represents a key\ndifference from methods that measure entailment, where context is evaluated relative to a specific\nground truth answer (see Table 1). Another source of errors arises when the autorater correctly\nidentifies that the necessary information is present, but the model fails to properly compose the\ninformation (e.g., in multihop questions or questions requiring arithmetic). In a substantial number of\ncases, however, determining the source of the error proves challenging.\n\n\n5 TECHNIQUES TO REDUCE HALLUCINATIONS WITH RAG\n\n\nFrom our previous analysis, we have seen that models may hallucinate rather than abstain and that\nthis happens more with RAG than in a closed-book setting. A natural next question is whether we\ncan prompt or fine-tune a model to perform closer to the ideal case", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_14850", "chunk_text": " have seen that models may hallucinate rather than abstain and that\nthis happens more with RAG than in a closed-book setting. A natural next question is whether we\ncan prompt or fine-tune a model to perform closer to the ideal case. Can we steer the model to either\n\n- utput the correct answer or abstain, while hallucinating an incorrect answer as little as possible?\n\n\n5.1 SELECTIVE RAG USING SUFFICIENT CONTEXT SIGNAL\n\n\nOne simple solution to improving RAG performance would be to use the sufficient context autorater\nto abstain given insufficient context. However, this heavy-handed approach can lower overall\nperformance, since all models answer some questions correctly even with insufficient context, as\ndescribed in Table 2 and demonstrated in Figure 3. Instead, we propose a method for combining\n\n\n8\n\n\nPublished as a conference paper at ICLR 2025\n\n\nthe sufficient context autorater outputs with model self-rated confidence scores to tune a selective\naccuracy-coverage trade-off, where \u201ccoverage\u201d denotes the portion of inputs on which the model does\nnot abstain. Specifically, we use these signals to train a simple linear model to predict hallucinations,\nand then use it to set coverage-accuracy trade-off thresholds.\n\n\nThis mechanism differs from other strategies for improving abstention in two key ways. First, because\nit operates independently from generation, it mitigates unintended downstream effects, whereas\nstrategies like fine-tuning to improve abstention can inadvertently worsen performance on certain\ninputs (see Section 5.2). Second, it offers a _controllable_ mechanism for tuning abstention, which\nallows for different operating settings in differing applications, such as strict accuracy compliance in\nmedical domains or maximal coverage on creative generation tasks.\n\n\n**Abstention Signals** We utilize two main signals for abstention: the self-rated probabilities as\nin Li et al. (2024); Kadavath et al. (2022) and the sufficient context autorater. For the self-rated\nprobabilities, we use two strategies: P(True) and P(Correct). P(True) requires sampling answers\nfrom the model multiple times, and then prompting the model multiple times to label each model as\ncorrect or incorrect, resulting in a final probability of correctness associated with each question as in\nKadavath et al. (2022). For proprietary models, where extensive querying is prohibitively expensive,\nwe use P(Correct) instead.", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_15300", "chunk_text": "\ncorrect or incorrect, resulting in a final probability of correctness associated with each question as in\nKadavath et al. (2022). For proprietary models, where extensive querying is prohibitively expensive,\nwe use P(Correct) instead. We adapt the probability-generating prompt from Li et al. (2024) to obtain\nthe model\u2019s response and its estimated probability of correctness. For the sufficient context signal,\nwe use the binary label from an autorater. Our hypothesis is that combining these signals should lead\nto more effective abstention, particularly in cases where the context is insufficient.\n\n\n**Methods.** We calculate P(True) by sampling 20 responses for each question and querying the model\n5 times to evaluate whether the answer is correct or incorrect (without using the ground truth) as\nin Kadavath et al. (2022). For P(Correct), the prompt requests the most likely and second most\nlikely answers along with their probabilities. We use string matching to extract the response and\nself-predicted probability, keeping the one with the highest probability. To determine sufficient\ncontext, we use FLAMe, a small and efficient model for determining the sufficient context label. We\ndivide the retrievals into chunks of 1600 tokens to fit in the context window and label the context as\nsufficient if any of these chunks are sufficient.\n\n\nWe combine the binary sufficient context label with the self-rated answer probability (P(True) for\n\n- pen-source models or P(Correct) for proprietary models) in a simple logistic regression model to\npredict hallucinations with 100 iterations of random hyperparameter search. At inference time, we\nuse the logistic regression model scores to threshold the outputs, abstaining when the score is below\na chosen threshold as in Joren et al. (2024). We measure the added value for selective accuracy\n\n- f the sufficient context signal ( **purple line** in Figure 4) by comparing it with the model self-rated\nconfidence alone ( **gray line** ).\n\n\n**Results.** We find that our approach leads to a better selective accuracy-coverage trade-off compared\nto using model confidence alone. In particular, see gains of over 10% for Gemma 27B on HotpotQA\nin the highest accuracy regions, and gains of over 5% for Gemini 1.5 Pro on the same dataset near the\n70% coverage region. These gains are less pronounced on", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_15750", "chunk_text": "% for Gemma 27B on HotpotQA\nin the highest accuracy regions, and gains of over 5% for Gemini 1.5 Pro on the same dataset near the\n70% coverage region. These gains are less pronounced on datasets with lower overall accuracy, such\nas when using Gemma 27B on Musique. In this scenario, the low overall performance (18.4%) likely\nmeans that most of the predictive gains are seen by using the self-rated confidence to predict errors\nfor the majority of samples. As a result, there is no added benefit from the sufficient context signal.\n\n\n**Discussion.** As expected, we see a downward trend in which higher coverage leads to lower selective\naccuracy for both methods. We conclude that the selective generation mechanism with sufficient\ncontext has an added benefit for accuracy-coverage trade-offs compared to self-rated confidence\nalone. As a prerequisite for our method, models should have a non-trivial accuracy on sufficient and\ninsufficient context instances. Then, intuitively, we can prioritize answering questions the model is\nlikely to get right before those the model struggles with. While ordering examples is impossible in\nreal settings, we can estimate a coverage level and use a threshold to choose when to answer.\n\n\n5.2 FINE-TUNING\n\n\nWe also consider fine-tuning models to increase their ability to abstain instead of outputting an\nincorrect answer. To do so, we train the models with some examples that contain \u201cI don\u2019t know\u201d\ninstead of their original ground truth answer. The intuition here is that training explicitly on such\n\n\n9\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n|Col1|Col2|P(Correct<br>P(Correct|)<br>) + Suff . C|onte|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n\n\n|Col1|Col2|P(Correct<br>P(Correct|)<br>) + Suff .|Conte|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n\n\n|Col1|Col2|Col3|P(True<br>P(True|)<br>) + Suff .|Contex|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Col2|P(Correct<br>P(Correct", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_16200", "chunk_text": " Suff .|Contex|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Col2|P(Correct<br>P(Correct|)<br>)+SuffC|onte|\n|---|---|---|---|---|\n||||.||\n||||||\n||||||\n||||||\n\n\n|Col1|Col2|P(Correct<br>P(Correct|)<br>)+Suff|Conte|\n|---|---|---|---|---|\n||||.||\n||||||\n||||||\n||||||\n\n\n|Col1|Col2|P(True<br>P(True|)<br>)+Suff|Contex|\n|---|---|---|---|---|\n||||.||\n||||||\n||||||\n||||||\n\n\n\n\n\n\n\nFigure 4: **Selective Generation: Coverage vs. Selective Accuracy.** For selective generation, we\nuse a linear combination of sufficient context and self-rated confidence ( **purple** ) or confidence alone\n( **gray** ). The x-axis shows coverage (% of questions answered); the y-axis shows accuracy at each\ncoverage (# correct / # answered). The combined approach matches or outperforms the baseline\nconfidence-only method, especially on HotpotQA, where our method improves accuracy for most\ncoverages. For Gemma 27B on Musique, the methods are identical (coeff. for stuff. context is 0).\n\n\ninputs could encourage the model to abstain instead of hallucinating. We also consider multiple\nsettings, such as only changing the answers when the example has insufficient context. We present\nfull details in Appendix B.1. The main takeaways are that fine-tuned models (i) have a higher rate of\ncorrect answers in many cases, but (ii) still hallucinate quite often and more than they abstain. Overall,\nit is likely possible to use fine-tuning to steer the model towards better abstention and correctness, but\nmore work is needed to determine develop a reliable strategy that can balance these objectives.\n\n\n6 CONCLUSION\n\n\nOur work provided a new lens on LLM responses in RAG systems centered around our notion of\nsufficient context. We constructed a sufficient context autorater, which enabled scalable insights into\nmodel performance on different types of instances. Our analysis revealed that even with sufficient\ncontext, LLMs frequently hallucinate answers. We also found, surprisingly,", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_16650", "chunk_text": "sufficient context. We constructed a sufficient context autorater, which enabled scalable insights into\nmodel performance on different types of instances. Our analysis revealed that even with sufficient\ncontext, LLMs frequently hallucinate answers. We also found, surprisingly, many cases where\na model will output a correct answer with access to only insufficient context. Qualitatively, we\ncategorized such instances, leading to a fuller picture of ways context can be useful. Finally, we\ndemonstrated a general-purpose selective generation method, which applies to Gemini, GPT, and\nGemma, and can reduce hallucinations by 2\u201310% on queries that the model answers.\n\n\n**Limitations.** Our analysis focuses on QA datasets, but summarization tasks also utilize context,\nwhich may or may not be sufficient. For example, models may behave differently on the prompt\n\u201cSummarize the reviews of 5-star hotels in Mallorca\u201d depending on whether the context mentions the\nhotel reviews, whether they are for 5-star hotels, etc. Another shortcoming is an exploration of how\n\n- ften different retrieval methods lead to sufficient context. Also to achieve the best performance, we\ncould have used our autorater to iteratively judge whether to retrieve more or answer the question.\n\n\n**Future Work.** One direction is a fine-grained sufficient context autorater, which outputs a score\ninstead of a binary label. This could be useful for ranking contexts after the retrieval step. Another\ndirection is to extend the definition of sufficient context to multi-modal RAG settings, such as for\nvisual QA (images) or document QA (pdf files). Finally, our selective generation results suggest that\nthere is room for improvement in reducing hallucinations by using auxiliary signals from the inputs.\n\n\n10\n\n\nPublished as a conference paper at ICLR 2025\n\n\nACKNOWLEDGMENTS\n\n\nWe thank Hrishikesh Garud, Vikram Gopali, Xun Sun, and Bruce Wang for annotating data. We\nthank Ranjay Krishna and Jacob Eisenstein for helpful discussions. We also thank Alyshia Olsen for\nhelp with the figure design and color palette. We thank the anonymous reviewers for suggestions to\nimprove the presentation.\n\n\nREFERENCES\n\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_17100", "chunk_text": " the presentation.\n\n\nREFERENCES\n\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint_\n_arXiv:2303.08774_, 2023.\n\n\nAnthropic. Claude 3.5 sonnet model card addendum, 2024.\n\n\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\ngenerate, and critique through self-reflection. _arXiv preprint arXiv:2310.11511_, 2023.\n\n\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau\nYih. Reliable, adaptable, and attributable language models with retrieval. _arXiv preprint arXiv:2403.03187_,\n2024.\n\n\nYung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and James Glass. Lookback lens:\nDetecting and mitigating contextual hallucinations in large language models using only attention maps. _arXiv_\n_preprint arXiv:2407.07071_, 2024.\n\n\nFlorin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek,\nNicola Tonellotto, and Fabrizio Silvestri. The power of noise: Redefining retrieval for rag systems. In\n_Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information_\n_Retrieval_, pp. 719\u2013729, 2024.\n\n\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A\nsurvey on rag meeting llms: Towards retrieval-augmented large language models. In _Proceedings of the 30th_\n_ACM SIGKDD Conference on Knowledge Discovery and Data Mining_,", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_17550", "chunk_text": "-Seng Chua, and Qing Li. A\nsurvey on rag meeting llms: Towards retrieval-augmented large language models. In _Proceedings of the 30th_\n_ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pp. 6491\u20136501, 2024.\n\n\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\n_arXiv preprint arXiv:2312.11805_, 2023.\n\n\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,\nL\u00e9onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u00e9, et al. Gemma 2: Improving open\nlanguage models at a practical size. _arXiv preprint arXiv:2408.00118_, 2024.\n\n\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-evaluating factual consistency\nevaluation. In _Proceedings of the 2022 Conference of the North American Chapter of the Association for_\n_Computational Linguistics: Human Language Technologies_, pp. 3905\u20133920, 2022.\n\n\nCheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long Le, Abhishek Kumar, James Glass,\nAlexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Found in the middle: Calibrating\npositional attention bias improves long context utilization. In Lun-Wei Ku, Andre Martins, and Vivek\nSrikumar (eds.), _Findings of the Association for Computational Linguistics ACL 2024_, pp. 14982\u201314995,\nBangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi:\n[10.18653/v", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_18000", "chunk_text": "Findings of the Association for Computational Linguistics ACL 2024_, pp. 14982\u201314995,\nBangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi:\n[10.18653/v1/2024.findings-acl.890. URL https://aclanthology.org/2024.findings-acl.](https://aclanthology.org/2024.findings-acl.890)\n[890.](https://aclanthology.org/2024.findings-acl.890)\n\n\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. In _International Conference on Learning_\n_Representations (ICLR)_, 2022.\n\n\nAQ Jiang, A Sablayrolles, A Mensch, C Bamford, DS Chaplot, D de las Casas, F Bressand, G Lengyel, G Lample,\nL Saulnier, et al. Mistral 7b (2023). _arXiv preprint arXiv:2310.06825_, 2023.\n\n\nZhouyu Jiang, Mengshu Sun, Lei Liang, and Zhiqiang Zhang. Retrieve, summarize, plan: Advancing multi-hop\nquestion answering with an iterative approach. _arXiv preprint arXiv:2407.13101_, 2024.\n\n\n11\n\n\nPublished as a conference paper at ICLR 2025\n\n\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for\nefficient retrieval-augmented generation research. _CoRR_ [, abs/2405.13576, 2024. URL https://arxiv.](https://arxiv.org/abs/2405.13576)\n\n[org/abs/2405.13576.](https://arxiv.org/abs/2405.13576)\n\n\nHailey Joren, Chirag Nagpal, Katherine A Heller, and Berk Ustun. Participatory personalization in classification. In _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_, volume 36, 2023. [URL https", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_18450", "chunk_text": ", Katherine A Heller, and Berk Ustun. Participatory personalization in classification. In _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_, volume 36, 2023. [URL https://proceedings.neurips.cc/paper_files/paper/2023/](https://proceedings.neurips.cc/paper_files/paper/2023/file/2dbb8bfe4cd3875609b23799830ee865-Paper-Conference.pdf)\n[file/2dbb8bfe4cd3875609b23799830ee865-Paper-Conference.pdf.](https://proceedings.neurips.cc/paper_files/paper/2023/file/2dbb8bfe4cd3875609b23799830ee865-Paper-Conference.pdf)\n\n\nHailey Joren, Charles Marx, and Berk Ustun. Classification with conceptual safeguards. In _The Twelfth Inter-_\n_national Conference on Learning Representations (ICLR)_ [, 2024. URL https://iclr.cc/virtual/](https://iclr.cc/virtual/2024/poster/17625)\n[2024/poster/17625.](https://iclr.cc/virtual/2024/poster/17625)\n\n\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer,\nZac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they\nknow. _arXiv preprint arXiv:2207.05221_, 2022.\n\n\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay,\nand Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. _arXiv_\n_preprint arXiv:2409.12941_, 2024.\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_18900", "chunk_text": ", Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nK\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledgeintensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459\u20139474, 2020.\n\n\nMoxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua. Think twice before assure:\nConfidence estimation for large language models through reflection on multiple answers. _arXiv preprint_\n_arXiv:2403.09972_, 2024.\n\n\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts. _Transactions of the Associ-_\n_ation for Computational Linguistics_, 12:157\u2013173, 2024. doi: 10.1162/tacl_a_00638. [URL https:](https://aclanthology.org/2024.tacl-1.9)\n[//aclanthology.org/2024.tacl-1.9.](https://aclanthology.org/2024.tacl-1.9)\n\n\nPaul Mineiro. Online joint fine-tuning of multi-agent flows. _arXiv preprint arXiv:2406.04516_, 2024.\n\n\nAbhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and\nHannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models. In _COLM 2024_,\n2024.\n\n\nCheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong\nZhang. RAGTruth: A hallucination corpus for developing trustworthy retrieval-augmented language models.\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting_\n\n_of the Association for Computational Linguistics (Volume 1: Long Papers)_,", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_19350", "chunk_text": "ugmented language models.\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting_\n\n_of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 10862\u201310878, Bangkok,\nThailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.585.\n[URL https://aclanthology.org/2024.acl-long.585.](https://aclanthology.org/2024.acl-long.585)\n\n\nLucas Monteiro Paes, Carol Xuan Long, Berk Ustun, and Flavio Calmon. On the epistemic limits of personalized\nprediction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural_\n_Information Processing Systems_ [, 2022. URL https://openreview.net/forum?id=Snp3iEj7NJ.](https://openreview.net/forum?id=Snp3iEj7NJ)\n\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. In-context retrieval-augmented language models. _Transactions of the Association for Computational_\n_Linguistics_, 11:1316\u20131331, 2023.\n\n\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov,\nGaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural language generation\nmodels. _Computational Linguistics_, 49(4):777\u2013840, 2023.\n\n\nDavid Rau, Herv\u00e9 D\u00e9jean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and\nSt\u00e9phane Clinchant. Bergen: A benchmarking library for retrieval-augmented generation. _arXiv preprint_\n_arXiv:2407.01102_, 2024.\n\n\nDongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng,", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_19800", "chunk_text": "iv preprint_\n_arXiv:2407.01102_, 2024.\n\n\nDongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang\nWang, Shichao Sun, Huanyu Li, et al. Ragchecker: A fine-grained framework for diagnosing retrievalaugmented generation. _arXiv preprint arXiv:2408.08067_, 2024.\n\n\n12\n\n\nPublished as a conference paper at ICLR 2025\n\n\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny\nZhou. Large language models can be easily distracted by irrelevant context. In _International Conference on_\n_Machine Learning_, pp. 31210\u201331227. PMLR, 2023a.\n\n\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen-tau Yih. Replug: Retrieval-augmented black-box language models. _arXiv preprint arXiv:2301.12652_,\n2023b.\n\n\nAdi Simhi, Jonathan Herzig, Idan Szpektor, and Yonatan Belinkov. Constructing benchmarks and interventions\nfor combating hallucinations in llms. _arXiv preprint arXiv:2404.09971_, 2024.\n\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Is multihop qa in dire condition?\nmeasuring and reducing disconnected reasoning. In _Proceedings of the 2020 Conference on Empirical_\n_Methods in Natural Language Processing (EMNLP)_, pp. 8846\u20138863, 2020.\n\n\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions\nvia single-hop question composition. _Transactions of the Association for Computational Linguistics_, 10:\n539\u2013554, 2022.\n\n\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei,", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_21150", "chunk_text": " North American Chapter of the Association for Computational Linguistics: Human_\n_Language Technologies (Volume 1: Long Papers)_, pp. 7106\u20137132, 2024a.\n\n\n13\n\n\nPublished as a conference paper at ICLR 2025\n\n\nJiahao Zhang, Haiyang Zhang, Dongmei Zhang, Liu Yong, and Shen Huang. End-to-end beam retrieval for\nmulti-hop question answering. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), _Proceedings of_\n_the 2024 Conference of the North American Chapter of the Association for Computational Linguistics:_\n_Human Language Technologies (Volume 1: Long Papers)_, pp. 1718\u20131731, Mexico City, Mexico, June\n[2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.96. URL https:](https://aclanthology.org/2024.naacl-long.96)\n[//aclanthology.org/2024.naacl-long.96.](https://aclanthology.org/2024.naacl-long.96)\n\n\nYujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrieval-augmented large\nlanguage models. In _Proceedings of the ACM on Web Conference 2024_, pp. 1453\u20131463, 2024.\n\n\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and\nreading: A comprehensive survey on open-domain question answering. _arXiv preprint arXiv:2101.00774_,\n2021.\n\n\n14\n\n\nPublished as a conference paper at ICLR 2025\n\n\nA SUPPORTING EXPERIMENT INFORMATION\n\n\nWe provide sufficient details to reproduce all of our experiments. We list all of the prompts that we\nuse in the next section.\n\n\nA.1 MODELS\n\n\n**GPT 4o** . We use the API-accessible gpt-4o-2024-08-06 model, released on August 6,\n2024 (Achiam et al., 2023).\n\n\n**Gemini 1.5 Pro.** We use the API-accessible gemini-1.5-pro-0514 model, released on May\n14, ", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_21600", "chunk_text": "6,\n2024 (Achiam et al., 2023).\n\n\n**Gemini 1.5 Pro.** We use the API-accessible gemini-1.5-pro-0514 model, released on May\n14, 2024 (Gemini Team et al., 2023).\n\n\n**Claude 3.5 Sonnet.** We use the only API-accessible claude-3-5-sonnet-20240620 model,\nreleased on June 20, 2024 (Anthropic, 2024).\n\n\n**Gemma 2 27B.** We use the publicly available instruction tuned gemma-2-27b-it model, released\n\n- n Jun 27, 2024 (Gemma Team et al., 2024).\n\n\n**Mistral 3 7B.** We use the publicly available instruction tuned Mistral-7B-Instruct-v0.3\nmodel, released on May 22, 2024 (Jiang et al., 2023).\n\n\n**FLAMe.** We use the published FLAMe-RM-24B model (Vu et al., 2024).\n\n\n**TRUE-NLI model.** Calculated the maximum probability over the chunks in the context. Use a\nthreshold of 0.05, where if the maximum probability is higher then this, then we classify as \u2018sufficient\ncontext\u2019. The threshold of 0.05 achieved the highest F1 score on our human labeled dataset. We use\nthe t5_xxl_true_nli_mixture version of their model (Honovich et al., 2022).\n\n\nA.2 FINE-TUNING SETTINGS\n\n\nIn our fine-tuning setup, we employed the LoRA adaptation technique (Hu et al., 2022) to fine-tune\nMistral-7B-Instruct-v0.3 [1] . We used either a 2,000-example random subset sampled from the training\nset of the Musique-Ans dataset or from the development set of the HotPotQA data [2] . The prompt\ntemplate for finetuning is provided in Appendix D.\n\n\nFor the LoRA parameters, we set the rank to 4 and alpha to 8 for all experiments. The models were\nfine-tuned over 2 epochs with a batch size of 16 and a learning rate of 1 _\u00d7_ 10 _[\u2212]_", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_22050", "chunk_text": " set the rank to 4 and alpha to 8 for all experiments. The models were\nfine-tuned over 2 epochs with a batch size of 16 and a learning rate of 1 _\u00d7_ 10 _[\u2212]_ [5] . We note that the\ntraining was not smooth, and different checkpoints led to very different results. To be systematic,\nwe chose the best checkpoint in terms of Correct % after either 1 or 2 epochs (where for Musique it\nturned out to be after 1 epoch, and for HotPotQA we found that 2 epochs was better).\n\n\nA.3 DATASETS\n\n\nWe sample 500 examples from HotPotQA and Musique-Ans dev sets, following prior work. We use\nall \u2018True Premise\u2019 questions from FreshQA.\n\n\n**Retrieval for HotpotQA.** We adopt the FlashRAG framework (Jin et al., 2024) to implement our\nRetrieval-Augmented Generation (RAG) process. Our retrieval corpus is based on the wiki-18 dataset,\nutilizing \u2018intfloat/e5-base-v2\u2018 from Hugging Face\u2019s model hub as a Dense Retriever [3] . For each query,\nwe retrieved the top 5 documents, which are subsequently concatenated with the query and placed\nwithin a prompt template for inference.\n\n\nTo explore advanced retrieval techniques, we also evaluated the REPLUG (Shi et al., 2023b) method.\nREPLUG enhances the generation quality by prepending each retrieved document individually to the\ninput context and ensembling output probabilities across different passes. The REPLUG method is\nalso implemented based on the FlashRAG framework (Jin et al., 2024).\n\n\n1Available at huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\n2We use dev set for HotPotQA since the training set had a much different distribution of sufficient context\nexamples. Namely, we found the train set to be over 88% sufficient context, while the dev set was only 44%.\n3huggingface.co/intfloat/e5-base-v2\n\n\n15\n\n\nPublished as a conference paper at ICLR 2025\n\n\n**Retrieval for FreshQA** We use the urls provided in the FreshQA dataset as retrieval for the\ncontext. We scraped each url and discarded extra HTML content such as headers, footers,", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_22500", "chunk_text": " conference paper at ICLR 2025\n\n\n**Retrieval for FreshQA** We use the urls provided in the FreshQA dataset as retrieval for the\ncontext. We scraped each url and discarded extra HTML content such as headers, footers, and\nnavigation. We include the title of each webpage in the text, convert any included tables to markdown,\nand include the table title immediately before the table in the text. When splitting the tables and text\nfor smaller context windows, we keep tables and sentences intact when possible. For large tables that\nrequire splitting, we duplicate the table row column headers to include them in each chunk.\n\n\n16\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 3: **Fine-tuned (FT) Mistral 3 7B Instruct** . We compare closed book and vanilla RAG with\nthree FT settings, measuring % Correct ( **%C** ), % Abstain (%A), and % Hallucinate ( **%H** ). Also,\n\u201cidk\u201d means we change the answer in training samples to be \u201cI don\u2019t know\u201d instead of the given\nanswer (either for 20% of random examples, or 20% of examples with insufficient context). Best\n**%C** for each model/dataset in bold.\n\n\n**Musique** **HotPotQA**\n**Model** **Variant** RAG **%C** **%A** **%H** **%C** **%A** **%H**\n\n\nMistral Closed Book 6.6 29.8 63.6 32 7.6 60.4\n\n\" Vanilla RAG \u2713 28.8 11.8 59.4 **46.6** 9.2 44.2\n\" FT GT answer (Data Mix 1) \u2713 **31.4** 0 68.6 43.4 0 56.6\n\" FT idk 20% rand. (Data Mix 2) \u2713 23 1.2 75.8 41.6 0.8 57.6\n\" FT idk 20% insuff. (Data Mix 3) \u2713 23 2.2 74.8 41.2 2 56.8\n\n\nB ADDITIONAL RESULTS\n\n\nB.1 FINE-TUNING FULL RESULTS\n\n\nOne aspect of our selective generation", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_22950", "chunk_text": " (Data Mix 3) \u2713 23 2.2 74.8 41.2 2 56.8\n\n\nB ADDITIONAL RESULTS\n\n\nB.1 FINE-TUNING FULL RESULTS\n\n\nOne aspect of our selective generation framework is that we use FLAMe, a 24B model, to provide\nsufficient context labels. However, we would incur significant overhead if we used a 24B model\nto improve the generation of a much smaller LLM. Instead, we try directly fine-tuning Mistral 3\n7B to increase accuracy with retrieval. Specifically, we experiment with different data mixtures to\nencourage the model to output \u201cI don\u2019t know\u201d instead of generating an incorrect response.\n\n\n**Fine-tuning Data.** We repeat the following process separately for each of the Musique-Ans and\nHotPotQA datasets to create three mixtures of training data with different answers for each dataset.\nFirst, we sample 2000 instances. Then, for Data Mix 1, we fine-tune on these instances and keep their\ngiven ground truth answer. For Data Mix 2, we choose 400 examples (20%) at random and change\nthe answer to \u201cI don\u2019t know\u201d before fine-tuning. For Data Mix 3, we instead randomly choose 400\nexamples (20%) that our autorater labels as insufficient context and change their answer to \u201cI don\u2019t\nknow\u201d while keeping the other answers as the ground truth. Our hypothesis is that fine-tuning on\nData Mix 2 and 3 should steer the model to abstain more and hallucinate less than with Data Mix 1.\n\n\n**Models, Methods, Metrics.** Using the three data mixtures described above, we fine-tune the Mistral\n3 7B Instruct model model with LoRA (details in Appendix A.2). At inference time, we use the\nstandard RAG setup where we add context to the prompt. As baselines, we also evaluate the model\nwithout fine-tuning in both the closed-book setting (w/o RAG) and the open-book setting (Vanilla\nRAG). Consistent with the prior experiments, we use an LLM with ground truth answers to classify\nresponses as Correct ( **%C** ), Abstention ( **%A** ), or Hallucination ( **%H** ).\n\n\n**Fine-tuning Results and Discussion.**", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_23400", "chunk_text": " we use an LLM with ground truth answers to classify\nresponses as Correct ( **%C** ), Abstention ( **%A** ), or Hallucination ( **%H** ).\n\n\n**Fine-tuning Results and Discussion.** Table 3 shows our experimental results. We verify that the FT\nvariants have a higher rate of generating correct answers ( **%C** ) compared to closed-book and Vanilla\nRAG for Musique but not for HotPotQA. On the other hand, refuting our hypothesis, Data Mix 2 and\n3 do not lead to more abstentions than Vanilla RAG. But, they do abstain more than with Data Mix 1,\nshowing the impact of adding \u201cI don\u2019t know\u201d in the training set. In general, FT models using RAG\n\n- utput incorrect answers ( **%H** ) much of the time, and often more than they abstain ( **%A** ).\n\n\nB.2 PERFORMANCE BREAKDOWN BY SUFFICIENT CONTEXT\n\n\nWe explore RAG performance by different models for various RAG benchmark datasets. Here,\nthe first column shows performance without RAG (closed-book) while the second column shows\nperformance with RAG (open-book). To better understand RAG performance, we use our sufficient\ncontext autorater to stratify the retrieval augmented generation (RAG) datasets into sufficient and\ninsufficient context. The third and fourth columns show the performance of the second column\nstratified by sufficient vs insufficient context respectively.\n\n\n17\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Correct, hallucination, and abstention fractions across models for dataset FreshQA, stratified\nby sufficient context. FreshQA includes hand-curated source URLs, which explains the larger\npercentage of sufficient context (77.4%). FreshQA also specifically explores questions with answers\nthat change based on the question\u2019s timestamp, which may explain the frequent abstentions without\nRAG (100% for Gemini 1.5 Pro).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Correct, hallucination, and abstention fractions across models for dataset HotpotQA,\nstratified by sufficient context. HotpotQA includes questions that are more likely to be answerable\nwithout context (e.g., yes or no questions, multiple choice questions, or questions with answers that\nmight be answerable due to pre-training). This explains the higher fraction of correct answers without", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_23850", "chunk_text": " includes questions that are more likely to be answerable\nwithout context (e.g., yes or no questions, multiple choice questions, or questions with answers that\nmight be answerable due to pre-training). This explains the higher fraction of correct answers without\nRAG (e.g., 48.0% for GPT 4o).\n\n\n18\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 4: **Performance Analysis of RAG Systems Using Human-Annotated Sufficient Context**\n**Labels.** These tables include results on a curated set of challenging context-dependent questions.\nTable (a) shows that while larger models generally achieve higher accuracy with sufficient context\n(present in 54.8% of cases), even top performers exhibit a 14-16% error rate. Table (b) reveals that\nwith insufficient context (45.2% of cases), models predominantly abstain from answering (50-73%\n\n- f instances), though significant hallucination rates (15-40%) persist. These patterns of contextdependent performance and hallucination risk are consistent with our analyses of HotpotQA, FreshQA,\nand Musique datasets, despite variations in absolute performance due to different task complexities.\n\n\n(a) Performance with Sufficient Context (54.8% of Dataset)\n\n\n**Model** **% Correct** **% Abstain** **% Hallucinate**\n\n\nGemini 1.5 Pro 84.1 1.6 14.3\n\nGPT 4o 82.5 4.8 12.7\n\nClaude 3.5 Sonnet 85.7 11.1 3.2\n\nGemini 1.5 Flash 77.8 4.8 17.5\n\nGemma 27B 71.4 3.2 25.4\n\n\n(b) Performance with Insufficient Context (45.2% of Dataset)\n\n\n**Model** **% Correct** **% Abstain** **% Hallucinate**\n\n\nGemini 1.5 Pro 9.6 50.0 40.4\n\nGPT 4o 23.1 61.5 15.4\n\nClaude 3.5 Sonnet 9.6 53.8 36.5\n\nGemini 1.5 Flash 7.7 73.1 19.2\n\nGemma 27B 9.6", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_24300", "chunk_text": "Claude 3.5 Sonnet 9.6 53.8 36.5\n\nGemini 1.5 Flash 7.7 73.1 19.2\n\nGemma 27B 9.6 55.8 34.6\n\n\n19\n\n\nPublished as a conference paper at ICLR 2025\n\n\nB.3 COMPARISON OF QA EVALUATION METRICS\n\n\nWe compare two the LLM-based QA Evaluator (LLMEval) used in the paper with a deterministic\nlexical matching metric (Contains Answer). The Contains Answer metric labels responses based on\nwhether they contain the exact ground truth answer, while LLMEval uses an LLM to assess semantic\n\ncorrectness.\n\n\nTable 5 presents model performance across three datasets (FreshQA, Musique, HotpotQA), split by\n\n- ur sufficient context autorater. The results show Contains Answer is generally stricter than LLMEval,\nthough both metrics reveal similar patterns in model behavior.\n\n\nTable 5: **Comparison of evaluation metrics across models and datasets.** We show results for\nchecking whether the response contains one of the ground truth answer strings (\"Contains\"), where\nwe report the % of responses that contain an answer. We compare this to our LLMEval method that\nuses an LLM to evaluate if the response is correct, abstain, or hallucinated, and we report % correct.\n\n\nFreshQA Musique HotpotQA\n\n\nModel Context Contains LLMEval Contains LLMEval Contains LLMEval\n\n\nGemini 1.5 Pro Suff 80.3% 89.1% 60.1% 83.4% 47.6% 67.5%\n\nInsuff 31.4% 41.2% 33.6% 49.5% 34.2% 49.4%\n\nGPT-4 Suff 84.3% 89.1% 64.6% 83.4% 52.4% 71.9%\n\nInsuff 36.3% 44.1% 44.4% 61.4% 46.1% 59.5%\n\nGemma 27B Suff 26.9% 27.5% 10.8% 23.3% 40.7% 64.1%\n\nInsuff 11.8% 6.9% 7.2%", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_24750", "chunk_text": "27B Suff 26.9% 27.5% 10.8% 23.3% 40.7% 64.1%\n\nInsuff 11.8% 6.9% 7.2% 10.1% 22.7% 37.9%\n\nClaude 3.5 Suff 67.9% 73.1% 48.9% 74.0% 46.3% 66.7%\n\nSonnet Insuff 26.5% 33.3% 19.9% 40.4% 29.0% 38.3%\n\n\nThe Contains Answer metric exhibits several characteristics when compared to LLMEval:\n\n\n1. Different formatting affects matching:\n\n\nQ: What date did the creator of Autumn Leaves die?\nGround Truth: 13 August 1896\nResponse: August 13, 1896.\nContains Answer: False\n\nLLMEval: Correct\n\n\n2. Semantic equivalents are not captured:\n\n\nQ: What former Los Angeles Lakers majority owner is the\nfather of Jeanie Marie Buss?\n\nGround Truth: Gerald Hatten Buss\n\nResponse: Jerry Buss.\nContains Answer: False\n\nLLMEval: Correct\n\n\n3. Partial matches can be marked as correct:\n\n\nQ: What is Amazon Prime Video\u2019s most watched premiere ever?\nGround Truth: The Rings of Power\nResponse: The series explores the forging of the Rings of Power,\nthe rise of Sauron...\n\nContains Answer: True\n\nLLMEval: Hallucinate\n\n\nThe LLM QA evaluator provides several practical advantages:\n\n\n20\n\n\nPublished as a conference paper at ICLR 2025\n\n\n    - Handles variations in model verbosity and formatting\n\n\n    - Distinguishes between correct, abstain, and incorrect responses\n\n\n    - Enables efficient evaluation across multiple datasets\n\n\nOur analysis shows two key findings that are consistent across both metrics: LLMs (i) exhibit\nhallucination even with sufficient context and (ii) struggle to abstain with insufficient context.\n\n\n21\n\n\nPublished as a conference paper at ICLR 2025\n\n\nC PROMPTS\n\n\nC.1 SUFFICIENT CONTEXT AUTORATER PROMPT\n\n\n\n\n\n22\n\n\nPublished as a conference paper at ICLR 2025\n\n\nC.2 FLAME PROMPT\n\n\nC.3 LLMEVAL PROMPT\n\n\nSince the questions in our datasets ask for free form answers, the LLM responses may not exactly\nmatch", "token_count": 500, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2411.06037_sufficient_context_joren:chunk_25200", "chunk_text": "Published as a conference paper at ICLR 2025\n\n\nC.2 FLAME PROMPT\n\n\nC.3 LLMEVAL PROMPT\n\n\nSince the questions in our datasets ask for free form answers, the LLM responses may not exactly\nmatch the GT answers. Hence, we use an LLM to determine: the answers are the same (Correct)\n\n- r the LLM does not answer the question (Abstain) or the answer is incorrect (Hallucinate). We\nnote that prior work has shown that Gemini 1.5 Pro has very high accuracy and correlation with\nhuman judgments for this evaluating free form responses (Krishna et al., 2024). Responses resulting\nin empty strings were classified as \"missing,\" while variations of \"I don\u2019t know\" were also treated\nas missing. We normalized both the ground truth answers and the model\u2019s responses by removing\npunctuation, converting to lowercase, and eliminating stop words.\n\n\n\n\n\n23\n\n\nPublished as a conference paper at ICLR 2025\n\n\nC.4 DATASET QUESTION ANSWER PROMPTS\n\n\nThe CoT prompt instructs the model to provide an accurate and concise answer based solely on\nthe given search results, using an unbiased and journalistic tone. The prompt includes an example\nquestion, references, and answer to guide the model\u2019s response format. To extract the final answer,\nwe implemented a pattern matching technique on the model\u2019s response, specifically targeting the text\nfollowing \"The answer is:\" for CoT prompts.\n\n\nChain of Thought (CoT)\n\n\n\n\n\nAnswer Only (AO)\n\n\n\n\n\nD FINE-TUNING AND RAG PROMPTS FOR MISTRAL\n\n\nFinetuning Prompt (FT)\n\n\n24\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nEvaluation Without RAG Prompt\n\n\n\n\n\nEvaluation With RAG Prompt\n\n\n\n\n\n25\n\n\n", "token_count": 367, "metadata": {"arxiv_id": "2411.06037", "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems", "authors": ["Hailey Joren", "Jianyi Zhang", "Chun-Sung Ferng", "Da-Cheng Juan", "Ankur Taly", "Cyrus Rashtchian"], "year": 2024, "url": "https://arxiv.org/pdf/2411.06037v3"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_0", "chunk_text": "## **Retrieval-Augmented Generation for** **Knowledge-Intensive NLP Tasks**\n\n**Patrick Lewis** _[\u2020\u2021]_ **, Ethan Perez** _[\u22c6]_ **,**\n\n\n**Aleksandra Piktus** _[\u2020]_ **, Fabio Petroni** _[\u2020]_ **, Vladimir Karpukhin** _[\u2020]_ **, Naman Goyal** _[\u2020]_ **, Heinrich K\u00fcttler** _[\u2020]_ **,**\n\n\n**Mike Lewis** _[\u2020]_ **, Wen-tau Yih** _[\u2020]_ **, Tim Rockt\u00e4schel** _[\u2020\u2021]_ **, Sebastian Riedel** _[\u2020\u2021]_ **, Douwe Kiela** _[\u2020]_\n\n\n_\u2020_ Facebook AI Research; _\u2021_ University College London; _\u22c6_ New York University;\n\n```\n                plewis@fb.com\n\n```\n\n**Abstract**\n\n\nLarge pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance\nlags behind task-specific architectures. Additionally, providing provenance for their\ndecisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric\nmemory have so far been only investigated for extractive downstream tasks. We\nexplore a general-purpose fine-tuning recipe for retrieval-augmented generation\n(RAG) \u2014 models which combine pre-trained parametric and non-parametric mem\n    - ry for language generation. We introduce RAG models where the parametric\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, and another which can use different passages\nper token. We fine-tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks,\n\n     - utperforming parametric seq2seq models and task-specific retrieve-and-extract\narch", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_450", "chunk_text": "une and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks,\n\n     - utperforming parametric seq2seq models and task-specific retrieve-and-extract\narchitectures. For language generation tasks, we find that RAG models generate\nmore specific, diverse and factual language than a state-of-the-art parametric-only\nseq2seq baseline.\n\n\n**1** **Introduction**\n\n\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\u2019t straightforwardly provide insight into\ntheir predictions, and may produce \u201challucinations\u201d [38]. Hybrid models that combine parametric\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\ninspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\n\n\n|Col1|Col2|Col3|z4<br>z3<br>z2|Col5|Col6|\n|---|---|---|---|---|---|\n||||**z3**<br>**z2**|**z3**<br>**z2**|**z3**<br>**z2**|\n||||**z2**|**z2**|**z2**|\n|||**z**|**z**|||\n|||**1**|**1**|||\n|||||||\n\n\n\nFigure 1: Overview of our approach. We combine a pre-trained retriever ( _Query Encoder_ + _Document_\n_Index_ ) with a pre-trained seq2seq model ( _Generator_ ) and fine-tune end-to-end. For query _x_, we use\nMaximum Inner Product Search (MIPS) to find the top-K documents _zi_ . For final prediction _y_, we\ntreat _z_ as a latent variable and marginalize over seq2seq predictions given different", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_900", "chunk_text": "x_, we use\nMaximum Inner Product Search (MIPS) to find the top-K documents _zi_ . For final prediction _y_, we\ntreat _z_ as a latent variable and marginalize over seq2seq predictions given different documents.\n\n\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\nand non-parametric memory to the \u201cworkhorse of NLP,\u201d i.e. sequence-to-sequence (seq2seq) models.\n\n\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\na general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG).\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\ncan be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\n\n\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\nmemory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stackaugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\npresent without additional training.\n\n\nOur results highlight the benefits of combining parametric and non-parametric memory with generation for _knowledge-intensive tasks_ - tasks that humans could not reasonably be expected to perform\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\n\n- n open Natural Questions [", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_1350", "chunk_text": " and non-parametric memory with generation for _knowledge-intensive tasks_ - tasks that humans could not reasonably be expected to perform\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\n\n- n open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\nextractive tasks, we find that unconstrained generation outperforms previous extractive approaches.\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\ngeneration, and we find that our models generate responses that are more factual, specific, and\ndiverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\nthe non-parametric memory can be replaced to update the models\u2019 knowledge as the world changes. [1]\n\n\n**2** **Methods**\n\n\nWe explore RAG models, which use the input sequence _x_ to retrieve text documents _z_ and use them\nas additional context when generating the target sequence _y_ . As shown in Figure 1, our models\nleverage two components: (i) a retriever _p\u03b7_ ( _z|x_ ) with parameters _\u03b7_ that returns (top-K truncated)\ndistributions over text passages given a query _x_ and (ii) a generator _p\u03b8_ ( _yi|x, z, y_ 1: _i\u2212_ 1) parametrized\n\n\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at `[https://github.com/huggingface/transformers/blob/master/](https://github.com/huggingface/transformers/blob/master/examples/rag/)`\n`[examples/rag/](https://github.com/huggingface/transformers/blob/master/examples/rag/)` . An interactive demo of RAG models can be found at `[https://huggingface.co/rag/](https://huggingface.co/rag/)`\n\n\n2\n\n\nby _\u03b8_ that generates a current token based on a context of the previous _i \u2212_ 1 tokens _y_ 1: _i", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_1800", "chunk_text": "face.co/rag/](https://huggingface.co/rag/)`\n\n\n2\n\n\nby _\u03b8_ that generates a current token based on a context of the previous _i \u2212_ 1 tokens _y_ 1: _i\u2212_ 1, the original\ninput _x_ and a retrieved passage _z_ .\n\n\nTo train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\nWe propose two models that marginalize over the latent documents in different ways to produce a\ndistribution over generated text. In one approach, _RAG-Sequence_, the model uses the same document\nto predict each target token. The second approach, _RAG-Token_, can predict each target token based\n\n- n a different document. In the following, we formally introduce both models and then describe the\n_p\u03b7_ and _p\u03b8_ components, as well as the training and decoding procedure.\n\n\n**2.1** **Models**\n\n\n**RAG-Sequence Model** The RAG-Sequence model uses the same retrieved document to generate\nthe complete _sequence_ . Technically, it treats the retrieved document as a single latent variable that\nis marginalized to get the seq2seq probability _p_ ( _y|x_ ) via a top-K approximation. Concretely, the\ntop K documents are retrieved using the retriever, and the generator produces the output sequence\nprobability for each document, which are then marginalized,\n\n\n\n_\u2248_ - _p\u03b7_ ( _z|x_ ) _p\u03b8_ ( _y|x, z_ ) = \n_z\u2208_ top- _k_ ( _p_ ( _\u00b7|x_ )) _z\u2208_ top- _k_ ( _p_\n\n\n\n_p_ RAG-Sequence( _y|x_ ) _\u2248_ \n\n\n\n - _p\u03b7_ ( _z|x_ )\n\n_z\u2208_ top- _k_ ( _p_ ( _\u00b7|x_ ))\n\n\n\n_N_\n\n- _p\u03b8_ ( _yi|x, z, y_ 1: _i\u2212_ 1)\n\n\n_i_\n\n\n\n**RAG-Token Model** In the RAG-Token model we can draw a different latent document for each\ntarget _token_ and marginalize accordingly. This allows the generator to choose content from several\ndocuments when producing an answer. Concretely, the top K documents are", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_2250", "chunk_text": " In the RAG-Token model we can draw a different latent document for each\ntarget _token_ and marginalize accordingly. This allows the generator to choose content from several\ndocuments when producing an answer. Concretely, the top K documents are retrieved using the\nretriever, and then the generator produces a distribution for the next output token for each document,\nbefore marginalizing, and repeating the process with the following output token, Formally, we define:\n\n\n\n\n - _p\u03b7_ ( _z|x_ ) _p\u03b8_ ( _yi|x, z, y_ 1: _i\u2212_ 1)\n\n_z\u2208_ top- _k_ ( _p_ ( _\u00b7|x_ ))\n\n\n\n_p_ RAG-Token( _y|x_ ) _\u2248_\n\n\n\n_N_\n\n\n\n_i_\n\n\n\nFinally, we note that RAG can be used for sequence classification tasks by considering the target class\nas a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.\n\n\n**2.2** **Retriever: DPR**\n\n\nThe retrieval component _p\u03b7_ ( _z|x_ ) is based on DPR [26]. DPR follows a bi-encoder architecture:\n\n_p\u03b7_ ( _z|x_ ) _\u221d_ exp      - **d** ( _z_ ) _[\u22a4]_ **q** ( _x_ )\ufffd **d** ( _z_ ) = BERT _d_ ( _z_ ) _,_ **q** ( _x_ ) = BERT _q_ ( _x_ )\n\n\nwhere **d** ( _z_ ) is a dense representation of a document produced by a BERTBASE _document encoder_ [8],\nand **q** ( _x_ ) a query representation produced by a _query encoder_, also based on BERTBASE. Calculating\ntop-k( _p\u03b7_ ( _\u00b7|x_ )), the list of _k_ documents _z_ with highest prior probability _p\u03b7_ ( _z|x_ ), is a Maximum Inner\nProduct Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\nretriever was trained to retrieve documents which contain answers to TriviaQA [24]", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_2700", "chunk_text": " in sub-linear time [23]. We use\na pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This\nretriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and\nNatural Questions [29]. We refer to the document index as the _non-parametric memory_ .\n\n\n**2.3** **Generator: BART**\n\n\nThe generator component _p\u03b8_ ( _yi|x, z, y_ 1: _i\u2212_ 1) could be modelled using any encoder-decoder. We use\nBART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input\n_x_ with the retrieved content _z_ when generating from BART, we simply concatenate them. BART was\npre-trained using a denoising objective and a variety of different noising functions. It has obtained\nstate-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5\nmodels [32]. We refer to the BART generator parameters _\u03b8_ as the _parametric memory_ henceforth.\n\n\n**2.4** **Training**\n\n\nWe jointly train the retriever and generator components without any direct supervision on what\ndocument should be retrieved. Given a fine-tuning training corpus of input/output pairs ( _xj, yj_ ), we\n\n\n3\n\n\nminimize the negative marginal log-likelihood of each target, [\ufffd] _j_ _[\u2212]_ [log] _[ p]_ [(] _[y][j][|][x][j]_ [)][ using stochastic]\ngradient descent with Adam [28]. Updating the document encoder BERT _d_ during training is costly as\nit requires the document index to be periodically updated as REALM does during pre-training [20].\nWe do not find this step necessary for strong performance, and keep the document encoder (and\nindex) fixed, only fine-tuning the query encoder BERT _q_ and the BART generator.\n\n\n**2.5** **Decoding**\n\n\nAt test time, RAG-Sequence and RAG-Token require different ways to approximate arg max _y p_ ( _y|x_ ).\n\n\n**RAG-Token** The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: _", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_3150", "chunk_text": " RAG-Token require different ways to approximate arg max _y p_ ( _y|x_ ).\n\n\n**RAG-Token** The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: _p_ _[\u2032]_ _\u03b8_ [(] _[y][i][|][x, y]_ [1:] _[i][\u2212]_ [1][) =][ \ufffd] _z\u2208_ top- _k_ ( _p_ ( _\u00b7|x_ )) _[p][\u03b7]_ [(] _[z][i][|][x]_ [)] _[p][\u03b8]_ [(] _[y][i][|][x, z][i][, y]_ [1:] _[i][\u2212]_ [1][)][ To]\n\ndecode, we can plug _p_ _[\u2032]_ _\u03b8_ [(] _[y][i][|][x, y]_ [1:] _[i][\u2212]_ [1][)][ into a standard beam decoder.]\n\n\n**RAG-Sequence** For RAG-Sequence, the likelihood _p_ ( _y|x_ ) does not break into a conventional pertoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\neach document _z_, scoring each hypothesis using _p\u03b8_ ( _yi|x, z, y_ 1: _i\u2212_ 1). This yields a set of hypotheses\n_Y_, some of which may not have appeared in the beams of all documents. To estimate the probability\n\n- f an hypothesis _y_ we run an additional forward pass for each document _z_ for which _y_ does not\nappear in the beam, multiply generator probability with _p\u03b7_ ( _z|x_ ) and then sum the probabilities across\nbeams for the marginals. We refer to this decoding procedure as \u201cThorough Decoding.\u201d For longer\n\n- utput sequences, _|Y |_ can become large, requiring many forward passes. For more efficient decoding,\nwe can make a further approximation that _p\u03b8_ ( _y|x, zi_ ) _\u2248_ 0 where _y_ was not generated during beam\nsearch from _x, zi_ . This avoids the need to run additional forward passes once the candidate set _Y_", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_3600", "chunk_text": "\u03b8_ ( _y|x, zi_ ) _\u2248_ 0 where _y_ was not generated during beam\nsearch from _x, zi_ . This avoids the need to run additional forward passes once the candidate set _Y_ has\nbeen generated. We refer to this decoding procedure as \u201cFast Decoding.\u201d\n\n\n**3** **Experiments**\n\n\nWe experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\na single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute an\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\nNavigable Small World approximation for fast retrieval [37]. During training, we retrieve the top\n_k_ documents for each query. We consider _k \u2208{_ 5 _,_ 10 _}_ for training and set _k_ for test time using dev\ndata. We now discuss experimental details for each task.\n\n\n**3.1** **Open-domain Question Answering**\n\n\nOpen-domain question answering (QA) is an important real-world application and common testbed\nfor knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs ( _x, y_ )\nand train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to\nthe popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved\ndocuments, relying primarily on non-parametric knowledge. We also compare to \u201cClosed-Book\nQA\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead\nrelying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural\nQuestions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\nmodel. We use the same train/dev/test splits as prior work [", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_4050", "chunk_text": "rec (CT) [2]. As\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\nmodel. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM)\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\n\n\n**3.2** **Abstractive Question Answering**\n\n\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\ntext generation. To test RAG\u2019s natural language generation (NLG) in a knowledge-intensive setting,\nwe use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages\nretrieved from a search engine for each question, and a full sentence answer annotated from the\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\n\n\n4\n\n\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\nanswered in a way that matches the reference answer without access to the gold passages, such as\n\u201cWhat is the weather in Volcano, CA?\u201d so performance will be lower without using gold passages.\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\nRAG can rely on parametric knowledge to generate reasonable responses.\n\n\n**3.3** **Jeopardy Question Generation**\n\n\nTo evaluate RAG\u2019s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist\n\n- f short, simple questions, we propose the more demanding task of generating Jeopardy questions.\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\nFor example, \u201cThe World Cup\u201d is the answer to the question \u201cIn 1986 Mexico scored as the first\ncountry to host this international sports competition twice.\u201d As Jeopardy questions are precise,\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\nchallenging knowledge-intensive generation task.\n\n\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\nthis is a new task,", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_4500", "chunk_text": " their answer entities constitutes a\nchallenging knowledge-intensive generation task.\n\n\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\nthis is a new task, we train a BART model for comparison. Following [67], we evaluate using the\nSQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for\nmatching entities and has higher correlation with human judgment for question generation than\nstandard metrics. We also perform two human evaluations, one to assess generation factuality, and\n\n- ne for specificity. We define factuality as whether a statement can be corroborated by trusted external\nsources, and specificity as high mutual dependence between the input and output [33]. We follow\nbest practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two\ngenerated questions, one from BART and one from RAG. They are then asked to pick one of four\n\n- ptions\u2014quuestion A is better, question B is better, both are good, or neither is good.\n\n\n**3.4** **Fact Verification**\n\n\nFEVER [56] requires classifying whether a natural language claim is supported or refuted by\nWikipedia, or whether there is not enough information to decide. The task requires retrieving\nevidence from Wikipedia relating to the claim and then reasoning over this evidence to classify\nwhether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem\ncoupled with an challenging entailment reasoning task. It also provides an appropriate testbed for\nexploring the RAG models\u2019 ability to handle classification rather than generation. We map FEVER\nclass labels (supports, refutes, or not enough info) to single output tokens and directly train with\nclaim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on\nretrieved evidence. In many real-world applications, retrieval supervision signals aren\u2019t available, and\nmodels that do not require such supervision will be applicable to a wider range of tasks. We explore\ntwo variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_4950", "chunk_text": " variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way\n(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.\n\n\n**4** **Results**\n\n\n**4.1** **Open-domain Question Answering**\n\n\nTable 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA\ntasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines\nthe generation flexibility of the \u201cclosed-book\u201d (parametric only) approaches and the performance of\n\"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results\nwithout expensive, specialized \u201csalient span masking\u201d pre-training [20]. It is worth noting that RAG\u2019s\nretriever is initialized using DPR\u2019s retriever, which uses retrieval supervision on Natural Questions\nand TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \u201ccrossencoder\u201d to re-rank documents, along with an extractive reader. RAG demonstrates that neither a\nre-ranker nor extractive reader is necessary for state-of-the-art performance.\n\n\nThere are several advantages to generating answers even when it is possible to extract them. Documents with clues about the answer but do not contain the answer verbatim can still contribute towards\na correct answer being generated, which is not possible with standard extractive approaches, leading\n\n\n5\n\n\nTable 1: Open-Domain QA Test Scores. For TQA,\nleft column uses the standard test set for OpenDomain QA, right column uses the TQA-Wiki\ntest set. See Appendix D for further details.\n\n\nModel NQ TQA WQ CT\n\n\nClosed T5-11B [52] 34.5  - /50.1 37.4  Book T5-11B+SSM[52] 36.6 - /60.5 44.7 \n\nOpen REALM [20] 40.4 - / - 40.7 46.8\nBook DPR [26] 41.5 **57.9** / - 41.1 50.6\n\n\nRAG-Token 44.1 55.2/66.1 **45.5** 50.", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_5400", "chunk_text": "8\nBook DPR [26] 41.5 **57.9** / - 41.1 50.6\n\n\nRAG-Token 44.1 55.2/66.1 **45.5** 50.0\nRAG-Seq. **44.5** 56.8/ **68.0** 45.2 **52.2**\n\n\n\nTable 2: Generation and classification Test Scores.\nMS-MARCO SotA is [4], FEVER-3 is [68] and\nFEVER-2 is [57] *Uses gold context/evidence.\nBest model without gold access underlined.\n\n\nModel Jeopardy MSMARCO FVR3 FVR2\nB-1 QB-1 R-L B-1 Label Acc.\n\n\nSotA - - **49.8** - **49.9** - **76.8** **92.2** \n\nBART 15.1 19.7 38.2 41.6 64.0 81.1\n\n\nRAG-Tok. **17.3** **22.2** 40.1 41.5\n72.5 89.5\nRAG-Seq. 14.7 21.4 40.8 44.2\n\n\n\nto more effective marginalization over documents. Furthermore, RAG can generate correct answers\neven when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such\ncases for NQ, where an extractive model would score 0%.\n\n\n**4.2** **Abstractive Question Answering**\n\n\nAs shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu\npoints and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is\nimpressive given that (i) those models access gold passages with specific information required to\ngenerate the reference answer, (ii) many questions are unanswerable without the gold passages, and\n(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers\nfrom our models. Qualitatively, we find that RAG models hallucinate less and generate factually\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_5850", "chunk_text": " 3 shows some generated answers\nfrom our models. Qualitatively, we find that RAG models hallucinate less and generate factually\ncorrect text more often than BART. Later, we also show that RAG generations are more diverse than\nBART generations (see \u00a74.5).\n\n\n**4.3** **Jeopardy Question Generation**\n\n\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\nthan RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and\nBART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on\nthe task over a state-of-the-art generation model. Evaluators also find RAG generations to be more\nspecific by a large margin. Table 3 shows typical generations from each model.\n\n\nJeopardy questions often contain two separate pieces of information, and RAG-Token may perform\nbest because it can generate responses that combine content from several documents. Figure 2 shows\nan example. When generating \u201cSun\u201d, the posterior is high for document 2 which mentions \u201cThe\nSun Also Rises\u201d. Similarly, document 1 dominates the posterior when \u201cA Farewell to Arms\u201d is\ngenerated. Intriguingly, after the first token of each book is generated, the document posterior flattens.\nThis observation suggests that the generator can complete the titles without depending on specific\ndocuments. In other words, the model\u2019s parametric knowledge is sufficient to complete the titles. We\n\n`Also Rises\"` indicating the title \"The Sun Also Rises\" is stored in BART\u2019s parameters. Similarly,\n\nhow parametric and non-parametric memories _work together_ - the non-parametric component helps\nto guide the generation, drawing out specific knowledge stored in the parametric memory.\n\n\n**4.4** **Fact Verification**\n\n\nTable 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of\nstate-of-the-art models, which are complex pipeline systems with domain-specific architectures and\nsubstantial engineering, trained using", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_6300", "chunk_text": " 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of\nstate-of-the-art models, which are complex pipeline systems with domain-specific architectures and\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\n\n\n6\n\n\n**Document 1** : his works are considered classics of American\nliterature ... His wartime experiences formed the basis for his novel\n**\u201dA Farewell to Arms\u201d** (1929) ...\n\n\n**Document 2** : ... artists of the 1920s \u201dLost Generation\u201d expatriate\ncommunity. His debut novel, **\u201dThe Sun Also Rises\u201d**, was published\nin 1926.\n\n\n\nDoc 1\n\nDoc 2\n\nDoc 3\n\nDoc 4\n\nDoc 5\n\n\n\nFigure 2: RAG-Token document posterior _p_ ( _zi|x, yi, y\u2212i_ ) for each generated token for input \u201cHemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\nwhen generating \u201cA Farewell to Arms\" and for document 2 when generating \u201cThe Sun Also Rises\".\n\n\nTable 3: Examples from generation tasks. RAG models generate more specific and factually accurate\nresponses. \u2018?\u2019 indicates factually incorrect responses, * indicates partially correct responses.\n\n\nTask Input Model Generation\n\n\n\nBART ?The middle ear is the part of the ear between the middle ear and the nose.\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\n\n\nBART The currency needed in Scotland is Pound sterling.\nRAG-T Pound is the currency needed in Scotland.\nRAG-S The currency needed in Scotland is the pound sterling.\n\n\n\nMS\nMARCO\n\n\nJeopardy\nQuestion\nGener\n\n- ation\n\n\n\ndefine middle\n\near\n\n\nwhat currency\nneeded in\n\nscotland\n\n\n\nThe Divine\nComedy\n\n\n\nBART ?This state has the largest number of counties in the U.S.\nWashington RAG-T It\u2019s the only U.S. state named for a U.S. president\nRAG-S It\u2019s the state where you\u2019ll find Mount Rainier National Park\n\n\n\nBART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\n", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_6750", "chunk_text": ".S. president\nRAG-S It\u2019s the state where you\u2019ll find Mount Rainier National Park\n\n\n\nBART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\nRAG-T Dante\u2019s \"Inferno\" is the first part of this epic poem\nRAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"\n\n\n\nFor 2-way classification, we compare against Thorne and Vlachos [57], who train RoBERTa [35]\nto classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy\nwithin 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.\nWe also analyze whether documents retrieved by RAG correspond to documents annotated as gold\nevidence in FEVER. We calculate the overlap in article titles between the top _k_ documents retrieved\nby RAG and gold evidence annotations. We find that the top retrieved document is from a gold article\nin 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n\n\n**4.5** **Additional Results**\n\n\n**Generation Diversity** Section 4.3 shows that RAG models are more factual and specific than\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n\n[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\ntotal ngrams generated by different models. Table 5 shows that RAG-Sequence\u2019s generations are\nmore diverse than RAG-Token\u2019s, and both are significantly more diverse than BART without needing\nany diversity-promoting decoding.\n\n\n**Retrieval Ablations** A key feature of RAG is learning to retrieve relevant information for the task.\nTo assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever\nduring training. As shown in Table 6, learned retrieval improves results for all tasks.\n\n\nWe compare RAG\u2019s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace\nRAG\u2019s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating\n_p_ ( _z|x_ ). Table 6 shows the", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_7200", "chunk_text": " overlap-based BM25 retriever [53]. Here, we replace\nRAG\u2019s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating\n_p_ ( _z|x_ ). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are\nheavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval\nimproves results on all other tasks, especially for Open-Domain QA, where it is crucial.\n\n\n**Index hot-swapping** An advantage of non-parametric memory models like RAG is that knowledge\ncan be easily updated at test time. Parametric-only models like T5 or BART need further training to\nupdate their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]\nWikipedia dump from December 2016 and compare outputs from RAG using this index to the newer\nindex from our main results (December 2018). We prepare a list of 82 world leaders who had changed\n\n\n7\n\n\nTable 4: Human assessments for the Jeopardy\nQuestion Generation Task.\n\n\nFactuality Specificity\n\n\nBART better 7.1% 16.8%\n\nRAG better **42.7%** **37.4%**\nBoth good 11.7% 11.8%\nBoth poor 17.7% 6.9%\nNo majority 20.8% 20.1%\n\n\n\nTable 5: Ratio of distinct to total tri-grams for\ngeneration tasks.\n\n\nMSMARCO Jeopardy QGen\n\n\nGold 89.6% 90.0%\n\nBART 70.7% 32.4%\n\nRAG-Token 77.8% 46.8%\nRAG-Seq. 83.5% 53.8%\n\n\n\nTable 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent.\n\n\nModel NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2\nExact Match B-1 QB-1 R-L B-1 Label Accuracy\n\n\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4\n**75.1** **91", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_7650", "chunk_text": "1 Label Accuracy\n\n\nRAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4\n**75.1** **91.6**\nRAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9\n\n\nRAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4\n72.9 89.4\nRAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3\n\n\nRAG-Token 43.5 54.8 **46.5** 51.9 **17.9** **22.6** 56.2 **49.4**\n74.5 90.6\nRAG-Sequence **44.0** **55.8** 44.9 **53.4** 15.3 21.5 **57.2** 47.5\n\n\nbetween these dates and use a template \u201cWho is {position}?\u201d (e.g. \u201cWho is the President of Peru?\u201d)\nto query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for\n2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched\nindices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).\nThis shows we can update RAG\u2019s world knowledge by simply replacing its non-parametric memory.\n\n\n**Effect of Retrieving more documents** Models are trained with either 5 or 10 retrieved latent\ndocuments, and we do not observe significant differences in performance between them. We have the\nflexibility to adjust the number of retrieved documents at test time, which can affect performance and\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\ndocuments", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_8100", "chunk_text": " which can affect performance and\nruntime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves\nOpen-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved\ndocuments. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for\nRAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.\n\n\n\n44\n\n\n43\n\n\n42\n\n\n41\n\n\n40\n\n\n39\n\n\n\n\n\n56\n\n\n54\n\n\n52\n\n\n50\n\n\n48\n\n\n\n\n\n\n\n80\n\n\n70\n\n\n60\n\n\n50\n\n\n40\n\n\n\n10 20 30 40 50\n\nK Retrieved Docs\n\n\n\n\n\n10 20 30 40 50\n\nK Retrieved Docs\n\n\n\nFigure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.\n\n\n**5** **Related Work**\n\n\n**Single-Task Retrieval** Prior work has shown that retrieval improves performance across a variety of\nNLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],\nfact checking [56], fact completion [48], long-form question answering [12], Wikipedia article\ngeneration [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our\nwork unifies previous successes in incorporating retrieval into individual tasks, showing that a single\nretrieval-based architecture is capable of achieving strong performance across several tasks.\n\n\n8\n\n\n**General-Purpose Architectures for NLP** Prior work on general-purpose architectures for NLP\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\nhas been shown to achieve strong performance on various classification tasks in the GLUE benchmarks [60, 61] after fine-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\nlanguage model could achieve strong performance across both discriminative and generative tasks.\nFor further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nand generative tasks. Our work aims to expand the space of possible tasks with a single", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_8550", "chunk_text": " [51, 52] propose a single, pre-trained encoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nand generative tasks. Our work aims to expand the space of possible tasks with a single, unified\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\n\n\n**Learned Retrieval** There is significant work on learning to retrieve documents in information\nretrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\nwork optimizes the retrieval module to aid in a specific, downstream task such as question answering,\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\ncan be fine-tuned for strong performance on a variety of tasks.\n\n\n**Memory-based Architectures** Our document index can be seen as a large external memory for\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\nwork. Other work improves the ability of dialog models to generate factual text by attending over\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\u2019s\nmemory by editing the document index. This approach has also been used in knowledge-intensive\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\nrather than end-to-end learnt retrieval [9].\n\n\n**Retrieve-and-Edit approaches** Our method shares some similarities with retrieve-and-edit style\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\nto provide a final output. These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_9000", "chunk_text": ". These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\ncould represent promising future work.\n\n\n**6** **Discussion**\n\n\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\nfound that people prefer RAG\u2019s generation over purely parametric BART, finding RAG more factual\nand specific. We conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\nanother objective. Our work opens up new research directions on how parametric and non-parametric\nmemories interact and how to most effectively combine them, showing promise in being applied to a\nwide variety of NLP tasks.\n\n\n9\n\n\n**Broader Impact**\n\n\nThis work offers several positive societal benefits over previous work: the fact that it is more\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less\nwith generations that are more factual, and offers more control and interpretability. RAG could be\nemployed in a wide variety of scenarios with direct benefit to society, for example by endowing it\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\neffective at their jobs.\n\n\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_9450", "chunk_text": " for GPT-2 [50] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or to automate the production of spam/phishing\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against\nmisleading content and automated spam/phishing.\n\n\n**Acknowledgments**\n\n\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\n\nprogram.\n\n\n**References**\n\n\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. _arXiv:1611.09268 [cs]_, November 2016. URL `[http:](http://arxiv.org/abs/1611.09268)`\n`[//arxiv.org/abs/1611.09268](http://arxiv.org/abs/1611.09268)` . arXiv: 1611.09268.\n\n\n[2] Petr Baudi\u0161 and Jan \u0160edivy. Modeling of the question answering task in the yodaqa system. In`\n_International Conference of the Cross-Language Evaluation Forum for European Languages_,\npages 222\u2013228. Springer, 2015. URL `[https://link.springer.com/chapter/10.1007%](https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20)`\n`[2F978-3-319-24027-5_20](https://link.springer.com/chapter/", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_9900", "chunk_text": ".com/chapter/10.1007%2F978-3-319-24027-5_20)`\n`[2F978-3-319-24027-5_20](https://link.springer.com/chapter/10.1007%2F978-3-319-24027-5_20)` .\n\n\n[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase\nfrom Question-Answer Pairs. In _Proceedings of the 2013 Conference on Empirical Methods_\n_in Natural Language Processing_, pages 1533\u20131544, Seattle, Washington, USA, October 2013.\nAssociation for Computational Linguistics. URL `[http://www.aclweb.org/anthology/](http://www.aclweb.org/anthology/D13-1160)`\n`[D13-1160](http://www.aclweb.org/anthology/D13-1160)` .\n\n\n[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. _ArXiv_, abs/2004.07159,\n2020. URL `[https://arxiv.org/abs/2004.07159](https://arxiv.org/abs/2004.07159)` .\n\n\n[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\nOpen-Domain Questions. In _Proceedings of the 55th Annual Meeting of the Association for_\n_Computational Linguistics (Volume 1: Long Papers)_, pages 1870\u20131879, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL\n`[https://www.aclweb.org/anthology/P17-1171](https://www.aclweb.org/anthology/P17-1171)` .\n\n\n[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and\nJonathan Berant. Coarse-to-fine question answering for long documents. In _Proceedings of the_\n_55th Annual Meeting of the Association for Computational Linguistics (Volume 1:", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_12600", "chunk_text": "55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_,\npages 1601\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\ndoi: 10.18653/v1/P17-1147. URL `[https://www.aclweb.org/anthology/P17-1147](https://www.aclweb.org/anthology/P17-1147)` .\n\n\n[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stackaugmented recurrent nets. In _Proceedings of the 28th International Conference on_\n_Neural Information Processing Systems - Volume 1_, NIPS\u201915, page 190\u2013198, Cambridge, MA, USA, 2015. MIT Press. URL `[https://papers.nips.cc/paper/](https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets)`\n`[5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets](https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets)` .\n\n\n[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. Dense passage retrieval for open-domain question answering. _arXiv preprint_\n_arXiv:2004.04906_, 2020. URL `[https://arxiv.org/abs/2004.04906](https://arxiv.org/abs/2004.04906)` .\n\n\n[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In _International Conference on_\n_Learning Representations_, 2020. URL `[https://openreview.net/forum?id=HklBjCEKvH](https://openreview.net/forum?id=HklBjCEKvH)` .\n\n\n[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua\nBengio and Yann LeCun, editors, _3rd International Conference on Learning", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_13950", "chunk_text": ". URL `[https://www.aclweb.org/anthology/](https://www.aclweb.org/anthology/N16-1014)`\n`[N16-1014](https://www.aclweb.org/anthology/N16-1014)` .\n\n\n[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation\nwith optimized questions and multi-turn comparisons. _ArXiv_, abs/1909.03087, 2019. URL\n`[https://arxiv.org/abs/1909.03087](https://arxiv.org/abs/1909.03087)` .\n\n\n[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine\ntranslation with joint textual and phonetic embedding. In _Proceedings of the 57th Annual_\n_Meeting of the Association for Computational Linguistics_, pages 3044\u20133049, Florence, Italy,\nJuly 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL\n`[https://www.aclweb.org/anthology/P19-1291](https://www.aclweb.org/anthology/P19-1291)` .\n\n\n[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. In _International_\n_Conference on Learning Representations_, 2018. URL `[https://openreview.net/forum?](https://openreview.net/forum?id=Hyg0vbWC-)`\n`[id=Hyg0vbWC-](https://openreview.net/forum?id=Hyg0vbWC-)` .\n\n\n[37] Yury A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search\nusing hierarchical navigable small world graphs. _IEEE Transactions on Pattern Analysis and_\n_Machine Intelligence_, 42:824\u2013836, 2016. URL `[https://arxiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320)` .\n\n\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. _arXiv", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_14400", "chunk_text": "xiv.org/abs/1603.09320](https://arxiv.org/abs/1603.09320)` .\n\n\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. _arXiv_\n_preprint arXiv:2002.06177_, 2020. URL `[https://arxiv.org/abs/2002.06177](https://arxiv.org/abs/2002.06177)` .\n\n\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\u00e4schel, Vassilis\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\nverifiability of generated text. _arXiv preprint arXiv:1911.03587_, 2019. URL `[https:](https://arxiv.org/abs/1911.03587)`\n`[//arxiv.org/abs/1911.03587](https://arxiv.org/abs/1911.03587)` .\n\n\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In _ICLR_, 2018. URL `[https://openreview.net/forum?id=r1gs9JgRZ](https://openreview.net/forum?id=r1gs9JgRZ)` .\n\n\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In _Proceedings of the 2018_\n_Conference on Empirical Methods in Natural Language Processing_, pages 2322\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-1255. URL `[https://www.aclweb.org/anthology/D18-1255](https://www.aclweb.org/anthology/D18-1255)` .\n\n\n[42] Preksha Nema and Mitesh M. Khapra. Towards a", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_14850", "chunk_text": "clweb.org/anthology/D18-1255](https://www.aclweb.org/anthology/D18-1255)` .\n\n\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\nsystems. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language_\n_Processing_, pages 3950\u20133959, Brussels, Belgium, October-November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/D18-1429. URL `[https://www.aclweb.org/](https://www.aclweb.org/anthology/D18-1429)`\n`[anthology/D18-1429](https://www.aclweb.org/anthology/D18-1429)` .\n\n\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne, editors,\n_Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic_\n\n\n13\n\n\n_approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing_\n_Systems (NIPS 2016), Barcelona, Spain, December 9, 2016_, volume 1773 of _CEUR Workshop_\n_Proceedings_ . CEUR-WS.org, 2016. URL `[http://ceur-ws.org/Vol-1773/CoCoNIPS_](http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf)`\n`[2016_paper9.pdf](http://ceur-ws.org/Vol-1773/CoCoNIPS_2016_paper9.pdf)` .\n\n\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. _arXiv preprint_\n_arXiv:1901.04085_, 2019. URL `[https://arxiv.org/abs/1901.04085](https://arxiv.org/abs/1901.04085)` .\n\n\n[45] Myle Ott", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_16200", "chunk_text": "51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. _arXiv e-prints_, 2019. URL `[https://arxiv.org/abs/1910.10683](https://arxiv.org/abs/1910.10683)` .\n\n\n[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into\nthe parameters of a language model? _arXiv e-prints_, 2020. URL `[https://arxiv.org/abs/](https://arxiv.org/abs/2002.08910)`\n`[2002.08910](https://arxiv.org/abs/2002.08910)` .\n\n\n[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and\nbeyond. _Found. Trends Inf. Retr._, 3(4):333\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/\n1500000019. URL `[https://doi.org/10.1561/1500000019](https://doi.org/10.1561/1500000019)` .\n\n\n[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\nRadford, and Jian-Bing Wang. Release strategies and the social impacts of language models.\n_ArXiv_, abs/1908.09203, 2019.\n\n\n[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances_\n_in Neural Information Processing Systems 28_, pages 2440\u20132448. Curran Associates, Inc., 2015.\nURL `[http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf)` .\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_17100", "chunk_text": "/paper/7181-attention-is-all-you-need.pdf)` .\n\n\n[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.\n_AAAI Conference on Artificial Intelligence_, 2018. URL `[https://www.aaai.org/ocs/index.](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329)`\n`[php/AAAI/AAAI18/paper/view/17329](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17329)` .\n\n\n[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding.\nIn _Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting_\n_Neural Networks for NLP_, pages 353\u2013355, Brussels, Belgium, November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/W18-5446. URL `[https://www.aclweb.org/](https://www.aclweb.org/anthology/W18-5446)`\n`[anthology/W18-5446](https://www.aclweb.org/anthology/W18-5446)` .\n\n\n[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for GeneralPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d\\textquotesingle Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information_\n_Processing Systems 32_, pages 3261\u20133275. Curran Associates, Inc., 2019. URL `[https://](https://arxiv.org/abs/1905.00537)`\n`[arxiv.org/abs/1905.00537](https://arxiv", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_18000", "chunk_text": "] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio\nand Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR_\n_2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL\n`[http://arxiv.org/abs/1410.3916](http://arxiv.org/abs/1410.3916)` .\n\n\n[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and refine: Improved sequence\ngeneration models for dialogue. In _Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd_\n_International Workshop on Search-Oriented Conversational AI_, pages 87\u201392, Brussels, Belgium,\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL\n`[https://www.aclweb.org/anthology/W18-5713](https://www.aclweb.org/anthology/W18-5713)` .\n\n\n15\n\n\n[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\u2019s transformers:\nState-of-the-art natural language processing. _ArXiv_, abs/1910.03771, 2019.\n\n\n[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semisupervised question answering. In _Proceedings of the 2019 Conference on Empirical Meth-_\n\n_ods in Natural Language Processing and the 9th International Joint Conference on Natural_\n_Language Processing (EMNLP-IJCNLP)_, pages 2495\u20132509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-125", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_18450", "chunk_text": "_Language Processing (EMNLP-IJCNLP)_, pages 2495\u20132509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL\n`[https://www.aclweb.org/anthology/D19-1253](https://www.aclweb.org/anthology/D19-1253)` .\n\n\n[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and\nJian Yin. Reasoning over semantic-level graph for fact checking. _ArXiv_, abs/1909.03745, 2019.\nURL `[https://arxiv.org/abs/1909.03745](https://arxiv.org/abs/1909.03745)` .\n\n\n16\n\n\n### Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n**A** **Implementation Details**\n\n\nFor Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models.\nFor RAG-Sequence models, we report test results using 50 retrieved documents, and we use the\nThorough Decoding approach since answers are generally short. We use greedy decoding for QA as\nwe did not find beam search improved results. For Open-MSMarco and Jeopardy question generation,\nwe report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence,\nand we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast\nDecoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance.\n\n\n**B** **Human Evaluation**\n\n\nFigure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions\nand a worked example appear when clicking \"view tool guide\".\n\n\nFigure 4 shows the user interface for human evaluation. To avoid any biases for screen position,\nwhich model corresponded to sentence A and sentence B was randomly selected for each example.\nAnnotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\ntheir annotations were", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_18900", "chunk_text": " the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to\nassess the accuracy of the annotators. Two annotators did not perform well on these examples and\ntheir annotations were removed from the results.\n\n\n**C** **Training setup Details**\n\n\nWe train all RAG models and BART baselines using Fairseq [45]. [2] We train with mixed precision\nfloating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though\ntraining and inference can be run on one GPU. We find that doing Maximum Inner Product Search\nwith FAISS is sufficiently fast on CPU, so we store document index vectors on CPU, requiring _\u223c_ 100\nGB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace\nTransformers [66] [3], which achieves equivalent performance to the previous version but is a cleaner\nand easier to use implementation. This version is also open-sourced. We also compress the document\nindex using FAISS\u2019s compression tools, reducing the CPU memory requirement to 36GB. Scripts to\nrun experiments with RAG can be found at `[https://github.com/huggingface/transformers/](https://github.com/huggingface/transformers/blob/master/examples/rag/README.md)`\n`[blob/master/examples/rag/README.md](https://github.com/huggingface/transformers/blob/master/examples/rag/README.md)` and an interactive demo of a RAG model can be found\nat `[https://huggingface.co/rag/](https://huggingface.co/rag/)`\n\n\n2 `[https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)`\n3 `[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)`\n\n\n17\n\n\n**D** **Further Details on Open-Domain QA**\n\n\nFor open-domain QA, multiple answer annotations are often available for a given question. These\nanswer annotations are exploited by extractive models during training as typically all the answer\nannotations are used to find matches within documents when preparing training data. For RAG, we\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\nthe model with each ( _q, a_ ) pair separately, leading to a small increase in", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_19350", "chunk_text": " matches within documents when preparing training data. For RAG, we\nalso make use of multiple annotation examples for Natural Questions and WebQuestions by training\nthe model with each ( _q, a_ ) pair separately, leading to a small increase in accuracy. For TriviaQA,\nthere are often many valid answers to a given question, some of which are not suitable training targets,\nsuch as emoji or spelling variants. For TriviaQA, we filter out answer candidates if they do not occur\nin top 1000 documents for the query.\n\n\n**CuratedTrec preprocessing** The answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20].\nTo overcome this, we use a pre-processing step where we first retrieve the top 1000 documents for\neach query, and use the answer that most frequently matches the regex pattern as the supervision\ntarget. If no matches are found, we resort to a simple heuristic: generate all possible permutations for\neach regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.\n\n\n**TriviaQA Evaluation setups** The open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading\ncompehension purposes. We report our results using the datasets splits used in DPR [26], which are\nconsistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public\nTriviaQA Web Development split. Roberts et al. [52] used the TriviaQA official Wikipedia test set\ninstead. F\u00e9vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See\nappendix of [14]). We report results on both test sets to enable fair comparison to both approaches.\nWe find that our performance is much higher using the official Wiki test set, rather than the more\nconventional open-domain test set, which we attribute to the official Wiki test set questions being\nsimpler to answer from Wikipedia.\n\n\n**E** **Further Details on FEVER**\n\n\nFor FEVER classification, we follow the practice from [32], and first re-generate the claim, and\nthen classify using the representation of the final hidden state, before finally marginalizing across\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks.", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_19800", "chunk_text": " practice from [32], and first re-generate the claim, and\nthen classify using the representation of the final hidden state, before finally marginalizing across\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\nfirst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\nwe explore in the main paper. FEVER\u2019s other sub-task involves extracting sentences from Wikipedia\nas evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to\nus, directly tackling this task is not straightforward. We hope to address this in future work.\n\n\n**F** **Null Document Probabilities**\n\n\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order\nto model cases where no useful information could be retrieved for a given input. Here, if _k_ documents\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\ndocument, before marginalizing over _k_ + 1 predictions. We explored modelling this null document\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\n(iii) a neural network to predict the logit. We did not find that these improved performance, so in\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\ndocuments for questions that are less likely to benefit from retrieval, suggesting that null document\nmechanisms may not be necessary for RAG.\n\n\n**G** **Parameters**\n\n\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n\n\n18\n\n\nTable 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\n\n\nTask Train Development Test\n\n\nNatural Questions 79169 8758 3611\nTriviaQA 78786 8838 11314\nWebQuestions 3418 362 2033\nCuratedTrec 635 134 635\nJeopardy", "token_count": 500, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_20250", "chunk_text": "\n\n\nNatural Questions 79169 8758 3611\nTriviaQA 78786 8838 11314\nWebQuestions 3418 362 2033\nCuratedTrec 635 134 635\nJeopardy Question Generation 97392 13714 26849\nMS-MARCO 153726 12468 101093*\nFEVER-3-way 145450 10000 10000\nFEVER-2-way 96966 6666 6666\n\n\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/nonparametric models require far fewer trainable parameters for strong open-domain QA performance.\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\npoint precision to manage memory and disk footprints.\n\n\n**H** **Retrieval Collapse**\n\n\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\nretrieval component would \u201ccollapse\u201d and learn to retrieve the same documents regardless of the\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\n\n\n**I** **Number of instances per dataset**\n\n\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\n\n\n19\n\n\n", "token_count": 453, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2005.11401_rag_lewis:chunk_20700", "chunk_text": ".\n\n\n19\n\n\n", "token_count": 3, "metadata": {"arxiv_id": "2005.11401", "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "authors": ["Patrick Lewis", "Ethan Perez", "Aleksandra Piktus", "Fabio Petroni", "Vladimir Karpukhin", "Naman Goyal", "Heinrich K\u00fcttler", "Mike Lewis", "Wen-tau Yih", "Tim Rockt\u00e4schel", "Sebastian Riedel", "Douwe Kiela"], "year": 2020, "url": "https://arxiv.org/pdf/2005.11401v4"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_0", "chunk_text": "1\n\n\n## Retrieval-Augmented Generation for Large Language Models: A Survey\n\nYunfan Gao [a], Yun Xiong [b], Xinyu Gao [b], Kangxiang Jia [b], Jinliu Pan [b], Yuxi Bi [c], Yi Dai [a], Jiawei Sun [a], Meng\nWang [c], and Haofen Wang [a,c]\n\n\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\ncCollege of Design and Innovation, Tongji University\n\n\n\n_**Abstract**_ **\u2014Large Language Models (LLMs) showcase impres-**\n**sive capabilities but encounter challenges like hallucination,**\n\n**outdated knowledge, and non-transparent, untraceable reasoning**\n**processes. Retrieval-Augmented Generation (RAG) has emerged**\n**as a promising solution by incorporating knowledge from external**\n**databases. This enhances the accuracy and credibility of the**\n**generation, particularly for knowledge-intensive tasks, and allows**\n**for continuous knowledge updates and integration of domain-**\n**specific information. RAG synergistically merges LLMs\u2019 intrin-**\n**sic knowledge with the vast, dynamic repositories of external**\n**databases. This comprehensive review paper offers a detailed**\n**examination of the progression of RAG paradigms, encompassing**\n**the Naive RAG, the Advanced RAG, and the Modular RAG.**\n**It meticulously scrutinizes the tripartite foundation of RAG**\n**frameworks, which includes the retrieval, the generation and the**\n**augmentation techniques. The paper highlights the state-of-the-**\n**art technologies embedded in each of these critical components,**\n**providing a profound understanding of the advancements in RAG**\n**systems. Furthermore, this paper introduces up-to-date evalua-**\n**tion framework and benchmark. At the end, this article delineates**\n**the challenges currently faced and points out prospective avenues**\n**for research and development** [1] **.**\n\n\n_**Index Terms**_ **\u2014Large language model, retrieval-augmented gen-**\n**eration, natural language processing, information retrieval**\n\n\nI. INTRODUCTION\n# L ARGE language models (LLMs) have achieved remark-able success, though they still face significant limitations,\n\nespecially in domain-specific", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_450", "chunk_text": "-augmented gen-**\n**eration, natural language processing, information retrieval**\n\n\nI. INTRODUCTION\n# L ARGE language models (LLMs) have achieved remark-able success, though they still face significant limitations,\n\nespecially in domain-specific or knowledge-intensive tasks [1],\nnotably producing \u201challucinations\u201d [2] when handling queries\nbeyond their training data or requiring current information. To\n\n- vercome challenges, Retrieval-Augmented Generation (RAG)\nenhances LLMs by retrieving relevant document chunks from\nexternal knowledge base through semantic similarity calculation. By referencing external knowledge, RAG effectively\nreduces the problem of generating factually incorrect content.\nIts integration into LLMs has resulted in widespread adoption,\nestablishing RAG as a key technology in advancing chatbots\nand enhancing the suitability of LLMs for real-world applications.\n\nRAG technology has rapidly developed in recent years, and\nthe technology tree summarizing related research is shown\n\n\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\n1Resources are available at [https://github.com/Tongji-KGLLM/](https://github.com/Tongji-KGLLM/RAG-Survey)\n[RAG-Survey](https://github.com/Tongji-KGLLM/RAG-Survey)\n\n\n\nin Figure 1. The development trajectory of RAG in the era\n\n- f large models exhibits several distinct stage characteristics.\nInitially, RAG\u2019s inception coincided with the rise of the\nTransformer architecture, focusing on enhancing language\nmodels by incorporating additional knowledge through PreTraining Models (PTM). This early stage was characterized\nby foundational work aimed at refining pre-training techniques\n\n[3]\u2013[5].The subsequent arrival of ChatGPT [6] marked a\npivotal moment, with LLM demonstrating powerful in context\nlearning (ICL) capabilities. RAG research shifted towards\nproviding better information for LLMs to answer more complex and knowledge-intensive tasks during the inference stage,\nleading to rapid development in RAG studies. As research\nprogressed, the enhancement of RAG was no longer limited\nto the inference stage but began to incorporate more with LLM\nfine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth,\nyet it has not been accompanied by a systematic synthesis that\ncould clarify its broader trajectory. This survey endeavors to\nfill this gap by mapping out the RAG", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_900", "chunk_text": "LM\nfine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth,\nyet it has not been accompanied by a systematic synthesis that\ncould clarify its broader trajectory. This survey endeavors to\nfill this gap by mapping out the RAG process and charting\nits evolution and anticipated future paths, with a focus on the\nintegration of RAG within LLMs. This paper considers both\ntechnical paradigms and research methods, summarizing three\nmain research paradigms from over 100 RAG studies, and\nanalyzing key technologies in the core stages of \u201cRetrieval,\u201d\n\u201cGeneration,\u201d and \u201cAugmentation.\u201d On the other hand, current\nresearch tends to focus more on methods, lacking analysis and\nsummarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks,\nand evaluation methods applicable to RAG. Overall, this\npaper sets out to meticulously compile and categorize the\nfoundational technical concepts, historical progression, and\nthe spectrum of RAG methodologies and applications that\nhave emerged post-LLMs. It is designed to equip readers and\nprofessionals with a detailed and structured understanding of\nboth large models and RAG. It aims to illuminate the evolution\n\n- f retrieval augmentation techniques, assess the strengths and\nweaknesses of various approaches in their respective contexts,\nand speculate on upcoming trends and innovations.\nOur contributions are as follows:\n\n\n_\u2022_ In this survey, we present a thorough and systematic\nreview of the state-of-the-art RAG methods, delineating\nits evolution through paradigms including naive RAG,\n\n\n2\n\n\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\n\n\n\nadvanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRet", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_1350", "chunk_text": ". This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, and\ndelve into their synergies, elucidating how these components intricately collaborate to form a cohesive and\neffective RAG framework.\n\n\n_\u2022_ We have summarized the current assessment methods of\n\nRAG, covering 26 tasks, nearly 50 datasets, outlining\nthe evaluation objectives and metrics, as well as the\ncurrent evaluation benchmarks and tools. Additionally,\nwe anticipate future directions for RAG, emphasizing\npotential enhancements to tackle current challenges.\n\n\nThe paper unfolds as follows: Section II introduces the\nmain concept and current paradigms of RAG. The following\nthree sections explore core components\u2014\u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, respectively. Section III focuses\n\n- n optimization methods in retrieval,including indexing, query\nand embedding optimization. Section IV concentrates on postretrieval process and LLM fine-tuning in generation. Section V\nanalyzes the three augmentation processes. Section VI focuses\n\n- n RAG\u2019s downstream tasks and evaluation system. Section VII mainly discusses the challenges that RAG currently\n\n\n\nfaces and its future development directions. At last, the paper\nconcludes in Section VIII.\n\n\nII. OVERVIEW OF RAG\n\n\nA typical application of RAG is illustrated in Figure 2.\nHere, a user poses a question to ChatGPT about a recent,\nwidely discussed news. Given ChatGPT\u2019s reliance on pretraining data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information\ngap by sourcing and incorporating knowledge from external\ndatabases. In this case, it gathers relevant news articles related\nto the user\u2019s query. These articles, combined with the original\nquestion, form a comprehensive prompt that empowers LLMs\nto generate a well-informed answer.\nThe RAG research paradigm is continuously evolving, and\nwe categorize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\n\n- f the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_1800", "chunk_text": " RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\n\n- f the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response to these specific shortcomings in Naive RAG.\n\n\n_A. Naive RAG_\n\n\nThe Naive RAG research paradigm represents the earliest methodology, which gained prominence shortly after the\n\n\n3\n\n\nFig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\nencoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the final answer.\n\n\n\nwidespread adoption of ChatGPT. The Naive RAG follows\na traditional process that includes indexing, retrieval, and\ngeneration, which is also characterized as a \u201cRetrieve-Read\u201d\nframework [7].\n\n_Indexing_ starts with the cleaning and extraction of raw data\nin diverse formats like PDF, HTML, Word, and Markdown,\nwhich is then converted into a uniform plain text format. To\naccommodate the context limitations of language models, text\nis segmented into smaller, digestible chunks. Chunks are then\nencoded into vector representations using an embedding model\nand stored in vector database. This step is crucial for enabling\nefficient similarity searches in the subsequent retrieval phase.\n_Retrieval_ . Upon receipt of a user query, the RAG system\nemploys the same encoding model utilized during the indexing\nphase to transform the query into a vector representation.\nIt then computes the similarity scores between the query\nvector and the vector of chunks within the indexed corpus.\nThe system prioritizes and retrieves the top K chunks that\ndemonstrate the greatest similarity to the query. These chunks\nare subsequently used as the expanded context in prompt.\n_Generation_ . The posed query and selected documents are\nsynthesized into a coherent prompt to which a large language\nmodel is tasked with formulating a response. The model\u2019s\napproach to answering may vary depending on task-specific\ncriteria, allowing it to either draw upon its inherent parametric\nknowledge or restrict its responses to the information contained within the provided documents. In cases of ongoing\ndialogues,", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_2250", "chunk_text": " The model\u2019s\napproach to answering may vary depending on task-specific\ncriteria, allowing it to either draw upon its inherent parametric\nknowledge or restrict its responses to the information contained within the provided documents. In cases of ongoing\ndialogues, any existing conversational history can be integrated\ninto the prompt, enabling the model to engage in multi-turn\ndialogue interactions effectively.\n\nHowever, Naive RAG encounters notable drawbacks:\n\n\n\n_Retrieval Challenges_ . The retrieval phase often struggles\nwith precision and recall, leading to the selection of misaligned\n\n- r irrelevant chunks, and the missing of crucial information.\n_Generation Difficulties_ . In generating responses, the model\nmay face the issue of hallucination, where it produces content not supported by the retrieved context. This phase can\nalso suffer from irrelevance, toxicity, or bias in the outputs,\ndetracting from the quality and reliability of the responses.\n_Augmentation Hurdles_ . Integrating retrieved information\nwith the different task can be challenging, sometimes resulting\nin disjointed or incoherent outputs. The process may also\nencounter redundancy when similar information is retrieved\nfrom multiple sources, leading to repetitive responses. Determining the significance and relevance of various passages and\nensuring stylistic and tonal consistency add further complexity.\nFacing complex issues, a single retrieval based on the original\nquery may not suffice to acquire adequate context information.\nMoreover, there\u2019s a concern that generation models might\n\n- verly rely on augmented information, leading to outputs that\nsimply echo retrieved content without adding insightful or\nsynthesized information.\n\n\n_B. Advanced RAG_\n\n\nAdvanced RAG introduces specific improvements to overcome the limitations of Naive RAG. Focusing on enhancing retrieval quality, it employs pre-retrieval and post-retrieval strategies. To tackle the indexing issues, Advanced RAG refines\nits indexing techniques through the use of a sliding window\napproach, fine-grained segmentation, and the incorporation of\nmetadata. Additionally, it incorporates several optimization\nmethods to streamline the retrieval process [8].\n\n\n4\n\n\nFig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\nchain-like", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_2700", "chunk_text": " mainly consists of three parts: indexing, retrieval and generation. (Middle)\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\ngeneration; it includes methods such as iterative and adaptive retrieval.\n\n\n\n_Pre-retrieval process_ . In this stage, the primary focus is\n\n- n optimizing the indexing structure and the original query.\nThe goal of optimizing indexing is to enhance the quality of\nthe content being indexed. This involves strategies: enhancing\ndata granularity, optimizing index structures, adding metadata,\nalignment optimization, and mixed retrieval. While the goal\n\n- f query optimization is to make the user\u2019s original question\nclearer and more suitable for the retrieval task. Common\n\nmethods include query rewriting query transformation, query\nexpansion and other techniques [7], [9]\u2013[11].\n\n\n_Post-Retrieval Process_ . Once relevant context is retrieved,\nit\u2019s crucial to integrate it effectively with the query. The main\nmethods in post-retrieval process include rerank chunks and\ncontext compressing. Re-ranking the retrieved information to\nrelocate the most relevant content to the edges of the prompt is\na key strategy. This concept has been implemented in frameworks such as LlamaIndex [2], LangChain [3], and HayStack [12].\nFeeding all relevant documents directly into LLMs can lead\nto information overload, diluting the focus on key details with\nirrelevant content.To mitigate this, post-retrieval efforts concentrate on selecting the essential information, emphasizing\ncritical sections, and shortening the context to be processed.\n\n\n[2https://www.llamaindex.ai](https://www.llamaindex.ai)\n[3https://www.langchain.com/](https://www.langchain.com/)\n\n\n\n_C. Modular RAG_\n\n\nThe modular RAG architecture advances beyond the former two RAG paradigms, offering enhanced adaptability and\nversatility. It incorporates diverse strategies for improving its\ncomponents, such as adding a search module for similarity\nsearches and refining the retriever through fine-tuning. Innovations like restructured RAG modules [", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_3150", "chunk_text": ", offering enhanced adaptability and\nversatility. It incorporates diverse strategies for improving its\ncomponents, such as adding a search module for similarity\nsearches and refining the retriever through fine-tuning. Innovations like restructured RAG modules [13] and rearranged\nRAG pipelines [14] have been introduced to tackle specific\nchallenges. The shift towards a modular RAG approach is\nbecoming prevalent, supporting both sequential processing and\nintegrated end-to-end training across its components. Despite\nits distinctiveness, Modular RAG builds upon the foundational\nprinciples of Advanced and Naive RAG, illustrating a progression and refinement within the RAG family.\n\n_1) New Modules:_ The Modular RAG framework introduces\nadditional specialized components to enhance retrieval and\nprocessing capabilities. The Search module adapts to specific scenarios, enabling direct searches across various data\nsources like search engines, databases, and knowledge graphs,\nusing LLM-generated code and query languages [15]. RAGFusion addresses traditional search limitations by employing\na multi-query strategy that expands user queries into diverse\nperspectives, utilizing parallel vector searches and intelligent\nre-ranking to uncover both explicit and transformative knowledge [16]. The Memory module leverages the LLM\u2019s memory\nto guide retrieval, creating an unbounded memory pool that\n\n\naligns the text more closely with data distribution through iterative self-enhancement [17], [18]. Routing in the RAG system\nnavigates through diverse data sources, selecting the optimal\npathway for a query, whether it involves summarization,\nspecific database searches, or merging different information\nstreams [19]. The Predict module aims to reduce redundancy\nand noise by generating context directly through the LLM,\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\nmodule tailors RAG to various downstream tasks, automating\nprompt retrieval for zero-shot inputs and creating task-specific\nretrievers through few-shot query generation [20], [21] .This\ncomprehensive approach not only streamlines the retrieval process but also significantly improves the quality and relevance\n\n- f the information retrieved, catering to a wide array of tasks\nand queries with enhanced precision and flexibility.\n_2) New Patterns:_ Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration\nto address specific challenges. This goes beyond the fixed\nstructures of Naive and Advanced RAG, characterized by a\nsimple \u201cRetrieve\u201d and \u201cRead\u201d mechanism", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_3600", "chunk_text": " Patterns:_ Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration\nto address specific challenges. This goes beyond the fixed\nstructures of Naive and Advanced RAG, characterized by a\nsimple \u201cRetrieve\u201d and \u201cRead\u201d mechanism. Moreover, Modular\nRAG expands this flexibility by integrating new modules or\nadjusting interaction flow among existing ones, enhancing its\napplicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read [7]model\nleverage the LLM\u2019s capabilities to refine retrieval queries\nthrough a rewriting module and a LM-feedback mechanism\nto update rewriting model., improving task performance.\nSimilarly, approaches like Generate-Read [13] replace traditional retrieval with LLM-generated content, while ReciteRead [22] emphasizes retrieval from model weights, enhancing the model\u2019s ability to handle knowledge-intensive tasks.\nHybrid retrieval strategies integrate keyword, semantic, and\nvector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings\n(HyDE) [11] seeks to improve retrieval relevance by focusing\n\n- n embedding similarities between generated answers and real\ndocuments.\n\nAdjustments in module arrangement and interaction, such\nas the Demonstrate-Search-Predict (DSP) [23] framework\nand the iterative Retrieve-Read-Retrieve-Read flow of ITERRETGEN [14], showcase the dynamic use of module outputs to bolster another module\u2019s functionality, illustrating a\nsophisticated understanding of enhancing module synergy.\nThe flexible orchestration of Modular RAG Flow showcases\nthe benefits of adaptive retrieval through techniques such as\nFLARE [24] and Self-RAG [25]. This approach transcends\nthe fixed RAG retrieval process by evaluating the necessity\n\n- f retrieval based on different scenarios. Another benefit of\na flexible architecture is that the RAG system can more\neasily integrate with other technologies (such as fine-tuning\n\n- r reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\n\n\n_D. RAG vs Fine-tuning_\n\nThe augmentation of LLMs has attracted considerable attention due to their growing prevalence. Among the optimization\n\n\n\n5\n\n\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt engineering. Each method has distinct characteristics", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_4050", "chunk_text": "The augmentation of LLMs has attracted considerable attention due to their growing prevalence. Among the optimization\n\n\n\n5\n\n\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt engineering. Each method has distinct characteristics as illustrated in Figure 4. We used a quadrant chart to\nillustrate the differences among three methods in two dimensions: external knowledge requirements and model adaption\nrequirements. Prompt engineering leverages a model\u2019s inherent\ncapabilities with minimum necessity for external knowledge\nand model adaption. RAG can be likened to providing a model\nwith a tailored textbook for information retrieval, ideal for precise information retrieval tasks. In contrast, FT is comparable\nto a student internalizing knowledge over time, suitable for\nscenarios requiring replication of specific structures, styles, or\nformats.\n\nRAG excels in dynamic environments by offering realtime knowledge updates and effective utilization of external\nknowledge sources with high interpretability. However, it\ncomes with higher latency and ethical considerations regarding\ndata retrieval. On the other hand, FT is more static, requiring\nretraining for updates but enabling deep customization of the\nmodel\u2019s behavior and style. It demands significant computational resources for dataset preparation and training, and\nwhile it can reduce hallucinations, it may face challenges with\nunfamiliar data.\n\nIn multiple evaluations of their performance on various\nknowledge-intensive tasks across different topics, [28] revealed that while unsupervised fine-tuning shows some improvement, RAG consistently outperforms it, for both existing knowledge encountered during training and entirely new\nknowledge. Additionally, it was found that LLMs struggle\nto learn new factual information through unsupervised finetuning. The choice between RAG and FT depends on the\nspecific needs for data dynamics, customization, and computational capabilities in the application context. RAG and\nFT are not mutually exclusive and can complement each\n\n- ther, enhancing a model\u2019s capabilities at different levels.\nIn some instances, their combined use may lead to optimal\nperformance. The optimization process involving RAG and FT\nmay require multiple iterations to achieve satisfactory results.\n\n\nIII. RETRIEVAL\n\n\nIn the context of RAG, it is crucial to efficiently retrieve\nrelevant documents from the data source. There are several\n\nkey issues involved, such as the retrieval source, retrieval\ngranularity, pre-processing of the retrieval, and selection of\nthe corresponding embedding model.\n\n\n_A. Retrieval Source_\n\n\nRAG", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_4500", "chunk_text": "relevant documents from the data source. There are several\n\nkey issues involved, such as the retrieval source, retrieval\ngranularity, pre-processing of the retrieval, and selection of\nthe corresponding embedding model.\n\n\n_A. Retrieval Source_\n\n\nRAG relies on external knowledge to enhance LLMs, while\nthe type of retrieval source and the granularity of retrieval\nunits both affect the final generation results.\n_1) Data Structure:_ Initially, text is s the mainstream source\n\n- f retrieval. Subsequently, the retrieval source expanded to include semi-structured data (PDF) and structured data (Knowledge Graph, KG) for enhancement. In addition to retrieving\nfrom original external sources, there is also a growing trend in\nrecent researches towards utilizing content generated by LLMs\nthemselves for retrieval and enhancement purposes.\n\n\n6\n\n\n\nTABLE I\n\nSUMMARY OF RAG METHODS\n\n\nRetrieval Retrieval Augmentation Retrieval\nMethod Retrieval Source\nData Type Granularity Stage process\n\n\nCoG [29] Wikipedia Text Phrase Pre-training Iterative\nDenseX [30] FactoidWiki Text Proposition Inference Once\nEAR [31] Dataset-base Text Sentence Tuning Once\nUPRISE [20] Dataset-base Text Sentence Tuning Once\nRAST [32] Dataset-base Text Sentence Tuning Once\nSelf-Mem [17] Dataset-base Text Sentence Tuning Iterative\nFLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive\nPGRA [33] Wikipedia Text Sentence Inference Once\nFILCO [34] Wikipedia Text Sentence Inference Once\nRADA [35] Dataset-base Text Sentence Inference Once\nFilter-rerank [36] Synthesized dataset Text Sentence Inference Once\nR-GQA [37] Dataset-base Text Sentence Pair Tuning Once\nLLM-R [38] Dataset-base Text Sentence Pair Inference Iterative\nTIGER [39] Dataset-base Text Item-base Pre-training Once\nLM-Indexer [40] Dataset-base Text Item-base Tuning Once\nBEQUE [9] Dataset-base Text Item-base Tuning Once\nCT-RAG [41] Synthesized dataset Text Item-base Tuning Once\nAtlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative\nRAVEN [43] Wikipedia Text Chunk Pre-training Once\nRETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative\nINSTRUCTRETRO [45] Pre-training corpus Text Chunk", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_5400", "chunk_text": "ing Once\nZemi [66] C4 Text Doc Tuning Once\nCRAG [67] Arxiv Text Doc Inference Once\n1-PAGER [68] Wikipedia Text Doc Inference Iterative\nPRCA [69] Dataset-base Text Doc Inference Once\nQLM-Doc-ranking [70] Dataset-base Text Doc Inference Once\nRecomp [71] Wikipedia Text Doc Inference Once\nDSP [23] Wikipedia Text Doc Inference Iterative\nRePLUG [72] Pile Text Doc Inference Once\nARM-RAG [73] Dataset-base Text Doc Inference Iterative\nGenRead [13] LLMs Text Doc Inference Iterative\nUniMS-RAG [74] Dataset-base Text Multi Tuning Once\nCREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once\nPKG [75] LLM Tabular,Text Chunk Inference Once\nSANTA [76] Dataset-base Code,Text Item Pre-training Once\nSURGE [77] Freebase KG Sub-Graph Tuning Once\nMK-ToD [78] Dataset-base KG Entity Tuning Once\nDual-Feedback-ToD [79] Dataset-base KG Entity Sequence Tuning Once\nKnowledGPT [15] Dataset-base KG Triplet Inference Muti-time\nFABULA [80] Dataset-base,Graph KG Entity Inference Once\nHyKGE [81] CMeKG KG Entity Inference Once\nKALMV [82] Wikipedia KG Triplet Inference Iterative\nRoG [83] Freebase KG Triplet Inference Iterative\nG-Retriever [84] Dataset-base TextGraph Sub-Graph Inference Once\n\n\n7\n\n\nFig. 4. RAG compared with other model optimization methods in the aspects of \u201cExternal Knowledge Required\u201d and \u201cModel Adaption Required\u201d. Prompt\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\n\n\n\n_Unstructured Data_, such as text, is the most widely used\nretrieval source, which are mainly gathered from corpus. For\n\n- pen-domain question-answering (ODQA", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_5850", "chunk_text": "AG has become more integrated with fine-tuning techniques.\n\n\n\n_Unstructured Data_, such as text, is the most widely used\nretrieval source, which are mainly gathered from corpus. For\n\n- pen-domain question-answering (ODQA) tasks, the primary\nretrieval sources are Wikipedia Dump with the current major\nversions including HotpotQA [4] (1st October, 2017), DPR [5] (20\nDecember, 2018). In addition to encyclopedic data, common\nunstructured data includes cross-lingual text [19] and domainspecific data (such as medical [67]and legal domains [29]).\n_Semi-structured data_ . typically refers to data that contains a\ncombination of text and table information, such as PDF. Handling semi-structured data poses challenges for conventional\nRAG systems due to two main reasons. Firstly, text splitting\nprocesses may inadvertently separate tables, leading to data\ncorruption during retrieval. Secondly, incorporating tables into\nthe data can complicate semantic similarity searches. When\ndealing with semi-structured data, one approach involves leveraging the code capabilities of LLMs to execute Text-2-SQL\nqueries on tables within databases, such as TableGPT [85].\nAlternatively, tables can be transformed into text format for\nfurther analysis using text-based methods [75]. However, both\n\n- f these methods are not optimal solutions, indicating substantial research opportunities in this area.\n_Structured data_, such as knowledge graphs (KGs) [86],\nwhich are typically verified and can provide more precise information. KnowledGPT [15] generates KB search queries and\nstores knowledge in a personalized base, enhancing the RAG\nmodel\u2019s knowledge richness. In response to the limitations of\nLLMs in understanding and answering questions about textual\ngraphs, G-Retriever [84] integrates Graph Neural Networks\n\n\n[4https://hotpotqa.github.io/wiki-readme.html](https://hotpotqa.github.io/wiki-readme.html)\n[5https://github.com/facebookresearch/DPR](https://github.com/facebookresearch/DPR)\n\n\n\n(GNNs), LLMs and RAG, enhancing graph comprehension\nand question-answering capabilities through soft prompting\n\n- f the LLM, and employs the Prize-Collecting Steiner Tree\n(PCST) optimization problem for targeted graph retrieval. On\nthe contrary, it requires additional effort", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_6300", "chunk_text": " graph comprehension\nand question-answering capabilities through soft prompting\n\n- f the LLM, and employs the Prize-Collecting Steiner Tree\n(PCST) optimization problem for targeted graph retrieval. On\nthe contrary, it requires additional effort to build, validate,\nand maintain structured databases. On the contrary, it requires\nadditional effort to build, validate, and maintain structured\n\ndatabases.\n\n_LLMs-Generated Content._ Addressing the limitations of\nexternal auxiliary information in RAG, some research has\nfocused on exploiting LLMs\u2019 internal knowledge. SKR [58]\nclassifies questions as known or unknown, applying retrieval\nenhancement selectively. GenRead [13] replaces the retriever\nwith an LLM generator, finding that LLM-generated contexts\n\n- ften contain more accurate answers due to better alignment\nwith the pre-training objectives of causal language modeling.\nSelfmem [17] iteratively creates an unbounded memory pool\nwith a retrieval-enhanced generator, using a memory selector to choose outputs that serve as dual problems to the\n\n- riginal question, thus self-enhancing the generative model.\nThese methodologies underscore the breadth of innovative\ndata source utilization in RAG, striving to improve model\nperformance and task effectiveness.\n_2) Retrieval Granularity:_ Another important factor besides\nthe data format of the retrieval source is the granularity of\nthe retrieved data. Coarse-grained retrieval units theoretically\ncan provide more relevant information for the problem, but\nthey may also contain redundant content, which could distract\nthe retriever and language models in downstream tasks [50],\n\n[87]. On the other hand, fine-grained retrieval unit granularity\nincreases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge. Choosing\n\n\nthe appropriate retrieval granularity during inference can be\na simple and effective strategy to improve the retrieval and\ndownstream task performance of dense retrievers.\nIn text, retrieval granularity ranges from fine to coarse,\nincluding Token, Phrase, Sentence, Proposition, Chunks, Document. Among them, DenseX [30]proposed the concept of\nusing propositions as retrieval units. Propositions are defined\nas atomic expressions in the text, each encapsulating a unique\nfactual segment and presented in a concise, self-contained natural language format. This approach aims to enhance retrieval\nprecision and relevance. On the Knowledge Graph (KG),\nretrieval granularity includes Entity, Triplet, and sub-Graph", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_6750", "chunk_text": " a unique\nfactual segment and presented in a concise, self-contained natural language format. This approach aims to enhance retrieval\nprecision and relevance. On the Knowledge Graph (KG),\nretrieval granularity includes Entity, Triplet, and sub-Graph.\nThe granularity of retrieval can also be adapted to downstream\ntasks, such as retrieving Item IDs [40]in recommendation tasks\nand Sentence pairs [38]. Detailed information is illustrated in\nTable I.\n\n\n_B. Indexing Optimization_\n\nIn the Indexing phase, documents will be processed, segmented, and transformed into Embeddings to be stored in a\nvector database. The quality of index construction determines\nwhether the correct context can be obtained in the retrieval\n\nphase.\n_1) Chunking Strategy:_ The most common method is to split\nthe document into chunks on a fixed number of tokens (e.g.,\n100, 256, 512) [88]. Larger chunks can capture more context,\nbut they also generate more noise, requiring longer processing\ntime and higher costs. While smaller chunks may not fully\nconvey the necessary context, they do have less noise. However, chunks leads to truncation within sentences, prompting\nthe optimization of a recursive splits and sliding window meth\n- ds, enabling layered retrieval by merging globally related\ninformation across multiple retrieval processes [89]. Nevertheless, these approaches still cannot strike a balance between\nsemantic completeness and context length. Therefore, methods\nlike Small2Big have been proposed, where sentences (small)\nare used as the retrieval unit, and the preceding and following\nsentences are provided as (big) context to LLMs [90].\n_2) Metadata Attachments:_ Chunks can be enriched with\nmetadata information such as page number, file name, author,category timestamp. Subsequently, retrieval can be filtered\nbased on this metadata, limiting the scope of the retrieval.\nAssigning different weights to document timestamps during\nretrieval can achieve time-aware RAG, ensuring the freshness\n\n- f knowledge and avoiding outdated information.\nIn addition to extracting metadata from the original documents, metadata can also be artificially constructed. For\nexample, adding summaries of paragraph, as well as introducing hypothetical questions. This method is also known as\nReverse HyDE. Specifically, using LLM to generate questions\nthat can be answered by the document, then calculating the\nsimilarity between the original question and the hypothetical\nquestion during retrieval to reduce the semantic gap between\nthe question and the", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_7200", "chunk_text": "\nReverse HyDE. Specifically, using LLM to generate questions\nthat can be answered by the document, then calculating the\nsimilarity between the original question and the hypothetical\nquestion during retrieval to reduce the semantic gap between\nthe question and the answer.\n_3) Structural Index:_ One effective method for enhancing\ninformation retrieval is to establish a hierarchical structure for\n\nthe documents. By constructing In structure, RAG system can\nexpedite the retrieval and processing of pertinent data.\n\n\n\n8\n\n\n_Hierarchical index structure_ . File are arranged in parentchild relationships, with chunks linked to them. Data summaries are stored at each node, aiding in the swift traversal\n\n- f data and assisting the RAG system in determining which\nchunks to extract. This approach can also mitigate the illusion\ncaused by block extraction issues.\n_Knowledge Graph index_ . Utilize KG in constructing the\nhierarchical structure of documents contributes to maintaining\nconsistency. It delineates the connections between different\nconcepts and entities, markedly reducing the potential for\nillusions. Another advantage is the transformation of the\ninformation retrieval process into instructions that LLM can\ncomprehend, thereby enhancing the accuracy of knowledge\nretrieval and enabling LLM to generate contextually coherent\nresponses, thus improving the overall efficiency of the RAG\nsystem. To capture the logical relationship between document\ncontent and structure, KGP [91] proposed a method of building\nan index between multiple documents using KG. This KG\nconsists of nodes (representing paragraphs or structures in the\ndocuments, such as pages and tables) and edges (indicating\nsemantic/lexical similarity between paragraphs or relationships\nwithin the document structure), effectively addressing knowledge retrieval and reasoning problems in a multi-document\nenvironment.\n\n\n_C. Query Optimization_\n\n\nOne of the primary challenges with Naive RAG is its\ndirect reliance on the user\u2019s original query as the basis for\nretrieval. Formulating a precise and clear question is difficult,\nand imprudent queries result in subpar retrieval effectiveness.\nSometimes, the question itself is complex, and the language\nis not well-organized. Another difficulty lies in language\ncomplexity ambiguity. Language models often struggle when\ndealing with specialized vocabulary or ambiguous abbreviations with multiple meanings. For instance, they may not\ndiscern whether \u201cLLM\u201d refers to _large language model_ - r a\n_Master of Laws_ in a legal context.\n_1) Query", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_7650", "chunk_text": "aling with specialized vocabulary or ambiguous abbreviations with multiple meanings. For instance, they may not\ndiscern whether \u201cLLM\u201d refers to _large language model_ - r a\n_Master of Laws_ in a legal context.\n_1) Query Expansion:_ Expanding a single query into multiple queries enriches the content of the query, providing\nfurther context to address any lack of specific nuances, thereby\nensuring the optimal relevance of the generated answers.\n_Multi-Query_ . By employing prompt engineering to expand\nqueries via LLMs, these queries can then be executed in\nparallel. The expansion of queries is not random, but rather\nmeticulously designed.\n_Sub-Query_ . The process of sub-question planning represents\nthe generation of the necessary sub-questions to contextualize\nand fully answer the original question when combined. This\nprocess of adding relevant context is, in principle, similar\nto query expansion. Specifically, a complex question can be\ndecomposed into a series of simpler sub-questions using the\nleast-to-most prompting method [92].\n_Chain-of-Verification(CoVe)_ . The expanded queries undergo\nvalidation by LLM to achieve the effect of reducing hallucinations. Validated expanded queries typically exhibit higher\nreliability [93].\n\n\n_2) Query Transformation:_ The core concept is to retrieve\nchunks based on a transformed query instead of the user\u2019s\n\n- riginal query.\n_Query Rewrite_ .The original queries are not always optimal\nfor LLM retrieval, especially in real-world scenarios. Therefore, we can prompt LLM to rewrite the queries. In addition to\nusing LLM for query rewriting, specialized smaller language\nmodels, such as RRR (Rewrite-retrieve-read) [7]. The implementation of the query rewrite method in the Taobao, known\nas BEQUE [9] has notably enhanced recall effectiveness for\nlong-tail queries, resulting in a rise in GMV.\nAnother query transformation method is to use prompt\nengineering to let LLM generate a query based on the original\nquery for subsequent retrieval. HyDE [11] construct hypothetical documents (assumed answers to the original query). It\nfocuses on embedding similarity from answer to answer rather\nthan seeking embedding similarity for the problem or query.\nUsing the Step-back Prompting method [10], the original\nquery is abstracted to generate a high-level concept question\n(step-back question). In the RAG system, both", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_8100", "chunk_text": " to answer rather\nthan seeking embedding similarity for the problem or query.\nUsing the Step-back Prompting method [10], the original\nquery is abstracted to generate a high-level concept question\n(step-back question). In the RAG system, both the step-back\nquestion and the original query are used for retrieval, and both\nthe results are utilized as the basis for language model answer\ngeneration.\n_3) Query Routing:_ Based on varying queries, routing to\ndistinct RAG pipeline,which is suitable for a versatile RAG\nsystem designed to accommodate diverse scenarios.\n_Metadata Router/ Filter_ . The first step involves extracting\nkeywords (entity) from the query, followed by filtering based\n\n- n the keywords and metadata within the chunks to narrow\ndown the search scope.\n_Semantic Router_ is another method of routing involves\nleveraging the semantic information of the query. Specific\napprach see Semantic Router [6] . Certainly, a hybrid routing\napproach can also be employed, combining both semantic and\nmetadata-based methods for enhanced query routing.\n\n\n_D. Embedding_\n\n\nIn RAG, retrieval is achieved by calculating the similarity\n(e.g. cosine similarity) between the embeddings of the question and document chunks, where the semantic representation\ncapability of embedding models plays a key role. This mainly\nincludes a sparse encoder (BM25) and a dense retriever (BERT\narchitecture Pre-training language models). Recent research\nhas introduced prominent embedding models such as AngIE,\nVoyage, BGE,etc [94]\u2013[96], which are benefit from multi-task\ninstruct tuning. Hugging Face\u2019s MTEB leaderboard [7] evaluates\nembedding models across 8 tasks, covering 58 datasests. Additionally, C-MTEB focuses on Chinese capability, covering\n6 tasks and 35 datasets. There is no one-size-fits-all answer\nto \u201cwhich embedding model to use.\u201d However, some specific\nmodels are better suited for particular use cases.\n_1) Mix/hybrid Retrieval :_ Sparse and dense embedding\napproaches capture different relevance features and can benefit from each other by leveraging complementary relevance\ninformation. For instance, sparse retrieval models can be used\n\n\n[6https://github.com/aurelio-labs/semantic-router](https://github.com/aurelio-labs/semantic-router)\n[7https://huggingface.co/spaces/mteb/leaderboard](https://hugging", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_8550", "chunk_text": "\n\n\n[6https://github.com/aurelio-labs/semantic-router](https://github.com/aurelio-labs/semantic-router)\n[7https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n\n\n\n9\n\n\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)\ncan be utilized to learn term weights to enhance sparse\nretrieval. Specifically, it also demonstrates that sparse retrieval\nmodels can enhance the zero-shot retrieval capability of dense\nretrieval models and assist dense retrievers in handling queries\ncontaining rare entities, thereby improving robustness.\n_2) Fine-tuning Embedding Model:_ In instances where the\ncontext significantly deviates from pre-training corpus, particularly within highly specialized disciplines such as healthcare,\nlegal practice, and other sectors replete with proprietary jargon,\nfine-tuning the embedding model on your own domain dataset\nbecomes essential to mitigate such discrepancies.\nIn addition to supplementing domain knowledge, another\npurpose of fine-tuning is to align the retriever and generator,\nfor example, using the results of LLM as the supervision signal\nfor fine-tuning, known as LSR (LM-supervised Retriever).\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\ngenerator to create task-specific retrievers, addressing challenges in supervised fine-tuning, particularly in data-scarce\ndomains. Another approach, LLM-Embedder [97], exploits\nLLMs to generate reward signals across multiple downstream\ntasks. The retriever is fine-tuned with two types of supervised\nsignals: hard labels for the dataset and soft rewards from\nthe LLMs. This dual-signal approach fosters a more effective\nfine-tuning process, tailoring the embedding model to diverse\ndownstream applications. REPLUG [72] utilizes a retriever\nand an LLM to calculate the probability distributions of the\nretrieved documents and then performs supervised training\nby computing the KL divergence. This straightforward and\neffective training method enhances the performance of the\nretrieval model by using an LM as the supervisory signal,\neliminating the need for specific cross-attention mechanisms.\nMoreover, inspired by RLHF (Reinforcement Learning from\nHuman Feedback), utilizing LM-based feedback to reinforce\nthe retriever through reinforcement learning.\n\n\n_E. Adapter_", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_9000", "chunk_text": " the supervisory signal,\neliminating the need for specific cross-attention mechanisms.\nMoreover, inspired by RLHF (Reinforcement Learning from\nHuman Feedback), utilizing LM-based feedback to reinforce\nthe retriever through reinforcement learning.\n\n\n_E. Adapter_\n\n\nFine-tuning models may present challenges, such as integrating functionality through an API or addressing constraints arising from limited local computational resources.\nConsequently, some approaches opt to incorporate an external\nadapter to aid in alignment.\nTo optimize the multi-task capabilities of LLM, UPRISE [20] trained a lightweight prompt retriever that can\nautomatically retrieve prompts from a pre-built prompt pool\nthat are suitable for a given zero-shot task input. AAR\n(Augmentation-Adapted Retriver) [47] introduces a universal\nadapter designed to accommodate multiple downstream tasks.\nWhile PRCA [69] add a pluggable reward-driven contextual\nadapter to enhance performance on specific tasks. BGM [26]\nkeeps the retriever and LLM fixed,and trains a bridge Seq2Seq\nmodel in between. The bridge model aims to transform the\nretrieved information into a format that LLMs can work with\n\neffectively, allowing it to not only rerank but also dynamically select passages for each query, and potentially employ\nmore advanced strategies like repetition. Furthermore, PKG\n\n\nintroduces an innovative method for integrating knowledge\ninto white-box models via directive fine-tuning [75]. In this\napproach, the retriever module is directly substituted to generate relevant documents according to a query. This method\nassists in addressing the difficulties encountered during the\nfine-tuning process and enhances model performance.\n\n\nIV. GENERATION\n\n\nAfter retrieval, it is not a good practice to directly input all\nthe retrieved information to the LLM for answering questions.\nFollowing will introduce adjustments from two perspectives:\nadjusting the retrieved content and adjusting the LLM.\n\n\n_A. Context Curation_\n\n\nRedundant information can interfere with the final generation of LLM, and overly long contexts can also lead LLM\nto the \u201cLost in the middle\u201d problem [98]. Like humans, LLM\ntends to only focus on the beginning and end of long texts,\nwhile forgetting the middle portion. Therefore, in the RAG\nsystem, we typically need to further process the retrieved\n\ncontent.\n\n_1) Reranking:_ Reranking fundamentally reorders document\nchunks to highlight the most pertinent results", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_9450", "chunk_text": " texts,\nwhile forgetting the middle portion. Therefore, in the RAG\nsystem, we typically need to further process the retrieved\n\ncontent.\n\n_1) Reranking:_ Reranking fundamentally reorders document\nchunks to highlight the most pertinent results first, effectively\nreducing the overall document pool, severing a dual purpose\nin information retrieval, acting as both an enhancer and a\nfilter, delivering refined inputs for more precise language\nmodel processing [70]. Reranking can be performed using\nrule-based methods that depend on predefined metrics like\nDiversity, Relevance, and MRR, or model-based approaches\nlike Encoder-Decoder models from the BERT series (e.g.,\nSpanBERT), specialized reranking models such as Cohere\nrerank or bge-raranker-large, and general large language models like GPT [12], [99].\n_2) Context Selection/Compression:_ A common misconception in the RAG process is the belief that retrieving as many\nrelevant documents as possible and concatenating them to form\na lengthy retrieval prompt is beneficial. However, excessive\ncontext can introduce more noise, diminishing the LLM\u2019s\nperception of key information .\n(Long) LLMLingua [100], [101] utilize small language\nmodels (SLMs) such as GPT-2 Small or LLaMA-7B, to\ndetect and remove unimportant tokens, transforming it into\na form that is challenging for humans to comprehend but\nwell understood by LLMs. This approach presents a direct\nand practical method for prompt compression, eliminating the\nneed for additional training of LLMs while balancing language\nintegrity and compression ratio. PRCA tackled this issue by\ntraining an information extractor [69]. Similarly, RECOMP\nadopts a comparable approach by training an information\ncondenser using contrastive learning [71]. Each training data\npoint consists of one positive sample and five negative samples, and the encoder undergoes training using contrastive loss\nthroughout this process [102] .\nIn addition to compressing the context, reducing the number of documents aslo helps improve the accuracy of the\nmodel\u2019s answers. Ma et al. [103] propose the \u201cFilter-Reranker\u201d\nparadigm, which combines the strengths of LLMs and SLMs.\n\n\n\n10\n\n\nIn this paradigm, SLMs serve as filters, while LLMs function\nas reordering agents. The research", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_9900", "chunk_text": " the \u201cFilter-Reranker\u201d\nparadigm, which combines the strengths of LLMs and SLMs.\n\n\n\n10\n\n\nIn this paradigm, SLMs serve as filters, while LLMs function\nas reordering agents. The research shows that instructing\nLLMs to rearrange challenging samples identified by SLMs\nleads to significant improvements in various Information\nExtraction (IE) tasks. Another straightforward and effective\napproach involves having the LLM evaluate the retrieved\ncontent before generating the final answer. This allows the\nLLM to filter out documents with poor relevance through LLM\ncritique. For instance, in Chatlaw [104], the LLM is prompted\nto self-suggestion on the referenced legal provisions to assess\ntheir relevance.\n\n\n_B. LLM Fine-tuning_\n\n\nTargeted fine-tuning based on the scenario and data characteristics on LLMs can yield better results. This is also one\n\n- f the greatest advantages of using on-premise LLMs. When\nLLMs lack data in a specific domain, additional knowledge can\nbe provided to the LLM through fine-tuning. Huggingface\u2019s\nfine-tuning data can also be used as an initial step.\nAnother benefit of fine-tuning is the ability to adjust the\nmodel\u2019s input and output. For example, it can enable LLM to\nadapt to specific data formats and generate responses in a particular style as instructed [37]. For retrieval tasks that engage\nwith structured data, the SANTA framework [76] implements\na tripartite training regimen to effectively encapsulate both\nstructural and semantic nuances. The initial phase focuses on\nthe retriever, where contrastive learning is harnessed to refine\nthe query and document embeddings.\nAligning LLM outputs with human or retriever preferences\nthrough reinforcement learning is a potential approach. For\ninstance, manually annotating the final generated answers\nand then providing feedback through reinforcement learning.\nIn addition to aligning with human preferences, it is also\npossible to align with the preferences of fine-tuned models\nand retrievers [79]. When circumstances prevent access to\npowerful proprietary models or larger parameter open-source\nmodels, a simple and effective method is to distill the more\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\nbe coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_10350", "chunk_text": " effective method is to distill the more\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\nbe coordinated with fine-tuning of the retriever to align preferences. A typical approach, such as RA-DIT [27], aligns the\nscoring functions between Retriever and Generator using KL\ndivergence.\n\n\nV. AUGMENTATION PROCESS IN RAG\n\n\nIn the domain of RAG, the standard practice often involves\na singular (once) retrieval step followed by generation, which\ncan lead to inefficiencies and sometimes is typically insufficient for complex problems demanding multi-step reasoning,\nas it provides a limited scope of information [105]. Many\nstudies have optimized the retrieval process in response to this\nissue, and we have summarised them in Figure 5.\n\n\n_A. Iterative Retrieval_\n\n\nIterative retrieval is a process where the knowledge base\nis repeatedly searched based on the initial query and the text\ngenerated so far, providing a more comprehensive knowledge\n\n\n11\n\n\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\n\n\n\nbase for LLMs. This approach has been shown to enhance\nthe robustness of subsequent answer generation by offering\nadditional contextual references through multiple retrieval\niterations. However, it may be affected by semantic discontinuity and the accumulation of irrelevant information. ITERRETGEN [14] employs a synergistic approach that leverages \u201cretrieval-enhanced generation\u201d alongside \u201cgenerationenhanced retrieval\u201d for tasks that necessitate the reproduction\n\n- f specific information. The model harnesses the content\nrequired to address the input task as a contextual basis for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n\n\n_B. Recursive Retrieval_\n\n\nRecursive retrieval is often used in information retrieval and\n\nNLP to improve the depth and relevance of search results", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_10800", "chunk_text": " for\nretrieving pertinent knowledge, which in turn facilitates the\ngeneration of improved responses in subsequent iterations.\n\n\n_B. Recursive Retrieval_\n\n\nRecursive retrieval is often used in information retrieval and\n\nNLP to improve the depth and relevance of search results.\nThe process involves iteratively refining search queries based\n\n- n the results obtained from previous searches. Recursive\nRetrieval aims to enhance the search experience by gradually converging on the most pertinent information through a\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\nthe retrieval process and refines the CoT with the obtained\nretrieval results. ToC [57] creates a clarification tree that\nsystematically optimizes the ambiguous parts in the Query. It\ncan be particularly useful in complex search scenarios where\nthe user\u2019s needs are not entirely clear from the outset or where\nthe information sought is highly specialized or nuanced. The\nrecursive nature of the process allows for continuous learning\nand adaptation to the user\u2019s requirements, often resulting in\nimproved satisfaction with the search outcomes.\nTo address specific data scenarios, recursive retrieval and\nmulti-hop retrieval techniques are utilized together. Recursive\n\n\n\nretrieval involves a structured index to process and retrieve\ndata in a hierarchical manner, which may include summarizing\nsections of a document or lengthy PDF before performing a\nretrieval based on this summary. Subsequently, a secondary\nretrieval within the document refines the search, embodying\nthe recursive nature of the process. In contrast, multi-hop\nretrieval is designed to delve deeper into graph-structured data\nsources, extracting interconnected information [106].\n\n\n_C. Adaptive Retrieval_\n\n\nAdaptive retrieval methods, exemplified by Flare [24] and\nSelf-RAG [25], refine the RAG framework by enabling LLMs\nto actively determine the optimal moments and content for\nretrieval, thus enhancing the efficiency and relevance of the\ninformation sourced.\n\nThese methods are part of a broader trend wherein\nLLMs employ active judgment in their operations, as seen\nin model agents like AutoGPT, Toolformer, and GraphToolformer [107]\u2013[109]. Graph-Toolformer, for instance, divides its retrieval process into distinct steps where LLMs\nproactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive\nstance allows LLMs to decide when to search", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_11250", "chunk_text": " for instance, divides its retrieval process into distinct steps where LLMs\nproactively use retrievers, apply Self-Ask techniques, and employ few-shot prompts to initiate search queries. This proactive\nstance allows LLMs to decide when to search for necessary\ninformation, akin to how an agent utilizes tools.\nWebGPT [110] integrates a reinforcement learning framework to train the GPT-3 model in autonomously using a\nsearch engine during text generation. It navigates this process\nusing special tokens that facilitate actions such as search\nengine queries, browsing results, and citing references, thereby\nexpanding GPT-3\u2019s capabilities through the use of external\nsearch engines. Flare automates timing retrieval by monitoring\nthe confidence of the generation process, as indicated by the\n\n\nprobability of generated terms [24]. When the probability falls\nbelow a certain threshold would activates the retrieval system\nto collect relevant information, thus optimizing the retrieval\ncycle. Self-RAG [25] introduces \u201creflection tokens\u201d that allow\nthe model to introspect its outputs. These tokens come in\ntwo varieties: \u201cretrieve\u201d and \u201ccritic\u201d. The model autonomously\ndecides when to activate retrieval, or alternatively, a predefined\nthreshold may trigger the process. During retrieval, the generator conducts a fragment-level beam search across multiple\nparagraphs to derive the most coherent sequence. Critic scores\nare used to update the subdivision scores, with the flexibility\nto adjust these weights during inference, tailoring the model\u2019s\nbehavior. Self-RAG\u2019s design obviates the need for additional\nclassifiers or reliance on Natural Language Inference (NLI)\nmodels, thus streamlining the decision-making process for\nwhen to engage retrieval mechanisms and improving the\nmodel\u2019s autonomous judgment capabilities in generating ac\ncurate responses.\n\n\nVI. TASK AND EVALUATION\n\n\nThe rapid advancement and growing adoption of RAG\nin the field of NLP have propelled the evaluation of RAG\nmodels to the forefront of research in the LLMs community.\nThe primary objective of this evaluation is to comprehend\nand optimize the performance of RAG models across diverse\napplication scenarios.This chapter will mainly introduce the\nmain downstream tasks of RAG, datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop/multi-hop QA, multiplechoice, domain-specific QA as well as", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_11700", "chunk_text": ", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_12150", "chunk_text": "fulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation methods [29], [161], [163].\n\n\n_C. Evaluation Aspects_\n\nContemporary evaluation practices of RAG models emphasize three primary quality scores and four essential abilities,\nwhich collectively inform the evaluation of the two principal\ntargets of the RAG model: retrieval and generation.\n_1) Quality Scores:_ Quality scores include context relevance, answer faithfulness, and answer relevance. These quality scores evaluate the efficiency of the RAG model from\ndifferent perspectives in the process of information retrieval\nand generation [164]\u2013[166].\n_Context Relevance_ evaluates the precision and specificity\n\n- f the retrieved context, ensuring relevance and minimizing\nprocessing costs associated with extraneous content.\n_Answer Faithfulness_ ensures that the generated answers\nremain true to the retrieved context, maintaining consistency\nand avoiding contradictions.\n_Answer Relevance_ requires that the generated answers are\ndirectly pertinent to the posed questions, effectively addressing\nthe core inquiry.\n_2) Required Abilities:_ RAG evaluation also encompasses\nfour abilities indicative of its adaptability and efficiency:\nnoise robustness, negative rejection, information integration,\nand counterfactual robustness [167], [168]. These abilities are\ncritical for the model\u2019s performance under various challenges\nand complex scenarios, impacting the quality scores.\n_Noise Robustness_ appraises the model\u2019s capability to manage noise documents that are question-related but lack substantive information.\n\n_Negative Rejection_ assesses the model\u2019s discernment in\nrefraining from responding when the retrieved documents do\nnot contain the necessary knowledge to answer a question.\n_Information Integration_ evaluates the model\u2019s proficiency in\nsynthesizing information from multiple documents to address\ncomplex questions.\n_Counterfactual Robustness_ tests the model\u2019s ability to rec\n- gnize and disregard known inaccuracies within documents,\neven when instructed about potential misinformation.\nContext relevance and noise robustness are important for\nevaluating the quality of retrieval, while answer faithfulness,\nanswer relevance, negative rejection, information integration,\nand counterfactual robustness are important for evaluating the\nquality of generation.\n\n\n13\n\n\nTABLE II\n\nDOWNSTREAM TASKS AND DATASETS OF RAG\n\n\nTask Sub Task Dataset Method\n\n\n[26], [30], [", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_12600", "chunk_text": ", information integration,\nand counterfactual robustness are important for evaluating the\nquality of generation.\n\n\n13\n\n\nTABLE II\n\nDOWNSTREAM TASKS AND DATASETS OF RAG\n\n\nTask Sub Task Dataset Method\n\n\n[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]\nQA Single-hop Natural Qustion(NQ) [111] [3], [4], [22], [27], [40], [43], [54], [62], [71], [112]\n\n[20], [44], [72]\n\n[13], [30], [34], [45], [50], [64]\nTriviaQA(TQA) [113] [4], [27], [59], [62], [112]\n\n[22], [25], [43], [44], [71], [72]\nSQuAD [114] [20], [23], [30], [32], [45], [69], [112]\nWeb Questions(WebQ) [115] [3], [4], [13], [30], [50], [68]\nPopQA [116] [7], [25], [67]\nMS MARCO [117] [4], [40], [52]\n\n\n[23], [26], [31], [34], [47], [51], [61], [82]\nMulti-hop HotpotQA [118]\n\n[7], [14], [22], [27], [59], [62], [69], [71], [91]\n2WikiMultiHopQA [119] [14], [24], [48], [59], [61], [91]\nMuSiQue [120] [14], [51], [61], [91]\n\n\nLong-form QA ELI5 [121] [27], [34], [43], [49], [51]\nNarrativeQA(NQA) [122] [45], [60], [63], [123]\nASQA [124] [24], [57]\nQMSum(QM) [125] [60], [123]\n\n\nDomain QA Qasper [126] [60], [63]\nCOVID-QA [127] [35], [46]\nCMB [128],MMCU Medical [129] [81]\n\n\nMulti-Choice QA QuALITY [130] [60], [63]\nARC [131] [", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_13050", "chunk_text": " [63]\nCOVID-QA [127] [35], [46]\nCMB [128],MMCU Medical [129] [81]\n\n\nMulti-Choice QA QuALITY [130] [60], [63]\nARC [131] [25], [67]\nCommonsenseQA [132] [58], [66]\n\n\nGraph QA GraphQA [84] [84]\n\n\nDialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42]\nPersonal Dialog KBP [134] [74], [135]\nDuleMon [136] [74]\nTask-oriented Dialog CamRest [137] [78], [79]\nRecommendation Amazon(Toys,Sport,Beauty) [138] [39], [40]\n\n\nIE Event Argument Extraction WikiEvent [139] [13], [27], [37], [42]\nRAMS [140] [36], [37]\nRelation Extraction T-REx [141],ZsRE [142] [27], [51]\n\n\nReasoning Commonsense Reasoning HellaSwag [143] [20], [66]\nCoT Reasoning CoT Reasoning [144] [27]\nComplex Reasoning CSQA [145] [55]\n\n\nOthers Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72]\nLanguage Modeling WikiText-103 [147] [5], [29], [64], [71]\nStrategyQA [148] [14], [24], [48], [51], [55], [58]\nFact Checking/Verification FEVER [149] [4], [13], [27], [34], [42], [50]\nPubHealth [150] [25], [67]\nText Generation Biography [151] [67]\nText Summarization WikiASP [152] [24]\nXSum [153] [17]\nText Classification VioLens [154] [19]\nTREC [155] [33]\nSentiment SST-2 [156] [20], [33], [38]\nCode Search CodeSearchNet [157] [76]\nRobustness Evaluation NoMIRACL [56] [56]\nMath GSM8K [158] [73]\nMachine Translation JRC-Acquis [159] [17]\n\n\n14\n\n\n\nTABLE III\n\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_13500", "chunk_text": "ACL [56] [56]\nMath GSM8K [158] [73]\nMachine Translation JRC-Acquis [159] [17]\n\n\n14\n\n\n\nTABLE III\n\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\n\n\nContext Faithfulness Answer Noise Negative Information Counterfactual\nRelevance Relevance Robustness Rejection Integration Robustness\n\n\nAccuracy \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nEM \u2713\n\nRecall \u2713\n\nPrecision \u2713 \u2713\n\nR-Rate \u2713\n\nCosine Similarity \u2713\nHit Rate \u2713\n\nMRR \u2713\n\nNDCG \u2713\n\nBLEU \u2713 \u2713 \u2713\n\nROUGE/ROUGE-L \u2713 \u2713 \u2713\n\n\n\nThe specific metrics for each evaluation aspect are summarized in Table III. It is essential to recognize that these\nmetrics, derived from related work, are traditional measures\nand do not yet represent a mature or standardized approach for\nquantifying RAG evaluation aspects. Custom metrics tailored\nto the nuances of RAG models, though not included here, have\nalso been developed in some evaluation studies.\n\n\n_D. Evaluation Benchmarks and Tools_\n\n\nA series of benchmark tests and tools have been proposed\nto facilitate the evaluation of RAG.These instruments furnish\n\nquantitative metrics that not only gauge RAG model performance but also enhance comprehension of the model\u2019s capabilities across various evaluation aspects. Prominent benchmarks\nsuch as RGB, RECALL and CRUD [167]\u2013[169] focus on\nappraising the essential abilities of RAG models. Concurrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens [8] employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\n\n\nVII. DISCUSSION AND FUTURE PROSPECTS\n\n\nDespite the considerable progress in RAG technology, several challenges persist that warrant in-depth research.This\nchapter will mainly introduce the current challenges and future\nresearch directions faced by RAG.\n\n\n_A. RAG vs Long Context_\n\n\nWith the deepening of related research, the context of LLMs\nis continuously expanding [170]\u2013[172]. Presently, LLMs can\neffortlessly manage contexts exceeding 200,000 tokens [9] . This\ncapability signifies that long-document question answering,\npreviously reliant on RAG, can now incorporate", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_13950", "chunk_text": " [170]\u2013[172]. Presently, LLMs can\neffortlessly manage contexts exceeding 200,000 tokens [9] . This\ncapability signifies that long-document question answering,\npreviously reliant on RAG, can now incorporate the entire\ndocument directly into the prompt. This has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n\n\n[8https://www.trulens.org/trulens eval/core concepts rag triad/](https://www.trulens.org/trulens_eval/core_concepts_rag_triad/)\n[9https://kimi.moonshot.cn](https://kimi.moonshot.cn)\n\n\n\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quickly locate the original\nreferences for LLMs to help users verify the generated answers. The entire retrieval and reasoning process is observable,\nwhile generation solely relying on long context remains a\nblack box. Conversely, the expansion of context provides new\n\n- pportunities for the development of RAG, enabling it to\naddress more complex problems and integrative or summary\nquestions that require reading a large amount of material to\nanswer [49]. Developing new RAG methods in the context of\nsuper-long contexts is one of the future research trends.\n\n\n_B. RAG Robustness_\n\n\nThe presence of noise or contradictory information during\nretrieval can detrimentally affect RAG\u2019s output quality. This\nsituation is figuratively referred to as \u201cMisinformation can\nbe worse than no information at all\u201d. Improving RAG\u2019s\nresistance to such adversarial or counterfactual inputs is gaining research momentum and has become a key performance\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\ntype of documents should be retrieved, evaluate the relevance\n\n- f the documents to the prompt, their position, and the\nnumber included in the context. The research findings reveal\nthat including irrelevant documents can unexpectedly increase\naccuracy by over 30%, contradicting the initial assumption\n\n- f reduced quality. These results underscore the importance\n\n- f developing specialized strategies to integrate retrieval with\n", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_14400", "chunk_text": " in the context. The research findings reveal\nthat including irrelevant documents can unexpectedly increase\naccuracy by over 30%, contradicting the initial assumption\n\n- f reduced quality. These results underscore the importance\n\n- f developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\n\n\n_C. Hybrid Approaches_\n\n\nCombining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-toend joint training\u2014and how to harness both parameterized\n\n\n15\n\n\n\nTABLE IV\n\nSUMMARY OF EVALUATION FRAMEWORKS\n\n\n**Evaluation Framework** **Evaluation Targets** **Evaluation Aspects** **Quantitative Metrics**\n\n\n\nAccuracy\nEM\n\nAccuracy\nAccuracy\n\n\n\nRetrieval Quality\nRGB _[\u2020]_\nGeneration Quality\n\n\n\nNoise Robustness\n\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\n\n\n\nRECALL _[\u2020]_ Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\n\n\n\n    \n    \nCosine Similarity\n\n\nAccuracy\nAccuracy\nAccuracy\n\n\n    \n    \n    \n\nBLEU\n\nROUGE-L\n\nBertScore\n\nRAGQuestEval\n\n\n\nRetrieval Quality\nRAGAS _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nARES _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nTruLens _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nCRUD _[\u2020]_\nGeneration Quality\n\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nCreative Generation\n\nKnowledge-intensive QA\nError Correction\n\nSummarization\n\n\n\n\n_\u2020 represents a benchmark, and \u2021 represents a tool. * denotes customized quantitative metrics, which deviate from traditional_\n_metrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these_\n_metrics, as required._\n\n\n\nand non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific\nfunctionalities into RAG and fine-tuned by the results of RAG\nsystem. For example, CRAG [67] trains a lightweight retrieval\nevaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval\nactions based on confidence levels", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_14850", "chunk_text": " fine-tuned by the results of RAG\nsystem. For example, CRAG [67] trains a lightweight retrieval\nevaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval\nactions based on confidence levels.\n\n\n_D. Scaling laws of RAG_\n\n\nEnd-to-end RAG models and pre-trained models based\n\n- n RAG are still - ne - f the focuses - f current re\nsearchers [173].The parameters of these models are one of\nthe key factors.While scaling laws [174] are established for\nLLMs, their applicability to RAG remains uncertain. Initial\nstudies like RETRO++ [44] have begun to address this, yet the\nparameter count in RAG models still lags behind that of LLMs.\nThe possibility of an Inverse Scaling Law [10], where smaller\nmodels outperform larger ones, is particularly intriguing and\nmerits further investigation.\n\n\n_E. Production-Ready RAG_\n\n\nRAG\u2019s practicality and alignment with engineering requirements have facilitated its adoption. However, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security\u2014such as preventing\n\n\n[10https://github.com/inverse-scaling/prize](https://github.com/inverse-scaling/prize)\n\n\n\ninadvertent disclosure of document sources or metadata by\nLLMs\u2014are critical engineering challenges that remain to be\naddressed [175].\nThe development of the RAG ecosystem is greatly impacted\nby the progression of its technical stack. Key tools like\nLangChain and LLamaIndex have quickly gained popularity\nwith the emergence of ChatGPT, providing extensive RAGrelated APIs and becoming essential in the realm of LLMs.The\nemerging technology stack, while not as rich in features as\nLangChain and LLamaIndex, stands out through its specialized\nproducts. For example, Flowise AI prioritizes a low-code\napproach, allowing users to deploy AI applications, including\nRAG, through a user-friendly drag-and-drop interface. Other\ntechnologies like HayStack, Meltano, and Cohere Coral are\nalso gaining attention for their unique contributions to the field.\nIn addition to AI-focused vendors, traditional software and\ncloud service providers are expanding their offerings to include\nRAG-centric services. Weaviate\u2019s Verba [11] is designed for\npersonal assistant applications, while Amazon\u2019s Kendra 12\n\n- ffers intelligent", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_15300", "chunk_text": " vendors, traditional software and\ncloud service providers are expanding their offerings to include\nRAG-centric services. Weaviate\u2019s Verba [11] is designed for\npersonal assistant applications, while Amazon\u2019s Kendra 12\n\n- ffers intelligent enterprise search services, enabling users to\nbrowse various content repositories using built-in connectors.\nIn the development of RAG technology, there is a clear\ntrend towards different specialization directions, such as: 1)\nCustomization - tailoring RAG to meet specific requirements.\n2) Simplification - making RAG easier to use to reduce the\n\n\n[11https://github.com/weaviate/Verba](https://github.com/weaviate/Verba)\n[12https://aws.amazon.com/cn/kendra/](https://aws.amazon.com/cn/kendra/)\n\n\nFig. 6. Summary of RAG ecosystem\n\n\ninitial learning curve. 3) Specialization - optimizing RAG to\nbetter serve production environments.\nThe mutual growth of RAG models and their technology\nstacks is evident; technological advancements continuously\nestablish new standards for existing infrastructure. In turn,\nenhancements to the technology stack drive the development\n\n- f RAG capabilities. RAG toolkits are converging into a\nfoundational technology stack, laying the groundwork for\nadvanced enterprise applications. However, a fully integrated,\ncomprehensive platform concept is still in the future, requiring\nfurther innovation and development.\n\n\n_F. Multi-modal RAG_\n\nRAG has transcended its initial text-based questionanswering confines, embracing a diverse array of modal data.\nThis expansion has spawned innovative multimodal models\nthat integrate RAG concepts across various domains:\n_Image_ . RA-CM3 [176] stands as a pioneering multimodal\nmodel of both retrieving and generating text and images.\nBLIP-2 [177] leverages frozen image encoders alongside\nLLMs for efficient visual language pre-training, enabling zeroshot image-to-text conversions. The \u201cVisualize Before You\nWrite\u201d method [178] employs image generation to steer the\nLM\u2019s text generation, showing promise in open-ended text\ngeneration tasks.\n_Audio and Video_ . The GSS method retrieves and stitches\n\ntogether audio clips to convert machine-translated data into\nspeech-translated data [179]. UEOP marks a significant advancement in end-to-end automatic speech recognition by\nincorporating external, offline strategies for voice-to-text conversion [180]. Additionally,", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_15750", "chunk_text": "gether audio clips to convert machine-translated data into\nspeech-translated data [179]. UEOP marks a significant advancement in end-to-end automatic speech recognition by\nincorporating external, offline strategies for voice-to-text conversion [180]. Additionally, KNN-based attention fusion leverages audio embeddings and semantically related text embeddings to refine ASR, thereby accelerating domain adaptation.\n\n\n\n16\n\n\nVid2Seq augments language models with specialized temporal\nmarkers, facilitating the prediction of event boundaries and\ntextual descriptions within a unified output sequence [181].\n_Code_ . RBPS [182] excels in small-scale learning tasks by\nretrieving code examples that align with developers\u2019 objectives\nthrough encoding and frequency analysis. This approach has\ndemonstrated efficacy in tasks such as test assertion generation and program repair. For structured knowledge, the CoK\nmethod [106] first extracts facts pertinent to the input query\nfrom a knowledge graph, then integrates these facts as hints\nwithin the input, enhancing performance in knowledge graph\nquestion-answering tasks.\n\n\nVIII. CONCLUSION\n\n\nThe summary of this paper, as depicted in Figure 6, emphasizes RAG\u2019s significant advancement in enhancing the capabilities of LLMs by integrating parameterized knowledge from\nlanguage models with extensive non-parameterized data from\nexternal knowledge bases. The survey showcases the evolution\n\n- f RAG technologies and their application on many different\ntasks. The analysis outlines three developmental paradigms\nwithin the RAG framework: Naive, Advanced, and Modular RAG, each representing a progressive enhancement over\nits predecessors. RAG\u2019s technical integration with other AI\nmethodologies, such as fine-tuning and reinforcement learning,\nhas further expanded its capabilities. Despite the progress in\nRAG technology, there are research opportunities to improve\nits robustness and its ability to handle extended contexts.\nRAG\u2019s application scope is expanding into multimodal domains, adapting its principles to interpret and process diverse\ndata forms like images, videos, and code. This expansion highlights RAG\u2019s significant practical implications for AI deployment, attracting interest from academic and industrial sectors.\n\n\nThe growing ecosystem of RAG is evidenced by the rise in\nRAG-centric AI applications and the continuous development\n\n- f supportive tools. As RAG\u2019s application landscape broadens,\nthere is a need to refine evaluation methodologies to keep\npace with its evolution. Ensuring accurate and representative\nperformance assessments is crucial for fully capturing RAG\u2019s\n", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_16200", "chunk_text": " continuous development\n\n- f supportive tools. As RAG\u2019s application landscape broadens,\nthere is a need to refine evaluation methodologies to keep\npace with its evolution. Ensuring accurate and representative\nperformance assessments is crucial for fully capturing RAG\u2019s\ncontributions to the AI research and development community.\n\n\nREFERENCES\n\n\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, \u201cLarge\nlanguage models struggle to learn long-tail knowledge,\u201d in _Interna-_\n_tional Conference on Machine Learning_ . PMLR, 2023, pp. 15 696\u2013\n15 707.\n\n[2] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\nY. Zhang, Y. Chen _et al._, \u201cSiren\u2019s song in the ai ocean: A survey on hallucination in large language models,\u201d _arXiv preprint arXiv:2309.01219_,\n2023.\n\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\nA. Sharma, \u201cGar-meets-rag paradigm for zero-shot information retrieval,\u201d _arXiv preprint arXiv:2310.20158_, 2023.\n\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K\u00a8uttler, M. Lewis, W.-t. Yih, T. Rockt\u00a8aschel _et al._, \u201cRetrievalaugmented generation for knowledge-intensive nlp tasks,\u201d _Advances in_\n_Neural Information Processing Systems_, vol. 33, pp. 9459\u20139474, 2020.\n\n[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark _et al._,\n\u201cImproving language models by retrieving from trillions of tokens,\u201d\nin _International conference on machine learning_ . PMLR", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_16650", "chunk_text": "che, J.-B. Lespiau, B. Damoc, A. Clark _et al._,\n\u201cImproving language models by retrieving from trillions of tokens,\u201d\nin _International conference on machine learning_ . PMLR, 2022, pp.\n2206\u20132240.\n\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray _et al._, \u201cTraining language\nmodels to follow instructions with human feedback,\u201d _Advances in_\n_neural information processing systems_, vol. 35, pp. 27 730\u201327 744,\n2022.\n\n[7] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, \u201cQuery rewriting for retrieval-augmented large language models,\u201d _arXiv preprint_\n_arXiv:2305.14283_, 2023.\n\n[8] I. ILIN, \u201cAdvanced rag techniques: an illustrated    - verview,\u201d [https://pub.towardsai.net/](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)\n[advanced-rag-techniques-an-illustrated-overview-04d193d8fec6,](https://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)\n2023.\n\n[9] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen _et al._,\n\u201cLarge language model based long-tail query rewriting in taobao\nsearch,\u201d _arXiv preprint arXiv:2311.03758_, 2023.\n\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le,\nand D. Zhou, \u201cTake a step back: Evoking reasoning via abstraction in\nlarge language models,\u201d _arXiv preprint arXiv:2310.06117_, 2023.\n\n[11] L. Gao, X. Ma, J. Lin,", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_17550", "chunk_text": " A. H. Raudaschl, \u201cForget rag, the future\nis rag-fusion,\u201d [https://towardsdatascience.com/](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1)\n[forget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1)\n\n[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, \u201cLift\nyourself up: Retrieval-augmented text generation with self memory,\u201d\n_arXiv preprint arXiv:2305.02437_, 2023.\n\n[18] S. Wang, Y. Xu, Y. Fang, Y. Liu, S. Sun, R. Xu, C. Zhu, and\nM. Zeng, \u201cTraining data is more valuable than you think: A simple\nand effective method by retrieving from training data,\u201d _arXiv preprint_\n_arXiv:2203.08773_, 2022.\n\n\n\n17\n\n\n[19] X. Li, E. Nie, and S. Liang, \u201cFrom classification to generation:\nInsights into crosslingual retrieval augmented icl,\u201d _arXiv preprint_\n_arXiv:2311.06595_, 2023.\n\n[20] D. Cheng, S. Huang, J. Bi, Y. Zhan, J. Liu, Y. Wang, H. Sun,\nF. Wei, D. Deng, and Q. Zhang, \u201cUprise: Universal prompt retrieval\nfor improving zero-shot evaluation,\u201d _arXiv preprint arXiv:2303.08518_,\n2023.\n\n[21] Z. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\nK. B. Hall, and M.-W. Chang, \u201cPromptagator: Few-shot dense retrieval\nfrom 8 examples,\u201d _arXiv preprint arXiv:2209.11755_, 2022.\n\n[22] Z. Sun, X. Wang, Y", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_18900", "chunk_text": ". Seo, J. Baek, J. Thorne, and S. J. Hwang, \u201cRetrieval-augmented\ndata augmentation for low-resource domain tasks,\u201d _arXiv preprint_\n_arXiv:2402.13482_, 2024.\n\n[36] Y. Ma, Y. Cao, Y. Hong, and A. Sun, \u201cLarge language model is not\na good few-shot information extractor, but a good reranker for hard\nsamples!\u201d _arXiv preprint arXiv:2303.08559_, 2023.\n\n[37] X. Du and H. Ji, \u201cRetrieval-augmented generative question answering\nfor event argument extraction,\u201d _arXiv preprint arXiv:2211.07067_, 2022.\n\n[38] L. Wang, N. Yang, and F. Wei, \u201cLearning to retrieve in-context\nexamples for large language models,\u201d _arXiv preprint arXiv:2307.07164_,\n2023.\n\n[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,\nL. Hong, Y. Tay, V. Q. Tran, J. Samost _et al._, \u201cRecommender systems\nwith generative retrieval,\u201d _arXiv preprint arXiv:2305.05065_, 2023.\n\n[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,\nY. Li, H. Lu _et al._, \u201cLanguage models as semantic indexers,\u201d _arXiv_\n_preprint arXiv:2310.07815_, 2023.\n\n[41] R. Anantha, T. Bethi, D. Vodianik, and S. Chappidi, \u201cContext tuning\nfor retrieval augmented generation,\u201d _arXiv preprint arXiv:2312.05708_,\n2023.\n\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, \u201c", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_20700", "chunk_text": ", S. Subramanian,\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, \u201cRetrieval meets long\ncontext large language models,\u201d _arXiv preprint arXiv:2310.03025_,\n2023.\n\n[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \u201cInterleaving retrieval with chain-of-thought reasoning for knowledge-intensive\nmulti-step questions,\u201d _arXiv preprint arXiv:2212.10509_, 2022.\n\n[62] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.R. Wen, and H. Wang, \u201cInvestigating the factual knowledge boundary\n\n   - f large language models with retrieval augmentation,\u201d _arXiv preprint_\n_arXiv:2307.11019_, 2023.\n\n[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\nManning, \u201cRaptor: Recursive abstractive processing for tree-organized\nretrieval,\u201d _arXiv preprint arXiv:2401.18059_, 2024.\n\n[64] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. LeytonBrown, and Y. Shoham, \u201cIn-context retrieval-augmented language\nmodels,\u201d _arXiv preprint arXiv:2302.00083_, 2023.\n\n[65] Y. Ren, Y. Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, \u201cRetrieve-andsample: Document-level event argument extraction via hybrid retrieval\naugmentation,\u201d in _Proceedings of the 61st Annual Meeting of the_\n_Association for Computational Linguistics (Volume 1: Long Papers)_,\n2023, pp. 293\u2013306.\n\n\n\n18\n\n\n[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, \u201cZemi: Learning\nzero-shot semi-parametric language models from multiple tasks,\u201d _arXiv_\n_preprint ar", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_21150", "chunk_text": " Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, \u201cZemi: Learning\nzero-shot semi-parametric language models from multiple tasks,\u201d _arXiv_\n_preprint arXiv:2210.00185_, 2022.\n\n[67] S.-Q. Yan, J.-C. Gu, Y. Zhu, and Z.-H. Ling, \u201cCorrective retrieval\naugmented generation,\u201d _arXiv preprint arXiv:2401.15884_, 2024.\n\n[68] P. Jain, L. B. Soares, and T. Kwiatkowski, \u201c1-pager: One pass answer\ngeneration and evidence retrieval,\u201d _arXiv preprint arXiv:2310.16568_,\n2023.\n\n[69] H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, \u201cPrca:\nFitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter,\u201d _arXiv preprint_\n_arXiv:2310.18347_, 2023.\n\n[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, \u201cOpen-source large\nlanguage models are strong zero-shot query likelihood models for\ndocument ranking,\u201d _arXiv preprint arXiv:2310.13243_, 2023.\n\n[71] F. Xu, W. Shi, and E. Choi, \u201cRecomp: Improving retrieval-augmented\nlms with compression and selective augmentation,\u201d _arXiv preprint_\n_arXiv:2310.04408_, 2023.\n\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W.-t. Yih, \u201cReplug: Retrieval-augmented black-box language models,\u201d _arXiv preprint arXiv:2301.12652_, 2023.\n\n[73] E. Melz, \u201cEnhancing llm intelligence with arm-rag: Auxiliary rationale memory for retrieval augmented generation,\u201d _arXiv preprint_\n_arXiv:2311.04177_, 2023.\n\n[74] H", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_22050", "chunk_text": ":2310.14528_, 2023.\n\n[80] P. Ranade and A. Joshi, \u201cFabula: Intelligence report generation\nusing retrieval-augmented narrative construction,\u201d _arXiv preprint_\n_arXiv:2310.13848_, 2023.\n\n[81] X. Jiang, R. Zhang, Y. Xu, R. Qiu, Y. Fang, Z. Wang, J. Tang,\nH. Ding, X. Chu, J. Zhao _et al._, \u201cThink and retrieval: A hypothesis\nknowledge graph enhanced medical large language models,\u201d _arXiv_\n_preprint arXiv:2312.15883_, 2023.\n\n[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\n\u201cKnowledge-augmented language model verification,\u201d _arXiv preprint_\n_arXiv:2310.12836_, 2023.\n\n[83] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, \u201cReasoning on graphs: Faithful\nand interpretable large language model reasoning,\u201d _arXiv preprint_\n_arXiv:2310.01061_, 2023.\n\n[84] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun,\nX. Bresson, and B. Hooi, \u201cG-retriever: Retrieval-augmented generation\nfor textual graph understanding and question answering,\u201d _arXiv preprint_\n_arXiv:2402.07630_, 2024.\n\n[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,\nX. Li, A. Su _et al._, \u201cTablegpt: Towards unifying tables, nature language\nand commands into one gpt,\u201d _arXiv preprint arXiv:2307.08674_, 2023.\n\n[86] M. Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, \u201cIseeq: Information\nseeking question generation using dynamic meta-information retrieval\nand knowledge graphs,\u201d in _Proceedings of the AAAI", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_22500", "chunk_text": " Gaur, K. Gunaratna, V. Srinivasan, and H. Jin, \u201cIseeq: Information\nseeking question generation using dynamic meta-information retrieval\nand knowledge graphs,\u201d in _Proceedings of the AAAI Conference on_\n_Artificial Intelligence_, vol. 36, no. 10, 2022, pp. 10 672\u201310 680.\n\n[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch\u00a8arli,\nand D. Zhou, \u201cLarge language models can be easily distracted by\nirrelevant context,\u201d in _International Conference on Machine Learning_ .\nPMLR, 2023, pp. 31 210\u201331 227.\n\n[88] R. Teja, \u201cEvaluating the ideal chunk size for a rag\nsystem using llamaindex,\u201d [https://www.llamaindex.ai/blog/](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)\n[evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,](https://www.llamaindex.ai/blog/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5)\n2023.\n\n\n[[89] Langchain, \u201cRecursively split by character,\u201d https://python.langchain.](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n[com/docs/modules/data connection/document transformers/recursive](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n[text splitter, 2023.](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n\n[90] S. Yang, \u201cAdvanced rag 01: Small-tobig retrieval,\u201d [https://towardsdatascience.com/](https://towardsdatascience.com/advanced-rag-01-small-to-big-retrieval-172181b396d4)\n[advanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.](https://", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_24750", "chunk_text": "Xiv:1705.03551_, 2017.\n\n[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \u201cSquad: 100,000+\nquestions for machine comprehension    - f text,\u201d _arXiv_ _preprint_\n_arXiv:1606.05250_, 2016.\n\n[115] J. Berant, A. Chou, R. Frostig, and P. Liang, \u201cSemantic parsing on\nfreebase from question-answer pairs,\u201d in _Proceedings of the 2013_\n_conference on empirical methods in natural language processing_, 2013,\npp. 1533\u20131544.\n\n[116] A. Mallen, A. Asai, V. Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\n\u201cWhen not to trust language models: Investigating effectiveness and\nlimitations of parametric and non-parametric memories,\u201d _arXiv preprint_\n_arXiv:2212.10511_, 2022.\n\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\nand L. Deng, \u201cMs marco: A human-generated machine reading comprehension dataset,\u201d 2016.\n\n[118] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning, \u201cHotpotqa: A dataset for diverse, explainable multi-hop question answering,\u201d _arXiv preprint arXiv:1809.09600_,\n2018.\n\n[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, \u201cConstructing a\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps,\u201d\n_arXiv preprint arXiv:2011.01060_, 2020.\n\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, \u201cMusique:\nMultihop questions via single-hop question composition,\u201d _Transactions_\n\n_of the Association for Computational Linguistics_, vol. 10, pp. 539\u2013554,\n2022.\n\n[121] A. Fan, Y.", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_26100", "chunk_text": " S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\n\u201cWizard of wikipedia: Knowledge-powered conversational agents,\u201d\n_arXiv preprint arXiv:1811.01241_, 2018.\n\n[134] H. Wang, M. Hu, Y. Deng, R. Wang, F. Mi, W. Wang, Y. Wang, W.C. Kwan, I. King, and K.-F. Wong, \u201cLarge language models as source\n\n\nplanner for personalized knowledge-grounded dialogue,\u201d _arXiv preprint_\n_arXiv:2310.08840_, 2023.\n\n[135] \u2014\u2014, \u201cLarge language models as source planner for personalized knowledge-grounded dialogue,\u201d _arXiv preprint arXiv:2310.08840_,\n2023.\n\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y. Niu, H. Wu, H. Wang, and S. Wang,\n\u201cLong time no see! open-domain conversation with long-term persona\nmemory,\u201d _arXiv preprint arXiv:2203.05797_, 2022.\n\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\nSu, S. Ultes, D. Vandyke, and S. Young, \u201cConditional generation\nand snapshot learning in neural dialogue systems,\u201d _arXiv preprint_\n_arXiv:1606.03352_, 2016.\n\n[138] R. He and J. McAuley, \u201cUps and downs: Modeling the visual evolution\n\n    - f fashion trends with one-class collaborative filtering,\u201d in _proceedings_\n\n_of the 25th international conference on world wide web_, 2016, pp.\n507\u2013517.\n\n[139] S. Li, H. Ji, and J. Han, \u201cDocument-level event argument extraction\nby conditional generation,\u201d _arXiv preprint arXiv:2104.05919_, 2021.\n\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, \u201cMultisentence argument linking,\u201d _arXiv preprint arXiv:1911.03766_, 2019", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_27000", "chunk_text": " A. Zou, M. Mazeika, D. Song, and\nJ. Steinhardt, \u201cMeasuring massive multitask language understanding,\u201d\n_arXiv preprint arXiv:2009.03300_, 2020.\n\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, \u201cPointer sentinel\nmixture models,\u201d _arXiv preprint arXiv:1609.07843_, 2016.\n\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\n\u201cDid aristotle use a laptop? a question answering benchmark with\nimplicit reasoning strategies,\u201d _Transactions of the Association for_\n_Computational Linguistics_, vol. 9, pp. 346\u2013361, 2021.\n\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, \u201cFever: a\nlarge-scale dataset for fact extraction and verification,\u201d _arXiv preprint_\n_arXiv:1803.05355_, 2018.\n\n[150] N. Kotonya and F. Toni, \u201cExplainable automated fact-checking for\npublic health claims,\u201d _arXiv preprint arXiv:2010.09926_, 2020.\n\n[151] R. Lebret, D. Grangier, and M. Auli, \u201cNeural text generation from\nstructured data with application to the biography domain,\u201d _arXiv_\n_preprint arXiv:1603.07771_, 2016.\n\n[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,\nand G. Neubig, \u201cWikiasp: A dataset for multi-domain aspect-based\nsummarization,\u201d _Transactions of the Association for Computational_\n_Linguistics_, vol. 9, pp. 211\u2013225, 2021.\n\n[153] S. Narayan, S. B. Cohen, and M. Lapata, \u201cDon\u2019t give me the details,\njust the summary! topic-aware convolutional neural networks for extreme summarization,\u201d _arXiv preprint arXiv:1808.08745_, 2018.\n\n[154", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_27450", "chunk_text": " M. Lapata, \u201cDon\u2019t give me the details,\njust the summary! topic-aware convolutional neural networks for extreme summarization,\u201d _arXiv preprint arXiv:1808.08745_, 2018.\n\n[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,\nS. I. Ahmed, N. Mohammed, and M. R. Amin, \u201cVio-lens: A novel\ndataset of annotated social network posts leading to different forms\n\n    - f communal violence and its evaluation,\u201d in _Proceedings of the First_\n_Workshop on Bangla Language Processing (BLP-2023)_, 2023, pp. 72\u2013\n84.\n\n[155] X. Li and D. Roth, \u201cLearning question classifiers,\u201d in _COLING 2002:_\n_The 19th International Conference on Computational Linguistics_, 2002.\n\n[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng,\nand C. Potts, \u201cRecursive deep models for semantic compositionality\n\n    - ver a sentiment treebank,\u201d in _Proceedings of the 2013 conference on_\n_empirical methods in natural language processing_, 2013, pp. 1631\u2013\n1642.\n\n\n\n20\n\n\n[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\n\u201cCodesearchnet challenge: Evaluating the state of semantic code\nsearch,\u201d _arXiv preprint arXiv:1909.09436_, 2019.\n\n[158] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\nM. Plappert, J. Tworek, J. Hilton, R. Nakano _et al._, \u201cTraining verifiers\nto solve math word problems,\u201d _arXiv preprint arXiv:2110.14168_, 2021.\n\n[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,\nand D", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2312.10997_rag_survey_gao:chunk_29250", "chunk_text": "symbolic language modeling with automaton-augmented retrieval,\u201d in\n_International Conference on Machine Learning_ . PMLR, 2022, pp.\n468\u2013485.\n\n[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,\nM. Lewis, L. Zettlemoyer, and W.-t. Yih, \u201cRetrieval-augmented multimodal language modeling,\u201d _arXiv preprint arXiv:2211.12561_, 2022.\n\n[177] J. Li, D. Li, S. Savarese, and S. Hoi, \u201cBlip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language\nmodels,\u201d _arXiv preprint arXiv:2301.12597_, 2023.\n\n[178] W. Zhu, A. Yan, Y. Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y.\nWang, \u201cVisualize before you write: Imagination-guided open-ended\ntext generation,\u201d _arXiv preprint arXiv:2210.03765_, 2022.\n\n[179] J. Zhao, G. Haffar, and E. Shareghi, \u201cGenerating synthetic speech from\nspokenvocab for speech translation,\u201d _arXiv preprint arXiv:2210.08174_,\n2022.\n\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, \u201cUsing external\n\n    - ff-policy speech-to-text mappings in contextual end-to-end automated\nspeech recognition,\u201d _arXiv preprint arXiv:2301.02736_, 2023.\n\n\n21\n\n\n\n\n[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,\nJ. Sivic, and C. Schmid, \u201cVid2seq: Large-scale pretraining of a visual\nlanguage model for dense video captioning,\u201d in _Proceedings of the_\n_IEEE/CVF Conference on Computer Vision and Pattern Recognition_,\n2023, pp. 10 714\u201310 726.\n\n[182] N. Nashid, M", "token_count": 500, "metadata": {"arxiv_id": "2312.10997", "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "authors": ["Yunfan Gao", "Yun Xiong", "Xinyu Gao", "Kangxiang Jia", "Jinliu Pan", "Yuxi Bi", "Yi Dai", "Jiawei Sun", "Meng Wang", "Haofen Wang"], "year": 2023, "url": "https://arxiv.org/pdf/2312.10997v5"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_0", "chunk_text": "## **Benchmarking Large Language Models in Retrieval-Augmented Generation**\n\n**Jiawei Chen** [1,3] **, Hongyu Lin** [1,*] **, Xianpei Han** [1,2,*] **, Le Sun** [1,2]\n\n1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n_{_ jiawei2020,hongyu,xianpei,sunle _}_ @iscas.ac.cn\n\n\n\n**Abstract**\n\n\nRetrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language\nmodels (LLMs). However, existing research lacks rigorous\nevaluation of the impact of retrieval-augmented generation\n\n  - n different large language models, which make it challenging to identify the potential bottlenecks in the capabilities\n\n  - f RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance\n\n  - f different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case.\nThen we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\n\n  - f noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n\n\n**Introduction**\n\n\nRecently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_450", "chunk_text": " (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao et al. 2020; Raunak, Menezes,\nand Junczys-Dowmunt 2021; Ji et al. 2023), knowledge outdating (He, Zhang, and Roth 2022), and the lack of domainspecific expertise (Li et al. 2023c; Shen et al. 2023).\nIncorporating external knowledge via information retrieval, i.e., Retrieval-Augmented Generation (RAG), has\nbeen regarded as a promising way to resolve the above challenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\n\n\n  - Corresponding authors.\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\n2022; Izacard et al. 2022). With the help of external knowledge, LLMs can generate more accurate and reliable responses. The most common method is to use a search engine\nas a retriever such as New Bing. Due to the vast amount of\ninformation available on the Internet, using a search engine\ncan provide more real-time information.\n\nHowever, Retrieval-Augmented Generation brings not\n\n- nly positive effects to LLMs (Liu, Zhang, and Liang 2023;\nMaynez et al. 2020). On one hand, there is a significant\namount of noise information even fake news in the content\navailable on the Internet, which poses challenges for search\nengines in accurately retrieving desirable knowledge. On the\n\n- ther hand, LLMs suffer from unreliable generation challenge. LLMs can be misled by incorrect information contained in the context (Bian et al. 2023) and also suffer from\nhallucination during the generation (Adlakha et al. 2023),\nresulting in generating content that goes beyond external in\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nformation. These challenges result in LLMs being unable to\nconsistently generate reliable and accurate responses. Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how\ncould each model survives from these drawbacks and improvement", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_900", "chunk_text": ". These challenges result in LLMs being unable to\nconsistently generate reliable and accurate responses. Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how\ncould each model survives from these drawbacks and improvement their performance via information retrieval. As a\nresult, there is a pressing need for a comprehensive evaluation of LLMs on their ability to effectively utilize retrieved\ninformation, as well as their ability to withstand the various\ndrawbacks present in information retrieval.\nTo this end, this paper conducts a comprehensive evaluation of RAG for current LLMs. Specifically, we create a new\nRetrieval-Augmented Generation Benchmark, namely RGB,\nin both English and Chinese. In order to ensure that the internal knowledge of LLMs does not introduce bias into the\nevaluation results, RGB chooses to aggregate the latest news\ninformation and constructs queries based on the news information. Then, based on these queries, we use Search API to\nfetch relevant documents and select most relevant snippets\nfrom the content as external retrieved documents. Finally,\nbased on different compositions of query and document-set\npairs, we expand the corpus and divided it into 4 testbeds to\nevaluate the following basic abilities of LLMs according to\nthe common challenges in RAG, as shown in Figure 1:\n\n\n - **Noise Robustness**, which means a LLM can extract useful information from noisy documents. In this paper, we\ndefine noisy documents as those that are relevant to the\nquestion but do not contain any information of the answer. For the instance in Figure 1, the noisy documents\nrelated to the question \u201cWho was awarded the 2022 Nobel Prize in Literature\u201d include reports about the 2021\nNobel Prize in Literature. To this end, the testbed for\nnoise robustness contains instances whose external documents contain a certain number of noisy documents\nbased on the desired noise ratio.\n\n - **Negative Rejection**, which means that a LLM should reject to answer the question when the required knowledge\nis not present in any retrieved document. The testbed for\nnegative rejection contains instances whose external documents are only with noisy documents. LLMs are expected to indicate \u201cinsufficient information\u201d or other rejection signals.\n\n - **Information Integration**, which evaluates whether\nLLMs can answer complex questions that require integrating information from multiple documents. For the instance in Figure 1, for the question \u201cWhen were the ChatGPT app", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_1350", "chunk_text": "ufficient information\u201d or other rejection signals.\n\n - **Information Integration**, which evaluates whether\nLLMs can answer complex questions that require integrating information from multiple documents. For the instance in Figure 1, for the question \u201cWhen were the ChatGPT app for iOS and ChatGPT api launched?\u201d, LLMs\nare expected to provide information of the launch dates\nfor both the ChatGPT iOS app and ChatGPT API. The\ntestbed for information integration contains instances\nthat can only be answered using multiple documents.\n\n - **Counterfactual Robustness**, which evaluates whether\nLLMs can identify risks of known factual errors in the\nretrieved documents when the LLMs are given warnings\nabout potential risks in the retrieved information through\ninstruction. The testbed for counterfactual robustness includes instances that can be answered directly by the\nLLMs, but the external documents contain factual errors.\n\n\n\nBased on RGB, we conduct evaluation on 6 state-ofthe-art large language models including ChatGPT (OpenAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM26B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),\nQwen-7B-Chat (QwenLM 2023), BELLE-7B (Yunjie Ji\n2023). We found that even though RAG can improve the response accuracy of LLMs, they still suffer from the abovementioned challenges significantly. Specifically, we found\nthat even though LLMs demonstrate some level of noise robustness, they tend to confuse similar information and frequently generate inaccurate answers when relevant information exists. For example, when faced with a question about\nthe 2022 Nobel Prize in Literature, if there are noisy documents about the 2021 Nobel Prize in Literature in external\ndocuments, LLMs may become confused and provide inaccurate answers. Besides, LLMs frequently fail to reject answering and generate incorrect answers when none of the\nexternal documents contain relevant information. Furthermore, LLMs lack the ability to summarize from multiple\ndocuments, and therefore if multiple documents are needed\nto answer a question, LLMs often fail to provide accurate\nanswer. Finally, we found that even when the LLMs contain\nthe required knowledge and are given warnings about potential risks in the retrieved information through instruction,\nthey still tend to trust and prioritize", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_1800", "chunk_text": " question, LLMs often fail to provide accurate\nanswer. Finally, we found that even when the LLMs contain\nthe required knowledge and are given warnings about potential risks in the retrieved information through instruction,\nthey still tend to trust and prioritize the retrieved information\n\n- ver their own existing knowledge. The experimental results\nmentioned above highlight the need for further resolution of\nimportant issues in the existing RAG method. Therefore, it\nis crucial to exercise caution and carefully design its usage.\nGenerally speaking, the contributions of this paper are [1] :\n\n\n - We proposed to evaluate four capabilities for retrievalaugmented generation  - f LLMs and created the\nRetrieval-Augmented Generation Benchmark in both English and Chinese. To best of our knowledge, it is the first\nbenchmark designed to assess these four capabilities for\nretrieval-augmented generation of LLMs.\n\n - We evaluated the existing LLMs using RGB and found\nthe limitations of them in the four different abilities.\n\n - We analyzed the responses of LLMs in RGB and identified their current shortcomings as well as suggested directions for improvement.\n\n\n**Related work**\n\n**Retrieval-augmented models** The knowledge stored in\nlarge language models is commonly out-of-date (He, Zhang,\nand Roth 2022) and they also sometimes generate hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023) i.e., they may generate irrelevant or factually incorrect contents. By using external\nknowledge as guidance, retrieval-augmented models can\ngenerate more accurate and reliable responses (Guu et al.\n2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\net al. 2022; Shi et al. 2023; Ren et al. 2023). Retrievalaugmented models have achieved remarkable results in various tasks such as open-domain QA (Izacard and Grave\n2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\n\n\n1Our code&data: https://github.com/chen700564/RGB.\n\n\n|Retrieve using<br>search engine|Col2|\n|---|---|\n|**Retrieve using**<br>**search engine**|{\"link\": \"https", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_2250", "chunk_text": "Our code&data: https://github.com/chen700564/RGB.\n\n\n|Retrieve using<br>search engine|Col2|\n|---|---|\n|**Retrieve using**<br>**search engine**|{\"link\": \"https://www.nobelprize.org/prizes/medicine/\", \"title\":<br>\"The Nobel Prize in Physiology or Medicine 2022\", \"snippet\": \"The<br>Nobel Assembly...\"}, ...|\n|**Retrieve using**<br>**search engine**|**Google Search API**|\n|**Retrieve using**<br>**search engine**|Query:Who was awarded the 2022 Nobel Prize for Physiology and<br>Medicine?\u201d,|\n\n\n\n\n\n**Data generation by**\n\n**ChatGPT**\n\n\n\n{\n\n\u201cQuestion\u201d: \u201cWho was awarded the 2022\nNobel Prize for Physiology and Medicine?\u201d,\n\n\u201cAnswer\u201d: ['Svante P\u00e4\u00e4bo','Svante Paabo\u2019]\n}\n\n\n|Col1|R ee yl ea it ne fd re m:v We ann it o?o: n2 :0 Sa2 vs2 nN to eab Pe \u00e4l \u00e4P br i oz e nf 2o d0r S2P v2h y ns toi eo l Peo alg aPy a ozn ed M ore Pd hic yi sn ioe<br>Q u s ti eo dn teh w a w r d e d t h ae N b br i f logy<br>a Kn d M i c i<br>o a a|\n|---|---|\n||**gpt-3.5-turbo api**|\n||We simulate the process of a user querying and obtaining<br>information. Suppose the user retrieves a current event news,<br>speculate the event that the user is concerned about and the<br>question that he/she may want to know, and generate the key<br>information corresponding to the answer to the question. \u2026<br>\u2026<br>News: The2022 Nobel Prize for Physiology and Medicine was \u2026|\n\n\n\n\n\n\n\nFigure 2: The process of data generation. Firstly, we use\nmodels to extract (event, question, answer) from news articles. Next, we utilize search engines to retrieve relevant\nweb pages. Finally, a dense retrieval model is employed to\nre-rank the content of these web pages.\n\n\net al. 2019a,b; Peng et al. 2023", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_2700", "chunk_text": ". Next, we utilize search engines to retrieve relevant\nweb pages. Finally, a dense retrieval model is employed to\nre-rank the content of these web pages.\n\n\net al. 2019a,b; Peng et al. 2023), domain-specific question answering (Cui et al. 2023) and code generation (Zhou\net al. 2023b). Recently, with the development of large models, a series of retrieval-enhanced tools and products have\ngained widespread attention, such as ChatGPT retrieval plugin, Langchain, New Bing, etc. However, in real-world scenarios, the retrieved text inevitably contains noise. Therefore, in this paper we conducted a systematic evaluation and\nanalysis of retrieval-augmented generation in LLMs.\n\n\n**Evaluation of LLMs** Evaluating LLMs has received significant attention due to their remarkable general capability (Chang et al. 2023). It enables us to gain a deeper understanding of the specific abilities and limitations of LLMs,\nwhile also providing valuable guidance for future research.\nIn the past, benchmarks such as GLUE (Wang et al. 2019b)\nand SuperCLUE (Wang et al. 2019a) primarily focused on\nevaluating NLP tasks, particularly in natural language understanding. However, these evaluations often fail to fully\ncapture the capabilities of LLMs. MMLU (Hendrycks et al.\n2021) was then proposed to measure the knowledge acquired\nby language models when pre-training. Recently, with the\ndevelopment of LLMs, a series of general evaluation benchmarks have emerged, such as AGIEval (Zhong et al. 2023),\nC-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),\nOpenLLM Leaderboard (Edward Beeching 2023), etc. In\naddition to general abilities, there are also specific benchmarks that focus on evaluating the capabilities of models.\nFor example, CValues (Xu et al. 2023a) focuses on the safety\n\n\n\nand responsibility of LLMs, M3Exam (Zhang et al. 2023)\nfocuses on human exam and ToolBench (Qin et al. 2023)\nevaluates how well LLMs use external tools. Recently, Adlakha et al. (2023)", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_3150", "chunk_text": " et al. 2023)\nfocuses on human exam and ToolBench (Qin et al. 2023)\nevaluates how well LLMs use external tools. Recently, Adlakha et al. (2023) evaluate the RAG of LLMs in exist QA\ndataset. Different from their work, we focus on 4 required\nabilities of RAG and create Retrieval-Augmented Generation Benchmark to evaluate the LLMs.\n\n\n**Retrieval-Augmented Generation Benchmark**\n\n\nIn this section, we first introduce the specific retrievalaugmented generation abilities we aim to evaluate. Next, we\n\n- utline the process of constructing the RAG benchmark for\nevaluation. Lastly, we present the evaluation metrics.\n\n\n**Required abilities of RAG**\n\n\nExternal knowledge is the key to resolving the problems\n\n- f LLMs such as hallucination and outdated knowledge,\nwhich can make LLMs generate more accurate and reliable\nresponses through retrieval-augmented generation (RAG).\nHowever, LLMs cannot always response as expected with\nRAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating\nthese external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the\nuseful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the\nretrieval-augmented generation of LLMs, and we concern\nabout 4 specific abilities:\n**Noise Robustness** is the robustness of LLMs in noisy\ndocuments. As retrievers are not perfect, the external knowledge they retrieve often contains a significant amount of\nnoise, i.e., documents which are relevant to the question but\ndo not contain any information about the answer. To effectively answer user questions, LLMs must be able to extract\nthe necessary information from documents despite there are\nnoisy documents.\n**Negative Rejection** is a measure of whether LLMs can\ndecline to answer a question when none of the contexts provide useful information. In real-world situations, the search\nengine often fails to retrieve documents containing the answers. In these cases, it is important for the", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_3600", "chunk_text": " measure of whether LLMs can\ndecline to answer a question when none of the contexts provide useful information. In real-world situations, the search\nengine often fails to retrieve documents containing the answers. In these cases, it is important for the model to have\nthe capability to reject recognition and avoid generating misleading content.\n**Information Integration** is a capacity to integrate answers from multiple documents. In many cases, the answer to a question may be contained in multiple documents.\nFor example, for the question _\u201cWho are the champions of_\n_the U.S. Open 2022 men\u2019s and women\u2019s singles?\u201d_, the two\nchampions may be mentioned in different documents. In order to provide better answers to complex questions, it is necessary for LLMs to have the ability to integrate information.\n**Counterfactual Robustness** refers to a capacity to handle errors in external knowledge. In the real world, there is\nan abundance of false information on the internet. Please\n\n\nnote that we only evaluate the situation that LLMs are given\nwarnings about potential risks in the retrieved information\nthrough instruction.\nIn real-world scenarios, it is not possible to obtain perfect documents with all the necessary external knowledge.\nTherefore, evaluating these four abilities of the model becomes essential in order to measure the RAG of LLMs.\n\n\n**Data construction**\n\nInspired by previous benchmarks for LLMs, RGB utilizes\na question-answering format for evaluation. We evaluate the\nLLMs by judging the retrieval-augmented responses of them\nto the questions. To simulate real-world scenarios, we construct question and answer data using actual news articles.\nDue to the abundance of knowledge contained within the\nLLMs there is a potential for bias when measuring the first\nthree abilities. To mitigate this, the instances of RGB are\nconstructed by latest news articles. Additionally, we retrieve\nexternal documents from Internet through search engines.\nFinally, we expand the corpus and divided it into 4 testbeds\nto evaluate the above basic abilities of LLMs. The overall\nprocedure of our data construction is illustrated in Figure 2.\n**QA instances generation.** We first collect latest news articles and use prompts to make ChatGPT generate events,\nquestions, and answers for each articles. For example, as\nshown in the Figure 2, for a report about \u201cThe 2022 Nobel\nPrize\u201d, ChatGPT will generate corresponding event, question and provide key", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_4050", "chunk_text": "PT generate events,\nquestions, and answers for each articles. For example, as\nshown in the Figure 2, for a report about \u201cThe 2022 Nobel\nPrize\u201d, ChatGPT will generate corresponding event, question and provide key information for answering it. By generating events, the model is able to preliminarily filter out\nnews articles that do not contain any events. After generation, we manually check the answer and filter out data that\nis difficult to retrieve through search engines.\n**Retrieve using search engine.** For each query, we use\nGoogle\u2019s API to fetch 10 relevant web pages and extract\ncorresponding snippets of text from them. Simultaneously,\nwe read these web pages and convert their textual content\ninto text chunks with a maximum length of 300 tokens. Using an existing dense retrieval model [2], we select the top-30\ntext chunks that match the query most effectively. These retrieved text chunks, along with the snippets provided by the\nsearch API, will serve as our external documents. These documents will be divided into positive documents and negative\ndocuments based on whether they contain the answer.\n**Testbeds construction for each ability.** We expand the\ncorpus and divided it into 4 testbeds to evaluate the above\nbasic abilities of LLMs. To evaluate the noise robustness,\nwe sample varying numbers of negative documents according to the desired ratio of noises. For negative rejection, all the external documents are sampled from negative\ndocuments. For the information integration ability, we further construct data based on the above generated questions.\nThis involves expanding or rewriting these questions so that\ntheir answers encompass multiple aspects. For example, the\nquestion \u201cWho won the MVP of Super Bowl 2023?\u201d can\nbe rewrite as \u201cWho won the MVPs of Super Bowl 2022\nand 2023?\u201d. Consequently, answering such questions re\n\n2Chinese: https://huggingface.co/moka-ai/m3e-base; English:\nhttps://huggingface.co/sentence-transformers/all-mpnet-base-v2.\n\n\n\nFigure 3: The instructions used in our experiments, which\ninclude a system instruction followed by a user input instruction. The \u201c _{_ DOCS _}_ \u201d and \u201c _{_ QUERY _}_ \u201d will be replaced by\nthe external documents and the question.\n\n\nquires utilizing information from various documents. Different from the first three abilities, the data of counterfactual\n", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_4500", "chunk_text": " The \u201c _{_ DOCS _}_ \u201d and \u201c _{_ QUERY _}_ \u201d will be replaced by\nthe external documents and the question.\n\n\nquires utilizing information from various documents. Different from the first three abilities, the data of counterfactual\nrobustness is constructed solely based on the internal knowledge of the model. Based on the aforementioned generated\nquestions mentioned above, we adopt ChatGPT to automatically generate its known knowledge. Specifically, we use\nprompts to allow the model to generate both questions and\nanswers that are already known. For example, based on the\nquestion \u201cWho was awarded the 2022 Nobel Prize for Physiology and Medicine?\u201d, the model will generate the known\nquestion \u201cWho was awarded the 2021 Nobel Prize in Literature?\u201d and answer \u201c _Abdulrazak Gurnah_ \u201d. We then manually verified the generated answers, and retrieve relevant\ndocuments as described above. In order to make documents\ncontain factual errors, we manually modify the answers and\nreplace the corresponding parts in the document.\nFinally, we collect totally 600 base questions in RGB,\nand 200 additional questions for the information integration\nability and 200 additional questions for counterfactual robustness ability. Half of the instances are in English, and the\n\n- ther half are in Chinese.\n\n\n**Evaluation metrics**\n\nThe core of this benchmark is to evaluate whether LLMs can\nutilize the provided external documents to acquire knowledge and generate reasonable answers. We evaluate the responses of LLMs in order to measure above-mentioned four\nabilities of them.\n**Accuracy** is used to measure noise robustness and information integration. We employ an exact matching approach\nwhere if the generated text contains an exact match to the\nanswer, it is considered as a correct answer.\n**Rejection rate** is used to measure negative rejection.\nWhen only noisy documents are provided, LLMs should\n\n- utput the specific content \u2013 \u201cI can not answer the question\nbecause of the insufficient information in documents.\u201d (We\nuse instructions to inform the model.). If the model generates this content, it indicates a successful rejection.\n**Error detection rate** measures whether the model can\n\ndetect the factual errors in the documents for counterfactual\nrobustness. When the provided documents contain factual\nerrors, the model should output the specific content \u2013 \u201cThere\nare factual errors in the provided documents.\u201d (We use in\n\n\n**English** **Chinese**\n\n\n\n**", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_4950", "chunk_text": " for counterfactual\nrobustness. When the provided documents contain factual\nerrors, the model should output the specific content \u2013 \u201cThere\nare factual errors in the provided documents.\u201d (We use in\n\n\n**English** **Chinese**\n\n\n\n**System instruction**\n\nYou are an accurate and reliable AI assistant that can\n\nanswer questions with the help of external documents.\nPlease note that external documents may contain noisy\n\n- r factually incorrect information. If the information in\nthe document contains the correct answer, you will give\nan accurate answer. If the information in the document\n\ndoes not contain the answer, you will generate \u2019I can not\nanswer the question because of the insufficient\ninformation in documents.\u2018 If there are inconsistencies\n\nwith the facts in some of the documents, please generate\nthe response 'There are factual errors in the provided\ndocuments.' and provide the correct answer.\n\n\n\n**System instruction**\n\u4f60\u662f\u4e00\u4e2a\u51c6\u786e\u548c\u53ef\u9760\u7684\u4eba\u5de5\u667a\u80fd\u52a9\u624b\uff0c\n\u80fd\u591f\u501f\u52a9\u5916\u90e8\u6587\u6863\u56de\u7b54\u95ee\u9898\uff0c\u8bf7\u6ce8\u610f\n\u5916\u90e8\u6587\u6863\u53ef\u80fd\u5b58\u5728\u566a\u58f0\u4e8b\u5b9e\u6027\u9519\u8bef\u3002\n\u5982\u679c\u6587\u6863\u4e2d\u7684\u4fe1\u606f\u5305\u542b\u4e86\u6b63\u786e\u7b54\u6848\uff0c\n\u4f60\u5c06\u8fdb\u884c\u51c6\u786e\u7684\u56de\u7b54\u3002\u5982\u679c\u6587\u6863\u4e2d\u7684\n\u4fe1\u606f\u4e0d\u5305\u542b\u7b54\u6848\uff0c\u4f60\u5c06\u751f\u6210\u201c\u6587\u6863\u4fe1\n\u606f\u4e0d\u8db3\uff0c\u56e0\u6b64\u6211\u65e0\u6cd5\u57fa\u4e8e\u63d0\u4f9b\u7684\u6587\u6863\n\u56de\u7b54\u8be5\u95ee\u9898\u3002\u201d\u5982\u679c\u90e8\u5206\u6587\u6863\u4e2d\u5b58\u5728\n\u4e0e\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u7684\u9519\u8bef\uff0c\u8bf7\u5148\u751f\u6210\u201c\u63d0\n\u4f9b\u6587\u6863\u7684\u6587\u6863\u5b58\u5728\u4e8b\u5b9e\u6027\u9519\u8bef\u3002\u201d\uff0c\n\u5e76\u751f\u6210\u6b63\u786e\u7b54\u6848\u3002\n\n\n**User input Instruction**\n\u6587\u6863\uff1a \\n{DOCS} \\n\\n \u95ee\u9898\uff1a \\n{QUERY}\n\n\n\n**User input Instruction**\nDocument:\\n{DOCS} \\n\\nQuestion:\\n{QUERY}\n\n\n|Col1|English|Chinese|\n|---|---|---|\n|Noise Ratio|0<br>0.2<br>0.4<br>0.6<br>0.8|0<br>0.2<br>0.4<br>0.6<br>0.8|\n|ChatGPT (OpenAI 2022)<br>ChatGLM-6B (THUDM 2023a)<br>", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_5400", "chunk_text": ">0.2<br>0.4<br>0.6<br>0.8|\n|ChatGPT (OpenAI 2022)<br>ChatGLM-6B (THUDM 2023a)<br>ChatGLM2-6B (THUDM 2023b)<br>Vicuna-7B-v1.3 (Chiang et al. 2023)<br>Qwen-7B-Chat (QwenLM 2023)<br>BELLE-7B-2M (Yunjie Ji 2023)|**96.33**<br>**94.67**<br>**94.00**<br>**90.00**<br>**76.00**<br>93.67<br>90.67<br>89.33<br>84.67<br>70.67<br>91.33<br>89.67<br>83.00<br>77.33<br>57.33<br>87.67<br>83.33<br>86.00<br>82.33<br>60.33<br>94.33<br>91.67<br>91.00<br>87.67<br>73.67<br>83.33<br>81.00<br>79.00<br>71.33<br>64.67|**95.67**<br>**94.67**<br>**91.00**<br>**87.67**<br>**70.67**<br>94.33<br>90.67<br>89.00<br>82.33<br>69.00<br>86.67<br>82.33<br>76.67<br>72.33<br>54.00<br>85.67<br>82.67<br>77.00<br>69.33<br>49.67<br>94.00<br>92.33<br>88.00<br>84.33<br>68.67<br>92.00<br>88.67<br>85.33<br>78.33<br>67.68|\n\n\nTable 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the\nincreasing noise rate poses a challenge for RAG in LLMs.\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Long-distance", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_5850", "chunk_text": "|\n\n\nTable 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the\nincreasing noise rate poses a challenge for RAG in LLMs.\n\n\n\n\n\n\n\n\n\n\n\n\n|Col1|Long-distance information.|Evidence uncertainty.|Concept confusion.|\n|---|---|---|---|\n|**Question**|Who did Iga Swiatek defeat to win the Qatar Open 2022?|What is the name of Apple\u2019s headset?|What was Tesla\u2019s revenue in Q1 2022?|\n|**Answer**|**Anett Kontaveit**|**Vision Pro**|**18.76 billion**|\n|**Documents**|_Positive document_<br>In February, Swiatek entered into the Qatar Open ...<br>In the fnal, she won ...** Anett Kontaveit** ...<br>_Negative document_<br>This time, she defeated Ons Jabeur 6-2, 7-6(5) to win<br>the 2022 US Open, ...|_Positive document_<br>Apple (AAPL.O) on Monday unveiled a costly<br>augmented-reality headset called the** Vision Pro** ...<br>_Negative document_<br>... is what Gurman believes will be called<br>Apple Reality Pro. ...|_Positive document_<br>Tesla, Inc. (TSLA) reported Q1 FY 2022 earnings results<br>... detailed revenues of $**18.76 billion** ...<br>_Negative document_<br>...frst-quarter earnings for 2022 ...<br>...Automotive revenue reached $16.86 billion...|\n|**Responses**|Iga Swiatek defeated Ons Jabeur in the second round<br>of the Qatar Open 2022 to win the tournament.|According to the document, the name of Apple\u2019s<br>headset is Apple Reality Pro.|According to the fnancial results provided in the article,<br>Tesla\u2019s revenue in Q1 2022 was $16.86 billion.|\n\n\n\nTable 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses\nare generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,\nwhile the red text highlights the non-matching parts.\n\n\n\nstructions to inform", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_6300", "chunk_text": " one negative document are shown. The responses\nare generated by ChatGLM2-6B. The blue text indicates the matching parts between the document and the question or answer,\nwhile the red text highlights the non-matching parts.\n\n\n\nstructions to inform the model.). If the model generates this\ncontent, it indicates that the model has detected erroneous\ninformation in the document.\n\n**Error correction rate** measures whether the model can\nprovide the correct answer after identifying errors for counterfactual robustness. The model is asked to generate the correct answer after identifying the factual errors. If the model\ngenerates the correct answer, it indicates that the model is\ncapable of correcting errors in the document.\nConsidering that the model may not fully adhere to instructions, for rejection rate and error detection rate, we\nalso use ChatGPT to conduct additional evaluation of the\nanswers. Specifically, we assess the model\u2019s responses by\nusing instructions and demonstrations to determine if they\ncan reflect information that is not present in the document or\nidentify any factual errors.\n\n\n**Experiments**\n\n\nIn this section, we evaluate the performance of various\nLLMs, analyze and discuss the results, summarizing the\nmain challenges that existing LLMs encounter when using\nexternal knowledge.\n\n\n**Settings**\n\n\n**Task formats.** Due to contextual limitations, we provide 5\nexternal documents for each question. In our experiments\n\n- n noise robustness, we evaluate scenarios with noise ratios ranging from 0 to 0.8. To comprehensively evaluate the\n\n- verall capabilities, we have adopted a unified instruction\nfor each language, as shown in Figure 3. The experiments\nwere conducted using an NVIDIA GeForce RTX 3090.\n\n\n\n**Models** We conduct evaluation on 6 state-of-the-art\nlarge language models which can generate both English and Chinese including ChatGPT (OpenAI 2022) [3],\nChatGLM-6B (THUDM 2023a), ChatGLM2-6B (THUDM\n2023b), Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7BChat (QwenLM 2023), BELLE-7B-2M (Yunjie Ji 2023).\n\n\n**Results on Noise Robustness**\n\nWe evaluated the accuracy based on the different noise ratios\nin external documents, and the results are shown in Table ", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_6750", "chunk_text": "3), BELLE-7B-2M (Yunjie Ji 2023).\n\n\n**Results on Noise Robustness**\n\nWe evaluated the accuracy based on the different noise ratios\nin external documents, and the results are shown in Table 1.\nWe can see that:\n**(1) RAG can effect improve the responses of LLMs.**\nLLMs have shown strong performance even in the presence\n\n- f noise, indicating that RAG is a promising way for LLMs\nto generate accurate and reliable responses.\n**(2) The increasing noise rate poses a challenge for**\n**RAG in LLMs.** Specifically, when the noise ratio exceeds\n80%, the accuracy decreases significantly at a significance\nlevel of 0.05. For example, the performance of ChatGPT has\ndecreased from 96.33% to 76.00%, while the performance\n\n- f ChatGLM2-6B has decreased from 91.33% to 57.33%.\n\n\n**Error Analysis.** To better comprehend the negative impact of noise on model generation, we examined the incorrect answers and found that these errors typically originate\nfrom three reasons, as shown in Table 2.\n**(1) Long-distance information.** LLMs often face difficulty in identifying the correct answer from external documents when the information related to the question is distant\nfrom the information related to the answer. This scenario\nis quite common as longer texts are frequently encountered\n\n\n3We use gpt-3.5-turbo api in the experiments.\n\n\n- n the internet. In such cases, it is typical for the question\u2019s\ninformation to be initially presented at the start of the document and subsequently referred to using pronouns. In Table 2, the question information (\u201cQatar Open 2022\u201d) is only\nmentioned once at the beginning and is far from where the\nanswer text \u201cAnett Kontaveit\u201d appears. This situation may\ncause LLMs to depend on information from other documents and create false impressions, i.e., hallucination.\n**(2) Evidence uncertainty.** Before highly anticipated\nevents, like the release of new Apple products or the announcement of the Oscars, there is often a significant\namount of speculative information circulating on the internet. Although the relevant documents explicitly state that\nit is uncertain or speculative content, they can still impact\n\n- n the retrieval-augmented generation of LLMs. In Table", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_7200", "chunk_text": " there is often a significant\namount of speculative information circulating on the internet. Although the relevant documents explicitly state that\nit is uncertain or speculative content, they can still impact\n\n- n the retrieval-augmented generation of LLMs. In Table 2,\nwhen the noise ratio increases, the content of erroneous\ndocuments is all about some people\u2019s predictions about the\nname of the headset (\u201cApple Reality Pro\u201d). Even if there is\na correct answer (\u201cVision Pro\u201d) in the relevant documents,\nLLMs can still be misled by uncertain evidences.\n**(3) Concept confusion.** The concepts in external documents may be similar to, but different from, the concepts in\nthe question. This can cause confusion for LLMs and make\nLLMs generate incorrect answers. In Table 2, the model answer focuses on the concept \u201cautomotive revenue\u201d in the\ndocument rather than \u201crevenue\u201d in the question.\nBased on the analysis above, we have identified certain\nlimitations in LLMs regarding retrieval-augmented generation. To effectively handle the vast amount of noise present\n\n- n the internet, further detailed enhancements are required\nfor the model such as long documents modeling and precise\nconcept comprehension.\n\n\n**Results on Negative Rejection testbed**\n\nWe evaluated the rejection rate when only noise documents\nwere provided. The results are shown in Table 3. In addition to evaluating the rejection rate through exact matching\n(Rej in Table 3), we also utilize ChatGPT to determine if\nthe responses from the LLMs contain any rejection information (Rej _[\u2217]_ in Table 3). We can see that: **Negative Rejection**\n**poses a challenge for RAG in LLMs.** The highest rejection\nrates for LLMs in English and Chinese were only 45% and\n43.33%, respectively. This suggests that LLMs can be easily\nmisled by noisy documents, leading to incorrect answers.\nIn addition, through comparing Rej and Rej _[\u2217]_, we found\nthat LLMs fail to strictly follow instructions, and they often\ngenerate unpredictable responses, which make it hard to use\nthem as state triggers (such as for recognizing rejection).\nWe conduct case studies in Table 4. The first error is\nbecause of **Evidence uncertainty** . Although the document\n\n- nly mentions contact with \u201cAdam McKay\u201d and does not\nexplicitly state that he", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_7650", "chunk_text": "such as for recognizing rejection).\nWe conduct case studies in Table 4. The first error is\nbecause of **Evidence uncertainty** . Although the document\n\n- nly mentions contact with \u201cAdam McKay\u201d and does not\nexplicitly state that he is the director of the movie, the\nmodel still concludes that he holds this role. The first error is because of **Concept confusion** . The information provided in the answer pertains to \u201cthe 2018 Winter Olympics\u201d\ninstead of \u201cthe 2022 Olympics\u201d mentioned in the question.\nRetrieval-augmented generation poses a greater challenge of\nnegative rejection compared to answer directly as it presents\nrelevant documents that could potentially mislead the LLMs\n\n\n\n|Languages|English|Chinese|\n|---|---|---|\n||Rej<br>Rej_\u2217_|Rej<br>Rej_\u2217_|\n|ChatGPT<br>ChatGLM-6B<br>ChatGLM2-6B<br>Vicuna-7B-v1.3<br>Qwen-7B-Chat<br>BELLE-7B-2M|24.67<br>**45.00**<br>9.00<br>25.00<br>10.33<br>41.33<br>17.00<br>33.33<br>**31.00**<br>35.67<br>5.67<br>32.33|5.33<br>**43.33**<br>6.33<br>17.00<br>6.33<br>36.33<br>3.37<br>24.67<br>**8.67**<br>25.33<br>5.33<br>13.67|\n\n\nTable 3: The result of negative rejection. Rej means the rejection rate (%) and Rej _[\u2217]_ means the rejection rate evaluated\nby ChatGPT. We can see that negative rejection poses a challenge for RAG in LLMs.\n\n\n\n\n\n\n\n\n|Question|Answer|Response|\n|---|---|---|\n|who will direct<br>Irredeemable flm?|**Jeymes**<br>**Samuel**|The information provided in the document<br>is that Fox has attached \u201cThe Big Short\u201d<br>writer-director Adam McKay to its movie<br>adaptation of \u201cIrredeemable\u201d", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_8100", "chunk_text": "mes**<br>**Samuel**|The information provided in the document<br>is that Fox has attached \u201cThe Big Short\u201d<br>writer-director Adam McKay to its movie<br>adaptation of \u201cIrredeemable\u201d from Boom!<br>Comics. Therefore, the answer is Adam McKay.|\n|Which country won the<br>most medals at the<br>2022 Winter Olympics?|**Norway**|Based on the information provided in the<br>document, the country that won the most medals<br>at the 2022 Winter Olympics is Germany.<br>Germany has won a total of 31 medals, including 14<br>gold medals, 14 bronze medals, and 31 silver medals.|\n\n\n\nTable 4: Error cases of negative rejection generated by\nChatGLM2-6B. The red text highlights the error answers.\n\n\nand result in incorrect responses. In future developments, it\nwill be crucial for LLMs to enhance their ability to accurately match questions with the appropriate documents.\n\n\n**Results on Information Integration testbed**\n\nWe evaluated the accuracy based on the different noise ratios\nin external documents, and the results are shown in Table 5.\nWhen comparing the model to Table 1, we observed that\nit has a weak information integration ability, which in turn\naffects its noise robustness. We can see that:\n**(1) Information integration poses a challenge for RAG**\n**in LLMs.** Even without noise, the highest accuracy of LLMs\ncan only reach 60% and 67% for English and Chinese,\nrespectively. After adding noise, the highest accuracy decreases to 43% and 55%. These results suggest that LLMs\nstruggle with integrating information effectively and are not\nwell-suited for directly answering complex questions.\n**(2) Complex questions are more challenging for RAG**\n**with noisy documents.** Performance decline becomes significant when the noise ratio is 0.4, but for simple problems,\na significant decline occurs only at a noise ratio of 0.8 at a\nsignificance level of 0.05. This indicates that complex problems are more vulnerable to interference from noise. We\nspeculate that this is because solving complex problems requires integrating information from multiple documents, and\nthis information can be considered as noise to each other,\nmaking it harder for the model to extract relevant information from the documents.\n\n\n**Error Analysis.", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_8550", "chunk_text": ". We\nspeculate that this is because solving complex problems requires integrating information from multiple documents, and\nthis information can be considered as noise to each other,\nmaking it harder for the model to extract relevant information from the documents.\n\n\n**Error Analysis.** We conducted an error analysis on\nChatGLM2-6B (noise ratio is 0). Apart from the similar errors founded in the noise robustness experiment (38% of the\ntotal), there are also three types of unique errors. We have\npresented these cases in Table 6.\n\n\n|Col1|English|Chinese|\n|---|---|---|\n|Noise Ratio|0<br>0.2<br>0.4|0<br>0.2<br>0.4|\n|ChatGPT<br>ChatGLM-6B<br>ChatGLM2-6B<br>Vicuna-7B-v1.3<br>Qwen-7B-Chat<br>BELLE-7B-2M|55<br>51<br>34<br>45<br>36<br>35<br>34<br>32<br>21<br>**60**<br>**53**<br>**43**<br>55<br>50<br>37<br>40<br>34<br>24|63<br>**58**<br>47<br>60<br>53<br>52<br>44<br>43<br>32<br>43<br>36<br>25<br>**67**<br>56<br>**55**<br>49<br>41<br>38|\n\n\nTable 5: The experimental result of information integration\nmeasured by accuracy (%) under different noise ratios. We\ncan see that information integration poses a challenge for\nRAG in LLMs.\n\n\n\n|Col1|Acc Accdoc ED ED\u2217 CR|\n|---|---|\n|ChatGPT-zh<br>Qwen-7B-Chat-zh<br>ChatGPT-en|91<br>**17**<br>1<br>3<br>33.33<br>77<br>12<br>5<br>4<br>25.00<br>89<br>9<br>**8**<br>**7**<br>**57.14**|\n\n\nTable 7: The result of counterfactual robustness. ACC is the\naccuracy (%) of LLMs", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_9000", "chunk_text": "<br>89<br>9<br>**8**<br>**7**<br>**57.14**|\n\n\nTable 7: The result of counterfactual robustness. ACC is the\naccuracy (%) of LLMs without external documents. ACCdoc\nis the accuracy (%) of LLMs with counterfactual documents.\nED and ED _[\u2217]_ are error detection rates evaluated by exact\nmatching and ChatGPT, respectively. CR is the error correction rate.\n\n\nthought approach to break down complex problems (Zhou\net al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). However, these methods slow down the inference speed and cannot provide timely responses.\n\n\n**Results on Counterfactual Robustness testbed**\n\nIn order to ensure that LLMs possess relevant knowledge,\nwe assess their performance by directly asking them questions. However, we found that most LLMs struggle to answer them correctly. To ensure a more reasonable evaluation, we only consider LLMs that have an accuracy rate of\n\n- ver 70% as this threshold is relatively high and encompasses more LLMs. The results are shown in Table 7. We\npresent the following metrics: accuracy without any documents, accuracy with counterfactual documents, error detection rates, and error correction rates. We can see that It\nis hard for LLMs to identify and correct factual errors in the\ndocuments. This suggests that the model can be easily misled by documents containing incorrect facts.\nIt is important to note that retrieval-augmented generation\nis not designed to automatically address factual errors within\na given context, as this contradicts the underlying assumption that the model lacks knowledge and relies on retrieved\ndocuments for additional information. However, this issue is\ncrucial in practical applications due to the abundance of fake\nnews on the internet. Existing LLMs do not have a safeguard\nto handle inaccurate responses caused by misinformation. In\nfact, they heavily depend on the information they retrieve.\nEven when LLMs contain the internal knowledge about the\nquestions, they often trust false information that is retrieved.\nThis presents significant a challenge for the future development of RAG in LLMs.\n\n\n**Conclusion**\n\nIn this paper, we evaluated four abilities of retrievalaugmented generation in LLMs: noise robustness, negative rejection, information integration, and counterfactual\nrobustness", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_9450", "chunk_text": " the future development of RAG in LLMs.\n\n\n**Conclusion**\n\nIn this paper, we evaluated four abilities of retrievalaugmented generation in LLMs: noise robustness, negative rejection, information integration, and counterfactual\nrobustness. To conduct the evaluation, we built RetrievalAugmented Generation Benchmark (RGB). The instances of\nRGB are generated from latest news articles and the external\ndocuments obtained from search engines. The experimental\nresults suggest that current LLMs have limitations in the 4\nabilities. This indicates that there is still a significant amount\n\n- f work needed to effectively apply RAG to LLMs. To ensure accurate and reliable responses from LLMs, it is crucial\nto exercise caution and carefully design for RAG.\n\n\n\n\n\n\n\n\n\n\n|Question|Answer|Response|Errors|\n|---|---|---|---|\n|What groupings are France and<br>Germany in World Cup 2022?|Group D<br>Group E|France and Germany are in<br>Group D of the 2022 FIFA<br>World Cup.|**Merging**<br>**Error**|\n|Who were the MVP of Super<br>Bowl 2022 and 2023?|Cooper Kupp<br>Patrick<br>Mahomes|According to the provided<br>document, the MVP of Super<br>Bowl LVI was Cooper Kupp<br>of the Los Angeles Rams.|**Ignoring**<br>**Error**|\n|What flms won the 2022 and<br>2023 Academy Awards for<br>Best Picture?|CODA<br>Everything<br>Everywhere<br>All at Once|The flm CODA won the<br>award for Best Picture at the<br>95th Academy Awards<br>ceremony held on 2023.|**Misalignment**<br>**Error**|\n\n\n\nTable 6: Error cases of information integration, the responses are generated by ChatGLM2-6B. The blue and red\ntexts represent the answers to two sub-questions.\n\n\n**(1) Merging Error (28% of the total).** The model sometimes merges the answers of the two sub-questions, resulting\nin an error. It mistakenly uses the answer from one question\nto address both two questions. At this point, the model will\ndisregard any documents related to one sub-question. For\nexample, in Table 6, it incorrectly states that Group", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_9900", "chunk_text": " error. It mistakenly uses the answer from one question\nto address both two questions. At this point, the model will\ndisregard any documents related to one sub-question. For\nexample, in Table 6, it incorrectly states that Group D is the\nWorld Cup group for both France and Germany, while in fact\nGermany is actually assigned to Group E.\n**(2) Ignoring Error (28% of the total).** Sometimes, the\nmodel may ignore one of the sub-questions and only answer\nthe other. This error occurs when the model lacks a complete\nunderstanding of the problem and fails to recognize that it\nconsists of multiple sub-problems. As a result, the model\n\n- nly considers relevant documents for one sub-problem in\n\n- rder to generate an answer, disregarding the question posed\nby another sub-problem. For example, in Table 6, the model\n\n- nly provides the answer for the MVP of Super Bowl 2022\nand does not consider 2023.\n**(3) Misalignment Error (6% of the total).** Sometimes,\nthe model incorrectly identifies the documents for one subquestion as the documents for another sub-question, leading\nto misaligned answers. For example, in Table 6, the third answer has two errors: an ignoring error and a misalignment error. Firstly, the model only mentioned the Best Picture of the\n2023 (95th) Academy Awards, completely disregarding the\n2022 awards. Additionally, it incorrectly stated that \u201cCODA\u201d\nis the Best Picture of 2023 when it was actually awarded as\nthe Best Picture in 2022.\nThe errors mentioned above are primarily caused by the\nlimited understanding of complex questions, which hinders\nthe ability to effectively utilize information from different\nsub-problems. The key lies in improving the model\u2019s reasoning capability. One possible solution is to use a chain-of\n\n**Acknowledgements**\n\n\nThis research work is supported by the National Natural\nScience Foundation of China under Grants no. 62122077,\n62106251, 62306303, the CAS Project for Young Scientists\nin Basic Research under Grant No.YSBR-040. Xianpei Han\nis sponsored by CCF- BaiChuan-Ebtech Foundation Model\nFund.\n\n\n**References**\n\nAdlakha, V.; BehnamGhader, P.; Lu, X. H.; Meade", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_10350", "chunk_text": "040. Xianpei Han\nis sponsored by CCF- BaiChuan-Ebtech Foundation Model\nFund.\n\n\n**References**\n\nAdlakha, V.; BehnamGhader, P.; Lu, X. H.; Meade, N.; and\nReddy, S. 2023. Evaluating Correctness and Faithfulness\n\n- f Instruction-Following Models for Question Answering.\narXiv:2307.16877.\n\nBang, Y.; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V.; Xu,\nY.; and Fung, P. 2023. A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination,\nand Interactivity. arXiv:2302.04023.\n\nBian, N.; Liu, P.; Han, X.; Lin, H.; Lu, Y.; He, B.; and\nSun, L. 2023. A Drop of Ink Makes a Million Think: The\nSpread of False Information in Large Language Models.\narXiv:2305.04812.\n\nBorgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Rutherford, E.; Millican, K.; van den Driessche, G.; Lespiau, J.-B.;\nDamoc, B.; Clark, A.; de Las Casas, D.; Guy, A.; Menick, J.;\nRing, R.; Hennigan, T.; Huang, S.; Maggiore, L.; Jones, C.;\nCassirer, A.; Brock, A.; Paganini, M.; Irving, G.; Vinyals,\nO.; Osindero, S.; Simonyan, K.; Rae, J. W.; Elsen, E.; and\nSifre, L. 2022. Improving language models by retrieving\nfrom trillions of tokens. arXiv:2112.04426.\n\nCai, D.; Wang, Y.; Bi, W.; Tu, Z.; Liu, X.; Lam, W.; and\nShi, S. 2019a. Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory. In _", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_10800", "chunk_text": " D.; Wang, Y.; Bi, W.; Tu, Z.; Liu, X.; Lam, W.; and\nShi, S. 2019a. Skeleton-to-Response: Dialogue Generation Guided by Retrieval Memory. In _Proceedings of the_\n_2019 Conference of the North American Chapter of the As-_\n_sociation for Computational Linguistics: Human Language_\n_Technologies, Volume 1 (Long and Short Papers)_, 1219\u2013\n1228. Minneapolis, Minnesota: Association for Computational Linguistics.\n\nCai, D.; Wang, Y.; Bi, W.; Tu, Z.; Liu, X.; and Shi, S.\n2019b. Retrieval-guided Dialogue Response Generation via\na Matching-to-Generation Framework. In _Proceedings of_\n_the 2019 Conference on Empirical Methods in Natural Lan-_\n_guage Processing and the 9th International Joint Confer-_\n_ence on Natural Language Processing (EMNLP-IJCNLP)_,\n1866\u20131875. Hong Kong, China: Association for Computational Linguistics.\n\nCao, M.; Dong, Y.; Wu, J.; and Cheung, J. C. K. 2020. Factual Error Correction for Abstractive Summarization Models. In _Proceedings of the 2020 Conference on Empirical_\n_Methods in Natural Language Processing (EMNLP)_, 6251\u2013\n6258. Online: Association for Computational Linguistics.\n\nChang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu,\nK.; Chen, H.; Yi, X.; Wang, C.; Wang, Y.; Ye, W.;\nZhang, Y.; Chang, Y.; Yu, P. S.; Yang, Q.; and Xie, X.\n2023. A Survey on Evaluation of Large Language Models.\narXiv:2307.03109.\n\n\n\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nCui, J.; Li,", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_11250", "chunk_text": " Stoica,\nI.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot\nImpressing GPT-4 with 90%* ChatGPT Quality.\nCui, J.; Li, Z.; Yan, Y.; Chen, B.; and Yuan, L. 2023. ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases. arXiv:2306.16092.\nDrozdov, A.; Sch\u00a8arli, N.; Aky\u00a8urek, E.; Scales, N.; Song,\nX.; Chen, X.; Bousquet, O.; and Zhou, D. 2023. Compositional Semantic Parsing with Large Language Models. In\n_The Eleventh International Conference on Learning Repre-_\n_sentations_ .\n\nEdward Beeching, N. H. S. H. N. L. N. R. O. S. L. T.\nT. W., Cl\u00b4ementine Fourrier. 2023. Open LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/\n\n- pen ~~l~~ lm ~~l~~ eaderboard.\nGuo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y.;\nYue, J.; and Wu, Y. 2023. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection. arXiv:2301.07597.\n\nGuu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W.\n2020. REALM: Retrieval-Augmented Language Model PreTraining. In _Proceedings of the 37th International Confer-_\n_ence on Machine Learning_, ICML\u201920. JMLR.org.\nHe, H.; Zhang, H.; and Roth, D. 2022. Rethinking\nwith Retrieval: Faithful Large Language Model Inference.\narXiv:2301.00303.\n\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding. In _International Conference_\n\n_on Learning Representations_ .\nHuang, Y.; Bai, Y", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_11700", "chunk_text": ", A.; Mazeika, M.;\nSong, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding. In _International Conference_\n\n_on Learning Representations_ .\nHuang, Y.; Bai, Y.; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.;\nLiu, J.; Lv, C.; Zhang, Y.; Lei, J.; Fu, Y.; Sun, M.; and He,\nJ. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese\nEvaluation Suite for Foundation Models. _arXiv preprint_\n_arXiv:2305.08322_ .\n\nIzacard, G.; and Grave, E. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In _Proceedings of the 16th Conference of_\n_the European Chapter of the Association for Computational_\n_Linguistics: Main Volume_, 874\u2013880. Online: Association for\nComputational Linguistics.\nIzacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni,\nF.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and\nGrave, E. 2022. Atlas: Few-shot Learning with Retrieval\nAugmented Language Models. arXiv:2208.03299.\nJi, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y.; Ishii, E.;\nBang, Y. J.; Madotto, A.; and Fung, P. 2023. Survey of Hallucination in Natural Language Generation. _ACM Comput._\n_Surv._, 55(12).\nLewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;\nGoyal, N.; K\u00a8uttler, H.; Lewis, M.; Yih, W.-t.; Rockt\u00a8aschel,\nT.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. In _Pro-_\n_ceedings of the 34th International Conference on Neural_\n_Information Processing Systems_, NIPS\u201920. Red Hook, NY,\nUSA", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_12150", "chunk_text": ". Retrieval-Augmented\nGeneration for Knowledge-Intensive NLP Tasks. In _Pro-_\n_ceedings of the 34th International Conference on Neural_\n_Information Processing Systems_, NIPS\u201920. Red Hook, NY,\nUSA: Curran Associates Inc. ISBN 9781713829546.\n\n\nLi, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;\nVeit, A.; Yu, F.; and Kumar, S. 2023a. Large Language\nModels with Controllable Working Memory. In _Findings of_\n_the Association for Computational Linguistics: ACL 2023_,\n1774\u20131793. Toronto, Canada: Association for Computational Linguistics.\n\nLi, X.; Zhang, T.; Dubois, Y.; Taori, R.; Gulrajani, I.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b. AlpacaEval: An Automatic Evaluator of Instruction-following\nModels. https://github.com/tatsu-lab/alpaca ~~e~~ val.\n\nLi, X.; Zhu, X.; Ma, Z.; Liu, X.; and Shah, S. 2023c. Are\nChatGPT and GPT-4 General-Purpose Solvers for Financial\nText Analytics? An Examination on Several Typical Tasks.\narXiv:2305.05862.\n\nLiu, N. F.; Zhang, T.; and Liang, P. 2023. Evaluating Verifiability in Generative Search Engines. arXiv:2304.09848.\n\nMaynez, J.; Narayan, S.; Bohnet, B.; and McDonald, R.\n2020. On Faithfulness and Factuality in Abstractive Summarization. In _Proceedings of the 58th Annual Meeting of_\n_the Association for Computational Linguistics_, 1906\u20131919.\nOnline: Association for Computational Linguistics.\n\nOpenAI. 2022. Chatgpt: Optimizing language models for\ndialogue. https://openai.com/blog/chatgpt.\n\nPeng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y.; Hu, Y.;\nHuang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023.\nCheck", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_13050", "chunk_text": "unaga, M.; Seo, M.; James, R.;\nLewis, M.; Zettlemoyer, L.; and tau Yih, W. 2023. REPLUG: Retrieval-Augmented Black-Box Language Models.\narXiv:2301.12652.\n\nTHUDM. 2023a. ChatGLM-6B. https://github.com/\nTHUDM/ChatGLM-6B.\n\nTHUDM. 2023b. ChatGLM2-6B. https://github.com/\nTHUDM/ChatGLM2-6B.\n\n\n\nTrivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,\nA. 2023. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In\n_Proceedings of the 61st Annual Meeting of the Associa-_\n_tion for Computational Linguistics (Volume 1: Long Papers)_,\n10014\u201310037. Toronto, Canada: Association for Computational Linguistics.\nWang, A.; Pruksachatkun, Y.; Nangia, N.; Singh, A.;\nMichael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a. _Su-_\n_perGLUE: A Stickier Benchmark for General-Purpose Lan-_\n_guage Understanding Systems_ . Red Hook, NY, USA: Curran\nAssociates Inc.\n\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019b. GLUE: A Multi-Task Benchmark\nand Analysis Platform for Natural Language Understanding.\nIn _International Conference on Learning Representations_ .\n\nXu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;\nGao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.;\nand Zhou, J. 2023a. CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility.\narXiv:2307.09705.\n\nXu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.S. 2023b. Search-in-the-Chain", "token_count": 500, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2309.01431_rgb_benchmark_chen:chunk_13500", "chunk_text": " Models from Safety to Responsibility.\narXiv:2307.09705.\n\nXu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.S. 2023b. Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledgeintensive Tasks. arXiv:2304.14732.\n\nYunjie Ji, Y. G. Y. P. Q. N. B. M. X. L., Yong Deng. 2023.\nBELLE: Bloom-Enhanced Large Language model Engine.\nhttps://github.com/LianjiaTech/BELLE.\nZhang, W.; Aljunied, S. M.; Gao, C.; Chia, Y. K.; and Bing,\nL. 2023. M3Exam: A Multilingual, Multimodal, Multilevel\nBenchmark for Examining Large Language Models.\nZhong, W.; Cui, R.; Guo, Y.; Liang, Y.; Lu, S.; Wang,\nY.; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval:\nA Human-Centric Benchmark for Evaluating Foundation\nModels. arXiv:2304.06364.\n\nZhou, D.; Sch\u00a8arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,\nX.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V.; and\nChi, E. H. 2023a. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. In _The Eleventh_\n_International Conference on Learning Representations_ .\n\nZhou, S.; Alon, U.; Xu, F. F.; Jiang, Z.; and Neubig, G.\n2023b. DocPrompting: Generating Code by Retrieving the\nDocs. In _The Eleventh International Conference on Learn-_\n_ing Representations_ .\n\n\n", "token_count": 424, "metadata": {"arxiv_id": "2309.01431", "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "authors": ["Jiawei Chen", "Hongyu Lin", "Xianpei Han", "Le Sun"], "year": 2023, "url": "https://arxiv.org/pdf/2309.01431v2"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_0", "chunk_text": "## **Query Rewriting for Retrieval-Augmented Large Language Models**\n\n**Xinbei Ma** [1,2,] _[\u2217]_ **, Yeyun Gong** [3, #, \u2020] **, Pengcheng He** [4, #] **, Hai Zhao** [1,2,\u2020] **, Nan Duan** [3]\n\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\n3Microsoft Research Asia 4Microsoft Azure AI\n\nsjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,\n{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com\n\n\n\n**Abstract**\n\n\nLarge Language Models (LLMs) play powerful, black-box readers in the _retrieve-then-_\n_read_ pipeline, making remarkable progress\nin knowledge-intensive tasks. This work introduces a new framework, _Rewrite-Retrieve-_\n_Read_ instead of the previous _retrieve-then-read_\nfor the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior\nstudies focusing on adapting either the retriever\n\n  - r the reader, our approach pays attention to\nthe adaptation of the search query itself, for\nthere is inevitably a gap between the input text\nand the needed knowledge in retrieval. We\nfirst prompt an LLM to generate the query,\nthen use a web search engine to retrieve contexts. Furthermore, to better align the query\nto the frozen modules, we propose a trainable\nscheme for our pipeline. A small language\nmodel is adopted as a trainable rewriter to cater\nto the black-box LLM reader. The rewriter is\n\ntrained using the feedback of the LLM reader\nby reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA\nand multiple-choice QA. Experiments results\nshow consistent performance improvement, indicating that our framework is proven effective\nand scalable, and brings a new framework for\nretrieval-augmented LLM [1] .\n\n\n**1** **Introduction**\n\n\nLarge Language Models (LLMs) have shown remarkable abilities for human language processing\nand extraordinary scalability and adaptability in\nfew- or zero-shot settings.(Ouyang et al., 2022;\nBrown et al., 2020; Chowdh", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_450", "chunk_text": " Language Models (LLMs) have shown remarkable abilities for human language processing\nand extraordinary scalability and adaptability in\nfew- or zero-shot settings.(Ouyang et al., 2022;\nBrown et al., 2020; Chowdhery et al., 2022). However, the training process depends on large-scale\nhigh-quality corpora but without the perception\n\n\n_\u2217_ Work done during an internship at [3] Microsoft Research\nAsia. # Equal contribution. \u2020Corresponding author.\nThis paper was partially supported by Joint Research\nProject of Yangtze River Delta Science and Technology Innovation Community (No. 2022CSJGG1400).\n1https://github.com/xbmxb/RAG-query-rewriting\n\n\n\n\n- f the real world. Thus, LLMs still have to face\nthe issue of hallucination (Yao et al., 2023; Bang\net al., 2023) and temporal misalignment (R\u00f6ttger\nand Pierrehumbert, 2021; Luu et al., 2022; Jang\net al., 2022). This affects the reliability of LLMs\nand hinders wider practical application, because\nthe consistency between the LLM responses with\nthe real world needs further validation. Exist\ning work has proved that incorporating external\nknowledge (i.e., non-parametric knowledge) with\ninternal knowledge (i.e., parametric knowledge)\ncan effectively alleviate hallucination, especially\nfor knowledge-intensive tasks. In fact, retrievalaugmented LLMs have been shown so effective\nthat they have been regarded as a standard solution to alleviate the factuality drawbacks in naive\nLLM generations. Retrieval augmentation is applied to select relative passages as external contexts\nfor the language model, which is _retrieve-then-read_\nframework (Lewis et al., 2020b; Karpukhin et al.,\n2020; Izacard et al., 2022). Take the open-domain\nQuestion-Answering task (open-domain QA) as\nan example, a retriever first searches for related\ndocuments for a question. Then the LLM receives\nthe question and the documents, then predicts an\n\nanswer.\n\n\nAs most LLMs are only accessible through inference APIs, they play the part of black-box frozen\nreaders in the pipeline. This makes previous retrieval augmentation methods that require complete\naccess (Lewis et al.,", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_900", "chunk_text": ", then predicts an\n\nanswer.\n\n\nAs most LLMs are only accessible through inference APIs, they play the part of black-box frozen\nreaders in the pipeline. This makes previous retrieval augmentation methods that require complete\naccess (Lewis et al., 2020b; Guu et al., 2020; Izacard et al., 2022) no longer feasible. Recent studies\n\n- n retrieval-augmented language models lean more\n\n- n the LLM-oriented adaptation. An idea is to train\na dense retrieval model to cater to the frozen lan\nguage model (Shi et al., 2023). By using feedback\nfrom the LLM as a training objective, the retrieval\nmodel is tuned for better LLM input contexts. An\n- ther research line focuses on the design of interactions between the retriever and the reader (Yao\net al., 2023; Khattab et al., 2022), where both the\n\n\nretriever and the reader are usually frozen. The idea\nis to trigger the emergent ability through carefully\ncrafted prompts or a sophisticated prompt pipeline.\nMultiple interactions with external knowledge allow the LLM to approach the correct answer step\nby step.\n\nHowever, there are still problems remaining to\nbe solved. Existing approaches overlook the adaptation of the query, i.e., the input of the _retrieve-_\n_then-read_ pipeline. The retrieval query is either\n\n- riginal from datasets or directly determined by the\nblack-box generation, thus is always fixed. However, there is inevitably a gap between the input\ntext and the knowledge that is really needed to\nquery. This limits performance and places a burden\n\n- n retrieval capability enhancement and prompt\nengineering.\n\nIn consideration of this issue, this paper proposes _Rewrite-Retrieve-Read_, a new framework for\nretrieval augmentation, which can be further tuned\nfor adapting to LLMs. In front of the retriever, a\nstep of _rewriting the input_ is added, filling the gap\nbetween the given input and retrieval need, as is\nshown in Figure 1. We adopt the off-the-shelf tool,\nan internet search engine, as the retriever, which\navoids the maintenance of the search index and\n\ncan access up-to-date knowledge (Lazaridou et al.,\n2022). Different from previous studies (Khattab\net al., 2022;", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_1350", "chunk_text": " as the retriever, which\navoids the maintenance of the search index and\n\ncan access up-to-date knowledge (Lazaridou et al.,\n2022). Different from previous studies (Khattab\net al., 2022; Yao et al., 2023) that require the mem\n- ry of multiple interaction rounds between the retriever and the LLM for each sample, the motivation of our rewriting step is to clarify the retrieval\nneed from the input text.\n\nWe also propose a trainable scheme for our\n_rewrite-retrieve-read_ framework (Figure 1 (c)).\nThe black-box retriever and the reader form a\n\nfrozen system. To further smooth the steps of\n\n- ur pipeline, we apply a small, trainable language\nmodel to perform the rewriting step, denoted as the\n_rewriter_ . The rewriter is trained by reinforcement\nlearning using the LLM performance as a reward,\nlearning to adapt the retrieval query to improve the\nreader on downstream tasks.\n\nOur proposed methods are evaluated  - n\nknowledge-intensive downstream tasks including\n\n- pen-domain QA (HotpoQA (Yang et al., 2018),\nAmbigNQ (Min et al., 2020), PopQA (Mallen\net al., 2022)) and multiple choice QA (MMLU\n(Hendrycks et al., 2021)). The experiments are\nimplemented on T5-large (Raffel et al., 2020) as\nthe rewriter, ChatGPT (Ouyang et al., 2022) and\n\n\n\nVicuna-13B (Chiang et al., 2023) as the LLM\nreader. The results show that query rewriting consistently improves the retrieve-augmented LLM\nperformance. The results also indicate that the\nsmaller language model can be competent for query\nrewriting.\nTo sum up, our proposed novel retrievalaugmentation method, _rewrite-retrieve-read_ is the\nfirst framework where the input text is adapted for\nthe frozen retriever and LLM reader. We introduce\n\na tuneable scheme with a small, trainable model,\nachieving performance gains with less resource\nconsumption.\n\n\n**2** **Related Work**\n\n\n**2.1** **Retrieval Augmentation**\n\n\nLanguage models require external knowledge to alleviate the factuality drawbacks. Retrieval augmentation has been regarded as the standard effective\nsolution. With a retrieval module", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_1800", "chunk_text": "**2** **Related Work**\n\n\n**2.1** **Retrieval Augmentation**\n\n\nLanguage models require external knowledge to alleviate the factuality drawbacks. Retrieval augmentation has been regarded as the standard effective\nsolution. With a retrieval module, related passages\nare provided to the language model as the context\n\n- f the original input. Thus factual information like\ncommon sense or real-time news helps with output\nprediction through contextualized reading comprehension.\n\nEarlier studies use sparse retriever (Chen et al.,\n2017) or dense retriever (Karpukhin et al., 2020)\nin front of a pre-trained language model (PrLM).\nThe neural retriever and reader are both PrLMs\n\n- f trainable size like BERT (Devlin et al., 2019)\n\n- r BART (Lewis et al., 2020a). Hence, the whole\n\n_retrieve-then-reader_ framework is a tuneable end\nto-end system, where the retrieved contexts can\nbe regarded as the intermediate results (Karpukhin\net al., 2020; Lewis et al., 2020b). Approaches to\nsmooth the two-step framework are proposed to optimize the retrieval and the reading comprehension\n(Sachan et al., 2021; Lee et al., 2022; Jiang et al.,\n2022). More recently, retrieval remains a powerful\nenhancement as the size of models and data scales\n\nrapidly (Mallen et al., 2022; Shi et al., 2023; Brown\net al., 2020). On the other hand, retrieval enhancement can compensate for the shortfall in parameter\nsize, compared to large-scale language models. For\nexample, by jointly training the retriever and the\nreader, Atlas (Izacard et al., 2022) shows few-shot\nperformance on par with 540B PalM (Chowdhery\net al., 2022) but be of 50 _\u00d7_ smaller size.\n**The Internet as a knowledge base** More related\nto our work, the search engine can assume the role\n\n- f the retriever and use the Internet as the source of\n\n\n|Col1|Input<br>Retriever|Col3|Col4|Col5|\n|---|---|---|---|---|\n||Docum|Docum|ent|s|\n||||||\n||||||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_2250", "chunk_text": " of\n\n\n|Col1|Input<br>Retriever|Col3|Col4|Col5|\n|---|---|---|---|---|\n||Docum|Docum|ent|s|\n||||||\n||||||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Retrieve-then-read  (b)Rewrite-retrieve-read         (c) Trainable rewrite-retrieve-read\n\n\nFigure 1: Overview of our proposed pipeline. From left to right, we show (a) standard _retrieve-then-read_ method,\n(b) LLM as a query rewriter for our _rewrite-retrieve-read_ pipeline, and (c) our pipeline with a trainable rewriter.\n\n\n\nexternal knowledge. Komeili et al. (2022) use an\ninternet search for relevant information based on\n\nthe dialogue history to perform dialogue response\ngeneration. SeeKeR (Shuster et al., 2022) use a\nsingle Transformer to iteratively perform search\nquery generation, then knowledge extraction for\ndialogue generation and sentence completion. For\nlarge-scale models, web search still shows effective for knowledge augmentation (Lazaridou et al.,\n2022), fact-checking (Menick et al., 2022), and\nLLM agent enhancement (Yao et al., 2023).\n\n\n**2.2** **Cooperation with Black-box LLMs**\n\n\nLarge Language Models, such as ChatGPT\n(Ouyang et al., 2022), Codex (Chen et al., 2021),\nPaLM (Chowdhery et al., 2022), emerge impressive natural language processing ability as well as\nremarkable scalability. This leads to a tendency\nto embrace LLMs on a wide range of NLP tasks.\nHowever, LLMs are only accessible as a black box\nin most cases, which is because (i) Some like ChatGPT are not open-source and kept private; (ii) The\nlarge parameter scale requires computational resources that are not always affordable to users. This\nconstraint means nothing is available except input\nand output texts.\nExisting studies have proved that LLMs\u2019 abilities can be better leveraged by carefully designed\ninteraction methods. GenRead (Yu et al., 2023)\nprompts an LLM to generate context instead of\ndeploying a retriever, showing that LLMs can retrieve internal knowledge by prompting. ReAct\n\n\n\n(Yao et al., 202", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_2700", "chunk_text": " GenRead (Yu et al., 2023)\nprompts an LLM to generate context instead of\ndeploying a retriever, showing that LLMs can retrieve internal knowledge by prompting. ReAct\n\n\n\n(Yao et al., 2023) and Self-Ask (Press et al., 2022)\ncombines the Chain-of-Thought (CoT) (Wei et al.,\n2022; Wang et al., 2022) and inter-actions with web\nAPIs. Only relying on prompt construction, ReAct provides novel baselines for interactive tasks.\nDemonstrate\u2013Search\u2013Predict (DSP) (Khattab et al.,\n2022) defines a sophisticated pipeline between an\nLLM and a retriever. Unlike ReAct, DSP integrates\nprompts for demonstration bootstrap besides multihop breakdown and retrieval.\nDespite the promising performance in the zero or\nfew-shot setting, the behavior of LLMs sometimes\nneeds adjustments. A feasible approach is to append trainable small models in front of or after the\nLLM. The small models, as a part of the parameters\n\n - f the system, can be fine-tuned for optimization.\nRePlug (Shi et al., 2023) is proposed to fine-tune a\ndense retriever for the frozen LLM in the _retrieve-_\n\n_then-read_ pipeline. The retriever is trained under\nthe LLM\u2019s supervision to retrieve documents that\nare suitable for the LLM. With the same purpose,\nDirectional Stimulus Prompting (Li et al., 2023)\ndeploys a small model to provide the LLM with\nstimulus (e.g., keywords for summarization, or dialogue actions for response generation), which is\nupdated according to the LLM reward.\nDifferent from the inspiring work mentioned\nabove, our proposed pipeline contains a query\nrewriting step in front of the _retrieve-then-read_\nmodule. We further propose a trainable scheme\nwith a small rewriting model, which is a novel\nenhancement for retrieval-augmented LLM by re\n\nconstructing the search query.\n\n\n**3** **Methodology**\n\n\nWe present _Rewrite-Retrieve-Read_, a pipeline that\nimproves the retrieval-augmented LLM from the\nperspective of query rewriting. Figure 1 shows an\n\n- verview. This section first introduces the pipeline\nframework in section 3.1, then the trainable scheme\n\n", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_3150", "chunk_text": "\nimproves the retrieval-augmented LLM from the\nperspective of query rewriting. Figure 1 shows an\n\n- verview. This section first introduces the pipeline\nframework in section 3.1, then the trainable scheme\n\nin section 3.2.\n\n\n**3.1** _**Rewrite-Retrieve-Read**_\n\n\nA task with retrieval augmentation can be denoted as follows. Given a dataset of a knowledgeintensive task (e.g., open-domain QA), _D_ =\n_{_ ( _x, y_ ) _i}, i_ = 0 _,_ 1 _,_ 2 _, . . ., N_, _x_ (e.g., a question)\nis the input to the pipeline, _y_ is the expected output\n(e.g., the correct answer). Our pipeline consists of\nthree steps. (i) Query rewrite: generate a query \u02dc _x_\nfor required knowledge based on the original input\n_x_ . (ii) Retrieve: search for related context, _doc_ . (iii)\nRead: comprehend the input along with contexts\n\n[ _doc, x_ ] and predict the output \u02c6 _y_ .\nA straightforward but effective method is to ask\nan LLM to rewrite queries to search for information that is potentially needed. We use a few-shot\nprompt to encourage the LLM to think, and the\n\n- utput can be none, one or more queries to search.\n\n\n**3.2** **Trainable Scheme**\n\n\nBesides, total reliance on a frozen LLM has shown\nsome drawbacks. Reasoning errors or invalid\nsearch hinders the performance (Yao et al., 2023;\nBehnamGhader et al., 2022). On the other hand,\nretrieved knowledge may sometimes mislead and\ncompromise the language model (Mallen et al.,\n2022). To better align to the frozen modules, it is\nfeasible to add a trainable model and adapt it by\ntaking the LLM reader feedback as a reward.\nBased on our framework, we further propose to\nutilize a trainable small language model to take\n\n- ver the rewriting step, as is shown in the right\npart of Figure 1. The trainable model is initialized with the pre-trained T5-large (770M) (Raffel\net al., 2020), denoted as _trainable", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_3600", "chunk_text": " rewriting step, as is shown in the right\npart of Figure 1. The trainable model is initialized with the pre-trained T5-large (770M) (Raffel\net al., 2020), denoted as _trainable rewriter_, _G\u03b8_ . The\nrewriter is first trained on pseudo data to warm up\n(\u00a73.2.1), then continually trained by reinforcement\nlearning (\u00a73.2.2).\n\n\n**3.2.1** **Rewriter Warm-up**\n\nThe task, query rewriting, is quite different from\nthe pre-training objective of sequence-to-sequence\ngenerative models like T5. First, we construct a\n\n\n\npseudo dataset for the query rewriting task. Inspired by recent distillation methods (Hsieh et al.,\n2023; Ho et al., 2022), we prompt the LLM to\nrewrite the original questions _x_ in the training set\nand collect the generated queries \u02dc _x_ as pseudo labels. The collected samples are then filtered: Those\nthat get correct predictions from the LLM reader\nare selected into the warm-up dataset, denoted as\n_DTrain_ = _{_ ( _x,_ \u02dc _x_ ) _|y_ \u02c6 = _y}_ . The rewriter _G\u03b8_ is finetuned on _DTrain_ with the standard log-likelihood\nas the training objective, denoted as\n\n\n\u02dc\n\n_Lwarm_ = _\u2212_   - _logp\u03b8_ ( _x_ [\u02c6] _t |_ \u02dc _x<t_, _x_ ) _._ (1)\n\n_t_\n\n\nThe rewriter model after warm-up shows modest performance, which depends on the pseudo\ndata quality and rewriter capability. Highly relying\n\n- n the human-written prompt line, \u02dc _x_ can be sub\n- ptimal. The relatively small scale of the rewriter\nsize is also a limitation of the performance after the\nwarm-up. Then we turn to reinforcement learning\nto align the rewriter to the following retriever and\nLLM reader.\n\n\n**3.2.2** **Reinforcement Learning**\n\n\nTo further fine-tune the rewriter to cater to the LLM\nreader, we adopt a policy gradient reinforcement\nlearning framework.\n\n**Task Formulation** In the context of reinforce\nment learning, the rewriter optimization", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_4050", "chunk_text": "inforcement Learning**\n\n\nTo further fine-tune the rewriter to cater to the LLM\nreader, we adopt a policy gradient reinforcement\nlearning framework.\n\n**Task Formulation** In the context of reinforce\nment learning, the rewriter optimization is formulated as a Markov Decision Process 5-tuple\n_\u27e8S, A, P, R, \u03b3\u27e9_ . (i) The state space _S_ is a finite set\nlimited by the vocabulary and the sequence length.\n(ii) The action space _A_ is equals to the vocabulary.\n(iii) The transition probability _P_ is determined by\nthe policy network, which is the rewriter model\n_G\u03b8_ . (iv) The reward function _R_ gives a reward\nvalue that depends on the current state. The policy gradient is derived from rewards, used as the\ntraining objective. (v) _\u03b3_ denotes the discount factor. More specifically, the rewriter _G\u03b8_ after the\nwarm-up is the initial policy model _\u03c0_ 0. At each\nstep _t_, the action _at_ is to generate the next token\n\u02c6\u02dc\n_xt_ based on the observation of the present state,\n_st_ = [ _x,_ _x_ \u02dc [\u02c6] _<t_ ]. When the generation is stopped by\nthe End-Of-Sentence token, one episode is ended.\nAfter finishing the retrieval and reading, a reward\nis computed by evaluating the final output, i.e., a\nscore for the LLM reader prediction.\n**Policy Optimization** We adopt Proximal Policy\nOptimization (PPO) (Schulman et al., 2017), following (Ramamurthy et al., 2022). Maximization\n\n\n- f the expectation of the reward _R_ is formulated as\n\n\n\u02dc\nmax _\u03b8_ E _x_ \u02c6\u02dc _\u223cp\u03b8_ ( _\u00b7|x_ )[ _R_ ( _x,_ _x_ [\u02c6] )] _,_\n\n\n\nmax _\u03b8_ E( _st,at_ ) _\u223c\u03c0\u03b8\u2032_ [ _min{kt,\u03b8A_ _[\u03b8][\u2032]_ ( _st, at_ ) ;\n\nclip ( _kt,\u03b8,_ 1 _\u2212_ _\u03b5,_ 1 + _\u03b5_ ) _A_ _[", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_4500", "chunk_text": " _min{kt,\u03b8A_ _[\u03b8][\u2032]_ ( _st, at_ ) ;\n\nclip ( _kt,\u03b8,_ 1 _\u2212_ _\u03b5,_ 1 + _\u03b5_ ) _A_ _[\u03b8][\u2032]_ ( _st, at_ ) _}_ ] _,_\n\n\n\n(2)\n\n\n\nin-context learning (Brown et al., 2020; Min et al.,\n2022). Our prompt follows the formulation of _[in-_\n_struction, demonstrations, input]_, where the input\nis _x_ . The instruction is straightforward and demonstrations are 1-3 random examples from training\nsets and are kept constant across all runs, mainly\nfor the task-specific output format illustration, i.e.,\na short phrase as an answer for HotpotQA, and an\n\n- ption as an answer for MMLU. For the training\nscheme in \u00a73.2, we fine-tuning a T5 as the rewriter.\n**Retriever** We use the Bing search engine as the\nretriever. It requires no candidate index construction like a dense retriever, nor candidates like a\ntextbook. But it allows for a wide knowledge scope\nand up-to-time factuality. With Bing API, the retrieval is performed in two approaches. (i) For all\nretrieved web pages, we concatenate the snippets\nthat are related sentences selected by Bing. This\nmethod is similar to using a search engine in a\nbrowser, input a query and press Enter, then collect the texts shown on the search result page. (ii)\nFor retrieved web pages, we request the URLs and\nparser to get all the texts. This is similar to clicking\n\n- n items on the search result page. Then we use\nBM25 to keep those with higher relevance scores\nwith the query, reducing the document length.\n**Reader** The reader is a frozen LLM, where we\nadopt ChatGPT (gpt-3.5-turbo) and Vicuna-13B.\nIt performs reading comprehension and prediction\nwith few-shot in-context learning. In our prompt,\nfollowing the brief instruction and the demonstrations, the input is _x_ - r [ _doc,_ _x_ \u02dc [\u02c6] ] with retrieval augmentation.\n\nIt has been proved that both the phrasing of\nprompt lines (Zhang et al., 2023a)", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_4950", "chunk_text": " input is _x_ - r [ _doc,_ _x_ \u02dc [\u02c6] ] with retrieval augmentation.\n\nIt has been proved that both the phrasing of\nprompt lines (Zhang et al., 2023a) and the selection\n\n- f demonstrations show effects on the in-context\n\nlearning performance (Su et al., 2022; Zhang et al.,\n2023b). As it is not the focus of this work, we pay\nno more attention to prompt editing.\n\n\n**5** **Experiments**\n\n\n**5.1** **Task Settings**\n\n\n**5.1.1** **Open-domain QA**\n\n\nThree open-domain QA datasets are used for evaluation. (i) HotPotQA (Yang et al., 2018) consists of\ncomplex questions that require multi-hop reasoning. We evaluate the full test set. (ii) AmbigNQ\n(Min et al., 2020) provides a disambiguated version\n\n- f Natural Questions (NQ) (Kwiatkowski et al.,\n2019). For ambiguous questions in NQ, minimal\nconstraints are added to break it into several similar\n\n\n\n_kt,\u03b8_ = _[p][\u03b8]_ [(] _[a][t]_ _[|][ s][t]_ [)]\n\n_p\u03b8\u2032_ ( _at | st_ ) _[,]_\n\n\nwhere _\u03b8_ _[\u2032]_ is the temporarily fixed policy for sampling and _\u03b8_ is updated. _A_ denotes the advantage\nfunction, which is formulated based on the estimation of value network _V\u03d5_ . The value network _V\u03d5_ is\ninitialized from the policy network _\u03c0_ 0. The formulation follows Generalized Advantage Estimation\n(GAE) (Schulman et al., 2015).\n\n\n_\u03b4t_ = _R_ ( _st, at_ ) + _V\u03d5_ ( _st_ +1) _\u2212_ _V\u03d5_ ( _st_ ) _,_\n\n\n\n_A_ \u02c6 _[\u03b8]_ _t_ [(] _[s][t][, a][t]_ [) =]\n\n\n\n_\u221e_ (3)\n\n- _\u03bb_ _[t][\u2032]_ _\u03b4t_ + _t\u2032,_\n\n_t_ _[\u2032]_ =0\n\n\n\nwhere _\u03bb_ is the bias", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_5400", "chunk_text": "_ [) =]\n\n\n\n_\u221e_ (3)\n\n- _\u03bb_ _[t][\u2032]_ _\u03b4t_ + _t\u2032,_\n\n_t_ _[\u2032]_ =0\n\n\n\nwhere _\u03bb_ is the bias-variance trade-off parameter.\nThe reward function _R_ reflects the quality of the\ngenerated queries, which needs to be consistent\nwith the final evaluation of the task. _x_ \u02dc [\u02c6] is fed to the\nretriever and the reader for a final prediction \u02c6 _y_ . A\npart of the reward function is the measures of \u02c6 _y_\ncompared to the golden label _y_ (e.g., exact match\nand F1 of the predicted answers), denoted as _Rlm_ .\nBesides, a KL-divergence regularization is added\nto prevent the model from deviating too far from\nthe initialization (Ramamurthy et al., 2022; Ziegler\net al., 2019).\n\n\n\u02dc\n_R_ ( _st, at_ ) = _Rlm_ ( _x, y_ [\u02c6] ) _\u2212_ _\u03b2_ KL ( _\u03c0\u03b8\u2225\u03c0_ 0) _._ (4)\n\n\nThe final loss function is composed of policy loss\nand value loss.\n\n\n\n1\n_L\u03b8_ = _\u2212_\n_|S| T_\n\n\n\n\n\n_\u03c4_ _\u2208S_\n\n\n\n_T_\n\n- min( _kt,\u03b8A_ _[\u03b8][\u2032]_ _,_ clip _A_ _[\u03b8][\u2032]_ ) _,_\n\n\n_t_ =0\n\n\n\n1\n_L\u03d5_ =\n_|S| T_\n\n\n\n\n\n_\u03c4_ _\u2208S_\n\n\n\n_T_\n\n- ( _V\u03d5_ ( _st_ ) _\u2212_ _Rt_ ) [2] _,_\n\n\n_t_ =0\n\n\n\n_Lppo_ = _L\u03b8_ + _\u03bbvL\u03d5._\n(5)\n\n\nHere, _S_ denotes the sampled set, and _T_ is for step\nnumbers.\n\n\n**4** **Implementation**\n\n\n**Rewriter** For the frozen pipeline in \u00a73.1, we\nprompt an LLM to rewrite the query with few-shot\n\n\n**Direct prompt**\n\n\nAnswer the question in the following format, end the answer with \u2019**\u2019. {demonstration} Question: { _", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_5850", "chunk_text": " pipeline in \u00a73.1, we\nprompt an LLM to rewrite the query with few-shot\n\n\n**Direct prompt**\n\n\nAnswer the question in the following format, end the answer with \u2019**\u2019. {demonstration} Question: { _x_ } Answer:\n\n\n**Reader prompt in retrieval-augment pipelines**\n\n\nAnswer the question in the following format, end the answer with \u2019**\u2019. {demonstration} Question: { _doc_ } { _x_ }\nAnswer:\n\n\n**Prompts for LLM as a frozen rewriter**\n\n\n_Open-domain QA:_ Think step by step to answer this question, and provide search engine queries for knowledge\nthat you need. Split the queries with \u2019;\u2019 and end the queries with \u2019**\u2019. {demonstration} Question: { _x_ } Answer:\n_Multiple choice QA:_ Provide a better search query for web search engine to answer the given question, end the\nqueries with \u2019**\u2019. {demonstration} Question: { _x_ } Answer:\n\n\nTable 1: Prompt lines used for the LLMs.\n\n\n\nbut specific questions. The first 1000 samples are\nevaluated in the test set. (iii) PopQA (Mallen et al.,\n2022) includes long-tail distributions as it contains\nmore low-popularity knowledge than other popular\nQA tasks. We split the dataset into 13k for training\nand 714 for testing.\nOpen-domain QA benchmarks are sets of\nquestion-answer pairs denoted as _{_ ( _q, a_ ) _i}_ . We use\nChatGPT for both the reader and the frozen rewriter.\n\nThe evaluation metrics are Exact Match ( _EM_ ) and\n_F_ 1 scores. For the reward function in RL, we use\n\nan indicator to reward if the retrieved content hits\n\nthe answer and penalize if misses the answer, denoted as _Hit_ . The total reward is a weighted sum\n\n- f EM, F1, and _Hit_ .\n\n\n\n_Hit_ =\n\n\n\n1 _a_ in _doc,_\n\n_\u2212_ 1 _else_\n\n\n\n\n(6)\n\n\n\nWe also use Vicuna-13B as the reader for evalua\ntion due to the rate limit issue of ChatGPT. More\n\ninformation on datasets and training setup are presented in the appendix.\n\n\n**5.2** **Baselines**\n\n\nThe following settings are implemented to evaluate and support our methods", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_6300", "chunk_text": " evalua\ntion due to the rate limit issue of ChatGPT. More\n\ninformation on datasets and training setup are presented in the appendix.\n\n\n**5.2** **Baselines**\n\n\nThe following settings are implemented to evaluate and support our methods. (i) **Direct** : The\nstandard in-context learning without any augmentations. (ii) **Retrieve-then-read** : The standard\nretrieval-augmented method. Retrieved documents\nare concatenated with the question. (iii) **LLM**\n**as a frozen rewriter** : As is introduced in \u00a73.1,\nwe prompt a frozen LLM to reason and generate\nqueries by few-shot in-context learning. (iv) **Train-**\n**able rewriter** : Applying the fine-tuned rewriter,\nthe output queries are used by the retriever and the\nreader. Table 1 presents prompt line forms. Please\nnote that the prompts for prediction are kept the\nsame for each task.\n\n\n**5.3** **Results**\n\n\nExperimental results on open-domain QA are reported in Table 2. For the three datasets, query\nrewriting consistently brings performance gain\nwith both a frozen rewriter and a trainable rewriter.\n\nOn AmbigNQ and PopQA, the standard retrieval\naugments the reader, indicating useful external\nknowledge is retrieved. On HotpotQA, the standard retrieval hurts the reader. This shows that\n\nusing complex questions as queries cannot compensate for the parametric knowledge, but bring\nnoises instead (Mallen et al., 2022). This suggests\nthat multi-hop questions are not suitable queries\nfor the web search engine. The scores increase by\nadding the rewriting step. On PopQA, our trainable\nrewriter surpasses standard retrieval while being\ninferior to the LLM rewriter. This indicates that the\n\n\n\n_Rlm_ = _EM_ + _\u03bbf_ _F_ 1 + _\u03bbhHit._\n\n\n**5.1.2** **Multiple-choice QA**\n\nFor multiple-choice QA, our evaluation is conducted on Massive Multi-task Language Understanding (MMLU) (Hendrycks et al., 2021), an\nexam question dataset including 4 categories: Humanities, STEM, Social Sciences, and Other. Each\ncategory is split into 80% for the training set and\n20% for the test set.\n\nMultiple-choice QA can be formulated as\n_{_ (", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_6750", "chunk_text": " question dataset including 4 categories: Humanities, STEM, Social Sciences, and Other. Each\ncategory is split into 80% for the training set and\n20% for the test set.\n\nMultiple-choice QA can be formulated as\n_{_ ( _q_ _[\u2032]_ _, a_ ) _i}_, where _q_ _[\u2032]_ = [ _q, c_ 0 _, c_ 1 _, c_ 2 _, c_ 3]. _c_ denotes\nthe options, generally there are four for each question. The retrieved documents that are included\n\nin the officially provided contaminated lists are\nignored. The questions with options are rewritten\ninto search queries. The answer is one option. _EM_\nis reported as metrics and used for the reward.\n\n\n_Rlm_ = _EM._ (7)\n\n\nWe use ChatGPT as a frozen rewriter and the reader.\n\n\ndistillation of query rewriting is sub-optimal.\nThe scores on multiple-choice QA are presented\nin Table 3. With ChatGPT as a reader, it can be observed that query rewriting improves the scores in\nmost of the settings, except for the social sciences\ncategory. With Vicuna as a reader, our method\nachieves more gains on the four categories compared to ChatGPT. This agrees with the intuition\nthat a more powerful reader has more parametric\nmemories, thus more difficult to compensate with\nexternal knowledge.\n\n\n**Model** **EM** **F** 1\n\n\n_HotpotQA_\nDirect 32.36 43.05\n\nRetrieve-then-read 30.47 41.34\n\nLLM rewriter 32.80 43.85\n\nTrainable rewriter 34.38 45.97\n\n\n_AmbigNQ_\nDirect 42.10 53.05\n\nRetrieve-then-read 45.80 58.50\n\nLLM rewriter 46.40 58.74\n\nTrainable rewriter 47.80 60.71\n\n\n_PopQA_\nDirect 41.94 44.61\n\nRetrieve-then-read 43.20 47.53\n\nLLM rewriter 46.00 49.74\n\nTrainable rewriter 45.72 49.51\n\n\nTable 2: Metrics of open-domain QA.\n\n\n**MMLU** **EM**\n\n\nHuman. STEM Other Social\n\n\n_ChatGPT", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_7200", "chunk_text": "46.00 49.74\n\nTrainable rewriter 45.72 49.51\n\n\nTable 2: Metrics of open-domain QA.\n\n\n**MMLU** **EM**\n\n\nHuman. STEM Other Social\n\n\n_ChatGPT_\n\nDirect 75.6 58.8 69.0 71.6\n\nRetrieve-then-read 76.7 63.3 70.0 78.2\n\nLLM rewriter 77.0 63.5 72.6 76.4\n\n\n_Vicuna-13B_\n\nDirect 39.8 34.9 50.2 46.6\n\nRetrieve-then-read 40.2 39.8 55.2 50.6\n\nLLM rewriter 42.0 41.5 57.1 52.2\n\nTrainable rewriter 43.2 40.9 59.3 51.2\n\n\nTable 3: Metrics of multiple choice QA.\n\n\n**6** **Analysis**\n\n\n**6.1** **Training Process**\n\n\nThe training process includes two stages, warm-up\nand reinforcement learning. This section shows\nthe validation scores of the three open-domain QA\ndatasets for further analysis. Figure 2 presents\nthe metric scores through training iterations in the\nprocess of reinforcement learning. As the rewriting\nmodels have been warmed up on the pseudo data\nbefore RL, scores at \u201c0 iteration\u201d denote the ability\nacquired from the warm-up training.\n\n\n|0 5 10 15 20 25 Interation Retrieve-then-read LLM rewriter (b)AmbigNQ|Col2|\n|---|---|\n|(b)AmbigNQ|(b)AmbigNQ|\n|||\n|||\n|Retrieve~~-~~then~~-~~read<br>LLM rewriter|Retrieve~~-~~then~~-~~read<br>LLM rewriter|\n|0<br>2<br>4<br>6<br>8<br>1|0|\n\n\n\n|0 2 4 6 8 10 Interation 4 5 Retrieve-then-read LLM rewriter (c)PopQA|Col2|Col3|Col4|\n|---|---|---|---|\n|0<br>2<br>4<br>6<br>8<br>10<br>12<br>0<br>1", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_7650", "chunk_text": " (c)PopQA|Col2|Col3|Col4|\n|---|---|---|---|\n|0<br>2<br>4<br>6<br>8<br>10<br>12<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>(c)PopQA<br>Retrieve~~-~~then~~-~~read<br>LLM rewriter|0<br>2<br>4<br>6<br>8<br>10<br>12<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>(c)PopQA<br>Retrieve~~-~~then~~-~~read<br>LLM rewriter|0<br>2<br>4<br>6<br>8<br>10<br>12<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>(c)PopQA<br>Retrieve~~-~~then~~-~~read<br>LLM rewriter|0<br>2<br>4<br>6<br>8<br>10<br>12<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>(c)PopQA<br>Retrieve~~-~~then~~-~~read<br>LLM rewriter|\n|0<br>2<br>4<br>6<br>8<br>10<br>12<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>(c)PopQA<br>Retrieve~~-~~then~~-~~read<br>LLM rewriter||||\n|0<br>2<br>4<br>6<br>8<br>10<br>12<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>(c)PopQA<br>Retrieve~~-~~then~~-~~read<br>LLM rewriter||||\n|0<br>2<br>4<br>6<br>8<br>10<br>12<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>(c)PopQA<br>Retrieve~~-~~then~~-~~read<br>LLM rewriter|Retrieve~~-~~then~~-~~read<br>LLM rewriter|Retrieve~~-~~then~~", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_8100", "chunk_text": "5<br>6<br>(c)PopQA<br>Retrieve~~-~~then~~-~~read<br>LLM rewriter|Retrieve~~-~~then~~-~~read<br>LLM rewriter|Retrieve~~-~~then~~-~~read<br>LLM rewriter|Retrieve~~-~~then~~-~~read<br>LLM rewriter|\n|0<br>2<br>4<br>6<br>8<br>10<br>12<br>0<br>1<br>2<br>3<br>4<br>5<br>6<br>(c)PopQA<br>Retrieve~~-~~then~~-~~read<br>LLM rewriter|Retrieve~~-~~then~~-~~read<br>LLM rewriter|1|2|\n\n\nFigure 2: Reinforcement learning validation scores of\n(a)HotpotQA, (b)AmbigNQ, and (c)PopQA. The solid\nlines show EM (red) and F1 (blue) numbers through\ntraining iterations. The dashed lines are EM scores\n\n- f the standard retrieve-then-read method (orange) and\nretrieval with an LLM as the rewriter (green).\n\n\nIt can be observed that the curves show upward\ntrends with some fluctuations on all the datasets. (i)\nFor multi-hop questions in HotpotQA, the standard\nretrieval is relatively weaker. Complex questions\ncan be not specific search queries and show a larger\ngap from rewritten queries, i.e., the green and red\nlines. (ii) On AmbigNQ and PopQA, our method\nsurpasses the baselines after several iterations (3\n\n- r 4). This indicates that the RL training stage can\ncompensate for the insufficiency of the distillation\n\n- n the pseudo data during warm-up training. (iii)\nIn particular, on PopQA, the trainable rewriter remains inferior to the LLM rewriter. This can be\n\nexplained as the dataset is constructed for adaptive\nretrieval (Mallen et al., 2022), which only uses retrieval where it helps to avoid harmful redundant\nretrieval. Thus, _\u201cNone\u201d_ is a possible query that\nmeans no retrieval. This causes more complexity and uncertainty. LLM rewriter knows better\nwhen the retrieval is needed for itself as a reader,\nalthough the rewriting step is not concatenated as\n\n\n\n\n\n\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_8550", "chunk_text": ", _\u201cNone\u201d_ is a possible query that\nmeans no retrieval. This causes more complexity and uncertainty. LLM rewriter knows better\nwhen the retrieval is needed for itself as a reader,\nalthough the rewriting step is not concatenated as\n\n\n\n\n\n\n\n\nthe input context of the reader.\nWe calculate the performance of query _\u201cNone\u201d_ .\nThe questions that can be correctly answered with\n- ut retrieval (i.e., the \u201cDirect\u201d method) are those\nsamples that need no more context. Comparing this\nretrieval-free set with those that are rewritten to\n\nbe _\u201cNone\u201d_ query, the F1 score of the LLM rewriter\nis 71.9% and the T5 rewriter score is 67.1%. If\n\nwe consider the questions that can be correctly answered without retrieval but go wrong with retrieval\nas the retrieval-free set, the F1 scores are 78.7% for\n\nLLM rewriter and 77.4% for T5.\n\n\n**Model** **EM** **F** 1 **Hit ratio**\n\n\nNo retrieval 42.10 53.05  Upper bound 58.40 69.45 100\n_Retrieve-then-read_\nw/ snippet 38.70 50.50 61.1\nw/ BM25 45.80 58.50 76.4\n\n_LLM rewriter_\nw/ snippet 39.80 52.64 63.5\nw/ BM25 46.40 58.74 77.5\n\n_Trainable rewriter_\nw/ BM25 [2] 47.80 60.71 82.2\n\n\nTable 4: Retrieval analysis on AmbigNQ.\n\n\n**6.2** **Retrieval Result**\n\n\nOur proposed method is a pipeline framework, instead of an end-to-end system. The query rewriting first affects the retrieved context, then the context makes a difference to the output of the reader.\nHence, QA metrics are indirect measurements. We\n\ntake a closer look at the retrieved context and the\n\nreader capability through the retrieval metric, hit\nratio. After text normalization, the hit rate is computed to measure whether the retrieved context contains the correct answers.\n\nTable 4 shows the scores on AmbigNQ. The\nscores in the second line are computed on a selection of the samples whose retrieved contexts hit\ncorrect answers", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_9000", "chunk_text": " the hit rate is computed to measure whether the retrieved context contains the correct answers.\n\nTable 4 shows the scores on AmbigNQ. The\nscores in the second line are computed on a selection of the samples whose retrieved contexts hit\ncorrect answers (under the standard retrieve-thenread setting). The scores show the approximate\nupper bound ability of the reader with retrieval augmentation, abbreviated as the \u201cupper bound\u201d score.\nThe effectiveness of retrieval is proved compared\nto the no retrieval setting (the first line). For each\nretrieval method, two settings are presented: (i)\ncollecting Bing snippets, (ii) selecting from URLs\nby BM25. The metrics show that content selection\nwith BM25 recalls better documents than snippets,\n\n\n2Our trainable rewriter is adapted to the retriever using\nBM25 during RL training. Using the output queries of the test\nset after training, the snippet hit rate is 73.4%.\n\n\n\nFigure 3: Examples for intuitive illustration. Q0 denotes\n\n- riginal input, Q1 is from the LLM rewriter, and Q2 is\nfrom the trained T5 rewriter. **Hit** means retriever recall\n\nthe answer, while **Correct** is for the reader output.\n\n\nwhile query rewriting makes progress on both settings. We also observed that the improvement in\nthe hit rate of the retriever is more significant than\nthe improvement in the reader. This is consistent\nwith the findings in related search (Mallen et al.,\n2022; Liu et al., 2023).\n\n\n**6.3** **Case Study**\n\n\nTo intuitively show how the query rewriting makes\na difference in the retrieved contexts and prediction\nperformance, we present examples in Figure 3 to\ncompare the original questions and the queries. In\nexample 1, the original question asks for a film that\n_the youngest daughter of Lady Mary-Gaye Curzon_\nco-stars with two certain actors. Both query 1 and\nquery 2 put the keyword _film_ forward, closely following _the youngest daughter of Lady Mary-Gaye_\n_Curzon_ . With both, the actress _Charlotte Calthorpe_\n\nand her movie information can be retrieved and\n\nthe answer is included. The second is an example\nwhere the query from the LLM rewriter failed but\n\n\n\nExample 1: multi-hop question\n\n\nQ0: The youngest daughter of Lady Mary-Gaye\nCurzon stars with", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_9450", "chunk_text": " retrieved and\n\nthe answer is included. The second is an example\nwhere the query from the LLM rewriter failed but\n\n\n\nExample 1: multi-hop question\n\n\nQ0: The youngest daughter of Lady Mary-Gaye\nCurzon stars with Douglas Smith and\nLucien Laviscount in what 2017 film?\n\n\nQ1: the youngest daughter of Lady Mary-Gaye\nCurzon; 2017 film stars Douglas Smith\nand Lucien Laviscount\n\n\nQ2: Lady Mary-Gaye Curzon youngest daughter\n2017 film with Douglas Smith and Lucien\nLaviscount\n\nExample 2:\n\n\nappear in?\n\n\nQ1: movie \"All Star\" 2000\n\n\nExample 3: multiple choice\n\nQ0: A car-manufacturing factory is considering\na new site for its next plant. Which of the\nfollowing would community planners be\nmost concerned with before allowing the\nplant to be built? Options: A. The amount\n - f materials stored in the plant B. The hours\n - f operations of the new plant C. The effect\nthe plant will have on the environment D.\nThe work environment for the employees\nat the plant\n\n\nconcerned with before allowing a carmanufacturing factory to be built?\n\n\n\nHit Correct\n\n\n\u274c \u274c\n\n\n\u2705 \u2705\n\n\n\u2705 \u2705\n\n\n\u274c \u274c\n\n\n\u274c \u274c\n\n\n\u2705 \u2705\n\n\nthe query from T5 gets the correct answer. The\nnumber _2000_ is misunderstood in query 1, while\nquery 2 keeps _200 movie_ together, avoiding meaningless retrieval. Example 3 is for multiple choice.\nThe query simplifies the background and enhances\nthe keyword _community planner_ . The retrieve contexts are mainly about _Introduction to Community_\n_Planning_ where the answer _environment_ appears\nseveral times.\n\n\n**7** **Conclusion**\n\n\nThis paper introduces the _Rewrite-Retrieve-Read_\npipeline, where a query rewriting step is added\nfor the retrieval-augmented LLM. This approach\nis applicable for adopting a frozen large language\nmodel as the reader and a real-time web search\n\nengine as the retriever. Further, we propose to apply a tuneable small language model the rewriter,\nwhich can be trained to cater to the frozen retriever\n\nand reader. The training implementation consists\n\n- f two stages, warm-up and reinforcement learning. Evaluation and analyses on open-domain", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_9900", "chunk_text": " propose to apply a tuneable small language model the rewriter,\nwhich can be trained to cater to the frozen retriever\n\nand reader. The training implementation consists\n\n- f two stages, warm-up and reinforcement learning. Evaluation and analyses on open-domain QA\nand multiple-choice QA show the effectiveness\n\n- f query rewriting. Our work proposes a novel\nretrieval-augmented black-box LLM framework,\nproves that the retrieval augmentation can be enhanced from the aspect of query rewriting, and\nprovides a new method for integrating trainable\nmodules into black-box LLMs.\n\n\n**Limitations**\n\n\nWe acknowledge the limitations of this work. (i)\nThere is still a trade-off between generalization and\nspecialization among downstream tasks. Adding\na training process, the scalability to direct transfer\nis compromised, compared to few-shot in-context\nlearning. (ii) The research line of _LLM agent_ has\nshown impressive performance but relies on multiple calls to the LLM for each sample (Khattab\net al., 2022; Yao et al., 2023), where the LLM\nplays as an agent to flexibly call the retriever multiple times, reads the context in earlier hops, and\ngenerates follow-up questions. Different from\nthese studies, our motivation is to enhance the one\nturn retriever-then-read framework with a trainable\n\nquery rewriter. (iii) Using a web search engine as\nthe retriever also leads to some limitations. Neu\nral dense retrievers that are based on professional,\nfiltered knowledge bases may potentially achieve\nbetter and controllable retrieval. More discussion\n\nis included in the appendix.\n\n\n\n**References**\n\n\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. _arXiv preprint_\n_arXiv:2302.04023_ .\n\n\nParishad BehnamGhader, Santiago Miret, and Siva\nReddy. 2022. Can retriever-augmented language\nmodels reason? the blame game between the retriever and", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_10350", "chunk_text": ".04023_ .\n\n\nParishad BehnamGhader, Santiago Miret, and Siva\nReddy. 2022. Can retriever-augmented language\nmodels reason? the blame game between the retriever and the language model. _arXiv preprint_\n_arXiv:2212.09146_ .\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. _Advances in neural information processing_\n_systems_, 33:1877\u20131901.\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer opendomain questions. In _Association for Computational_\n_Linguistics (ACL)_ .\n\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pond\u00e9 de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\n[Sutskever, and Wojciech Zaremba. 2021. Evaluat-](http://arxiv.org/abs/2107.03374", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_10800", "chunk_text": ", Dario Amodei, Sam McCandlish, Ilya\n[Sutskever, and Wojciech Zaremba. 2021. Evaluat-](http://arxiv.org/abs/2107.03374)\n[ing large language models trained on code.](http://arxiv.org/abs/2107.03374) _CoRR_,\nabs/2107.03374.\n\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\n[Stoica, and Eric P. Xing. 2023. Vicuna: An open-](https://lmsys.org/blog/2023-03-30-vicuna/)\n[source chatbot impressing gpt-4 with 90%* chatgpt](https://lmsys.org/blog/2023-03-30-vicuna/)\n[quality.](https://lmsys.org/blog/2023-03-30-vicuna/)\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. _arXiv preprint_\n_arXiv:2204.02311_ .\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\n[Kristina Toutanova. 2019. BERT: pre-training of](https://doi.org/10.18653/v1/n19-1423)\n\n\n[deep bidirectional transformers for language under-](https://doi.org/10.18653/v1/n19-1423)\n[standing. In](https://doi.org/10.18653/v1/n19-1423) _Proceedings of the 2019 Conference of_\n_the North American Chapter of the Association for_\n_Computational Linguistics: Human Language Tech-_\n_nologies, NAACL-HLT 2019, Minneapolis, MN, USA,_\n_June 2-7, 2019, Volume 1 (Long and Short Papers)_,\npages 4171\u20134186. Association", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_11250", "chunk_text": "-_\n_nologies, NAACL-HLT 2019, Minneapolis, MN, USA,_\n_June 2-7, 2019, Volume 1 (Long and Short Papers)_,\npages 4171\u20134186. Association for Computational\nLinguistics.\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In _International confer-_\n_ence on machine learning_, pages 3929\u20133938. PMLR.\n\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language\nunderstanding. _Proceedings of the International Con-_\n_ference on Learning Representations (ICLR)_ .\n\n\nNamgyu Ho, Laura Schmid, and Se-Young Yun. 2022.\nLarge language models are reasoning teachers. _arXiv_\n_preprint arXiv:2212.10071_ .\n\n\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,\nHootan Nakhost, Yasuhisa Fujii, Alexander J. Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister.\n2023. Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller\nmodel sizes. _ArXiv_, abs/2305.02301.\n\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, and Edouard\n[Grave. 2022. Few-shot Learning with Retrieval Aug-](http://arxiv.org/abs/2208.03299)\n[mented Language Models.](http://arxiv.org/abs/2208.03299)\n\n\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models.\n\n\nZhengbao Jiang,", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_11700", "chunk_text": "bo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models.\n\n\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\nZhiruo Wang, Jamie Callan, and Graham Neubig.\n2022. Retrieval as attention: End-to-end learning of\nretrieval and reading within a single transformer. In\n_Conference on Empirical Methods in Natural Lan-_\n_guage Processing (EMNLP)_, Abu Dhabi, UAE.\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n[Wen-tau Yih. 2020. Dense passage retrieval for open-](https://doi.org/10.18653/v1/2020.emnlp-main.550)\n[domain question answering. In](https://doi.org/10.18653/v1/2020.emnlp-main.550) _Proceedings of the_\n_2020 Conference on Empirical Methods in Natural_\n_Language Processing (EMNLP)_, pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\n\n\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP. _arXiv preprint_\n_arXiv:2212.14024_ .\n\n\n\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\n\n[Internet-augmented dialogue generation. In](https://doi.org/10.18653/v1/2022.acl-long.579) _Proceed-_\n_ings of the 60th Annual Meeting of the Association_\n_for Computational Linguistics (Volume 1: Long Pa-_\n_pers)_, pages 8460\u20138478, Dublin, Ireland. Association\nfor Computational Linguistics.\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_12150", "chunk_text": " Computational Linguistics.\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. _Transactions of the_\n_Association for Computational Linguistics_ .\n\n\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot\nprompting for open-domain question answering.\n_arXiv preprint arXiv:2203.05115_ .\n\n\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher Manning, and Kyoung-Gu Woo.\n[2022. You only need one model for open-domain](https://aclanthology.org/2022.emnlp-main.198)\n[question answering. In](https://aclanthology.org/2022.emnlp-main.198) _Proceedings of the 2022 Con-_\n_ference on Empirical Methods in Natural Language_\n_Processing_, pages 3047\u20133060, Abu Dhabi, United\nArab Emirates. Association for Computational Linguistics.\n\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\n[BART: denoising sequence-to-sequence pre-training](https://doi.org/10.18653/v1/2020.acl-main.703)\n[for natural language generation, translation, and com-](https://doi.org/10.18653/v1/2020.acl-main.703)\n[prehension. In](https://doi.org/10.18653/v1/2020.acl-main.703) _Proceedings of the 58th Annual Meet-_\n_ing of the Association for Computational Linguistics,_\n_ACL 2020, Online, July 5-10, 2020_, pages 7871\u20137880.\nAssociation for Computational Linguistics.\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpuk", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_13050", "chunk_text": "h Hajishirzi, and Daniel Khashabi. 2022.\nWhen not to trust language models: Investigating\neffectiveness and limitations of parametric and nonparametric memories. _arXiv preprint_ .\n\n\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy CampbellGillingham, Geoffrey Irving, et al. 2022. Teaching\nlanguage models to support answers with verified\nquotes. _arXiv preprint arXiv:2203.11147_ .\n\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle[moyer. 2022. Rethinking the role of demonstrations:](https://aclanthology.org/2022.emnlp-main.759)\n[What makes in-context learning work? In](https://aclanthology.org/2022.emnlp-main.759) _Proceed-_\n_ings of the 2022 Conference on Empirical Methods in_\n_Natural Language Processing_, pages 11048\u201311064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\n\n\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In _EMNLP_ .\n\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instructions with human feedback. _Advances in Neural_\n_Information Processing Systems_, 35:27730\u201327744.\n\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. _arXiv preprint arXiv:2210.03350_ .\n\n\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, X", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_13500", "chunk_text": "Xiv:2210.03350_ .\n\n\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\n[Bill Qian, et al. 2023. Toolllm: Facilitating large](https://arxiv.org/abs/2307.16789)\n[language models to master 16000+ real-world apis.](https://arxiv.org/abs/2307.16789)\n_ArXiv preprint_, abs/2307.16789.\n\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\n[Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the](http://jmlr.org/papers/v21/20-074.html)\n[limits of transfer learning with a unified text-to-text](http://jmlr.org/papers/v21/20-074.html)\n[transformer.](http://jmlr.org/papers/v21/20-074.html) _Journal of Machine Learning Research_,\n21(140):1\u201367.\n\n\nRajkumar Ramamurthy, Prithviraj Ammanabrolu,\nKiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian\nBauckhage, Hannaneh Hajishirzi, and Yejin Choi.\n[2022. Is reinforcement learning (not) for natural](https://arxiv.org/abs/2210.01241)\n[language processing?: Benchmarks, baselines, and](https://arxiv.org/abs/2210.01241)\n[building blocks for natural language policy optimiza-](https://arxiv.org/abs/2210.01241)\n[tion.](https://arxiv.org/abs/2210.01241)\n\n\n[Paul R\u00f6ttger and Janet Pierrehumbert. 2021. Temporal](https://doi.org/10.18653/v1/2021.findings-emnlp.206)\n[adaptation of BERT and performance on downstream](https://doi.org/10.18653/v1/2021.findings-emnlp.206)\n[document classification: Insights from social media.](https://doi.org", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_13950", "chunk_text": ".206)\n[adaptation of BERT and performance on downstream](https://doi.org/10.18653/v1/2021.findings-emnlp.206)\n[document classification: Insights from social media.](https://doi.org/10.18653/v1/2021.findings-emnlp.206)\nIn _Findings of the Association for Computational_\n_Linguistics: EMNLP 2021_, pages 2400\u20132412, Punta\nCana, Dominican Republic. Association for Computational Linguistics.\n\n\n\nDevendra Singh Sachan, Siva Reddy, William L. Hamil[ton, Chris Dyer, and Dani Yogatama. 2021. End-to-](https://proceedings.neurips.cc/paper/2021/hash/da3fde159d754a2555eaa198d2d105b2-Abstract.html)\n[end training of multi-document reader and retriever](https://proceedings.neurips.cc/paper/2021/hash/da3fde159d754a2555eaa198d2d105b2-Abstract.html)\n[for open-domain question answering. In](https://proceedings.neurips.cc/paper/2021/hash/da3fde159d754a2555eaa198d2d105b2-Abstract.html) _Advances_\n_in Neural Information Processing Systems 34: An-_\n_nual Conference on Neural Information Processing_\n_Systems 2021, NeurIPS 2021, December 6-14, 2021,_\n_virtual_, pages 25968\u201325981.\n\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\n_arXiv preprint arXiv:2302.04761_ .\n\n\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael\nJordan, and Pieter Abbeel. 2015. High-dimensional\ncontinuous control using generalized advantage estimation. _arXiv preprint arXiv:1506.02438_ .\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_14400", "chunk_text": "\ncontinuous control using generalized advantage estimation. _arXiv preprint arXiv:1506.02438_ .\n\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. _arXiv preprint_\n_arXiv:1707.06347_ .\n\n\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. _arXiv preprint arXiv:2303.17580_ .\n\n\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Replug: Retrievalaugmented black-box language models. _arXiv_\n_preprint arXiv:2301.12652_ .\n\n\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason Weston.\n[2022. Language models that seek for knowledge:](https://aclanthology.org/2022.findings-emnlp.27)\n[Modular search & generation for dialogue and](https://aclanthology.org/2022.findings-emnlp.27)\n[prompt completion. In](https://aclanthology.org/2022.findings-emnlp.27) _Findings of the Association_\n_for Computational Linguistics: EMNLP 2022, Abu_\n_Dhabi, United Arab Emirates, December 7-11, 2022_,\npages 373\u2013393. Association for Computational Linguistics.\n\n\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, et al. 2022. Selective annotation makes language models better fewshot learners. _arXiv preprint arXiv:2209.01975_ .\n\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\nLe, Ed H. Chi, and Denny", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_14850", "chunk_text": " better fewshot learners. _arXiv preprint arXiv:2209.01975_ .\n\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\nLe, Ed H. Chi, and Denny Zhou. 2022. [Self-](https://doi.org/10.48550/arXiv.2203.11171)\n[consistency improves chain of thought reasoning in](https://doi.org/10.48550/arXiv.2203.11171)\n[language models.](https://doi.org/10.48550/arXiv.2203.11171) _CoRR_, abs/2203.11171.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\n[and Denny Zhou. 2022. Chain-of-thought prompt-](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)\n[ing elicits reasoning in large language models. In](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)\n_NeurIPS_ .\n\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo[pher D. Manning. 2018. HotpotQA: A dataset for](https://doi.org/10.18653/v1/D18-1259)\n\n\n[diverse, explainable multi-hop question answering.](https://doi.org/10.18653/v1/D18-1259)\nIn _Proceedings of the 2018 Conference on Empiri-_\n_cal Methods in Natural Language Processing_, pages\n2369\u20132380, Brussels, Belgium. Association for Computational Linguistics.\n\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReAct: Synergizing reasoning and acting in language\nmodels. In _International Conference on Learning", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_15300", "chunk_text": " Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReAct: Synergizing reasoning and acting in language\nmodels. In _International Conference on Learning_\n_Representations (ICLR)_ .\n\n\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In _International Confer-_\n_ence for Learning Representation (ICLR)_ .\n\n\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. 2023a. Tempera:\nTest-time prompt editing via reinforcement learning.\nIn _The Eleventh International Conference on Learn-_\n_ing Representations_ .\n\n\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023b. Automatic chain of thought prompting in large language models. In _The Eleventh In-_\n_ternational Conference on Learning Representations_\n_(ICLR 2023)_ .\n\n\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. _arXiv_\n_preprint arXiv:1909.08593_ .\n\n\n**A** **Warm-up Dataset**\n\n\nFor the warm-up training of the tuneable rewriter,\nwe construct a pseudo dataset for the query rewriting task. For benchmarks that provide official training and test splits (HotpotQA and AmbigNQ), we\nuse the whole training set. For those that have no\n\n- fficial splits (PopQA and MMLU), we randomly\nsplit the full dataset. In detail, PopQA contains 16\ntypes of questions, thus split into 13k for training\nand 714 for testing following stratified sampling.\nFor MMLU, each of the 4 categories is randomly\nsplit into 80% for the training set and 20% for\nthe test set. Then the training sets of each benchmark are used to derive the pseudo dataset for the\nquery rewriting, i.e", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_15750", "chunk_text": " each of the 4 categories is randomly\nsplit into 80% for the training set and 20% for\nthe test set. Then the training sets of each benchmark are used to derive the pseudo dataset for the\nquery rewriting, i.e., _DTrain_ = _{_ ( _x,_ \u02dc _x_ ) _|y_ \u02c6 = _y}_ .\nWe present the statistics of the splits and warm-up\ndataset in Table 5.\n\n\n**B** **Setup Details**\n\n\nFor warm-up, we train the T5-large with 3e-5 learning rate, {16, 20} batch size, for {6,8,12} epochs.\nFor reinforcement learning, we set the sampling\n\n\n\nTask Training Set Warm-up Test Set\n\n\nHotpotQA 90.4k 37.5k 7.4k\nAmbigNQ 19.4k 8.6k 1k\nPopQA 13.0k 6.0k 0.7k\nHumanities 3.8k 1.5k 0.9k\n\nSTEM 2.4k 0.9k 0.6k\n\nOther 2.6k 1.3k 0.6k\n\nSocial Science 2.4k 1.3k 0.6k\n\n\nTable 5: Metrics of multiple choice QA.\n\n\nsteps to 5120, 10 threads, 512 steps for each. After\nsampling, the policy network is trained for {2,3,4}\nepochs, with learning rate as 2e-6 and batch size\nas {8,16}. _\u03bbf_ and _\u03bbh_ are 1.0. _\u03b2_ in Eq. 4 is dynamically adapted according to Ramamurthy et al.\n(2022); Ziegler et al. (2019),\n\n\nKL ( _\u03c0\u2225\u03c0_ 0) _\u2212_ KLtarget\n_et_ = clip _, \u2212_ 0 _._ 2 _,_ 0 _._ 2 _,_\n\n   - KLtarget    \n_\u03b2t_ +1 = _\u03b2t_ (1 + K _\u03b2et_ ) _,_\n\n\nwhere KL _target_ is set to 0.2, K _\u03b2_ is set to 0.1. _\u03b2_ 0\nis initialized", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_16200", "chunk_text": " = _\u03b2t_ (1 + K _\u03b2et_ ) _,_\n\n\nwhere KL _target_ is set to 0.2, K _\u03b2_ is set to 0.1. _\u03b2_ 0\nis initialized to be 0.001. The generation strategy follows the 4-beam search and returns the one\nsequence. In the implementation of the BM25based retriever, the textboxes from searched URLs\nare parsed from HTML code. We compute BM25\nscores between the paragraph from each textbox\nand the query following the scikit-learn package,\nthen keep those with higher scores until the reserved context reaches a max length. In reinforcement learning, the results of AmbigNQ are with\nthe BM25 method, while others use snippets as\n\ncontext.\n\n\n**C** **Web Search: Tool Use**\n\n\nOur proposed pipeline integrates an externally built\nweb search engine as the retriever module. We\npresent more discussion on the advantages and disadvantages here.\nThe usage of external tools expands the ability boundary of language models, compensating\nfor the parametric knowledge, and grounding the\ncapabilities of language models to interact with environments (Qin et al., 2023; Schick et al., 2023).\nRecent studies show a trend to leverage plug-andplay tools like search engines to enhance language\nagents (Lazaridou et al., 2022; Menick et al., 2022;\nShuster et al., 2022; Shen et al., 2023). Search\nengine APIs are well-developed retrievers, saving\nefforts to build and maintain another retriever, like\n\na Contriever. Accessible to the whole Internet, the\nweb search retrieves from a wide-range, up-to-date\n\n\nknowledge base. The temporal misalignment problem on a fixed candidate database can be alleviated.\nOn the other hand, web search APIs are commercial products requiring subscriptions. Also, the vast\namount of knowledge on the web can be difficult\nto control. The retrieved context from the Internet\n\ncan be occasionally inconsistent, redundant, and\ntoxic, which hinders the LLM reader.\nBeyond retrieval augmentation, in a general\nscope, other tools called by LLMs, like code interpreters, online models, and expert applications,\nare all similar to search engines, without trainable\nparameters to optimize. There could be a gap between the LM and these tools. This paper", "token_count": 500, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2305.14283_query_rewriting_ma:chunk_16650", "chunk_text": "scope, other tools called by LLMs, like code interpreters, online models, and expert applications,\nare all similar to search engines, without trainable\nparameters to optimize. There could be a gap between the LM and these tools. This paper proposes\nan idea to align them through a trainable small\nmodel.\n\n\n", "token_count": 64, "metadata": {"arxiv_id": "2305.14283", "title": "Query Rewriting for Retrieval-Augmented Large Language Models", "authors": ["Xinbei Ma", "Yeyun Gong", "Pengcheng He", "Hai Zhao", "Nan Duan"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14283v3"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_0", "chunk_text": "## CONTEXTUAL DOCUMENT EMBEDDINGS\n\n\n\n**John X. Morris**\nCornell University\njxm3@cornell.edu\n\n\n\n**Alexander M. Rush**\nCornell University\narush@cornell.edu\n\n\n\nABSTRACT\n\n\nDense document embeddings are central to neural retrieval. The dominant paradigm\nis to train and construct embeddings by running encoders directly on individual\ndocuments. In this work, we argue that these embeddings, while effective, are\nimplicitly out-of-context for targeted use cases of retrieval, and that a document\nembedding should take into account both the document and neighboring documents\nin context \u2013 analogous to contextualized word embeddings. We propose two complementary methods for contextualized document embeddings: first, an alternative\ncontrastive learning objective that explicitly incorporates document neighbors into\nthe intra-batch contextual loss; second, a new contextual architecture that explicitly\nencodes neighbor document information into the encoded representation. Results\nshow that both methods achieve better performance than biencoders in several\nsettings, with differences especially pronounced out-of-domain. We achieve state\n    - f-the-art results on the MTEB benchmark with no hard negative mining, score\ndistillation, dataset-specific instructions, intra-GPU example-sharing, or extremely\nlarge batch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n\n\n1 INTRODUCTION\n\n\nMachine learning approaches to text retrieval aim to learn an embedded representation for indexing\ndocuments. Classically, this area was dominated by statistical approaches using sparse lexical\nmatching methods based on n-gram frequencies such as BM25 (Robertson & Zaragoza, 2009).\nOnly recently have neural networks become competitive with state-of-the-art models on retrieval\ntasks (Karpukhin et al., 2020; Thakur et al., 2021). The primary neural method is a _dual encoder_\narchitecture that independently encodes both a document and query to a dense latent space for\nretrieval lookup. This document embedding space can improve upon a statistical model since it is\nlearned end-to-end for retrieval.\n\n\nHowever, there is at least one notable benefit of statistical approaches that is lost by neural models.\nStatistical models can easily incorporate prior corpus statistics such as inverse document frequency\n(IDF), into their representation. This prior term imparts context-dependence onto the model, since it\ncan be updated based on information specific to retrieval in a given domain at test time. We", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_450", "chunk_text": " incorporate prior corpus statistics such as inverse document frequency\n(IDF), into their representation. This prior term imparts context-dependence onto the model, since it\ncan be updated based on information specific to retrieval in a given domain at test time. We contrast\nthis contextual formulation with neural document encoders that are by definition a function of the\ndocument itself. For example consider the following document:\n\n\nThe National Football League Draft is an annual event in which the National\nFootball League (NFL) teams select eligible college football players...\n\n\nDepending on the retrieval domain, e.g. Wikipedia search, sports articles, or televised events, IDF\nmay weight terms such as NFL, draft or annual higher; a neural document embedding model\nwould need to select a global weighting for this document.\n\n\nIn this work, we explore contextualization of document embeddings produced by dense encoders.\nThe goal is to produce embeddings that are better able to handle retrieval tasks in specific challenging\ncontexts. We propose two complementary changes to document encoders: a contextual training\nprocedure and architecture.\n\n\nFor contextual training, we aim to build a notion of neighboring documents directly into the contrastive\nlearning process. We propose a method that uses fast query-document clustering to produce a group\n\n\n1\n\n\nFigure 1: Overview of our system for contextual document embeddings (CDE). Our model operates\nin two stages: a first stage used to characterize the dataset from samples, and a second stage used to\nembed the final document.\n\n\n- f neighbors for each training batch. Each update for training is constructed purely from neighboring\ndocuments to ensure that embeddings can distinguish documents even in the most challenging\n\ncontexts.\n\n\nFor the architecture, we propose a new encoder that injects information about the contextual documents during embedding. The proposed architecture augments the standard BERT-style encoder with\nadditional conditioning that provides aggregated document-level information about neighboring documents. We call our method Contextual Document Embedding (CDE). Analogously to pre-computed\ncorpus-level statistics, this method provides a manner for the embedding to take into account the\nrelative frequency of terms in context. The final output is still an embedding of the same size, so this\ndoes not require any additional storage or other changes to the retrieval process. When indexing, we\nutilize information from the corpus to produce document and query embeddings that are specific to a\nparticular domain.\n\n\nExperiments compare these two extensions to standard approaches for training document embeddings.\nOur results show that contextual contrastive training improves", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_900", "chunk_text": " When indexing, we\nutilize information from the corpus to produce document and query embeddings that are specific to a\nparticular domain.\n\n\nExperiments compare these two extensions to standard approaches for training document embeddings.\nOur results show that contextual contrastive training improves standard text embedding model training,\nand can be run without other approaches such as additional hard negatives. With the contextual\nencoder architecture, we see additional improvements over a baseline model in all settings tested,\nwith larger improvements in highly specific domains such as small datasets of financial and medical\ndocuments. When trained at industry-scale, our model achieves state-of-the-art results for small\n( _<_ 250M parameter) models on the MTEB benchmark.\n\n\n2 RELATED WORK\n\n\n**Text retrieval.** Our work is related to the general field of text retrieval; we propose specific\nimprovements to the training of \u201cbiencoder\u201d text embedding models such as DPR (Karpukhin et al.,\n2020), GTR (Ni et al., 2021), Contriever (Izacard et al., 2022), LaPraDoR (Xu et al., 2022), Instructor\n(Su et al., 2023), Nomic-Embed (Nussbaum et al., 2024), E5 (Wang et al., 2024), and GTE (Li et al.,\n2023). We focus on the problem of adapting these text retrieval models to new corpora at test time;\nsome prior work has noted this problem (Dai et al., 2022; Sciavolino, 2021) and proposed solutions\nsuch as unsupervised span-sampling and training on test corpora (Gao & Callan, 2021) and distillation\n\n- n the test corpus from a reranker (Sung et al., 2023). Late interaction methods (Khattab & Zaharia,\n2020; Santhanam et al., 2022) also offer one way to improve out-of-domain retrieval performance,\nbut increase the runtime and complexity of search. We propose a better sampling scheme that can be\nused to train any biencoder or late interaction model as well as a _training-free_ method for test-time\nadaptation.\n\n\n2\n\n\n**Contrastive learning.** Much research has focused on the effect of hard negatives on the performance\n\n- f contrastive learning methods Chen et al. (2020); Qu et al", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_1350", "chunk_text": "-free_ method for test-time\nadaptation.\n\n\n2\n\n\n**Contrastive learning.** Much research has focused on the effect of hard negatives on the performance\n\n- f contrastive learning methods Chen et al. (2020); Qu et al. (2021); Robinson et al. (2021); Wang\net al. (2023). (Zhang & Stratos, 2021) observe that harder negatives provide a better approximation\n\n- f the overall cross-entropy loss, but do not consider _batch_ - level optimizations for negative selection.\nHofstatter et al. (2021) cluster queries before training and show that this improves performance.\u00a8\nSachidananda et al. (2023) also consider contrastive batch sampling as a global optimization problem,\nbut do not apply their technique to state-of-the-art transformer-based text embedding models. (Ma\net al., 2024) use a clustering algorithm to partition a dataset into several sub-datasets, but train a\ndifferent model on each sub-dataset. Solatorio (2024) also use a pre-trained model to address the\nproblem of in-batch false negatives from randomly sampled batches. Our training algorithm aims to\nfind the hardest possible high-quality batches to train text embedding models.\n\n\n**Test-time adaptation.** Our method can be compared to other solutions to test-time adaptation, a\nproblem that has been well-studied across a variety of domains (Jang et al., 2023). In retrieval, one\nform of test-time adaptation is pseudo-relevance feedback (PRF) (Rocchio, 1971; Li et al., 2018;\nWang et al., 2021), where documents relevant to the query are used to construct a final, enhanced\nquery representation. The query side of our model can be seen as a form of pseudo-relevance\nfeedback; however, we train from scratch to support a more general form of PRF natively, on the\ndocument representation as well as the query.\n\n\n**Non-parametric modeling.** Our contextual document model can be seen as a form of nonparametric modeling. This shows connections with the a large body of deep learning research\nsuch as the non-parametric transformer (NPT) (Kossen et al., 2022) and the subfield of Neural\nProcesses (Garnelo et al., 2018; Kim et al., 201", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_1800", "chunk_text": " learning research\nsuch as the non-parametric transformer (NPT) (Kossen et al., 2022) and the subfield of Neural\nProcesses (Garnelo et al., 2018; Kim et al., 2019; Nguyen & Grover, 2023). Semi-parametric models\nhave been recently applied in NLP, specifically to the task of language modeling (Borgeaud et al.,\n2022; Khandelwal et al., 2020). Instead of using a retrieval model to build a semi-parametric langauge\nmodel, we build a semi-parametric model specifically for the task of retrieval.\n\n\n3 BACKGROUND\n\n\nWe can view text retrieval methods probabilistically as computing a distribution over potential\ndocuments based on a scalar score function _f_ ( _d, q_ ) matching documents and queries:\n\n\nexp _f_ ( _d,_ _q_ )\n_p_ ( _d | q_ ) = (1)\n\n~~\ufffd~~ _d_ _[\u2032]_ _\u2208D_ [exp] _[ f]_ [(] _[d][\u2032][, q]_ [)]\n\n\nwhere _D_ is a finite set of documents in a dataset. There is a wide variety of different definitions for _f_\nincluding full pairwise neural parameterizations (Nogueira & Cho, 2020). In this work, we focus on\nefficient retrieval methods using vector-based methods, also known as embedding models.\n\n\nVector retrieval methods assume that _f_ ( _d, q_ ) can be factored into two embedding terms, _\u03d5_ ( _d_ ) _\u00b7 \u03c8_ ( _q_ ),\nthe document and query embedding respectively. This factorization allows precomputation of the\ndocument embeddings _\u03d5_ ( _d_ ) for all _d \u2208D_ . This is critical for facilitating fast computation of\narg max _d p_ ( _d | q_ ) or top-k variants (Douze et al., 2024).\n\n\nIn statistical retrieval, _\u03d5_ and _\u03c8_ are closed-form functions of the data, often representing unigram or\nbigram counts by the relative frequency of word types. Notably for this work, these methods can also\nutilize distributional properties of the test dataset as a prior, for example through inverse document\nfrequency (IDF). We represent this integration of dataset-level information by writing", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_2250", "chunk_text": " frequency of word types. Notably for this work, these methods can also\nutilize distributional properties of the test dataset as a prior, for example through inverse document\nfrequency (IDF). We represent this integration of dataset-level information by writing the vector\nproduct _\u03d5_ ( _d_ ; _D_ ) _\u00b7 \u03c8_ ( _q_ ; _D_ ).\n\n\nIn neural retrieval, we instead learn the representation as a dense vector. We assume access to a\ntraining corpus of document and query pairs (these may be supervised, i.e. gold-standard annotations,\n\n- r unsupervised, i.e. noised synthetic examples), _DT_ = _{_ ( _d_ [1] _, q_ [1] ) _, ...,_ ( _d_ _[J]_ _, q_ _[J]_ ) _}_, with the aim of\nlearning the embedding function _\u03d5_ and _\u03c8_ .\n\n\nTraining can be motivated as maximizing likelihood of the document corresponding to each query, i.e.\n\n- _j_ [log] _[ p]_ [(] _[d][j][ |][ q][j]_ [)][. Unfortunately, since retrieval datasets can have] _[ |D|]_ [ exceed millions of documents,]\n\ncomputing the normalizer in Eq 1 at each training step is not an option. Instead contrastive learning is\nused where the likelihood is replaced with a biased approximation calculated from negative samples:\n\n\n3\n\n\nexp _f_ ( _d_ _[j]_ _,_ _q_ _[j]_ )\nlog\n_j_ ~~\ufffd~~ _d_ _[\u2032]_ _\u2208H_ ( _q_ _[j]_ ) [exp] _[ f]_ [(]\n\n\n\n_d_ _[\u2032]_ _\u2208H_ ( _q_ _[j]_ ) [exp] _[ f]_ [(] _[d][\u2032][, q][j]_ [)]\n\n\n\nmax\n_\u03d5,\u03c8_\n\n\n\n\n\n\n\nlog _p_ ( _d_ _[j]_ _| q_ _[j]_ ) _\u2248_  _j_ _j_\n\n\n\nwhere _H_ is a set of examples used to approximate the normalizing constant. In implementation,\nin addition to these hard negative examples, other examples from the mini-b", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_2700", "chunk_text": "j]_ ) _\u2248_  _j_ _j_\n\n\n\nwhere _H_ is a set of examples used to approximate the normalizing constant. In implementation,\nin addition to these hard negative examples, other examples from the mini-batch are also used to\ncompute the normalizer since it requires no additional compute for calculating _\u03d5_ ( _d_ ).\n\n\n4 METHODS\n\n\nIn our work, we are interested in integrating contextual information into our embedding functions\n_\u03d5_ and _\u03c8_ . The standard neural _\u03d5_ is purely a function of the document _\u03d5_ ( _d_ ) and does not take into\naccount any notion of context. This contrasts with the statistical model _\u03d5_ ( _\u00b7_ ; _D_ ) and _\u03c8_ ( _\u00b7_ ; _D_ ). Arguably\nthis is not an issue if retrieval is completely in domain, as _\u03d5_ is capable of learning statistics such as\nIDF and average document length on the training set through gradient descent.\n\n\nHowever, in many retrieval benchmarks, models are trained over a single set of documents _D_ and\nthen tested in many other domains _D_ that differs significantly from _DT_ . In this setting, training on\n_DT_ alone may not be able to provide robust embeddings when used in contexts such as _D_ .\n\n\n4.1 CONTEXTUAL TRAINING WITH ADVERSARIAL CONTRASTIVE LEARNING\n\n\nReturning to the example from the introduction, we assume that in a general purpose training corpus\n_DT_, the term NFL is a rare word appearing in relatively few documents and a useful signal. However,\nif at test time _D_ is a corpus of sports articles, this word would be exceedingly common. Evaluation\nin this domain is, in a statistical sense, adversarial to the original dataset. To handle this issue,\nmeta-learning-style objectives have shown to be effective for training document embedders. In these\napproaches, instead of sampling documents-query pairs iid, the objective first sample a domain and\nthen sample a batch of examples. This ensures that the model mostly sees related training points in\neach domain.\n\n\nWe propose a training objective that synthesizes a large set of fine-grained domains to train the model\n\n- n. Formally, our aim is to partition the training dataset _DT_ into groups ( _B_ [1] _, . . . B_ _[", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_3150", "chunk_text": " that synthesizes a large set of fine-grained domains to train the model\n\n- n. Formally, our aim is to partition the training dataset _DT_ into groups ( _B_ [1] _, . . . B_ _[B]_ ) such that each\ngroup represents a self-similar pseudo-domain:\n\n\n\n\n - log _p_ ( _d | q_ ) = max\n\n_\u03d5,\u03c8_\n( _d,q_ ) _\u2208B_ _[b]_\n\n\n\n\n\n\n\n\n\n\n_b_\n\n\n\nexp _f_ ( _d,_ _q_ )\n\n - log\n\n( _d,q_ ) _\u2208B_ _[b]_ ~~\ufffd~~ ( _d_ _[\u2032]_ _,\u00b7_ ) _\u2208B_ _[b]_ [ exp] _[ f]_\n\n\n\n\n\n\n\n( _d_ _[\u2032]_ _,\u00b7_ ) _\u2208B_ _[b]_ [ exp] _[ f]_ [(] _[d][\u2032][, q]_ [)]\n\n\n\nmax\n_\u03d5,\u03c8_\n\n\n\n\n\n\n_b_\n\n\n\nComputationally, the inner term can be implemented as a single batch and computed efficiently\nwithout the need for separate hard negatives ( _H_ ). Ideally we want groups that are as challenging as\npossible. Zhang & Stratos (2021) show that increasing the partition term improves the contrastive\napproximation to the maximum likelihood of the gradient. We can formalize this search for the most\ndifficult configuration of batches as an optimization problem:\n\n\n\n\n - ( _f_ ( _d, q_ _[\u2032]_ ) + _f_ ( _d_ _[\u2032]_ _, q_ )) = ( _B_ [1] max _,...B_ _[B]_ )\n\n( _d,q_ ) _\u2208B_ _[b]_\n\n( _d_ _[\u2032]_ _,q_ _[\u2032]_ ) _\u2208B_ _[b]_\n\n\n\n\n\n\n_b_\n\n\n\n\n \n( _d,q_ ) _\u2208B_ _[b]_\n\n( _d_ _[\u2032]_ _,q_ _[\u2032]_ ) _\u2208B_ _[b]_\n\n\n\n( _\u03d5_ ( _d_ ) _\u00b7 \u03c8_ ( _q_ _[\u2032]_ ) +", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_3600", "chunk_text": "[\u2032]_ _,q_ _[\u2032]_ ) _\u2208B_ _[b]_\n\n\n\n( _\u03d5_ ( _d_ ) _\u00b7 \u03c8_ ( _q_ _[\u2032]_ ) + _\u03d5_ ( _d_ _[\u2032]_ ) _\u00b7 \u03c8_ ( _q_ ))\n\n\n(2)\n\n\n\nmax\n( _B_ [1] _,...B_ _[B]_ )\n\n\n\n\n\n\n_b_\n\n\n\nSolving this combinatorial objective exactly is intractable, but we can approximate a solution using\nclustering. We first move from a maximization to a minimization by replacing the two dot products\nwith L2 distance _m_ (( _d, q_ ) _,_ ( _d_ _[\u2032]_ _, q_ _[\u2032]_ )) = _||\u03d5_ ( _d_ ) _\u2212_ _\u03c8_ ( _q_ _[\u2032]_ ) _||_ + _||\u03d5_ ( _[\u2032]_ _d_ ) _\u2212_ _\u03c8_ ( _q_ ) _||_ (which is equivalent for\nnormalized embeddings). We then note when that treated as symmetric pairs, this term obeys the\ntriangle inequality for any other pair _m_ :\n\n\n_m_ (( _d, q_ ) _, m_ ) + _m_ ( _m,_ ( _d_ _[\u2032]_ _, q_ _[\u2032]_ )) _\u2265_ _m_ (( _d, q_ ) _,_ ( _d_ _[\u2032]_ _, q_ _[\u2032]_ ))\n\n\n4\n\n\nThis implies that the following centroid-based objective represents an upper-bound on our original\n\n- bjective:\nmin             -             - _m_ (( _d, q_ ) _, m_ _[b]_ ) (3)\n(( _mB_ [1][1] _,...,m,...B_ _[B][B]_ )) _b_ ( _d,q_ ) _\u2208B_ _[b]_\n\nFor known _B_, this search defines an asymmetric K-Means clustering problem. A solution can be\nefficiently computed using extremely fast Euclidean K-Means packages be treating each data point as\ntwo separate vectors _\u03d5_ ( _d_ ) _\u2295_", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_4050", "chunk_text": " asymmetric K-Means clustering problem. A solution can be\nefficiently computed using extremely fast Euclidean K-Means packages be treating each data point as\ntwo separate vectors _\u03d5_ ( _d_ ) _\u2295_ _\u03c8_ ( _q_ ) and _\u03c8_ ( _q_ ) _\u2295_ _\u03d5_ ( _d_ ), where _\u2295_ is concatenation.\n\n\n**Cluster Embeddings.** Since clustering is performed before training, we do not have dense encoders\n_\u03d5_ and _\u03c8_ when constructing the groups. Borrowing methods from hard-negative mining (Robinson\net al., 2021) we can replace the _\u03d5_ and _\u03c8_ with a simpler embedding model when constructing groups.\nWe experiment with a sparse vector representation and with pretrained dense representations, settling\n\n- n GTR (Ni et al., 2021), a popular and generic text embedding model.\n\n\n**Filtering False Negatives.** Our method is especially sensitive to false negatives, as they will be\nmore likely to be included in a given batch. Unfortunately, traditional retrieval datasets are not\ndesigned with this type of global objective in mind: false negatives are common in most retrieval\ndatasets and their prevalence increases with dataset scale. As one datapoint, Qu et al. (2021) found\nthat over 70% of top-retrieved passages in MS Marco are false negatives.\n\n\nTo avoid a situation where each batch contains a large number of false negatives, we compute an\nequivalence class: _S_ ( _q, d_ ) = _{d_ _[\u2032]_ _\u2208D | f_ ( _q, d_ _[\u2032]_ ) _\u2265_ _f_ ( _q, d_ ) + _\u03f5}_ for some surrogate scoring function\n_f_ and boundary term _\u03f5_ . At training time, we alter the partition function for _d_ so that it no longer\nincludes the elements of _S_ ( _q, d_ ), which are not definitively negative examples:\n\n\nexp _f_ ( _d,_ _q_ )\nlog _p_ ( _d | q_ ) = (4)\nexp _f_ ( _d, q_ ) + ~~[\ufffd]~~ _d_ _[\u2032]_ _\u2208/S", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_4500", "chunk_text": ",_ _q_ )\nlog _p_ ( _d | q_ ) = (4)\nexp _f_ ( _d, q_ ) + ~~[\ufffd]~~ _d_ _[\u2032]_ _\u2208/S_ ( _q,d_ ) [exp] _[ f]_ [(] _[d][\u2032][, q]_ [)]\n\n\nFor simplicity, we again select _f_ to be a simple pre-trained embedding model. This method likely\n\n- ver-prunes some potential true negatives found by the surrogate model; however we found it to be\ncritical to model accuracy.\n\n\n**Packing.** Clusters found by our algorithm will be of varying sizes, and need to be packed into\nequal-sized batches. We apply a post-hoc procedure. We consider both random partitioning and\ngrouping via greedy cluster-level traveling salesman, similar to Shi et al. (2024). In both cases, we\nsplit large group into into smaller batches, and merge close small batches from within the same\ndomain into evenly-sized batches. This has an added benefit of introducing randomness into the\ngroups when training for multiple epochs. We leave it to future work to analyze the full effects of\ndifferent packing strategies such as expensive Balanced K-Means or heuristic approaches such as\nEqual K-Means (Gururangan et al., 2023).\n\n\n4.2 CONTEXTUAL DOCUMENT EMBEDDING (CDE)\n\n\nContextualization can also be added directly to the architecture. Taking inspiration from sparse vector\nretrieval which uses corpus statistics to determine the form of the embedding, we modify the encoders\nto have access to the corpus itself, i.e. _\u03d5_ ( _d_ ; _D_ ) and _\u03c8_ ( _d_ ; _D_ ). This effectively augments the biencoder\nmodel to give it the ability to contextualize documents directly.\n\n\nThe main challenge is how to design a neural architecture that can take into account dataset contextualization. On one extreme, we could follow methods like BM25 and precompute a fixed set of\ncorpus statistics that could be fed to the document encoder. On the other extreme, we could allow the\nencoder full access to the entire corpus, through some form of cross attention. The latter approach\nhas been explored on a small scale in methods like neural processes (Garnelo et al., 2018); however", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_4950", "chunk_text": " extreme, we could allow the\nencoder full access to the entire corpus, through some form of cross attention. The latter approach\nhas been explored on a small scale in methods like neural processes (Garnelo et al., 2018); however,\nit would be difficult to scale to larger datasets.\n\n\nWe opt for a middleground that allows the model to learn corpus statistics, but is also relatively\nefficient to compute, shown in Figure 1. Specifically, we note that document embeddings retain\na surprising amount of lexical information even after embedding (Morris et al., 2023). Therefore,\nif we pre-embed a subset of the corpus, we believe we can still dynamically calculate key dataset\ninformation during encoding.\n\n\n5\n\n\n\n\n\n\n\n\n - _m_ (( _d, q_ ) _, m_ _[b]_ ) (3)\n\n( _d,q_ ) _\u2208B_ _[b]_\n\n\n\n\n\n\n\n_b_\n\n\nWe produce contextualized embeddings via a two-stage process:\n\n\n_**First stage:**_ _Gather and embed context._ Given context documents _d_ [1] _, ..., d_ _[J]_ _\u2208D_, we embed each\nusing a unique embedding model and concatenate embeddings into a sequence _M_ 1( _d_ [1] ) _...M_ 1( _d_ _[J]_ ).\n\n\n_**Second stage:**_ _Embed document with additional context tokens._ To compute _\u03d5_ for document _d_ _[\u2032]_ we\nintegrate contextual embedding sequence at the input of second-stage embedding model _M_ 2:\n\n\n_\u03d5_ ( _d_ _[\u2032]_ ; _D_ ) = _M_ 2( _M_ 1( _d_ [1] ) _, . . ., M_ 1( _d_ _[J]_ ) _, E_ ( _d_ _[\u2032]_ 1 [)] _[, . . ., E]_ [(] _[d][\u2032]_ _T_ [))] (5)\n\n\nHere _M_ 1 is the first-stage encoder model, _M_ 2 is a second-stage encoder model, and _E_ is the token\nembedding matrix of _M_ 2 applied to each token in _d_ _[\u2032]_ . In practice, we parameterize both _M_ 1", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_5400", "chunk_text": "2 is a second-stage encoder model, and _E_ is the token\nembedding matrix of _M_ 2 applied to each token in _d_ _[\u2032]_ . In practice, we parameterize both _M_ 1 and\n_M_ 2 using traditional bidirectional transformers, so our model is comprised of two biencoder-like\nbackbones called in sequence.\n\n\nThere is a similar contextualized model for the query encoder _\u03c8_ which is also given document context\n(as we do not have query context at test time):\n\n\n_\u03d5_ ( _q_ ; _D_ ) = _M_ 2( _M_ 1( _d_ [1] ) _, . . ., M_ 1( _d_ _[J]_ ) _, E_ ( _q_ 1) _, . . ., E_ ( _qT_ )) (6)\n\n\nWe note several implementation properties of this architecture. During training, computing contextual\nembeddings for each contextual document for each training instance would naively increase training\nby a computational factor proportional to _J_, the number of documents in context. This time increase\nwould not be tractable, since contrastive training can already take many days. We overcome this\ndifficulty by sharing context _d_ [1] _, ..., d_ _[J]_ within a batch of documents; this allows us to compute\nrepresentations just once per training step and reuse them between documents via computational\ngraph. [1]\n\n\nWhen indexing a new corpus _D_, first stage representations _M_ 1( _d_ [1] ) _...M_ 1( _d_ _[J]_ ) can be computed once\nand cached, so _M_ 1 does not add parameters or runtime to the search process. Query representations\ncan also use the cached context, which only require additional inputs to the encoder. (Our model does\nnot include contextualized queries, only documents, as we typically do not assume access to example\nqueries at test-time.)\n\n\n**Embedding** _**without**_ **context.** Individual corpora during training may not have sufficient or available\ncontext. To improve our model\u2019s generalization, we use _sequence dropout_, where we randomly replace\ncontext embeddings _M_ 1( _d_ _[\u2217]_ ) with some null token _v\u2205_", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_5850", "chunk_text": " available\ncontext. To improve our model\u2019s generalization, we use _sequence dropout_, where we randomly replace\ncontext embeddings _M_ 1( _d_ _[\u2217]_ ) with some null token _v\u2205_ according to some a uniform probability _p_ .\n\n\nAt test time, if no corpus information is available, our model can now function as a non-contextual\nbiencoder simply by replacing all sequence token inputs with _v\u2205_ .\n\n\n**Position-agnostic embedding.** Since documents of _D_ are unordered, we remove all positionality\nfrom the neural encodings. When parameterizing _\u03b8_ with a traditional transformer, this can be\nachieved by omitting positional embeddings at the positions corresponding to _D_ . In practice, we use\ntransformers implementations dependent on FlashAttention with rotary positional embeddings at\neach self-attention layer. Full details of how we disable positionality are available in Section 10.4.\n\n\n**Two-stage gradient caching.** To improve training we employ a gradient-caching technique analogous to a two-stage version of GradCache (Gao et al., 2021). This technique allows us to fit larger\nbatches, longer sequences with more contextual samples without running out of memory. Essentially,\nwe compute first-stage and second-stage representations independently without gradients. We then\nuse these frozen representations to compute the loss, and gradients with respect to the second-stage\nrepresentations. We then re-run the second stage with gradients enabled and use the output gradients\nto backpropagate through the second-stage model, and obtain gradients for the first-stage representations. We repeat this process for the first-stage representations. This allows us to tradeoff computation\n(running each transformer forward pass twice) for memory.\n\n\n1Context reuse is only feasible because documents within the same batch typically share a large amount of\ncontext anyway, since they are clustered.\n\n\n6\n\n\n5 EXPERIMENTAL SETUP\n\n\nWe consider a range of retrieval experiments across different scales. To run experiments across a suitable number of settings, we devise a small setting: six-layer transformer, maximum sequence length\n\n- f 64, and maximum number of 64 additional contextual tokens. In this scenario, we evaluate on a\ntruncated version of the BEIR benchmark (Thakur et al., 2021). Given the low cost of each experiment,\nwe are able to pre-train and fine-tune both biencoder and contextual models", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_6300", "chunk_text": ", we evaluate on a\ntruncated version of the BEIR benchmark (Thakur et al., 2021). Given the low cost of each experiment,\nwe are able to pre-train and fine-tune both biencoder and contextual models across a variety of batch\nsizes in _{_ 256 _,_ 512 _,_ 1024 _,_ 2048 _,_ 4096 _}_ and cluster sizes _{_ 64 _,_ 256 _,_ 1024 _,_ 4096 _, ...,_ 2097152 _,_ 4194304 _}_ .\nAs typical state-of-the-art text embedding models are trained in two phases, a large weakly-supervised\npre-training phase and a short supervised phase, we run all experiments for both phases.\n\n\nFor the large setting, we use the best settings found via small experiments. We train a single model\n\n- n sequences of length 512 with 512 contextual documents, evaluating on the full MTEB benchmark\n(Muennighoff et al., 2022). This includes tasks from retrieval as well as tasks like classification,\nclustering, and reranking. We compare our model\u2019s performance to the top small-size (under 250M\nparameters) models on MTEB (Nussbaum et al., 2024; Xiao et al., 2024; Solatorio, 2024; Li et al.,\n2023).\n\n\n**Training Data and Metrics** We train on the meta-datasets collected in Nussbaum et al. (2024) for\ntraining text embedding models. This collection of datasets includes data from 24 datasets scraped\nfrom web sources such as Wikipedia and Reddit. Our unsupervised training phase trains on 200M\nweakly-supervised datapoints scraped from large internet sources such as Reddit and Wikipedia.\nThe supervised training phase includes 1.8M human-written query-document pairs intended for text\nretrieval, and is aggregated from popular retrieval datasets such as HotpotQA and MS MARCO (Yang\net al., 2018; Bajaj et al., 2018). For our full model, we also consider supervised training on the BGE\nmeta-datasets (Xiao et al., 2024). We evaluate our models using NDCG@10, a conventional retrieval\nmetric that enables comparison across many disparate datasets.\n\n\n**Implementation** When partitioning our dataset into batches, we encode documents and queries\n", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_6750", "chunk_text": "Xiao et al., 2024). We evaluate our models using NDCG@10, a conventional retrieval\nmetric that enables comparison across many disparate datasets.\n\n\n**Implementation** When partitioning our dataset into batches, we encode documents and queries\nusing GTR (Ni et al., 2021) and implement our clustering algorithm on top of FAISS (Douze et al.,\n2024). We cluster per-domain for 100 steps and take the best clustering out of 3 attempts. We\nselect NomicBERT as our pre-trained model backbone (Nussbaum et al., 2024), which has 137M\nparameters. We prepend all texts with short task-specific prefixes to identify each task; prefixes are\nlisted in Section 10.7. When pooling, we pool over text tokens only, never contextual tokens.\n\n\n**Training** We initialize both _M_ 1 and _M_ 2 using the BERT-base model from Nussbaum et al. (2024)\nthat includes flash attention. Weights are shared between _\u03d5_ and _\u03c8_, but notably not between _M_ 1\nand _M_ 2. For all experiments, we train with the Adam optimizer with 1000 steps of warmup to a\nlearning rate of 2 _\u00b7_ 10 _[\u2212]_ [5] and linearly decay to 0 throughout training. For the filtering model we select\nnomic-embed-v1 which was trained on the same datasets (Nussbaum et al., 2024). We train for\nthree epochs unless otherwise specified. We set the maximum sequence length for all inputs to 512\nand the number of contextual inputs to 512 (so the second-stage model has an input length of 1024).\nWhen computing contrastive loss, we use a fixed temperature of _\u03c4_ = 0 _._ 02. When sequence dropout is\nenabled in our contextual architecture, we set contextual input tokens to null vectors with a uniform\nprobability _p_ = 0 _._ 005. If the batch size exceeds the number of contextual documents, we randomly\nsample to produce contextual inputs.\n\n\n6 RESULTS\n\n\nThe main results are highlighted in Table 1 and Section 6. In the smaller setting, we observe that both\nadversarial contrastive learning and our contextual architecture improve performance compared to\nvanilla biencoder training. We observe the largest improvement when we", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_7200", "chunk_text": " results are highlighted in Table 1 and Section 6. In the smaller setting, we observe that both\nadversarial contrastive learning and our contextual architecture improve performance compared to\nvanilla biencoder training. We observe the largest improvement when we combine these techniques.\n\n\n**Contextual batching** After controlling for batch size and filtering for false negatives, we observe a\nstrong correlation (visualized in Figure 2) between batch difficulty and downstream performance:\n_reordering datapoints to make batches harder definitively enhances overall learning_ . This corrob\n- rates prior findings (Xiong et al., 2020; Qu et al., 2021) and theory (Zhang & Stratos, 2021) that\n\n\n7\n\n\nContextual\n\nBatch Arch Batch Size Cluster Size Train loss Train acc. NDCG@10\n\n\n16384            - 0.39 90.3 59.9\n\n\u2713 512 512 0.81 77.7 61.7\n\n\u2713 16384      - 0.37 90.7 62.4\n\n\u2713 \u2713 512 512 0.68 80.9 **63.1**\n\n\nTable 1: Performance of our small models with and without the two improvements proposed in this\npaper, measured on a shortened version of the BEIR benchmark. Numbers are NDCG@10.\n\n\nClssfctn Cluster PairCls Rerank Retrvl STS Summ. Mean\n\n\nnomic-embed-v1 74.1 43.9 85.2 55.7 52.8 82.1 30.1 62.39\n\nstella-base-en-v2 75.3 44.9 86.5 58.8 50.1 83.0 32.5 62.61\nbge-base-en-v1.5 75.5 45.8 86.6 58.9 53.3 82.4 31.1 63.56\nGIST-Embedding-v0 76.0 46.2 86.3 59.4 52.3 83.5 30.9 63.71\ngte-base-en-v1.5 77.2 46.8 85.3 57.7 54.1 82.0 31.", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_7650", "chunk_text": "52.3 83.5 30.9 63.71\ngte-base-en-v1.5 77.2 46.8 85.3 57.7 54.1 82.0 31.2 64.11\n\n\ncde-small-v1\n\n[Random] 81.3 46.6 84.1 55.3 51.1 81.4 31.6 63.81\n\n[Contextual] 81.7 48.3 84.7 56.7 53.3 81.6 31.2 **65.00**\n\n\nTable 2: Performance of models with 250M or fewer parameters on the MTEB benchmark for\ntext embedding models. \u201cRandom\u201d indicates the performance of our model with random training\ndocuments included instead of per-domain contextual documents.\n\n\nmore difficult batches in contrastive learning form a better overall gradient approximation and learn\nmore effectively.\n\n\nSection 6 showcases model performance across batch and cluster sizes after both phases of training.\nWe observe that although a large batch and cluster size are useful when filtering is not enacted, when\nincluding filtering, smaller cluster (and harder) are clearly better, and large batches do not add much.\nWhen comparing filtered to non-filtered models (Figure 4), filtering false negatives clearly improves\nperformance.\n\n\n**Contextual architecture** In addition to adversarial batching, we compare our contextual architecture to a biencoder across the datasets of BEIR in Table 1 (full results in appendix). Our architecture\ngenerally matches or improves performance on all downstream datasets, with largest improvements\nin ArguAna and SciFact, two of the smaller and more out-of-domain datasets.\n\n\n**Full-scale training** Figure 5 shows our models\u2019 performance when trained for multiple epochs on\nthe supervised datasets, relative to the best similar-sized embedding model (dashed line). We find\nbest performance when training for four epochs on the BGE meta-datasets. Although our best model\ndoes use a single hard negative per query, we are still able to to achieve state-of-the-art performance\nwithout using _any_ hard negatives.\n\n\nFor our final model (cde-small-v1), we select the best of the supervised models, which comes\nfrom finetuning on the BGE dataset. On MTEB, cde-small-v1 obtains state-of-the-art results\ncompared to models", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_8100", "chunk_text": " (cde-small-v1), we select the best of the supervised models, which comes\nfrom finetuning on the BGE dataset. On MTEB, cde-small-v1 obtains state-of-the-art results\ncompared to models of the same size. Although inspired by problems in the specific domain of text\nretrieval, we observe that our approach improves embedding performance in all domains, including\nclustering, classification, and semantic similarity. We also evaluate a \u201crandom documents\u201d baseline,\nwhere we sample random documents from the training dataset to simulate a scenario where we lack\naccess to the test corpus. In this setting, we drop around 1 _._ 2 points on average across all tasks; the\nSTS tasks in particular appear to produce representations that are close to context-agnostic.\n\n\n8\n\n\nFigure 2: Performance vs. average batch difficulty (as measured by loss at the end of pre-training and\nsupervised training) across batch sizes, after supervised contrastive training. Within a given batch\nsize, we observe a clear increase in performance by making individual batches harder. Correlations\nare Pearson.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|Scor|Col2|re (NDCG@1|Col4|10), fil|lter|red|Col8|\n|---|---|---|---|---|---|---|---|\n|Sco|Sco|re (NDCG@|re (NDCG@|10), fi|lte|red|red|\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n|||||||||\n\n\n|Sco|ore (NDCG@10|Col3|0), unf|filter|red|\n|---|---|---|---|---|---|\n|Sc|ore (NDCG@1|ore (NDCG@1|0), un|filte|red|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\nFigure 3: Biencoder performance with filtering (left) and without (right) across batch and cluster\nsizes during unsupervised contrastive pre-training. With filtering, small cluster sizes clearly improve\nperformance, and larger batch sizes do not.\n\n\n7 ANALYSIS\n\n\n**How hard are our clusters?** To analysis the relationship between cluster size in our clustering\nalgorithm and the overall average", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_8550", "chunk_text": "ive pre-training. With filtering, small cluster sizes clearly improve\nperformance, and larger batch sizes do not.\n\n\n7 ANALYSIS\n\n\n**How hard are our clusters?** To analysis the relationship between cluster size in our clustering\nalgorithm and the overall average difficulty of in-batch negatives, we measure the average difficulty\n\n- f 1000 batches across a variety of batch and cluster sizes and plot the data in Figure 6. We observe\n\n\n9\n\n\nFigure 4: Impact of filtering during training\nacross various batch and cluster sizes. Each dot\nis a biencoder pretrained with a different batch\nand cluster size.\n\n\nFigure 6: Average difficulty of in-batch negatives\nas measured by a surrogate model as cluster size\nand batch size change.\n\n\n\nFigure 5: Performance on MTEB across epochs\n\n- f supervised training on the Nomic and BGE\nsupervised meta-datasets.\n\n\nFigure 7: Impact of context by testing our\nmodel with different Stackexchange forum\ninput types. Y-axis indicates the input domain, X-axis indicates the test domain. Dark\nsquares come within one point NDCG@10.\n\n\n\nthat larger batches bring easier non-negative examples, and decreasing cluster size clearly increases\nthe average hardness of negative examples in a given cluster.\n\n\n**Which contextual documents help?** To confirm that the CDE model is utilizing contextual\ninformation from _D_ we consider how different contextual documents help for a given docuent _d_ .\nFigure 7 measures results on CQADupstack, a collection of Stack Exchange forum posts. We\nrandomly sample inputs to from _D_ from a domain (x-axis) and use them as input to the downstream\ntask _d_ marked along the y-axis. We mark a square as red if its score comes within 1 point of NDCG of\nthe top score for its domain. Generally utilizing in-domain works best, but there are some crossover\ninteractions.\n\n\n8 CONCLUSION\n\n\nWe propose two improvements to traditional biencoder models for generating embeddings. The\nfirst improvement involves an algorithm for reordering training datapoints to make batches harder\n\n\n10\n\n\nand improves vanilla training with minimal changes. Our second improvement involves a new\ncorpus-aware architecture for retrieval and allows us to train a state-of-the-art text embedding model.\n\n\n9 ACKNOWLEDGEMENTS\n\n\nThanks to Orion Weller, Vin Sachidananda, and Zach Nussbaum for valuable feedback on this\nresearch", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_9000", "chunk_text": "corpus-aware architecture for retrieval and allows us to train a state-of-the-art text embedding model.\n\n\n9 ACKNOWLEDGEMENTS\n\n\nThanks to Orion Weller, Vin Sachidananda, and Zach Nussbaum for valuable feedback on this\nresearch. We would also like to acknowledge to Nomic and Hyperbolic for providing the compute\nnecessary to conduct this research. This work was partially supported by Intelligence Advanced\nResearch Projects Activity (IARPA), via the HIATUS Program #2022-22072200003. JM is supported\nby an NSF GFRP fellowship.\n\n\n11\n\n\nREFERENCES\n\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica,\nSaurabh Tiwary, and Tong Wang. Ms marco: A human generated machine reading comprehension\n[dataset, 2018. URL https://arxiv.org/abs/1611.09268.](https://arxiv.org/abs/1611.09268)\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego\nde Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren\nMaggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol\nVinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. Improving\nlanguage models by retrieving from trillions of tokens, 2022.\n\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations, 2020.\n\n\nWilliam Coster and David Kauchak. Simple English Wikipedia: A new text simplification task.\nIn Dekang Lin, Yuji Matsumoto, and Rada Mihalcea (eds.), _Proceedings of the 49th Annual_\n_Meeting of the Association for Computational Linguistics: Human Language Technologies", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_9450", "chunk_text": " text simplification task.\nIn Dekang Lin, Yuji Matsumoto, and Rada Mihalcea (eds.), _Proceedings of the 49th Annual_\n_Meeting of the Association for Computational Linguistics: Human Language Technologies_, pp.\n665\u2013669, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL\n[https://aclanthology.org/P11-2117.](https://aclanthology.org/P11-2117)\n\n\nZhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu,\nKeith B. Hall, and Ming-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples,\n2022.\n\n\nMatthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel\nMazar\u00b4e, Maria Lomeli, Lucas Hosseini, and Herv\u00b4e J\u00b4egou. The faiss library, 2024.\n\n\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. Open Question Answering Over Curated and\nExtracted Knowledge Bases. In _KDD_, 2014.\n\n\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nlong form question answering. In Anna Korhonen, David R. Traum, and Llu\u00b4\u0131s Marquez (eds.),`\n_Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019,_\n_Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers_, pp. 3558\u20133567. Association for\n[Computational Linguistics, 2019. doi: 10.18653/v1/p19-1346. URL https://doi.org/10.](https://doi.org/10.18653/v1/p19-1346)\n[18653/v1/p19-1346.](https://doi.org/10.18653/v1/p19-1346)\n\n\nKatja Filippova and Yasemin Altun. Overcoming the lack of parallel data in sentence compression.\nIn David Yarowsky, Timothy Baldwin,", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_9900", "chunk_text": "](https://doi.org/10.18653/v1/p19-1346)\n\n\nKatja Filippova and Yasemin Altun. Overcoming the lack of parallel data in sentence compression.\nIn David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard (eds.),\n_Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_, pp.\n1481\u20131491, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\n[URL https://aclanthology.org/D13-1155.](https://aclanthology.org/D13-1155)\n\n\n[Wikimedia Foundation. Wikimedia downloads, 2024. URL https://dumps.wikimedia.org.](https://dumps.wikimedia.org)\n\n\nLuyu Gao and Jamie Callan. Unsupervised corpus aware language model pre-training for dense\npassage retrieval, 2021.\n\n\nLuyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. Scaling deep contrastive learning batch size\nunder memory limited setup, 2021.\n\n\nMarta Garnelo, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, and S. M. Ali Eslami. Conditional neural processes,\n2018.\n\n\nMansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam, and Zachary C Lipton.\nAmazonqa: A review-based question answering task, 2019.\n\n\nSuchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A. Smith, and Luke\nZettlemoyer. Scaling expert language models with unsupervised domain discovery, 2023.\n\n\n12\n\n\nFelix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. news-please: A generic news\ncrawler and extractor. In _Proceedings of the 15th International Symposium of Information Science_,\npp. 218\u2013223, March 2017. doi: 10.5281/zenodo.4120316.\n\n\nChristopher Hidey and Kathy McKeown. Identifying causal relations using parallel Wikipedia\narticles. In Katrin Erk and Noah A. Smith", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_10350", "chunk_text": ", March 2017. doi: 10.5281/zenodo.4120316.\n\n\nChristopher Hidey and Kathy McKeown. Identifying causal relations using parallel Wikipedia\narticles. In Katrin Erk and Noah A. Smith (eds.), _Proceedings of the 54th Annual Meeting of_\n_the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 1424\u20131433, Berlin,\nGermany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1135.\n[URL https://aclanthology.org/P16-1135.](https://aclanthology.org/P16-1135)\n\n\nSebastian Hofstatter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. Efficiently\u00a8\n[teaching an effective dense retriever with balanced topic aware sampling, 2021. URL https:](https://arxiv.org/abs/2104.06967)\n[//arxiv.org/abs/2104.06967.](https://arxiv.org/abs/2104.06967)\n\n\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt.\nCodeSearchNet challenge: Evaluating the state of semantic code search. _arXiv preprint_\n_arXiv:1909.09436_, 2019.\n\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. Unsupervised dense information retrieval with contrastive learning, 2022.\n\n\nMinguk Jang, Sae-Young Chung, and Hye Won Chung. Test-time adaptation via self-training with\nnearest neighbor information, 2023.\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\u02d8\nChen, and Wen tau Yih. Dense passage retrieval for open-domain question answering, 2020.\n\n\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough memorization: Nearest neighbor language models, 2020.\n\n\nDaniel", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_11700", "chunk_text": " the 2019 Conference on Empirical Methods in Natural Language Processing and_\n_the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pp.\n188\u2013197, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:\n[10.18653/v1/D19-1018. URL https://aclanthology.org/D19-1018.](https://aclanthology.org/D19-1018)\n\n\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez\u00b4 Abrego, Ji Ma, Vincent Y. Zhao, [\u00b4]\nYi Luan, Keith B. Hall, Ming-Wei Chang, and Yinfei Yang. Large dual encoders are generalizable\nretrievers, 2021.\n\n\nRodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert, 2020.\n\n\nZach Nussbaum, John X. Morris, Brandon Duderstadt, and Andriy Mulyar. Nomic embed: Training\na reproducible long context text embedder, 2024.\n\n\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua\nWu, and Haifeng Wang. Rocketqa: An optimized training approach to dense passage retrieval for\n\n - pen-domain question answering, 2021.\n\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. _arXiv e-prints_, art. arXiv:1606.05250, 2016.\n\n\nNils Reimers, Elliot Choi, Amr Kayid, Alekhya Nandula, Manoj Govindassamy, and Abdullah Elkady. Introducing embed v3, Nov 2023. [URL https://txt.cohere.com/](https://txt.cohere.com/introducing-embed-v3/)\n[introducing-embed-v3/.](https://txt.cohere.com/introducing-embed-v3/)\n\n\nStephen Robertson and Hugo Zaragoza. _The Probabilistic Relevance Framework: BM25 and Beyond_ .\nNow Publishers Inc., 2009.\n\n\nJoshua Robinson, Ching-Yao Chuang, Suv", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_12150", "chunk_text": "-embed-v3/)\n\n\nStephen Robertson and Hugo Zaragoza. _The Probabilistic Relevance Framework: BM25 and Beyond_ .\nNow Publishers Inc., 2009.\n\n\nJoshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learning with\nhard negative samples, 2021.\n\n\nJ. J. Rocchio. Relevance feedback in information retrieval. 1971. [URL https://api.](https://api.semanticscholar.org/CorpusID:61859400)\n[semanticscholar.org/CorpusID:61859400.](https://api.semanticscholar.org/CorpusID:61859400)\n\n\nVin Sachidananda, Ziyi Yang, and Chenguang Zhu. Global selection of contrastive batches via optimization on sample permutations. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), _Proceedings of the 40th International_\n_Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pp.\n[29542\u201329562. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.press/v202/](https://proceedings.mlr.press/v202/sachidananda23a.html)\n[sachidananda23a.html.](https://proceedings.mlr.press/v202/sachidananda23a.html)\n\n\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Colbertv2:\n[Effective and efficient retrieval via lightweight late interaction, 2022. URL https://arxiv.](https://arxiv.org/abs/2112.01488)\n\n[org/abs/2112.01488.](https://arxiv.org/abs/2112.01488)\n\n\nChristopher Sciavolino. Towards universal dense retrieval for open-domain question answering, 2021.\n\n\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\npointer-generator networks. In _Proceedings of the 55th Annual Meeting of the Association for_\n_Computational Linguistics (Volume 1: Long Papers)_, pp. 1073\u20131083, Vancouver,", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_12600", "chunk_text": " Summarization with\npointer-generator networks. In _Proceedings of the 55th Annual Meeting of the Association for_\n_Computational Linguistics (Volume 1: Long Papers)_, pp. 1073\u20131083, Vancouver, Canada, July\n[2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL https:](https://www.aclweb.org/anthology/P17-1099)\n[//www.aclweb.org/anthology/P17-1099.](https://www.aclweb.org/anthology/P17-1099)\n\n\n14\n\n\nWeijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Gergely Szilvasy, Rich James,\nXi Victoria Lin, Noah A. Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. In-context\npretraining: Language modeling beyond document boundaries, 2024.\n\n\nAivin V. Solatorio. Gistembed: Guided in-sample selection of training negatives for text embedding\n[fine-tuning, 2024. URL https://arxiv.org/abs/2402.16829.](https://arxiv.org/abs/2402.16829)\n\n\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih,\nNoah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned\ntext embeddings, 2023.\n\n\nMujeen Sung, Jungsoo Park, Jaewoo Kang, Danqi Chen, and Jinhyuk Lee. Optimizing test-time\nquery representations for dense retrieval, 2023.\n\n\nNandan Thakur, Nils Reimers, Andreas Ruckl\u00a8 e, Abhishek Srivastava, and Iryna Gurevych. Beir: A\u00b4\nheterogenous benchmark for zero-shot evaluation of information retrieval models, 2021.\n\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval,\n2023.\n\n\nLiang Wang, Nan Yang, Xiaolong Huang", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_13050", "chunk_text": " Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. Simlm: Pre-training with representation bottleneck for dense passage retrieval,\n2023.\n\n\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024.\n\n\nXiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. Pseudo-relevance feedback for\nmultiple representation dense retrieval. In _Proceedings of the 2021 ACM SIGIR International_\n_Conference on Theory of Information Retrieval_, ICTIR \u201921. ACM, July 2021. doi: 10.1145/\n[3471158.3472250. URL http://dx.doi.org/10.1145/3471158.3472250.](http://dx.doi.org/10.1145/3471158.3472250)\n\n\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack:\n[Packaged resources to advance general chinese embedding, 2024. URL https://arxiv.org/](https://arxiv.org/abs/2309.07597)\n[abs/2309.07597.](https://arxiv.org/abs/2309.07597)\n\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and\nArnold Overwijk. Approximate nearest neighbor negative contrastive learning for dense text\n[retrieval, 2020. URL https://arxiv.org/abs/2007.00808.](https://arxiv.org/abs/2007.00808)\n\n\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Laprador: Unsupervised pretrained dense\nretriever for zero-shot text retrieval, 2022.\n\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\n[ans", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_13500", "chunk_text": " Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\n[answering, 2018. URL https://arxiv.org/abs/1809.09600.](https://arxiv.org/abs/1809.09600)\n\n\nWenzheng Zhang and Karl Stratos. Understanding hard negatives in noise contrastive estimation,\n2021.\n\n\n15\n\n\n10 SUPPLEMENTARY MATERIAL\n\n\n10.1 COMPUTATIONAL RESOURCE USAGE\n\n\nWe pre-train all models on 8 NVIDIA H100 GPUs. In the slowest setting, training a biencoder for\na single unsupervised epoch (235M pairs) takes approximately one day. Training our contextual\narchiecture for a single epoch takes approximately two days. Shorter sequence-length experiments\nare 10-20x faster, and can be run on a single GPU.\n\n\n10.2 INITIAL EXPERIMENTS\n\n\nWe conducted two preliminary experiments to verify (i) the need for contextual training strategy and\n(ii) the need for in-batch false negative filtering when doing adversarial contrastive learning on a real\ndataset.\n\n\n**Preliminary experiment (i).** We conduct a preliminary experiment to verify this issue. Starting\nfrom several trained retrieval systems we compute performance on a variety of different tasks from\nthe BEIR dataset. Additionally we compute the IDF statistics from the datasets, and compare the\ndivergence from the base IDF statistics of the training set. Figure 8 shows that datasets with highdivergence have very high correlation with the accuracy degradation of models when measured in\ncomparison to BM25, which is able to measure and adapt to statistics of the test corpus.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|0.20 webis-touche2020 GenQ|webis-touche2020 GenQ|Col3|Col4|\n|---|---|---|---|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br> <br>0.15<br>0.10<br>0.05<br>0.00<br>0.05<br>0.10<br>0.15<br>.<br> - enQ<br>argua<br>climate~~", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_13950", "chunk_text": ">0.15<br>0.10<br>0.05<br>0.00<br>0.05<br>0.10<br>0.15<br>.<br> - enQ<br>argua<br>climate~~-~~fever<br>fever<br>fiqa<br>hotpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|argua<br>climate~~-~~fever<br>fever<br>fiqa<br>hotpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|argua<br>climate~~-~~fever<br>fever<br>fiqa<br>hotpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|na<br>nfcorp<br>cifact|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br> <br>0.15<br>0.10<br>0.05<br>0.00<br>0.05<br>0.10<br>0.15<br>.<br> - enQ<br>argua<br>climate~~-~~fever<br>fever<br>fiqa<br>hotpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|argua<br>climate~~-~~fever<br>fever<br>fiqa<br>hotpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|0.8<br>1.0<br>1.2<br>|0.8<br>1.0<br>1.2<br>|\n\n\n|0.0 0.2 0.4 0.6 0.", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_14400", "chunk_text": "0.8<br>1.0<br>1.2<br>|0.8<br>1.0<br>1.2<br>|\n\n\n|0.0 0.2 0.4 0.6 0.8 1.0 1.2 IDF Distance from MSMARCO 0.20 webis-touche2020 SPARTA|0.0 0.2 0.4 0.6 0.8 1.0 1.2 IDF Distance from MSMARCO webis-touche2020 SPARTA|Col3|1e7|\n|---|---|---|---|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br> <br>0.10<br>0.05<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>BM25- SPARTA<br>argua<br>climate~~-~~fever<br>fever<br>fiqa<br>~~h~~otpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|argua<br>climate~~-~~fever<br>fever<br>fiqa<br>~~h~~otpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|argua<br>climate~~-~~fever<br>fever<br>fiqa<br>~~h~~otpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|na<br>nfcorp<br>cifact|\n|0.0<br>0.2<br>0.4<br>0.6<br>0.8<br>1.0<br>1.2<br> <br>0.10<br>0.05<br>0.00<br>0.05<br>0.10<br>0.15<br>0.", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_14850", "chunk_text": "0.8<br>1.0<br>1.2<br> <br>0.10<br>0.05<br>0.00<br>0.05<br>0.10<br>0.15<br>0.20<br>BM25- SPARTA<br>argua<br>climate~~-~~fever<br>fever<br>fiqa<br>~~h~~otpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|argua<br>climate~~-~~fever<br>fever<br>fiqa<br>~~h~~otpotqa<br>msmarco<br>nq<br>quora<br>scidocs<br>s<br>trec~~-~~covid<br>webis~~-~~touche2020|0.8<br>1.0<br>1.2<br>|0.8<br>1.0<br>1.2<br>|\n\n\n|Col1|na<br>cinffaccotrp|\n|---|---|\n|0.8<br>1.0<br>1.2<br>|0.8<br>1.0<br>1.2<br>|\n\n\n\nFigure 8: Analysis of domain shift for popular neural retrieval methods. Performance difference\nfrom BM25 (y-axis) correlates with the different in IDF of the test corpus _D_ form the training corpus\n_DT_ .\n\n\n**Preliminary experiment (ii).** We select a random document from an unsupervised corpus and\nlook at its nearest neighbors, displayed in Table 3. We observe that the nearest neighbors to a given\ndocument in a large corpus are very close; in fact, many of them could be considered valid documents\nfor the given query as well.\n\n\nThis challenge motivates our embedding contextualization. In this section, we describe two complementary methods for remediation, (a) a contextual training method, (b) a contextual encoding\nmethod.\n\n\n10.3 INTERACTIONS BETWEEN CONTRASTIVE LOSS AND DISTRIBUTED DATA PARALLEL\n\n\nThe authors note that it can be notoriously difficult to train models using both contrastive loss and\nthe distributed data parallel (DDP) setting. In particular, when aggregating samples between GPUs,\nif any artifact reveals which GPU a model came from (for", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_15300", "chunk_text": " authors note that it can be notoriously difficult to train models using both contrastive loss and\nthe distributed data parallel (DDP) setting. In particular, when aggregating samples between GPUs,\nif any artifact reveals which GPU a model came from (for example, if the GPU model weights are\ninitialized slightly differently) than the model can quickly deteriorate to a suboptimal solution, each\n\n\n16\n\n\nTable 3: Nearest-neighbors to a single query in a large unsupervised dataset.\n\nQuery Document\n\n\nlooks like my card payment was duplicated after\nall. [...]\n\n\nwhy is there an extra ~~C~~ 1 fee in my statement? why is there an extra charge on my statement?\n\n\nwhat is this fee for card payment? why was a fee charged for my card payment?\n\n\nwhy do i have duplicate transactions for one pur- why was my transaction charged twice?\nchase?\n\n\ni have two of the same charges on my account! why was my transaction charged twice?\n\n\nmy transaction went through but i was charged a why was a fee charged for my transfer?\nfee. why?\n\n\nmy account shows i have been charged twice for\nthe same meal. [...]\n\n\nwill i get extra charges? why was a fee charged for my transfer?\n\n\ni got charged in double and want a refund why was my transaction charged twice?\n\n\nwhere do i pay with my debit or credit card? why is my card not accepted?\n\n\nwhy did i get charged a fee for my card payment? why was a fee charged for my card payment?\n\n\nmy statement shows different transaction times. why was my transaction charged twice?\n\n\nGPU learning a different final model and \u201ccheating\u201d to classify samples based on which GPU they\ncame from.\n\n\nThis issue is made extra difficult by the fact that gradient-syncing must be disabled for large-batch\ncontrastive learning to work efficiently. If gradient syncing becomes totally disabled, the training\nsilently diverges as each model learns a degenerate solution. We advise practitioners to take care\nwhen controlling gradient-syncing and run many control experiments to determine performance\nequivalence between DDP and non-DDP scenarios.\n\n\nOne potential benefit of our method is that it greatly decreases the number of hard negatives required\nper batch, which means that negative-sharing across GPUs may not be necessary in most settings. If\npossible, the most sanity-preserving way to perform contrastive training could be to\n\n\n10.4 REMOVING POSITIONALITY WITH ROTARY EMBEDDINGS\n\n\nOne", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_15750", "chunk_text": " that negative-sharing across GPUs may not be necessary in most settings. If\npossible, the most sanity-preserving way to perform contrastive training could be to\n\n\n10.4 REMOVING POSITIONALITY WITH ROTARY EMBEDDINGS\n\n\nOne detail of our model architecture is that it does not track positionality between dataset input\ntokens. Although disabling positionality would be trivial an a BERT-like encoder model that uses\nlearned positional embeddings, we use a version of BERT with _rotary_ positional embeddings which\ninject positional information at each layer of the transformer. To circumvent this step, we modify the\nmodel internals to set dataset input tokens to zero for the self-attention step only, and add a residual\nconnection propagating the dataset input tokens past the attention phase.\n\n\n10.5 ADDITIONAL RESULTS\n\n\nSection 10.5 show sweeps over batch and cluster sizes under our small experimental settings when\nperforming unsupervised pretraining with contextual architecture. We see similar trends to those\n\n- bserved with the biencoder architecture, however we note that performance is higher across the\nboard and our transductive model is able to perform well even at higher cluster sizes and low batch\nsizes.\n\n\nOne confounding factor in these experiments is that since the number of contextual documents is\nfixed, the number of different contextual inputs seen during training decreases with higher batch size.\n\n\n17\n\n\n|core (|Col2|Col3|@ )<br>,|Col5|ere|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n|core ( @|Col2|Col3|), u|n ere|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\nFigure 9: Contextual performance with filtering (left) and without (right) across batch and cluster\nsizes during unsupervised contrastive pre-training. Here, clustering with small cluster sizes clearly\nimproves performance, and larger batch sizes do not.\n\n\nFigure 10: Correlation between batch difficulty and perforamnce after supervised training.\n\n\nThis might explain part of why performance stagnates with higher batch sizes; increasing", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_16200", "chunk_text": " with small cluster sizes clearly\nimproves performance, and larger batch sizes do not.\n\n\nFigure 10: Correlation between batch difficulty and perforamnce after supervised training.\n\n\nThis might explain part of why performance stagnates with higher batch sizes; increasing the batch\nsize decreases the total number of learning examples seen by our contextual model.\n\n\n**Supervised training: difficulty correlations.** In Section 10.5 we plot the correlation between\nbatch difficulty and downstream performance across cluster sizes (and within batch sizes) in the\nsupervised setting. In this case we also see the best performance through the most difficult clusters.\n\n\n18\n\n\nFigure 11: Performance of all supervised models, across numbers of hard negatives.\n\n\n\nFigure 12: Model performance vs. cluster size\nwith and without filtering. When false negative\nfiltering is enabled, we see more improvements\nin performance from clustering at small cluster\nsizes.\n\n\n\nFigure 13: Model performance vs. batch size\nwith and without filtering. With and without\nfiltering, the optimal batch size ranges between\n10 [2] and 10 [4] ; performance starts to decrease as\nbatch size grows too large.\n\n\n\n**Supervised training: full results.** We plot the full results of all supervised training experiments in\nSection 10.5. Our experiments in this setting (using the mined negatives from the Nomic supervised\nmeta-datasets) generally show _decreasing_ performance with additional hard negatives.\n\n\n**TSP Packing.** We compare randomly packing clusters into batches vs. a greedy traveling salesmanstyle solution, similar to (Shi et al., 2024). In our scenario, we first cluster datapoints, then find the\ncentroid embedding of each cluster. We begin packing by randomly selecting a cluster, and then\nchoose the next cluster by finding the cluster with the closest centroid to the current one. Results\nare shown in Figure 14. Although these results appear slightly noisy, we see an improvement from\nTSP-style packing especially at smaller cluster sizes (where packing has an outsized impact). We\ntherefore opt to use this packing procedure for our main model.\n\n\n**Impact of context size** We consider contextual embeddings might move in space as their conditioning varies. Section 10.5 displays a few qualitative examples. We generate embeddings for randomly\nsampled documents from the TREC-Covid dataset and visualize their embeddings with PCA, where\nunique document inputs with different contextual embeddings are visualized in the same", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_16650", "chunk_text": ". Section 10.5 displays a few qualitative examples. We generate embeddings for randomly\nsampled documents from the TREC-Covid dataset and visualize their embeddings with PCA, where\nunique document inputs with different contextual embeddings are visualized in the same color. By\nchanging only the conditioning we reshape the embedding space and our model produces different\nembedding for the same text. Note that although the embeddings are clearly moving in response to\nchanging the contextual inputs, they still remain closer to each other than to different documents.\n\n\n19\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|||||Rand<br>TSP|om||\n||||||||\n||||||||\n||||||||\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Pre-training with TSP vs. random batching across cluster sizes.\n\n\n\nFigure 15: Each color indicates a single document input _d_ . Different points represent different values _\u03d5_ ( _d_ ; _D_ ) for different contexts.\n\n\n\nFigure 16: Performance of CDE model as the\nnumber of contextual examples increases.\n\n\n\nWe also consider how additional context is improving our model. Because the model includes an\n\n- ptional null token, we can supply any number of contextual inputs. We plot our model\u2019s performance\nacross context sizes in Figure 10.5. We see that our model is able to utilize partial context window\nsizes, and even perform reasonably with no context (i.e. all null token inputs) but offers the best\nperformance given a full context window size.\n\n\n10.6 CLUSTER TEXT EXAMPLES\n\n\nWe include random examples from a cluster gathered from our supervised dataset, shown in Table 4.\nThis particular cluster appears to be a combination of documents about county populations in the\nUntied States (in Kentucky, Iowa, Pennsylvania, etc.) and documents about criminal trials (mentioning\nhearings, depositions, and courts).\n\n\n10.7 TASK PREFIXES\n\n\nPrefixes are hand-written for each dataset in both meta-training sets. We follow the same prefix\nselection procedure as Nussbaum et al. (2024), inspired by Reimers et al. (2023):\n\n\n   - search ~~q~~ uery\n\n   - search ~~d~~   - cument\n\n   - classification\n\n\n20\n\n\nquery document\n\n\npopulation of breckenridge mi breckenridge, michigan. breckenridge is", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_17100", "chunk_text": "2023):\n\n\n   - search ~~q~~ uery\n\n   - search ~~d~~   - cument\n\n   - classification\n\n\n20\n\n\nquery document\n\n\npopulation of breckenridge mi breckenridge, michigan. breckenridge is a village\nin gratiot county in the u. s. state of michigan. the\npopulation was 1, 328 at the 2010 census. the village\nis located in wheeler township.\ncan a deposition be used in a criminal case depositions are commonly used in civil litigation\n(suits for money damages or equitable relief) [...]\nwhat cases require strict scrutiny the strict scrutiny standard is one of three employed\nby the courts in reviewing laws and government policies. the rational basis [...]\nfunction of state supreme courts it has also initiated several programs designed to\nimprove the effectiveness of the court system. a\nprimary function of the supreme court is to ensure\n\n[...]\nwhat is the population in idaho idaho \u2019 s population grows to nearly 1. 7 million.\nidaho \u2019 s population grew by 1. 2 percent between\nmid - 2014 and mid - 2015, the 12th strongest increase among the states and four - tenths of a percentage point ahead of the national growth rate.\nwhat is the population of manson, ia manson, iowa. manson is a city in calhoun county,\niowa, united states. the population was 1, 690 at the\n2010 census.\nwhat happens after a sentencing hearing find answers. sentencing. after a criminal defendant\nis convicted or pleads guilty, a judge will decide [...]\nflathead county population flathead county, montana. flathead county is a county\nlocated in the u. s. state of montana. as of the 2010\ncensus, the population was 90, 928, making it [...]\nwhiting, ks population the city of whiting had a population of 177 as of july\n1, 2017. whiting ranks in the lower quartile for population density and diversity index when compared to\nthe other cities, towns [...]\nwhat is the population of lewiston id lewiston, id population and races. as of 2010 - 2014,\nthe total population of lewiston is 32, 178, which is\n4. 12% more than it was", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_17550", "chunk_text": " of lewiston id lewiston, id population and races. as of 2010 - 2014,\nthe total population of lewiston is 32, 178, which is\n4. 12% more than it was in 2000. [...]\nwhat happens if you don\u2019t show up for jury what happens if you don\u2019t show up for jury duty in\ncalifornia? a : according to california courts, judicial\nbranch of california, if a citizen fails to show up for\njury duty, the juror can accrue fines up to $1,500.\nif service presents an undue hardship, a juror can\nrequest a postponement or to be excused. otherwise,\ncitizens are not exempt from jury duty.\npopulation of clearfield county pa clearfield is a borough and the county seat of\nclearfield county, pennsylvania, united states. the\npopulation was 6, 215 at the 2010 census, and the\nborough is part of the dubois, pa micropolitan statistical area, as well as the larger state college - dubois,\npa combined statistical area.\nhow long can it take for a trial the preliminary hearing phase of the trial usually\ntakes place 5 - 6 days after an arraignment. in the\ncase of a misdemeanor [...]\npopulation clinton ky clinton county is a county located in the u. s. state\n\n                            - f kentucky. as of the 2010 census, the population\nwas 10, 272. its county seat is albany. the county\nwas formed in 1835 and named for dewitt clinton,\nthe seventh governor of new york. it is a prohibition\n\n                             - r dry county.\npopulation of iosco county michigan with 25, 420 people, iosco county is the 55th most\npopulated county in the state of michigan out of\n83 counties. but watch out, iosco county, because\ngladwin county with 25, 411 people and manistee\ncounty with 24, 420 people are right behind you.\n\n\nTable 4: Sixteen samples from a cluster our algorithm finds in the supervised training data. The full\ncluster size is 256 points out of a dataset of 1 _._ 5 _M_ .\n\n\n21\n\n\nTable 5: Distribution of pretraining datasets curated in Nussbaum et al. (", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_18000", "chunk_text": " the supervised training data. The full\ncluster size is 256 points out of a dataset of 1 _._ 5 _M_ .\n\n\n21\n\n\nTable 5: Distribution of pretraining datasets curated in Nussbaum et al. (2024).\nDataset Datapoints % Dataset\n\n\nReddit _[a]_ 64,978,944 0.28\nPAQ Lewis et al. (2021) 52,953,088 0.23\nAmazon Reviews Ni et al. (2019) 38,682,624 0.16\nS2ORC Title Abstract Lo et al. (2020) 35438592 0.15\nWikiAnswers Fader et al. (2014) 9,912,320 0.04\nS2ORC Citation Titles Lo et al. (2020) 7,585,792 0.03\nS2ORC Abstract Citation Lo et al. (2020) 7,503,872 0.03\nS2ORC Abstract Body Lo et al. (2020) 6,389,760 0.03\nWikipedia Title Body Foundation (2024) 6,078,464 0.03\nGooaq Khashabi et al. (2021) 1,245,184 0.01\nCodesearch Husain et al. (2019) 835,584 _<_ .01\nAGNews **?** 409,600 _<_ .01\nCCNews Hamborg et al. (2017) 344,064 _<_ .01\nNPR _[b]_ 344,064 _<_ .01\nCNN See et al. (2017) 278,528 _<_ .01\nYahoo Title-Answer _[c]_ 262,144 _<_ .01\nAmazonQA Gupta et al. (2019) 212,992 _<_ .01\nYahoo Title-Question _[d]_ 196,608 _<_ .01\nSentence Compression Filippova & Altun (2013) 163,840 _<_ .01\nYahooQA _[e]_ 131,072 _<_ .01\nELI5 Fan et al. (2019) 98,304 _<_ .01\nAltlex Hidey & McKeown (2016) 98,304", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_18450", "chunk_text": " _[e]_ 131,072 _<_ .01\nELI5 Fan et al. (2019) 98,304 _<_ .01\nAltlex Hidey & McKeown (2016) 98,304 _<_ .01\nWikihow Koupaee & Wang (2018) 81,920 _<_ .01\nSimpleWiki Coster & Kauchak (2011) 81,920 _<_ .01\nStackExchange Duplicate Questions _[f]_ 65,536 _<_ .01\nStackExchange Title Body _[g]_ 65,536 _<_ .01\nStackExchange Body Body _[h]_ 65,536 _<_ .01\nQuora Duplicate Questions _[i]_ 32,768 _<_ .01\nSQuAD Rajpurkar et al. (2016) 16,384 _<_ .01\n\nTotal 234,553,344 1\n\n\n_a_ [https://huggingface.co/datasets/sentence-transformers/](https://huggingface.co/datasets/sentence-transformers/reddit-title-body)\n[reddit-title-body](https://huggingface.co/datasets/sentence-transformers/reddit-title-body)\n_b_ [https://files.pushshift.io/news/](https://files.pushshift.io/news/)\n_c_ [https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset)\n_d_ [https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset)\n_e_ [https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset](https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset)\n_f_ [https://data.stackexchange.com/apple/query/fork/1456963](https://data.stackexchange.com/apple/query/fork/1456963)\n_g_ [https://data.stackexchange.com/apple/query/fork/1456963](https://data.stackexchange.com/apple/query/fork/1456963)\n_h_ [https://data.stackexchange.com/apple/query/fork/1456963](https://data.stackexchange", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_18900", "chunk_text": ".stackexchange.com/apple/query/fork/1456963](https://data.stackexchange.com/apple/query/fork/1456963)\n_h_ [https://data.stackexchange.com/apple/query/fork/1456963](https://data.stackexchange.com/apple/query/fork/1456963)\n_i_ [https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs)\n\n\n   - clustering\n\n\n10.8 UNSUPERVISED TRAINING DATASETS\n\n\nWe train on 234 _M_ weakly supervised query-document pairs collected for training text embedding\nmodels in Nussbaum et al. (2024). The full distribution of 29 datasets is shown in Table 5. Reddit\nalone makes up over 25% of the data distribution, with 19 of the datasets comprising under 1% of the\ntotal data.\n\n\n22\n\n\nTable 6: Distribution of BEIR evaluation datasets used, ordered by corpus size.\n\n\nDataset Queries Documents\n\n\nNFCorpus 323 3,633\nSciFact 300 5,183\nArguAna 1,406 8,674\nSciDocs 1,000 25,657\nTREC-COVID 50 171,332\nQuora 5,000 522,931\nNatural Questions 3,452 2,681,468\nMS MARCO 6,980 8,841,823\n\n\nFigure 17: System performance (training accuracy) as we scale the size of the first-stage model\nencoder only.\n\n\n10.9 BEIR EVALUATION DATASETS\n\n\nOur initial experiments involve evaluating on nine datasets from the BEIR benchmark. Datasets are\ndetailed in Table 6. To enable fast evaluation at this stage, we obtain the top 1024 relevant documents\nto each document with GTR (Ni et al., 2021) and rerank only these documents at evaluation time.\n\n\n10.10 ADDITIONAL MODELING ABLATIONS\n\n\n**First-stage model size.** One consideration is whether we can improve our system without affecting\nsearch inference time by scaling the number of parameters in the backbone model only. We study\nthis affect by scaling the number of layers in the transformer backbone of the first-stage model from 1\nto the full 12. Resulting performance is", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_19350", "chunk_text": "\nsearch inference time by scaling the number of parameters in the backbone model only. We study\nthis affect by scaling the number of layers in the transformer backbone of the first-stage model from 1\nto the full 12. Resulting performance is shown in Section 10.10.\n\n\nOur results show that scaling the first-stage model has a small positive influence on model performance.\nHowever, since the total improvement from a 12x increase in first-stage model size is less than one\npercent, we conclude that the second-stage model size has a much larger impact on performance.\n\n\n10.11 HOW MANY TOKENS PER DOCUMENT?\n\n\nWe consider the question of how many tokens per document is ideal while keeping the total number of\ndocument tokens fixed. Results per the nine evaluation datasets of BEIR are shown in Section 10.11.\n\n\n10.12 MTEB RETRIEVAL EVALUATION PERFORMANCE\n\n\nTo evaluate on MTEB, we subsample contextual documents from the full corpus available in each\ndataset and modality. For retrieval, this corresponds to the corpus itself (importantly, not the queries);\nfor other modalities, we choose the default \u201ctext\u201d field in each casel. For classification tasks, we\nsample from the text side (not the classification labels themselves).\n\n\n23\n\n\n|argu|Col2|ana|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|\n\n\n|fiq|Col2|qa|Col4|\n|---|---|---|---|\n|fi|fi|qa|qa|\n|||||\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|\n\n\n|msma|Col2|rco|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|2<br>1<br>2", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_19800", "chunk_text": "1<br>2<br>3<br>2<br>5|\n\n\n|msma|Col2|rco|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|nfcorp|Col2|pus|Col4|\n|---|---|---|---|\n|nfcor|nfcor|pus|pus|\n|||||\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|\n\n\n|n|Col2|q|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|\n\n\n|2 1 2 3 quor|Col2|2 5 ra|Col4|\n|---|---|---|---|\n|quo|quo|ra|ra|\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n|scid|Col2|ocs|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_20250", "chunk_text": "---|---|\n|||||\n|||||\n|||||\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|\n\n\n|sci|Col2|fact|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|\n\n\n|trec-c|Col2|ovid|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|2<br>1<br>2<br>3<br>2<br>5|\n\n\n\nFigure 18: Performance per-dataset as we scale tokens-per-document, while keeping the total number\n\n- f contextual tokens fixed. Different domains prefer a different number of tokens per document.\n\n\nMethod Arg CQA CFEVER DBP FEVER FiQA HPQA MSMRC NFC NQ QUORA SCID SCIF TREC TOUCHE Mean\n\n\n**Unsupervised**\nBaseline 54.8 41.4 24.7 40.2 74.4 39.9 63.8 35.0 35.7 48.6 88.2 20.2 72.0 62.2 19.2 48.0\n\nContextual 54.9 43.1 24.4 40.7 79.6 42.1 68.8 38.9 36.5 57.8 88.9 21.1 72.8 77.1 21.9 51.2\n\n\n**Supervised**\nBaseline 49.3 40.5", "token_count": 500, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2410.02525_contextual_embeddings_nussbaum:chunk_20700", "chunk_text": " 38.9 36.5 57.8 88.9 21.1 72.8 77.1 21.9 51.2\n\n\n**Supervised**\nBaseline 49.3 40.5 38.3 45.0 85.0 38.4 73.6 43.1 35.0 59.4 87.7 18.3 70.5 79.9 28.2 52.8\n\nContextual 53.8 41.2 38.8 43.3 89.2 40.1 73.9 42.2 35.9 61.6 87.1 20.1 72.7 82.6 27.8 54.0\n\n\nTable 7: Results (NDCG@10) on the retrieval setting of the MTEB benchmark.\n\n\nTable 7 shows our model performance on all datasets in the MTEB retrieval category. We see largest\nimprovements over the baseline on the ArguAna and TREC-Covid datasets.\n\n\n24\n\n\n", "token_count": 239, "metadata": {"arxiv_id": "2410.02525", "title": "Contextual Document Embeddings", "authors": ["John X. Morris", "Alexander M. Rush"], "year": 2024, "url": "https://arxiv.org/pdf/2410.02525v4"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_0", "chunk_text": "## **Precise Zero-Shot Dense Retrieval without Relevance Labels**\n\n**Luyu Gao** _[\u2217]_ _[\u2020]_ **Xueguang Ma** _[\u2217\u2021]_ **Jimmy Lin** _[\u2021]_ **Jamie Callan** _[\u2020]_\n\n_\u2020_ Language Technologies Institute, Carnegie Mellon University\n\n_\u2021_ David R. Cheriton School of Computer Science, University of Waterloo\n{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca\n\n\n\n**Abstract**\n\n\nWhile dense retrieval has been shown effec\ntive and efficient across tasks and languages,\nit remains difficult to create effective fully\nzero-shot dense retrieval systems when no relevance label is available. In this paper, we\nrecognize the difficulty of zero-shot learning\nand encoding relevance. Instead, we propose to pivot through Hypothetical Document\nEmbeddings (HyDE). Given a query, HyDE first\nzero-shot instructs an instruction-following\nlanguage model (e.g. InstructGPT) to generate a _hypothetical_ document. The document captures relevance patterns but is unreal\nand may contain false details. Then, an unsupervised contrastively learned encoder (e.g.\nContriever) encodes the document into an\nembedding vector. This vector identifies a\nneighborhood in the corpus embedding space,\nwhere similar _real_ documents are retrieved\n\nbased on vector similarity. This second step\nground the generated document to the actual\ncorpus, with the encoder\u2019s dense bottleneck\nfiltering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense\nretriever Contriever and shows strong performance comparable to fine-tuned retrievers,\nacross various tasks (e.g. web search, QA, fact\nverification) and languages (e.g. sw, ko, ja). [1]\n\n\n**1** **Introduction**\n\n\nDense retrieval (Lee et al., 2019; Karpukhin et al.,\n2020), the method of retrieving documents using\nsemantic embedding similarities, has been shown\nsuccessful across tasks like web search, question\nanswering, and fact verification. A variety of meth\n- ds such as negative mining (Xiong et al., 2021; Qu\net al., 2021", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_450", "chunk_text": " been shown\nsuccessful across tasks like web search, question\nanswering, and fact verification. A variety of meth\n- ds such as negative mining (Xiong et al., 2021; Qu\net al., 2021), distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofst\u00e4tter et al., 2021) and task-specific\n\n\n_\u2217_ Equal contribution.\n1No models were trained or fine-tuned in making this pre[print. Our open source code is available at https://github.](https://github.com/texttron/hyde)\n[com/texttron/hyde.](https://github.com/texttron/hyde)\n\n\n\npre-training (Izacard et al., 2021; Gao and Callan,\n2021; Lu et al., 2021; Gao and Callan, 2022; Liu\nand Shao, 2022) have been proposed to improve the\neffectiveness of supervised dense retrieval models.\n\n\nOn the other hand, zero-shot dense retrieval still\nremains difficult. Many recent works consider the\nalternative transfer learning setup, where the dense\nretrievers are trained on a high-resource dataset and\nthen evaluated on queries from new tasks. The MSMARCO collection (Bajaj et al., 2016), a massive\njudged dataset with a large number of judged querydocument pairs, is arguably the most commonly\nused. As argued by Izacard et al. (2021), in practice, however, the existence of such a large dataset\ncannot always be assumed. Even MS-MARCO restricts commercial use and cannot be adopted in a\nvariety of real-world search scenarios.\n\nIn this paper, we aim to build effective fully\nzero-shot dense retrieval systems that require **no**\n**relevance** supervision, work out-of-box and generalize across tasks. As supervision is not available,\nwe start by examining self-supervised representation learning methods. Modern deep learning enables two distinct learning algorithms. At the token\nlevel, generative large language models (LLM) pretrained on large corpus have demonstrated strong\nnatural language understanding (NLU) and generation (NLG) capabilities (Brown et al., 2020;\nChen et al., 2021; Rae et al., 2021; Hoffmann\net al., 2022; Thoppilan et al.,", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_900", "chunk_text": ") and generation (NLG) capabilities (Brown et al., 2020;\nChen et al., 2021; Rae et al., 2021; Hoffmann\net al., 2022; Thoppilan et al., 2022; Chowdhery\net al., 2022). At the document level, text (chunk)\nencoders pre-trained with contrastive objectives\nlearn to encode document-document similarity into\ninner-product (Izacard et al., 2021; Gao and Callan,\n2022). On top of these, one extra insight into LLM\nis borrowed: the LLMs further trained to follow\n\ninstructions can _zero-shot_ generalize to diverse unseen instructions (Ouyang et al., 2022; Sanh et al.,\n2022; Min et al., 2022; Wei et al., 2022). Ouyang\net al. (2022) show that with a small amount of data,\nGPT-3 (Brown et al., 2020) models can be aligned\n\n\n**instruction** **query** **generated document** **real document**\n\n\n\n\n\nFigure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries\nwithout changing the underlying GPT-3 and Contriever/mContriever models.\n\n\n\nto human intent to follow instructions.\n\nWith these ingredients, we propose to\npivot through Hypothetical Document\nEmbeddings (HyDE), and decompose dense\nretrieval into two tasks, a generative task performed by an instruction-following language\nmodel and a document-document similarity task\nperformed by a contrastive encoder (Figure 1).\nFirst, we feed the query to the generative model\nand instruct it to \"write a document that answers\n\nthe question\", i.e. a hypothetical document.\nWe expect the generative process to capture\n\"relevance\" by giving an example; the generated\ndocument **is not** real, can contain factual errors but\nis like a relevant document. In the second step,\nwe use an unsupervised contrastive encoder to\nencode this document into an embedding vector.\nHere, we expect the encoder\u2019s dense bottleneck\nto serve a lossy compressor, where the extra\n(hallucinated) details are filtered out from the\nembedding. We use this vector to search against\nthe corpus embeddings. The most similar _real", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_1350", "chunk_text": " we expect the encoder\u2019s dense bottleneck\nto serve a lossy compressor, where the extra\n(hallucinated) details are filtered out from the\nembedding. We use this vector to search against\nthe corpus embeddings. The most similar _real_\ndocuments are retrieved and returned. The retrieval\n\nleverages document-document similarity encoded\nin the inner-product during contrastive training.\nNote that, interestingly, with HyDE factorization,\nthe query-document similarity score is no longer\nexplicitly modeled nor computed. Instead, the\nretrieval task is cast into two NLU and NLG tasks.\n\nHyDE appears unsupervised. **No** model is trained\nin HyDE: both the generative model and the contrastive encoder remain intact. Supervision signals\nwere only involved in instruction learning of our\nbackbone LLM.\n\nIn our experiments, we show HyDE using InstructGPT (Ouyang et al., 2022) and Contriever (Izacard\net al., 2021) as backbone models significantly outperforms the previous state-of-the-art Contriever\n- nly zero-shot no-relevance system on 11 queries\n\n\n\nsets, covering tasks like Web Search, Question\nAnswering, Fact Verification and languages like\nSwahili, Korean, Japanese.\n\n\n**2** **Related Works**\n\n\n**Dense Retrieval** (Lee et al., 2019; Karpukhin\net al., 2020) has been extensively studied after the\nemergence of pre-trained Transformer language\nmodels (Devlin et al., 2019). Researchers studied the metric learning problems, such as training\nloss (Karpukhin et al., 2020) and negative sampling (Xiong et al., 2021; Qu et al., 2021), and also\nintroduced distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofst\u00e4tter et al., 2021). Later works studied\nthe second stage pre-training of language model\nspecifically for retrieval (Izacard et al., 2021; Gao\nand Callan, 2021; Lu et al., 2021; Gao and Callan,\n2022; Liu and Shao, 2022).\nThe popularity of dense retrieval can be partially\nattributed to the rich and successful research in very\nefficient minimum inner", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_1800", "chunk_text": "; Lu et al., 2021; Gao and Callan,\n2022; Liu and Shao, 2022).\nThe popularity of dense retrieval can be partially\nattributed to the rich and successful research in very\nefficient minimum inner product search (MIPS) at\nvery large (billion) scales (Johnson et al., 2017).\n\n\n**Instructions-Following** **Language** **Models**\nSoon after the emergence of LLMs, several groups\n\n- f researchers discover that LLMs trained on data\n\nconsisting of instructions and their execution can\nzero-shot generalize to perform new tasks with new\ninstructions (Ouyang et al., 2022; Sanh et al., 2022;\nMin et al., 2022; Wei et al., 2022). This can be\ndone by standard supervised sequence-to-sequence\nlearning or more effectively with reinforcement\nlearning (Ouyang et al., 2022).\nConcurrent to us, Asai et al. (2022) studied\n\u201cTask-aware Retrieval with Instructions\u201d. They\n_fine-tuned dense encoders_ that can also encode\ntask-specific instruction prepended to query. In\ncomparison, we use an unsupervised encoder and\nhandle different tasks and their instruction with an\n\n\ninstruction following generative LLM, as described\nabove.\n\n\n**Zero-Shot Dense Retrieval** The tasks of zero\nshot (dense) retrieval are arguably empirically defined by Thakur et al. (2021) for the neural retrieval community. Their BEIR benchmark consists of diverse retrieval tasks. The paper and\nmany follow-up research generally consider the\nTransfer Learning setup where the dense retriever is first learned using a diverse and richly\nsupervised corpus and query collection, namely\nMS-MARCO (Thakur et al., 2021; Wang et al.,\n2022; Yu et al., 2022).\n\n\nHowever, as stated by Izacard et al. (2021), such\na large collection can rarely be assumed. In this\npaper, therefore, we study the problem of building\neffective dense retrieval systems without relevance\nlabels. Similar to Izacard et al. (2021), we also\ndo not assume access to the test time corpora for\ntraining. This is a more realistic setup and prevents\n\n- ver-engineering on the test corpora.\n\n\nBy the definition in Sachan", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_2250", "chunk_text": "ard et al. (2021), we also\ndo not assume access to the test time corpora for\ntraining. This is a more realistic setup and prevents\n\n- ver-engineering on the test corpora.\n\n\nBy the definition in Sachan et al. (2022), our\nsetup can be roughly considered as **\u201cunsuper-**\n**vised\u201d** . Strictly, as with Sachan et al. (2022), the\n\n- nly supervision resides in the LLM, in the processing of learning to follow instructions.\n\n\n**Generative Retrieval** Generative search is a new\n\nclass of retrieval methods that use neural generative\nmodels as search indices (Metzler et al., 2021; Tay\net al., 2022; Bevilacqua et al., 2022; Lee et al.,\n2022). These models use (constrained) decoding\nto generate document identifiers, such as id and\nsub-string, which map directly to _real_ documents.\nThey have to go through special training procedures\n\n- ver relevance data; effective search may also need\nto use novel forms of search indices (Bevilacqua\net al., 2022; Lee et al., 2022). In comparison, our\nmethod uses the standard MIPS index and requires\nno training or training data. Our generative model\nproduces an intermediate hypothetical document\nto be fed into a dense encoder, instead of a real\n\ndocument.\n\n\n**3** **Methodology**\n\n\nIn this section, we first formally define the problem of (zero-shot) dense retrieval. Then we will\nintroduce how HyDE is designed to solve it.\n\n\n\n**3.1** **Preliminaries**\n\n\nDense retrieval models similarity between query\nand document with inner product similarity. Given\na query _q_ and document _d_, it uses two encoder\nfunction enc _q_ and enc _d_ to map them into _d_ dimension vectors **vq** _,_ **vd**, whose inner product is used\nas similarity measurement.\n\n\nsim(q _,_ d) = _\u27e8_ enc _q_ (q) _,_ enc _d_ (d) _\u27e9_ = _\u27e8_ **vq** _,_ **vd** _\u27e9_ (1)\n\n\nFor zero-shot retrieval, we consider _L_ query sets\n_Q_ 1 _, Q_ 2 _, ..., Q", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_2700", "chunk_text": "d) _\u27e9_ = _\u27e8_ **vq** _,_ **vd** _\u27e9_ (1)\n\n\nFor zero-shot retrieval, we consider _L_ query sets\n_Q_ 1 _, Q_ 2 _, ..., QL_ and their corresponding search corpus, document sets _D_ 1 _, D_ 2 _, ..., DL_ . Denote the\n_j_ - th query from _i_ - th set query set _Qi_ as q _ij_ . We\nneed to fully define mapping _functions_ enc _q_ and\nenc _d_ without access to any query set _Qi_, document\nset _Di_, or any relevance judgment _rij_ .\nThe difficulty of zero-shot dense retrieval lies\nprecisely in Equation 1: it requires learning of two\nembedding functions (for query and document respectively) into the _same_ embedding space where\ninner product captures _relevance_ . Without relevance judgments/scores to fit, learning becomes\nintractable.\n\n\n**3.2** **HyDE**\n\n\nHyDE circumvents the aforementioned learning\nproblem by performing search in document\n- nly embedding space that captures documentdocument similarity. This can be easily learned\nusing unsupervised contrastive learning (Izacard\net al., 2021; Gao et al., 2021; Gao and Callan,\n2022). We set document encoder enc _d_ directly as a\ncontrastive encoder enccon.\n\n\n_f_ = enc _d_ = enccon (2)\n\n\nThis function is also denoted as _f_ for simplicity. This unsupervised contrastive encoder will\nbe shared by all incoming document corpus.\n\n\n**vd** = _f_ ( _d_ ) _\u2200d \u2208_ _D_ 1 _\u222a_ _D_ 2 _\u222a_ _... \u222a_ _DL_ (3)\n\n\nTo build the query vector, we consider in addition\nan instruction following LM, InstructLM. It takes a\nquery _q_ and a textual instruction INST and follows\nthem to perform the task specified by INST. For\nsimplicity, denote,\n\n\n_g_ ( _q,_ INST) = InstructLM( _q,_ INST) (4)\n\n\nNow we can use _g_ to map queries to \"hypothetical\" documents by sampling from _g_, setting INST\n\n\nto be \u201cwrite a", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_3150", "chunk_text": " ( _q,_ INST) = InstructLM( _q,_ INST) (4)\n\n\nNow we can use _g_ to map queries to \"hypothetical\" documents by sampling from _g_, setting INST\n\n\nto be \u201cwrite a paragraph that answers the\nquestion\u201d. The generated document _is not_ real,\ncan and is likely to be ungrounded factually (Brown\net al., 2020; Thoppilan et al., 2022). We _only_ require it to capture relevance pattern. This is done\nby generating documents, i.e. providing examples. Critically, here we **offload** relevance modeling from representation learning model to an\nNLG model that generalizes significantly more easily, naturally, and effectively (Brown et al., 2020;\nOuyang et al., 2022). Generating examples also\nreplaces explicit modeling of relevance scores.\nWe can now encode the generated document using\nthe document encoder _f_ . Write,\n\n\nE[ **v** _qij_ ] = E[ _f_ ( _g_ ( _qij,_ INST _i_ ))] (5)\n\n\nFormally, _g_ defines a probability distribution based\n\n- n the chain rule. In this paper, we simply consider\nthe expectation value, assuming the distribution of\n**v** _qij_ is uni-modal, i.e. the query is not ambiguous.\nThe study of ambiguous queries and diversity is\nleft to future work. We estimate Equation 5 by\nsampling _N_ documents from _g_, [ _d_ [\u02c6] 1 _,_ _d_ [\u02c6] 2 _, ...,_ _d_ [\u02c6] _N_ ].\n\n\n\n\u02c6\n**v** _qij_ = _N_ [1]\n\n\n= [1]\n\n_N_\n\n\n\n\n - _f_ ( _dk_ ) (6)\n\n\n\u02c6\n_dk\u223cg_ ( _qij_ _,_ INST _i_ )\n\n\n_N_\n\n- _f_ ( _d_ [\u02c6] _k_ ) (7)\n\n\n_k_ =1\n\n\n\n\n\n\n\n_N_\n\n\n\n\nWe also consider the query as a possible hypothesis,\n\n\n\n_N_\n\n\n\n\n**v** \u02c6 _qij_ = _N_ + 11 [[]\n\n\n\n\n- _f_ ( _d_ [\u02c6] _k_ ) + _f_ ( _qij", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_3600", "chunk_text": " a possible hypothesis,\n\n\n\n_N_\n\n\n\n\n**v** \u02c6 _qij_ = _N_ + 11 [[]\n\n\n\n\n- _f_ ( _d_ [\u02c6] _k_ ) + _f_ ( _qij_ )] (8)\n\n\n_k_ =1\n\n\n\nsample from InstructGPT using the OpenAI playground default temperature of 0.7 for open-ended\ngenerations. We use the English-only Contriever\nmodel for English retrieval tasks and multilingual\nmContriever for non-English tasks. We conducted\nretrieval experiments with the Pyserini toolkit (Lin\net al., 2021a).\n\n\n**Datasets** We consider web search query sets\nTREC DL19 (Craswell et al., 2020a) and\nDL20 (Craswell et al., 2020b); they are based on\nthe MS-MARCO dataset (Bajaj et al., 2016). We\nalso use a diverse collection of 6 low-resource\n\ndatasets from the BEIR dataset (Thakur et al.,\n2021). For non-English retrieval, we consider\nSwahili, Korean, Japanese, and Bengali from the\nMr.Tydi dataset (Zhang et al., 2021).\nWe use different instructions for each dataset.\n\nThey share a similar structure but have different\nquantifiers to control the exact form of the generated hypothetical documents. These instructions\ncan be found in subsection A.1.\n\n\n**Compared** **Systems** Contriever models,\nContriever and mContriever, serve as our major\nbaseline. They are trained using unsupervised\ncontrastive learning. HyDE retrievers share the\n_exact_ same embedding spaces with them. The\n\n- nly difference is how the query vector is built.\nThese comparisons allow us to easily examine\nthe effect of HyDE. The classical heuristic-based\n\nlexical retriever BM25 is also included.\n\nSeveral systems that involve fine-tuning on massive _relevance_ data are also included as refer\nences. We consider models fine-tuned on MSMARCO and transferred, DPR and ANCE, from\nthe BEIR paper. For multilingual, we include\nthe mDPR model from Mr.Tydi paper and MSMARCO fine-tuned mBERT and XLM-R from\nthe Contriever paper. We also include the state", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_4050", "chunk_text": "\nthe BEIR paper. For multilingual, we include\nthe mDPR model from Mr.Tydi paper and MSMARCO fine-tuned mBERT and XLM-R from\nthe Contriever paper. We also include the state-ofthe-art transfer learning models: Contriever and\nmContriever fine-tuned on MS-MARCO, denoted\nContriever [FT] and mContriever [FT] . These mod\nels have run through the state-of-the-art retrieval\nmodel training pipeline that involves second-stage\nretrieval-specific pre-training (Lee et al., 2019) and\na few rounds of fine-tuning (Qu et al., 2021); they\nshould be considered empirical upper bounds.\n\n\n**4.2** **Web Search**\n\n\nIn Table 1, we show retrieval results on TREC\nDL19 and TREC DL20. We see HyDE bring sizable\nimprovements to Contriever across the board for\n\n\n\nInner product is computed between \u02c6 **v** _qij_ and the\nset of all document vectors _{f_ ( _d_ ) _|d \u2208_ _Di}_ . The\nmost similar documents are retrieved. Here the\n\nencoder function _f_ serves as a lossy compressor\nthat outputs dense vectors, where the extra details\nare filtered and left out from the vector. It further\ngrounds the hypothetical vector to the actual corpus\nand the real documents. The full HyDE system is\nillustrated in Figure 1.\n\n\n**4** **Experiments**\n\n\n**4.1** **Setup**\n\n\n**Implementation** We implement HyDE using\nInstructGPT, a GPT-3 model from the instruct\nseries (text-davinci-003; Ouyang et al. (2022))\nand Contriever models (Izacard et al., 2021). We\n\n\nDL19 DL20\nmap ndcg@10 recall@1k map ndcg@10 recall@1k\n\n\n_w/o relevance judgement_\nBM25 30.1 50.6 75.0 28.6 48.0 78.6\n\nContriever 24.0 44.5 74.6 24.0 42.1 75.4\nHyDE **41.8** **61.3** **88.0** **", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_4500", "chunk_text": "78.6\n\nContriever 24.0 44.5 74.6 24.0 42.1 75.4\nHyDE **41.8** **61.3** **88.0** **38.2** **57.9** **84.4**\n\n\n_w/ relevance judgement_\nDPR 36.5 62.2 76.9 41.8 **65.3** 81.4\n\nANCE 37.1 **64.5** 75.5 40.8 64.6 77.6\nContriever [FT] 41.7 62.1 83.6 **43.6** 63.2 **85.8**\n\n\nTable 1: Results for web search on DL19/20. Best performing w/o relevance and overall system(s) are marked\n**bold** . DPR, ANCE and Contriever [FT] are in-domain _supervised_ models that are finetuned on MS MARCO training\ndata.\n\n\nScifact Arguana Trec-Covid FiQA DBPedia TREC-NEWS\n\n\n_nDCG@10_\n_w/o relevance judgement_\nBM25 67.9 39.7 **59.5** 23.6 31.8 39.5\n\nContriever 64.9 37.9 27.3 24.5 29.2 34.8\nHyDE **69.1** **46.6** 59.3 **27.3** **36.8** **44.0**\n\n\n_w/ relevance judgement_\nDPR 31.8 17.5 33.2 29.5 26.3 16.1\n\nANCE 50.7 41.5 **65.4** 30.0 28.1 38.2\nContriever [FT] 67.7 44.6 59.6 **32.9** **41.3** 42.8\n\n\n_Recall@100_\n_w/o relevance judgement_\nBM25 92.5 93.2 **49.8** 54.0 46.8 44.7\n\nContriever 92.6 90.1 17.2 56.2 45.3 42.3\nHyDE", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_4950", "chunk_text": ".2 **49.8** 54.0 46.8 44.7\n\nContriever 92.6 90.1 17.2 56.2 45.3 42.3\nHyDE **96.4** **97.9** 41.4 **62.1** **47.2** **50.9**\n\n\n_w/ relevance judgement_\nDPR 72.7 75.1 21.2 34.2 34.9 21.5\n\nANCE 81.6 93.7 45.7 58.1 31.9 39.8\nContriever [FT] 94.7 97.7 40.7 **65.6** **54.1** 49.2\n\n\nTable 2: Low resource tasks from BEIR. Best performing w/o relevance and overall system(s) are marked **bold** .\n\n\n\nboth precision-oriented and recall metrics. While\nunsupervised Contriever can underperform the\nclassical BM25 approach, HyDE outperforms BM25\nby large margins.\n\n\nHyDE remains competitive even when compared\nto fine-tuned models. Note that TREC DL19/20\nare search tasks defined on MS-MARCO and\nthere, all the fine-tuned models are richly _super-_\n_vised_ . On TREC DL19, HyDE shows comparable\nmap and ndcg@10 to Contriever [FT] and best recall@1k. On DL20, HyDE gets around 10% lower\nmap and ndcg@10 than Contriever [FT] and similar recall@1k. The ANCE model shows better\n\nndcg@10 numbers than HyDE but lower recall, suggesting it may be biased to a subset of queries\nand/or relevant documents.\n\n\n\n**4.3** **Low Resource Retrieval**\n\n\nIn Table 2, we show retrieval results on low\nresource tasks from BEIR. Similar to web\n\nsearch, HyDE again brings sizable improvements to\nContriever across the board in terms of both ndcg\nand recall. HyDE is only outperformed by BM25 on\n\n- ne dataset, TREC-Covid but with a tiny 0.2 margin; in comparison, the underlying Contriever\nunderperforms by more than 50%.\nWe also observe", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_5400", "chunk_text": " is only outperformed by BM25 on\n\n- ne dataset, TREC-Covid but with a tiny 0.2 margin; in comparison, the underlying Contriever\nunderperforms by more than 50%.\nWe also observe HyDE demonstrates strong\nperformance compared to fine-tuned models.\nHyDE generally shows better performance than\nANCE and DPR, even though the two are\nfine-tuned on MS-MARCO and ANCE also involves some sophisticated hard negative techniques.\nContriever [FT] shows performance advantages on\nFiQA and DBPedia. These involve retrieval of financial posts or entities respectively. We believe\nthe performance difference can be attributed to the\n\n\nSwahili Korean Japanese Bengali\n\n\n_w/o relevance judgement_\nBM25 38.9 28.5 21.2 **41.8**\n\nmContriever 38.3 22.3 19.5 35.3\nHyDE **41.7** **30.6** **30.7** 41.3\n\n\n_w/ relevance judgement_\nmDPR 7.3 21.9 18.1 25.8\n\nmBERT 37.4 28.1 27.1 35.1\n\nXLM-R 35.1 32.2 24.8 41.7\nmContriever [FT] **51.2** **34.2** **32.4** **42.3**\n\n\nTable 3: MRR@100 on Mr.Tydi. Best performing w/o\nrelevance and overall system(s) are marked **bold** .\n\n\nunder-specification of the instruction; more elaborative instructions may help.\n\n\n**4.4** **Multilingual Retrieval**\n\n\nMultilingual setup poses several additional challenges to HyDE. The small-sized contrastive encoder gets saturated as the number of languages\nscales (Conneau et al., 2020; Izacard et al., 2021).\nMeanwhile, our generative LLM faces an opposite\nissue: with languages of not as high resource as\nEnglish or French, the high capacity LLM can get\nunder-trained (Hoffmann et al., 2022).\nNevertheless, in Table 3, we still find HyDE\nable to improve the mContriever model. It can\n\n- utperform non-Contriever models fine-tuned on\nand", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_5850", "chunk_text": "Hoffmann et al., 2022).\nNevertheless, in Table 3, we still find HyDE\nable to improve the mContriever model. It can\n\n- utperform non-Contriever models fine-tuned on\nand transferred from MS-MARCO. On the other\n\nhand, we do observe some margins between HyDE\nand fine-tuned mContriever [FT] . Since HyDE and\nmContriever [FT] use similar contrastive encoders,\nwe hypothesize this is because the non-English languages we considered are under-trained in both\npre-training and instruction learning stages.\n\n\n**5** **Analysis**\n\n\nThe generative LLM and contrastive encoder make\nup the backbone of HyDE. In this section, we study\nthe effect of changing their realizations. In particular, we consider smaller language models (LM)\nand fine-tuned encoders. We conduct our studies\n\n- n TREC DL19/20.\n\n\n**5.1** **Effect of Different Generative Models**\n\n\nIn Table 4, we show HyDE using - ther\ninstruction-following language models. In\nparticular, we consider a 52-billion Cohere\nmodel (command-xlarge-20221108) and a\n11-billion FLAN model (FLAN-T5-xxl; Wei\net al. (2022)). [2] Generally, we observe that all\n\n\n[2Model sizes are from https://crfm.stanford.edu/](https://crfm.stanford.edu/helm/v1.0/?models)\n[helm/v1.0/?models.](https://crfm.stanford.edu/helm/v1.0/?models)\n\n\n\nModel DL19 DL20\n\n\nContriever 44.5 42.1\nContriever [FT] 62.1 63.2\n\n\nHyDE\nw/ Contriever\n\nw/ Flan-T5 (11b) 48.9 52.9\nw/ Cohere (52b) 53.8 53.8\nw/ GPT (175b) **61.3** **57.9**\nw/ Contriever [FT]\n\nw/ Flan-T5 (11b) 60.2 62.1\nw/ Cohere (52b) 61.4 63.1\nw/ GPT (175b) **", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_6300", "chunk_text": "triever [FT]\n\nw/ Flan-T5 (11b) 60.2 62.1\nw/ Cohere (52b) 61.4 63.1\nw/ GPT (175b) **67.4** **63.5**\n\n\nTable 4: NDCG@10 on TREC DL19/20. Effect\n\n- f changing different instruction LMs and using finetuned encoder. Best w/o relevance and overall models\n\nare marked **bold** .\n\n\nmodels bring improvement to the unsupervised\nContriever, with larger models bringing larger\nimprovements. At the time when this paper is\nwritten, the Cohere model is still experimental\nwithout much detail disclosed. We can only\ntentatively hypothesize that training techniques\nmay have also played some role in the performance\ndifference.\n\n\n**5.2** **HyDE with Fine-tuned Encoder**\n\n\nTo begin with, HyDE with fine-tuned encoder is\n_not_ the intended usage: HyDE is more powerful\nand irreplaceable when few relevance labels are\npresent. Here we are interested to find out if\nand how HyDE embedding can affect fine-tuned encoders. In Table 4, we see that less powerful instruction LMs can negatively impact the overall performance of the fine-tuned retriever. (To remind our\nreaders, Contriever [FT] is in-domain supervisedly\nfine-tuned for TREC DL19/20). The performance\ndegradations remain small. On the other hand, we\nalso observe the InstructGPT model able to fur\nther bring up the performance, especially on DL19.\nThis suggests that there may still exist certain factors not captured by the fine-tuned encoder but only\nby the generative model.\n\n\n**6** **Conclusion**\n\n\nAt the end of the paper, we encourage the readers\nto take a moment and reflect on the HyDE model.\nCompare it to some of the other recently seen retrievers or re-ranker. These other models probably\ndiffer in their architecture, training method, and/or\ntask, but probably all of them involve modeling\nrelevance scores between a pair of query and docu\n\nment. Dense retrievers consider vector similarities\n\nwhile self-attentive re-rankers regression scores. In\ncomparison, the concept of relevance in HyDE is\ncaptured by an", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_6750", "chunk_text": "levance scores between a pair of query and docu\n\nment. Dense retrievers consider vector similarities\n\nwhile self-attentive re-rankers regression scores. In\ncomparison, the concept of relevance in HyDE is\ncaptured by an NLG model and the language generation process. We demonstrate in many cases, HyDE\ncan be as effective as dense retrievers that learn to\n\nmodel numerical relevance scores. So, is numerical relevance just a statistical artifact of language\nunderstanding? Will a weak retriever theoretically\nsuffice as the NLU & NLG models rapidly become\nstronger? Rushing to conclusions is not smart;\nmore works need to be done to get answers. With\nthis paper, we just want to raise these questions.\nConcretely in this paper, we introduce a new\nparadigm of interactions between LLM and dense\nencoder/retriever. We demonstrate (part of) relevance modeling and instruction understanding\ncan be delegated to the more powerful and flexible LLM. As a consequence, the need for relevance labels is removed. We are excited to see\n\nhow this can be generalized further to more sophisticated tasks like multi-hop retrieval/QA and\nconversational search.\n\nWe argue HyDE is also of practical use though not\nnecessarily over the entire lifespan of a search system. At the very beginning of the life of the search\nsystem, serving queries using HyDE offers performance comparable to a fine-tuned model, which\nno other relevance-free model can offer. As the\n\nsearch log grows, a supervised dense retriever can\nbe gradually rolled out. As the dense retriever\ngrows stronger, more queries will be routed to it,\nwith only less common and emerging ones going\nto HyDE backend.\n\n\n**References**\n\n\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. [Task-aware re-](https://doi.org/10.48550/ARXIV.2211.09260)\n[trieval with instructions.](https://doi.org/10.48550/ARXIV.2211.09260)\n\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_7200", "chunk_text": "/10.48550/ARXIV.2211.09260)\n\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder,\nAndrew McNamara, Bhaskar Mitra, Tri Nguyen,\nMir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti[wary, and Tong Wang. 2016. Ms marco: A human](https://doi.org/10.48550/ARXIV.1611.09268)\n[generated machine reading comprehension dataset.](https://doi.org/10.48550/ARXIV.1611.09268)\n\n\nMichele Bevilacqua, Giuseppe Ottaviano, Patrick\nLewis, Wen-tau Yih, Sebastian Riedel, and Fabio\n[Petroni. 2022. Autoregressive search engines: Gen-](https://doi.org/10.48550/arXiv.2204.10628)\n[erating substrings as document identifiers.](https://doi.org/10.48550/arXiv.2204.10628) _CoRR_,\nabs/2204.10628.\n\n\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\n[Amodei. 2020. Language models are few-shot learn-](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)\n[ers. In](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_7650", "chunk_text": "cb4967418bfb8ac142f64a-Abstract.html)\n[ers. In](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html) _Advances in Neural Information Processing_\n_Systems 33: Annual Conference on Neural Informa-_\n_tion Processing Systems 2020, NeurIPS 2020, De-_\n_cember 6-12, 2020, virtual_ .\n\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\n[Ilya Sutskever, and Wojciech Zaremba. 2021. Eval-](https://doi.org/10.48550/ARXIV.2107.03374)\n[uating large language models trained on code.](https://doi.org/10.48550/ARXIV.2107.03374)\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, K", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_8100", "chunk_text": " Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin\n - dkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\n[Palm: Scaling language modeling with pathways.](https://doi.org/10.48550/ARXIV.2204.02311)\n\n\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle\n\n[moyer, and Veselin Stoyanov. 2020. Unsupervised](https://doi.org/10.18653/v1/2020.acl-main.747)\n[cross-lingual representation learning at scale.](https://doi.org/10.18653/v1/2020.acl-main.747) In\n_Proceedings of", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_8550", "chunk_text": "18653/v1/2020.acl-main.747)\n[cross-lingual representation learning at scale.](https://doi.org/10.18653/v1/2020.acl-main.747) In\n_Proceedings of the 58th Annual Meeting of the Asso-_\n_ciation for Computational Linguistics_, pages 8440\u2013\n8451, Online. Association for Computational Linguistics.\n\n\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel\n[Campos, and Ellen M. Voorhees. 2020a. Overview](https://doi.org/10.48550/ARXIV.2003.07820)\n\n[of the trec 2019 deep learning track.](https://doi.org/10.48550/ARXIV.2003.07820)\n\n\nNick Craswell, Bhaskar Mitra, Emine Yilmaz,\nDaniel Fernando Campos, and Ellen M. Voorhees.\n2020b. Overview of the trec 2020 deep learning\ntrack. _ArXiv_, abs/2003.07820.\n\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. [BERT: Pre-training of](https://doi.org/10.18653/v1/N19-1423)\n[deep bidirectional transformers for language under-](https://doi.org/10.18653/v1/N19-1423)\n[standing.](https://doi.org/10.18653/v1/N19-1423) In _Proceedings of the 2019 Conference_\n\n_of the North American Chapter of the Association_\n_for Computational Linguistics: Human Language_\n_Technologies, Volume 1 (Long and Short Papers)_,\npages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\n\n[Luyu Gao and Jamie Callan. 2021. Condenser: a pre-](https://doi.org/10.18653/v1/2021.emnlp-main.75)\n[training architecture for dense retrieval. In](https://doi.org/10.18653/v1/2021.emnlp-main.75) _Proceed-_\n_ings of the 2021 Conference on Empirical Methods_\n_in Natural Language Processing_, pages 981\u2013993,\nOnline and Punta Cana, Dominican", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_9000", "chunk_text": "18653/v1/2021.emnlp-main.75) _Proceed-_\n_ings of the 2021 Conference on Empirical Methods_\n_in Natural Language Processing_, pages 981\u2013993,\nOnline and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n\n[Luyu Gao and Jamie Callan. 2022. Unsupervised cor-](https://doi.org/10.18653/v1/2022.acl-long.203)\n[pus aware language model pre-training for dense](https://doi.org/10.18653/v1/2022.acl-long.203)\n[passage retrieval. In](https://doi.org/10.18653/v1/2022.acl-long.203) _Proceedings of the 60th Annual_\n_Meeting of the Association for Computational Lin-_\n_guistics (Volume 1: Long Papers)_, pages 2843\u20132853,\nDublin, Ireland. Association for Computational Linguistics.\n\n\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\n\n[SimCSE: Simple contrastive learning of sentence](https://doi.org/10.18653/v1/2021.emnlp-main.552)\n[embeddings. In](https://doi.org/10.18653/v1/2021.emnlp-main.552) _Proceedings of the 2021 Conference_\n\n_on Empirical Methods in Natural Language Process-_\n_ing_, pages 6894\u20136910, Online and Punta Cana, Dominican Republic. Association for Computational\nLinguistics.\n\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\n[and Laurent Sifre. 2022. Training compute-optimal](https://doi.org/10.48550/ARXIV.2203.15556)\n[large language models.](https://doi.org/10.48550/ARXIV.", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_9900", "chunk_text": "-domain question answering. In](https://doi.org/10.18653/v1/2020.emnlp-main.550) _Proceedings of_\n_the 2020 Conference on Empirical Methods in Nat-_\n_ural Language Processing (EMNLP)_, pages 6769\u2013\n6781, Online. Association for Computational Linguistics.\n\n\nHyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon\n[Seo. 2022. Generative multi-hop retrieval.](https://doi.org/10.48550/ARXIV.2204.13596)\n\n\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n[2019. Latent retrieval for weakly supervised open](https://doi.org/10.18653/v1/P19-1612)\n[domain question answering. In](https://doi.org/10.18653/v1/P19-1612) _Proceedings of the_\n_57th Annual Meeting of the Association for Com-_\n_putational Linguistics_, pages 6086\u20136096, Florence,\nItaly. Association for Computational Linguistics.\n\n\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira.\n2021a. Pyserini: A Python toolkit for reproducible\ninformation retrieval research with sparse and dense\nrepresentations. In _Proceedings of the 44th Annual_\n_International ACM SIGIR Conference on Research_\n_and Development in Information Retrieval (SIGIR_\n_2021)_, pages 2356\u20132362.\n\n\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n[2021b. In-batch negatives for knowledge distillation](https://doi.org/10.18653/v1/2021.repl4nlp-1.17)\n[with tightly-coupled teachers for dense retrieval. In](https://doi.org/10.18653/v1/2021.repl4nlp-1.17)\n_Proceedings of the 6th Workshop on Representation_\n_Learning for NLP (RepL4NLP-2021)_, pages 163\u2013\n173, Online. Association for Computational Linguistics.\n\n\nZheng Liu and Yingxia Shao. 2022. Retromae: Pretraining retrieval-oriented transformers via masked\n", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_10350", "chunk_text": "RepL4NLP-2021)_, pages 163\u2013\n173, Online. Association for Computational Linguistics.\n\n\nZheng Liu and Yingxia Shao. 2022. Retromae: Pretraining retrieval-oriented transformers via masked\nauto-encoder. _ArXiv_, abs/2205.12035.\n\n\nShuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed\nMalik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu,\nand Arnold Overwijk. 2021. [Less is more: Pre-](https://doi.org/10.18653/v1/2021.emnlp-main.220)\n[train a strong Siamese encoder for dense text re-](https://doi.org/10.18653/v1/2021.emnlp-main.220)\n[trieval using a weak decoder. In](https://doi.org/10.18653/v1/2021.emnlp-main.220) _Proceedings of the_\n_2021 Conference on Empirical Methods in Natural_\n_Language Processing_, pages 2780\u20132791, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\n\n\nDonald Metzler, Yi Tay, Dara Bahri, and Marc Najork.\n[2021. Rethinking search: making domain experts](https://doi.org/10.1145/3476415.3476428)\n\n[out of dilettantes.](https://doi.org/10.1145/3476415.3476428) _SIGIR Forum_, 55(1):13:1\u201313:27.\n\n\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han[naneh Hajishirzi. 2022. MetaICL: Learning to learn](https://doi.org/10.18653/v1/2022.naacl-main.201)\n[in context. In](https://doi.org/10.18653/v1/2022.naacl-main.201) _Proceedings of the 2022 Conference of_\n_the North American Chapter of the Association for_\n_Computational Linguistics: Human Language Tech-_\n_nologies_, pages 2791\u20132809, Seattle, United States.\nAssociation for Computational Linguistics.\n\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Alme", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_10800", "chunk_text": " for_\n_Computational Linguistics: Human Language Tech-_\n_nologies_, pages 2791\u20132809, Seattle, United States.\nAssociation for Computational Linguistics.\n\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan\n[Lowe. 2022. Training language models to follow in-](https://doi.org/10.48550/ARXIV.2203.02155)\n[structions with human feedback.](https://doi.org/10.48550/ARXIV.2203.02155)\n\n\nYingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang\nRen, Wayne Xin Zhao, Daxiang Dong, Hua Wu,\nand Haifeng Wang. 2021. [RocketQA: An opti-](https://doi.org/10.18653/v1/2021.naacl-main.466)\n[mized training approach to dense passage retrieval](https://doi.org/10.18653/v1/2021.naacl-main.466)\n[for open-domain question answering. In](https://doi.org/10.18653/v1/2021.naacl-main.466) _Proceed-_\n_ings of the 2021 Conference of the North Ameri-_\n_can Chapter of the Association for Computational_\n_Linguistics: Human Language Technologies_, pages\n5835\u20135847, Online. Association for Computational\nLinguistics.\n\n\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_11250", "chunk_text": " Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich\nElsen, Siddhant Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nMichela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato,\nAngeliki Lazaridou, Arthur Mensch, Jean-Baptiste\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac,\nEd Lockhart, Simon Osindero, Laura Rimell, Chris\nDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray\nKavukcuoglu, and Geoffrey Irving. 2021. [Scal-](https://doi.org/10.48550/ARXIV.2112.11446)\n[ing language models: Methods, analysis & insights](https://doi.org/10.48550/ARXIV.2112.11446)\n[from training gopher.](https://doi.org/10.48550/ARXIV.2112.11446)\n\n\nDevendra Singh Sachan, Mike Lewis, Mandar Joshi,\nArmen Aghajanyan, Wen-tau Yih, Joelle Pineau, and\nLuke Zettlemoyer. 2022. [Improving passage re-](https://arxiv.org/abs/2204.07496)\n[trieval with", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_11700", "chunk_text": "an, Wen-tau Yih, Joelle Pineau, and\nLuke Zettlemoyer. 2022. [Improving passage re-](https://arxiv.org/abs/2204.07496)\n[trieval with zero-shot question generation.](https://arxiv.org/abs/2204.07496)\n\n\n\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F\u00e9vry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella\nBiderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. [Multitask prompted training](https://openreview.net/forum?id=9Vrb9D0WI4)\n[enables zero-shot task generalization. In](https://openreview.net/forum?id=9Vrb9D0WI4) _The Tenth_\n_International Conference on Learning Representa-_\n_tions, ICLR 2022, Virtual Event, April 25-29, 2022_ .\nOpenReview.net.\n\n\nYi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni,\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Prakash Gupta, Tal Schuster, William W.\nCohen, and Donald Metzler. 2022. [Transformer](http://arxiv.org/abs/2202.06991)\n[memory as a differentiable search index.](http://arxiv.org/abs/2202.06991) _CoRR_,\nabs", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_12150", "chunk_text": " 2022. [Transformer](http://arxiv.org/abs/2202.06991)\n[memory as a differentiable search index.](http://arxiv.org/abs/2202.06991) _CoRR_,\nabs/2202.06991.\n\n\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab[hishek Srivastava, and Iryna Gurevych. 2021. BEIR:](http://arxiv.org/abs/2104.08663)\n[A heterogenous benchmark for zero-shot evalu-](http://arxiv.org/abs/2104.08663)\n[ation of information retrieval models.](http://arxiv.org/abs/2104.08663) _CoRR_,\nabs/2104.08663.\n\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris,\nTulsee Doshi, Renelito Delos Santos, Toju Duke,\nJohnny Soraker, Ben Zevenbergen, Vinodkumar\nPrabhakaran, Mark Diaz, Ben Hutchinson, Kristen\nOlson, Alejandra Molina, Erin Hoffman-John, Josh\nLee, Lora Aroyo, Ravi Rajakumar, Alena Butryna,\nMatthew Lamm, Viktoriya Kuzmina, Joe Fenton,\nAaron Cohen, Rachel Bernstein, Ray Kurzweil,\nBlaise Aguera-Arcas, Claire Cui, Marian Croak,\nEd H. Chi, and Quoc Le. 2022. [Lamda:](http://arxiv.org/abs/2201.08239) Lan[guage models for dialog applications.](http://", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_12600", "chunk_text": " Marian Croak,\nEd H. Chi, and Quoc Le. 2022. [Lamda:](http://arxiv.org/abs/2201.08239) Lan[guage models for dialog applications.](http://arxiv.org/abs/2201.08239) _CoRR_,\nabs/2201.08239.\n\n\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2022. [GPL: Generative pseudo label-](https://doi.org/10.18653/v1/2022.naacl-main.168)\n[ing for unsupervised domain adaptation of dense re-](https://doi.org/10.18653/v1/2022.naacl-main.168)\n[trieval. In](https://doi.org/10.18653/v1/2022.naacl-main.168) _Proceedings of the 2022 Conference of_\n_the North American Chapter of the Association for_\n_Computational Linguistics: Human Language Tech-_\n_nologies_, pages 2345\u20132360, Seattle, United States.\nAssociation for Computational Linguistics.\n\n\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An\n\n[drew M. Dai, and Quoc V. Le. 2022. Finetuned lan-](https://openreview.net/forum?id=gEZrGCozdqR)\n[guage models are zero-shot learners. In](https://openreview.net/forum?id=gEZrGCozdqR) _The Tenth_\n_International Conference on Learning Representa-_\n_tions, ICLR 2022, Virtual Event, April 25-29, 2022_ .\nOpenReview.net.\n\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\n[Arnold Overwijk. 2021. Approximate nearest neigh-](https://openreview.net/forum?id=zeFrfgyZln)\n[bor negative contrastive learning for dense text re-](https://openreview.net/forum?id=zeFrfgyZln)\n[trieval. In](https://openreview.net/forum?id=zeFrfgyZln", "token_count": 500, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2212.10496_hyde_gao:chunk_13050", "chunk_text": "Zln)\n[bor negative contrastive learning for dense text re-](https://openreview.net/forum?id=zeFrfgyZln)\n[trieval. In](https://openreview.net/forum?id=zeFrfgyZln) _9th International Conference on Learning_\n_Representations, ICLR 2021, Virtual Event, Austria,_\n_May 3-7, 2021_ . OpenReview.net.\n\n\nYue Yu, Chenyan Xiong, Si Sun, Chao Zhang, and\nArnold Overwijk. 2022. Coco-dr: Combating distribution shifts in zero-shot dense retrieval with contrastive and distributionally robust learning. In _Pro-_\n_ceedings of the 2022 Conference on Empirical Meth-_\n\n_ods in Natural Language Processing_ .\n\n\nXinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin.\n2021. Mr. TyDi: A multi-lingual benchmark for\ndense retrieval. _arXiv:2108.08787_ .\n\n\n**A** **Appendix**\n\n\n**A.1** **Instructions**\n\n\n**A.1.1** **Web Search**\n\n\n\n\n\n**A.1.2** **SciFact**\n\n\n\n\n\n**A.1.3** **Arguana**\n\n\n\n\n\n**A.1.4** **TREC-COVID**\n\n\n\n\n\n**A.1.5** **FiQA**\n\n\n\n\n\n**A.1.6** **DBPedia-Entity**\n\n\n\n\n\n**A.1.7** **TREC-NEWS**\n\n\n\n\n\n**A.1.8** **Mr.TyDi**\n\n\n\n\n", "token_count": 344, "metadata": {"arxiv_id": "2212.10496", "title": "Precise Zero-Shot Dense Retrieval without Relevance Labels", "authors": ["Luyu Gao", "Xueguang Ma", "Jimmy Lin", "Jamie Callan"], "year": 2022, "url": "https://arxiv.org/pdf/2212.10496v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_0", "chunk_text": "Published as a conference paper at ICLR 2024\n\n## RAPTOR: RECURSIVE ABSTRACTIVE PROCESSING\n\n### FOR TREE-ORGANIZED RETRIEVAL\n\n\n**Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, Christopher D. Manning**\nStanford University\npsarthi@cs.stanford.edu\n\n\nABSTRACT\n\n\nRetrieval-augmented language models can better adapt to changes in world state\nand incorporate long-tail knowledge. However, most existing methods retrieve\n\n     - nly short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of\nrecursively embedding, clustering, and summarizing chunks of text, constructing\na tree with differing levels of summarization from the bottom up. At inference\ntime, our RAPTOR model retrieves from this tree, integrating information across\nlengthy documents at different levels of abstraction. Controlled experiments show\nthat retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks. On question-answering tasks\nthat involve complex, multi-step reasoning, we show state-of-the-art results; for\nexample, by coupling RAPTOR retrieval with the use of GPT-4, we can improve\nthe best performance on the QuALITY benchmark by 20% in absolute accuracy.\n\n\n1 INTRODUCTION\n\n\nLarge Language Models (LLMs) have emerged as transformative tools showing impressive performance on many tasks. With the growing size of LLMs, they can serve standalone as very effective\nknowledge stores, with facts encoded within their parameters (Petroni et al., 2019; Jiang et al., 2020;\nTalmor et al., 2020; Rae et al., 2021; Hoffmann et al., 2022; Chowdhery et al., 2022; Bubeck et al.,\n2023; Kandpal et al., 2023) and models can be further improved with fine-tuning on downstream\ntasks (Roberts et al., 2020). Nevertheless, even a large model does not contain sufficient domainspecific knowledge for particular tasks and the world continues to change, invalidating facts in the\nLLM. Updating the knowledge of these models through additional fine-tuning or editing is difficult,\nparticularly when dealing with vast text corpora (Lewis et al., 2020", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_450", "chunk_text": " particular tasks and the world continues to change, invalidating facts in the\nLLM. Updating the knowledge of these models through additional fine-tuning or editing is difficult,\nparticularly when dealing with vast text corpora (Lewis et al., 2020; Mitchell et al., 2022). An alternative approach, pioneered in open domain question answering systems (Chen et al., 2017; Yu et al.,\n2018), is to index large quantities of text, after splitting it into chunks (paragraphs), in a separate\ninformation retrieval system. Retrieved information is then presented to the LLM along with the\nquestion as context (\u201cretrieval augmentation\u201d, Lewis et al., 2020; Izacard et al., 2022; Min et al.,\n2023; Ram et al., 2023), making it easy to provide a system with current knowledge particular to\nsome domain and enabling easy interpretability and provenance tracking, whereas the parametric\nknowledge of LLMs is opaque and difficult to trace back to its source (Akyurek et al., 2022).\n\n\nNevertheless, existing retrieval-augmented approaches also have flaws. The one we tackle is that\nmost existing methods retrieve only a few short, contiguous text chunks, which limits their ability\nto represent and leverage large-scale discourse structure. This is particularly relevant for thematic\nquestions that require integrating knowledge from multiple parts of a text, such as understanding\nan entire book, as in the NarrativeQA dataset (Ko\u02c7cisk`y et al., 2018). Consider the fairy tale of\nCinderella, and the question \u201cHow did Cinderella reach her happy ending?\u201d. The top- _k_ retrieved\nshort contiguous texts will not contain enough context to answer the question.\n\n\nTo address this, we design an indexing and retrieval system that uses a tree structure to capture both\nhigh-level and low-level details about a text. As shown in Figure 1, our system, RAPTOR, clusters\nchunks of text, generates text summaries of those clusters, and then repeats, generating a tree from\nthe bottom up. This structure enables RAPTOR to load into an LLM\u2019s context chunks representing\nthe text at different levels so that it can effectively and efficiently answer questions at different levels.\n\n\n1\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: **Tree construction process:** RAPTOR recursively clusters chunks of text", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_900", "chunk_text": "\nthe text at different levels so that it can effectively and efficiently answer questions at different levels.\n\n\n1\n\n\nPublished as a conference paper at ICLR 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: **Tree construction process:** RAPTOR recursively clusters chunks of text based on their\nvector embeddings and generates text summaries of those clusters, constructing a tree from the\nbottom up. Nodes clustered together are siblings; a parent node contains the text summary of that\ncluster.\n\n\nOur main contribution is the idea of using text summarization to allow retrieval augmentation of\ncontext at different scales, and to show its effectiveness in experiments on collections of long documents. Controlled experiments with three language models (UnifiedQA (Khashabi et al., 2020),\nGPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023)) show that RAPTOR outperforms current\nretrieval augmentation. Moreover, RAPTOR coupled with GPT-4, and sometimes even with UnifiedQA, gives new state-of-the-art results on three QA tasks: free text response questions on books\nand movies (NarrativeQA, Ko\u02c7cisk`y et al. 2018), full-text NLP papers (QASPER, Dasigi et al. 2021),\nand multiple-choice questions based on medium-length passages (QuALITY, Pang et al. 2022). [1]\n\n\n2 RELATED WORK\n\n\n**Why Retrieval?** Recent advances in hardware and algorithms have indeed expanded the context lengths that models can handle, leading to questions about the need for retrieval systems (Dai\net al., 2019; Dao et al., 2022; Liu et al., 2023). However, as Liu et al. (2023) and Sun et al. (2021)\nhave noted, models tend to underutilize long-range context and see diminishing performance as context length increases, especially when pertinent information is embedded within a lengthy context.\nMoreover, practically, use of long contexts is expensive and slow. This suggests that selecting the\nmost relevant information for knowledge-intensive tasks is still crucial.\n\n\n**Retrieval Methods** Retrieval-augmented language models (RALMs) have seen improvements in\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\nhave transitioned from traditional term-based techniques like **TF-IDF** (Sp\u00a8arck Jones,", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_1350", "chunk_text": "Ms) have seen improvements in\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\nhave transitioned from traditional term-based techniques like **TF-IDF** (Sp\u00a8arck Jones, 1972) and\n**BM25** (Robertson et al., 1995; Roberts et al., 2020) to deep learning\u2013based strategies (Karpukhin\net al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using\nlarge language models as retrievers due to their ability to memorize extensive knowledge (Yu et al.,\n2022; Sun et al., 2022). Research on the reader component includes **Fusion-in-Decoder (FiD)**\n(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages\nindependently in the encoder and **RETRO** (Borgeaud et al., 2022; Wang et al., 2023), which utilizes\ncross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context.\n\n\nEnd-to-end system training work includes **Atlas** (Izacard et al., 2022), which fine-tunes an encoderdecoder model in conjunction with the retriever; **REALM** (Guu et al., 2020), a bidirectional, masked\nLM fine-tuned for open-domain question answering; and **RAG (Retrieval-Augmented Genera-**\n**tion)** (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural\nretriever. Min et al. (2021) introduced **Joint Passage Retrieval (JPR)** model which uses a treedecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. **Dense Hi-**\n**erarchical Retrieval (DHR)** and **Hybrid Hierarchical Retrieval (HHR)** represent advancements\nin retrieval accuracy by combining document and passage level retrievals and integrating sparse and\ndense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023).\n\n\n[1We will release the code of RAPTOR publicly here.](https://github.com/parthsarthi03/raptor)\n\n\n2\n\n\nPublished as a conference paper at", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_1800", "chunk_text": "1; Arivazhagan et al., 2023).\n\n\n[1We will release the code of RAPTOR publicly here.](https://github.com/parthsarthi03/raptor)\n\n\n2\n\n\nPublished as a conference paper at ICLR 2024\n\n\nDespite a diversity in methods, the retrieving components of models predominantly rely on standard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this\napproach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous segmentation might not capture the complete semantic depth of the text. Reading extracted snippets\nfrom technical or scientific documents may lack important context making them difficult to read or\neven misleading. (Cohan & Goharian, 2017; Newman et al., 2023; Zhang et al., 2023).\n\n\n**Recursive summarization as Context** Summarization techniques provide a condensed view of\ndocuments, enabling more focused engagement with the content (Angelidis & Lapata, 2018). The\nsummarization/snippet model by Gao et al. (2023) uses summarizations and snippets of passages,\nwhich improves correctness on most datasets but can sometimes be a lossy means of compression.\nThe recursive-abstractive summarization model by Wu et al. (2021) employs task decomposition\nto summarize smaller text chunks, which are later integrated to form summaries of larger sections.\nWhile this method is effective for capturing broader themes, it can miss granular details. LlamaIndex\n(Liu, 2022) mitigates this issue by similarly summarizing adjacent text chunks but also retaining\nintermediate nodes thus storing varying levels of detail, keeping granular details. However, both\nmethods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still\n\n- verlook distant interdependencies within the text, which we can find and group with RAPTOR.\n\n\n3 METHODS\n\n\n**Overview of RAPTOR** Building on the idea that long texts often present subtopics and hierarchical structures (Cao & Wang, 2022; Dong et al., 2023b), RAPTOR addresses the issue of semantic\ndepth and connection in reading by building a recursive tree structure that balances broader thematic\ncomprehension with granular details and which allows nodes to be grouped based on semantic similarity not just order in the text.\n\n\nConstruction of the RAPTOR tree begins with segment", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_2250", "chunk_text": " and connection in reading by building a recursive tree structure that balances broader thematic\ncomprehension with granular details and which allows nodes to be grouped based on semantic similarity not just order in the text.\n\n\nConstruction of the RAPTOR tree begins with segmenting the retrieval corpus into short, contiguous\ntexts of length 100, similar to traditional retrieval augmentation techniques. If a sentence exceeds the\n100-token limit, we move the entire sentence to the next chunk, rather than cutting it mid-sentence.\nThis preserves the contextual and semantic coherence of the text within each chunk. These texts\nare then embedded using SBERT, a BERT-based encoder (multi-qa-mpnet-base-cos-v1)\n(Reimers & Gurevych, 2019). The chunks and their corresponding SBERT embeddings form the\nleaf nodes of our tree structure.\n\n\nTo group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model\nis used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle\n\n- f embedding, clustering, and summarization continues until further clustering becomes infeasible,\nresulting in a structured, multi-layered tree representation of the original documents. An important\naspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build\ntime and token expenditure, making it suitable for processing large and complex corpora. For a\ncomprehensive discussion on RAPTOR\u2019s scalability, please refer to the Appendix A.\n\n\nFor querying within this tree, we introduce two distinct strategies: tree traversal and collapsed tree.\nThe tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant\nnodes at each level. The collapsed tree method evaluates nodes collectively across all layers to find\nthe most relevant ones.\n\n\n**Clustering Algorithm** Clustering plays a key role in building the RAPTOR tree, organizing text\nsegments into cohesive groups. This step groups related content together, which helps the subsequent retrieval process.\n\n\nOne of the unique aspects of our clustering approach is the use of soft clustering, where nodes can\nbelong to multiple clusters without requiring a fixed number of clusters. This flexibility is essential because individual text segments often contain information relevant to various topics, thereby\nwarranting their inclusion in multiple summaries.\n\n\nOur clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers\nboth flexibility and a probabilistic framework. GMMs assume", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_2700", "chunk_text": " contain information relevant to various topics, thereby\nwarranting their inclusion in multiple summaries.\n\n\nOur clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers\nboth flexibility and a probabilistic framework. GMMs assume that data points are generated from a\nmixture of several Gaussian distributions.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2024\n\n\nGiven a set of _N_ text segments, each represented as a _d_ - dimensional dense vector embedding, the\nlikelihood of a text vector, **x**, given its membership in the _k_ _[th]_ Gaussian distribution, is denoted by\n_P_ ( **x** _|k_ ) = _N_ ( **x** ; _\u00b5k,_ **\u03a3** _k_ ). The overall probability distribution is a weighted combination _P_ ( **x** ) =\n\n- _Kk_ =1 _[\u03c0][k][N]_ [(] **[x]** [;] _[ \u00b5][k][,]_ **[ \u03a3]** _[k]_ [)][, where] _[ \u03c0][k]_ [ signifies the mixture weight for the] _[ k]_ [th][ Gaussian distribution.]\n\n\nThe high dimensionality of vector embeddings presents a challenge for traditional GMMs, as distance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Aggarwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection\n(UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018). The\nnumber of nearest neighbors parameter, _n_ ~~_n_~~ _eighbors_, in UMAP determines the balance between\nthe preservation of local and global structures. Our algorithm varies _n_ ~~_n_~~ _eighbors_ to create a hierarchical clustering structure: it first identifies global clusters and then performs local clustering within\nthese global clusters. This two-step clustering process captures a broad spectrum of relationships\namong the text data, from broad themes to specific details.\n\n\nShould a local cluster\u2019s combined context ever exceed the summarization model\u2019s token threshold,\n\n- ur algorithm recursively applies clustering within the cluster, ensuring that the context remains\nwithin the token threshold.\n\n\nTo determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)\nfor model selection. BIC not only penal", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_3150", "chunk_text": " threshold,\n\n- ur algorithm recursively applies clustering within the cluster, ensuring that the context remains\nwithin the token threshold.\n\n\nTo determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC)\nfor model selection. BIC not only penalizes model complexity but also rewards goodness of fit\n(Schwarz, 1978). The BIC for a given GMM is _BIC_ = ln( _N_ ) _k \u2212_ 2 ln( _L_ [\u02c6] ), where _N_ is the number\n\n- f text segments (or data points), _k_ is the number of model parameters, and _L_ [\u02c6] is the maximized\nvalue of the likelihood function of the model. In the context of GMM, the number of parameters _k_\nis a function of the dimensionality of the input vectors and the number of clusters.\n\n\nWith the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm\nis then used to estimate the GMM parameters, namely the means, covariances, and mixture weights.\n\n\nWhile the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which\n\n- ften exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an\neffective model for our purpose. We run an ablation comparing GMM Clustering with summarizing\ncontiguous chunks and provide details in Appendix B.\n\n\n**Model-Based Summarization** After clustering the nodes using Gaussian Mixture Models, the\nnodes in each cluster are sent to a language model for summarization. This step allows the model\nto transform large chunks of text into concise, coherent summaries of the selected nodes. For our\nexperiments, we use gpt-3.5-turbo to generate the summaries. The summarization step condenses the potentially large volume of retrieved information into a manageable size. We provide\nstatistics on the compression due to the summarization in Appendix C and the prompt used for\nsummarization in Appendix D.\n\n\nWhile the summarization model generally produces reliable summaries, a focused annotation study\nrevealed that about 4% of the summaries contained minor hallucinations. These did not propagate\nto parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis\n\n- f hallucinations, refer to the appendix E.\n\n\n**Querying** In this section, we elaborate on the two", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_3600", "chunk_text": " not propagate\nto parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis\n\n- f hallucinations, refer to the appendix E.\n\n\n**Querying** In this section, we elaborate on the two querying mechanisms employed by RAPTOR:\ntree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered\nRAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We\nprovide the pseudocode of both methods in Appendix F. Note that we embed all nodes using SBERT.\n\n\nThe **tree traversal** method first selects the top-k most relevant root nodes based on their cosine\nsimilarity to the query embedding. The children of these selected nodes are considered at the next\nlayer and the top-k nodes are selected from this pool again based on their cosine similarity to the\nquery vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected\nnodes is concatenated to form the retrieved context. The algorithm\u2019s steps are outlined below:\n\n\n1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the\nquery embedding and the embeddings of all nodes present at this initial layer.\n\n\n2. Choose the top- _k_ nodes based on the highest cosine similarity scores, forming the set _S_ 1.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2024\n\n\nFigure 2: **Illustration of the tree traversal and collapsed tree retrieval mechanisms.** Tree traversal starts at the root level of the tree and retrieves the top- _k_ (here, top-1) node(s) based on cosine\nsimilarity to the query vector. At each level, it retrieves the top- _k_ node(s) from the child nodes of\nthe previous layer\u2019s top- _k_ . Collapsed tree collapses the tree into a single layer and retrieves nodes\nuntil a threshold number of tokens is reached, based on cosine similarity to the query vector. The\nnodes on which cosine similarity search is performed are highlighted in both illustrations.\n\n\n3. Proceed to the child nodes of the elements in set _S_ 1. Compute the cosine similarity between\nthe query vector and the vector embeddings of these child nodes.\n\n\n4. Select the top _k_ child nodes with the highest cosine similarity scores to the query, forming\nthe set _S_ 2.\n\n\n5. Continue this process recursively for", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_4050", "chunk_text": "\nthe query vector and the vector embeddings of these child nodes.\n\n\n4. Select the top _k_ child nodes with the highest cosine similarity scores to the query, forming\nthe set _S_ 2.\n\n\n5. Continue this process recursively for _d_ layers, producing sets _S_ 1 _, S_ 2 _, . . ., Sd_ .\n\n\n6. Concatenate sets _S_ 1 through _Sd_ to assemble the relevant context to the query.\n\n\nBy adjusting the depth _d_ and the number of nodes _k_ selected at each layer, the tree traversal method\n\n- ffers control over the specificity and breadth of the information retrieved. The algorithm starts with\na broad outlook by considering the top layers of the tree and progressively focuses on finer details\nas it descends through the lower layers.\n\n\nThe **collapsed tree** approach offers a simpler way to search for relevant information by considering\nall nodes in the tree simultaneously, as depicted in Figure 2. Instead of going layer-by-layer, this\nmethod flattens the multi-layered tree into a single layer, essentially bringing all the nodes onto the\nsame level for comparison. The steps for this method are outlined below:\n\n\n1. First, collapse the entire RAPTOR tree into a single layer. This new set of nodes, denoted\nas _C_, contains nodes from every layer of the original tree.\n\n\n2. Next, calculate the cosine similarity between the query embedding and the embeddings of\nall nodes present in the collapsed set _C_ .\n\n\n3. Finally, pick the top- _k_ nodes that have the highest cosine similarity scores with the query.\nKeep adding nodes to the result set until you reach a predefined maximum number of\ntokens, ensuring you don\u2019t exceed the model\u2019s input limitations.\n\n\nWe tested both approaches on 20 stories from the QASPER dataset. Figure 3 shows the performance\n\n- f tree traversal with different top- sizes and collapsed tree with different maximum token numbers.\nThe collapsed tree approach consistently performs better. We believe collapsed tree retrieval is\nbetter due to offering greater flexibility than tree traversal; i.e., by searching through all the nodes\nsimultaneously, it retrieves information that is at the correct level of granularity for a given question.\nIn comparison, while using tree traversal with the same values of _d_ and _k_, the ratio of nodes from\neach level of the tree will be constant. So, the", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_4500", "chunk_text": " that is at the correct level of granularity for a given question.\nIn comparison, while using tree traversal with the same values of _d_ and _k_, the ratio of nodes from\neach level of the tree will be constant. So, the ratio of higher-order thematic information to granular\ndetails will remain the same regardless of the question.\n\n\n5\n\n\nPublished as a conference paper at ICLR 2024\n\n\nOne drawback, however, of the collapsed tree approach is that it requires cosine similarity search to\nbe performed on all nodes in the tree. However, this can be made more efficient with fast _k_ - nearest\nneighbor libraries such as FAISS (Johnson et al., 2019).\n\n\nOverall, given the collapsed tree approach\u2019s\ngreater flexibility and its superior performance\n\n                         - n the subset of the QASPER dataset, this is\nthe querying approach with which we proceed.\nSpecifically, we use the collapsed tree with\n2000 maximum tokens, which approximately\nequates to retrieving the top-20 nodes. Using a\ntoken-based approach ensures the context does\nnot exceed model context constraints as token\ncounts can vary across nodes. For experiments\nwith the UnifiedQA model, we provide 400 tokens of context, as UnifiedQA has a max context length of 512 tokens. We provide the same\namount of tokens of context to RAPTOR and to\n\nthe baselines.\n\nFigure 3: **Comparison of querying methods.**\nResults on 20 stories from the QASPER dataset\n\n**Qualitative Study** We conduct a qualitative\n\nusing tree traversal with different top-k values,\n\nanalysis to understand the benefits of RAP\nand collapsed tree with different context lengths.\n\nTOR\u2019s retrieval process compared to Dense\n\nCollapsed tree with 2000 tokens produces the best\n\nPassage Retrieval (DPR) methods. Our study\n\nresults, so we use this querying strategy for our\n\nfocuses on thematic, multi-hop questions using\n\nmain results.\n\na 1500-word Cinderella fairytale. As illustrated\nin Figure 4, RAPTOR\u2019s tree-based retrieval allows it to choose nodes from different tree layers,\nmatching the question\u2019s detail level. This approach often yields more relevant and comprehensive\ninformation for downstream tasks than DPR. For a detailed discussion and examples, including the\ntext retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G.\n\n\n4 EXPERIMENTS\n\n\n**Datasets** We", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_4950", "chunk_text": " and comprehensive\ninformation for downstream tasks than DPR. For a detailed discussion and examples, including the\ntext retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G.\n\n\n4 EXPERIMENTS\n\n\n**Datasets** We measure RAPTOR\u2019s performance across three question-answering datasets: NarrativeQA, QASPER, and QuALITY.\n\n\nNarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books\nand movie transcripts, totaling 1,572 documents (Ko\u02c7cisk`y et al., 2018; Wu et al., 2021). The\nNarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order\nto accurately answer its questions, thus testing the model\u2019s ability to comprehend longer texts in\nthe literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),\nROUGE (R-L), and METEOR (M) metrics. Please see appendix H for more details on the NarrativeQA evaluation script used in our experiments.\n\n\nThe QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing\nfor information embedded within the full text (Dasigi et al., 2021). The answer types in QASPER\nare categorized as Answerable/Unanswerable, Yes/No, Abstractive, and Extractive. Accuracy is\nmeasured using standard F1.\n\n\nLastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context\npassages averaging approximately 5,000 tokens in length (Pang et al., 2022). This dataset calls for\nreasoning over the entire document for QA tasks, enabling us to measure the performance of our retrieval system on medium-length documents. The dataset includes a challenging subset, QuALITYHARD, which contains questions that a majority of human annotators answered incorrectly in a\nspeed-setting. We report accuracies for both the entire test set and the HARD subset.\n\n\n**Controlled Baseline Comparisons** We first present controlled comparisons using the UnifiedQA\n3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),\nand DPR (Karpukhin et al., 2020) as the embedding models with and without the R", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_5400", "chunk_text": "imers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),\nand DPR (Karpukhin et al., 2020) as the embedding models with and without the RAPTOR tree\nstructure, on three datasets: QASPER, NarrativeQA, and QuALITY. As shown in Tables 1 and 2,\n\n\n6\n\n\nPublished as a conference paper at ICLR 2024\n\n\nFigure 4: **Querying Process:** Illustration of how RAPTOR retrieves information for two questions\nabout the Cinderella story: \u201cWhat is the central theme of the story?\u201d and \u201cHow did Cinderella find\na happy ending?\u201d. Highlighted nodes indicate RAPTOR\u2019s selections, while arrows point to DPR\u2019s\nleaf nodes. Notably, RAPTOR\u2019s context often encompasses the information retrieved by DPR, either\ndirectly or within higher-layer summaries.\n\n\n- ur results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms\nthe respective retriever across all datasets. [2]\n\n\nSince RAPTOR with SBERT has the best performance, we use it in all subsequent experiments.\nWe now compare RAPTOR with BM25 and DPR, using three different LLMs: GPT-3, GPT-4, and\nUnifiedQA. As shown in Table 3, RAPTOR consistently outperforms BM25 and DPR across all\nthree Language Models on the QASPER dataset. RAPTOR\u2019s F-1 Match scores are 53.1%, 55.7%,\nand 36.6% when using GPT-3, GPT-4, and UnifiedQA, respectively. These scores surpass DPR by\nmargins of 1.8, 2.7, and 4.5 points, and outdo BM25 by 6.5, 5.5, and 10.2 points across the respective\nLLMs. QASPER requires synthesizing information within NLP papers, so it is unsurprising that\nRAPTOR\u2019s higher-level summary nodes would allow it to outperform methods that can only extract\nthe top- _k_ most similar raw chunks of text, which may not contain the correct response in isolation.\n\n\nTable 1: **NarrativeQA Performance With + Without RAPTOR:** Performance comparison of\nvarious retrieval methods (SBERT, BM", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_5850", "chunk_text": "- _k_ most similar raw chunks of text, which may not contain the correct response in isolation.\n\n\nTable 1: **NarrativeQA Performance With + Without RAPTOR:** Performance comparison of\nvarious retrieval methods (SBERT, BM25, DPR) with and without RAPTOR on the NarrativeQA\ndataset, using UnifiedQA-3B as the language model. RAPTOR outperforms baselines of each respective retrieval method.\n\n\n**Model** **ROUGE** **BLEU-1** **BLEU-4** **METEOR**\n\n\n**SBERT with RAPTOR** **30.87%** **23.50%** **6.42%** **19.20%**\n\nSBERT without RAPTOR 29.26% 22.56% 5.95% 18.15%\n\n**BM25 with RAPTOR** **27.93%** **21.17%** **5.70%** **17.03%**\n\nBM25 without RAPTOR 23.52% 17.73% 4.65% 13.98%\n\n**DPR with RAPTOR** **30.94%** **23.51%** **6.45%** **19.05%**\n\nDPR without RAPTOR 29.56% 22.84% 6.12% 18.44%\n\n\nLikewise, in the QuALITY dataset as shown in Table 4, RAPTOR achieves an accuracy of 62.4%,\nwhich is a 2% and 5.1% improvement over DPR and BM25. Similar trends are observed when UnifiedQA is employed, with RAPTOR outperforming DPR and BM25 by 2.7% and 6.7%, respectively.\n\n\nFinally, in the NarrativeQA dataset, as presented in Table 6, RAPTOR excels across multiple metrics. For ROUGE-L, it surpasses BM25 and DPR by 7.3 and 2.7 points, respectively. In other\nmetrics like BLEU-1, BLEU-4, and METEOR, RAPTOR outperforms BM25 and DPR by margins\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\n\n\n2For the DPR experiments in Tables 1 and 2, we used the", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_6300", "chunk_text": "forms BM25 and DPR by margins\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\n\n\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\nthe performance observed in Karpukhin et al. (2020), where dpr-multiset-base showed superior results.\n\n\n7\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTable 2: **QuALITY and QASPER Performance With + Without RAPTOR:** Performance comparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outperforms baselines of each respective retrieval method for both datasets.\n\n\n**Model** **Accuracy (QuALITY)** **Answer F1 (QASPER)**\n\n\n**SBERT with RAPTOR** **56.6%** **36.70%**\n\nSBERT without RAPTOR 54.9% 36.23%\n\n**BM25 with RAPTOR** **52.1%** **27.00%**\n\nBM25 without RAPTOR 49.9% 26.47%\n\n**DPR with RAPTOR** **54.7%** **32.23%**\n\nDPR without RAPTOR 53.1% 31.70%\n\n\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different language models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column \u201dTitle +\nAbstract\u201d reflects performance when only the title and abstract of the papers are used for context.\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\nSpecifically, RAPTOR\u2019s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\nhigher than BM25.\n\n\n**Retriever** **GPT-3 F-1 Match** **GPT-4 F-1 Match** **UnifiedQA F-1 Match**\n\n\nTitle + Abstract 25.2 ", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_6750", "chunk_text": "% points\nhigher than BM25.\n\n\n**Retriever** **GPT-3 F-1 Match** **GPT-4 F-1 Match** **UnifiedQA F-1 Match**\n\n\nTitle + Abstract 25.2 22.2 17.5\n\nBM25 46.6 50.2 26.4\n\nDPR 51.3 53.0 32.1\n\n**RAPTOR** **53.1** **55.7** **36.6**\n\n\n\n**Comparison** **to** **State-of-the-art** **Systems**\nBuilding upon our controlled comparisons,\nwe examine RAPTOR\u2019s performance relative\nto other state-of-the-art models. As shown\nin Table 5, RAPTOR with GPT-4 sets a new\nbenchmark on QASPER, with a 55.7% F-1\nscore, surpassing the CoLT5 XL\u2019s score of\n53.9%.\n\n\nIn the QuALITY dataset, as shown in Table 7,\nRAPTOR paired with GPT-4 sets a new state\n- f-the-art with an accuracy of 82.6%, surpassing the previous best result of 62.3%. In particular, it outperforms CoLISA by 21.5% on\nQuALITY-HARD, which represents questions\nthat humans took unusually long to correctly\nanswer, requiring rereading parts of the text,\ndifficult reasoning, or both.\n\n\n\nLongT5 XL (Guo et al., 2022) 53.1\nCoLT5 XL (Ainslie et al., 2023) 53.9\n**RAPTOR + GPT-4** **55.7**\n\n\n\nTable 4: Comparison of accuracies on the QuALITY dev dataset for two different language models (GPT-3, UnifiedQA 3B) using various retrieval\nmethods. RAPTOR outperforms the baselines of\nBM25 and DPR by at least 2.0% in accuracy.\n\n\n**Model** **GPT-3 Acc.** **UnifiedQA Acc.**\n\n\nBM25 57.3 49.9\n\nDPR 60.4 53.9\n\n**RAPTOR** **62.4** **56.6**\n\n\nTable 5: Results on F-1 Match scores of various\nmodels on the QASPER", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_7200", "chunk_text": "49.9\n\nDPR 60.4 53.9\n\n**RAPTOR** **62.4** **56.6**\n\n\nTable 5: Results on F-1 Match scores of various\nmodels on the QASPER dataset.\n\n\n**Model** **F-1 Match**\n\n\n\nFor the NarrativeQA dataset, as represented in **RAPTOR + GPT-4** **55.7**\nTable 6, RAPTOR paired with UnifiedQA sets\na new state-of-the-art METEOR score. When compared to the recursively summarizing model by\nWu et al. (2021), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While\nWu et al. (2021) rely solely on the summary in the top root node of the tree structure, RAPTOR\nbenefits from its intermediate layers and clustering approaches, which allows it to capture a range of\ninformation, from general themes to specific details, contributing to its overall strong performance.\n\n\n\n4.1 CONTRIBUTION OF THE TREE STRUCTURE\n\n\nWe examine the contribution of each layer of nodes to RAPTOR\u2019s retrieval capabilities. We hypothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring\na broader understanding of the text.\n\n\n8\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTable 6: Performance comparison on the NarrativeQA dataset across multiple models, focusing\n\n- n four metrics: ROUGE-L, BLEU-1, BLEU-4, and METEOR. RAPTOR, when paired with UnifiedQA 3B, not only surpasses retrieval methods like BM25 and DPR but also sets a new state-ofthe-art in the METEOR metric.\n\n\n**Model** **ROUGE-L** **BLEU-1** **BLEU-4** **METEOR**\n\n\nBiDAF (Ko\u02c7cisk`y et al., 2018) 6 _._ 2 5 _._ 7 0 _._ 3 3 _._ 7\nBM25 + BERT (Mou et al., 2020) 15 _._ 5 14 _._ 5 1 _._ 4 5 _._ 0\nRecursively Summarizing Books (Wu et al., 2021) 21 _._ 6 22 _._ 3 4 _", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_7650", "chunk_text": " 14 _._ 5 1 _._ 4 5 _._ 0\nRecursively Summarizing Books (Wu et al., 2021) 21 _._ 6 22 _._ 3 4 _._ 2 10 _._ 6\nRetriever + Reader (Izacard & Grave, 2022) **32.0** **35.3** **7.5** 11 _._ 1\n**RAPTOR + UnifiedQA** 30.8 23.5 6.4 **19.1**\n\n\nTable 7: Accuracies of the QuALITY dataset on both the overall test set and the more challenging\nhard subset. GPT-4 with RAPTOR sets a new state-of-the-art.\n\n\n**Accuracy**\n**Model**\n\n\n**Test Set** **Hard Subset**\n\n\nLongformer-base (Beltagy et al., 2020) 39 _._ 5 35 _._ 3\nDPR and DeBERTaV3-large (Pang et al., 2022) 55 _._ 4 46 _._ 1\nCoLISA (DeBERTaV3-large) (Dong et al., 2023a) 62 _._ 3 54 _._ 7\n**RAPTOR + GPT-4** **82.6** **76.2**\n\n\nTable 8: Performance of RAPTOR when querying different tree layers for Story 1 from the QuALITY dataset. Columns represent different starting points (highest layer) and rows represent different\nnumbers of layers queried.\n\n\n**Layers Queried / Start Layer** **Layer 0 (Leaf Nodes)** **Layer 1** **Layer 2**\n\n\n1 layer 57.9 57.8 57.9\n2 layers        - 52.6 63.15\n3 layers        -        - **73.68**\n\n\nWe validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in\nappendix G. To quantitatively understand the contribution of the upper-level nodes, we used stories\nfrom the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in\nSection 3. However, during retrieval, we limit the search to different subsets of layers. For example,\nwe exclusively retrieve from the leaf", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_8100", "chunk_text": " QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in\nSection 3. However, during retrieval, we limit the search to different subsets of layers. For example,\nwe exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous\nsubsets of the layers. We show findings specific to one story in Table 8, revealing that a full-tree\nsearch, utilizing all layers, outperformed retrieval strategies that focused only on specific layers.\n\n\nThese findings highlight the importance of the full tree structure in RAPTOR. By providing both\nthe original text and higher-level summaries for retrieval, RAPTOR can effectively handle a wider\nrange of questions, from higher-order thematic queries to detail-oriented questions. Detailed results\nfor additional stories and an ablation study on layer contributions can be found in Appendix I.\n\n\n5 CONCLUSION\n\n\nIn this paper, we have presented RAPTOR, a novel tree-based retrieval system that augments the\nparametric knowledge of large language models with contextual information at various levels of\nabstraction. By employing recursive clustering and summarization techniques, RAPTOR creates a\nhierarchical tree structure that is capable of synthesizing information across various sections of the\nretrieval corpora. During the query phase, RAPTOR leverages this tree structure for more effective\nretrieval. Our controlled experiments demonstrated that RAPTOR not only outperforms traditional\nretrieval methods but also sets new performance benchmarks on several question-answering tasks.\n\n\n9\n\n\nPublished as a conference paper at ICLR 2024\n\n\n6 REPRODUCIBILITY STATEMENT\n\n\n**Language Models for QA and Summarization** Four language models are used in our RAPTOR\nexperiments: GPT-3 and GPT-4 for QA tasks, and GPT-3.5-turbo for summarization. The gpt-3,\n[gpt-4, and gpt-3.5-turbo models can be accessed via API calls (OpenAI API). UnifiedQA,](https://beta.openai.com/examples/)\n[which is used for QA tasks, is publicly available at Hugging Face.](https://huggingface.co/allenai/unifiedqa-v2-t5-3b-1363200)\n\n\n**Evaluation Datasets** [The three evaluation datasets used in our experiments\u2014QuALITY,](https://github.com/", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_8550", "chunk_text": "](https://huggingface.co/allenai/unifiedqa-v2-t5-3b-1363200)\n\n\n**Evaluation Datasets** [The three evaluation datasets used in our experiments\u2014QuALITY,](https://github.com/nyu-mll/quality)\n[QASPER, and NarrativeQA\u2014are all publicly accessible. These datasets ensure that the retrieval](https://allenai.org/data/qasper)\nand QA tests conducted in this study can be replicated.\n\n\n**Source Code** [The source code for RAPTOR will be publicly available here.](https://github.com/parthsarthi03/raptor)\n\n\nREFERENCES\n\n\nCharu C Aggarwal, Alexander Hinneburg, and Daniel A Keim. On the Surprising Behavior of Distance Metrics in High Dimensional Space. In _Database Theory\u2014ICDT 2001: 8th International_\n_Conference London, UK, January 4\u20136, 2001 Proceedings 8_, pp. 420\u2013434. Springer, 2001. URL\n[https://link.springer.com/chapter/10.1007/3-540-44503-x_27.](https://link.springer.com/chapter/10.1007/3-540-44503-x_27)\n\n\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta\u02dcn\u00b4on, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. CoLT5: Faster long-range\ntransformers with conditional computation. _arXiv preprint arXiv:2303.09752_, 2023. URL\n[https://arxiv.org/abs/2303.09752.](https://arxiv.org/abs/2303.09752)\n\n\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and\nKelvin Guu. Towards tracing knowledge in language models back to the training data. In\n_Findings of the Association for Computational Linguistics: EMNLP 2022_, pp. 2429\u20132446,\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n[doi: 10.18653/v1/2022.findings-emn", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_9000", "chunk_text": "LP 2022_, pp. 2429\u20132446,\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.\n[doi: 10.18653/v1/2022.findings-emnlp.180. URL https://aclanthology.org/2022.](https://aclanthology.org/2022.findings-emnlp.180)\n[findings-emnlp.180.](https://aclanthology.org/2022.findings-emnlp.180)\n\n\nStefanos Angelidis and Mirella Lapata. Summarizing opinions: Aspect extraction meets sentiment\nprediction and they are both weakly supervised. _arXiv preprint arXiv:1808.08858_, 2018. URL\n[https://arxiv.org/abs/1808.08858.](https://arxiv.org/abs/1808.08858)\n\n\nManoj Ghuhan Arivazhagan, Lan Liu, Peng Qi, Xinchi Chen, William Yang Wang, and Zhiheng\nHuang. Hybrid hierarchical retrieval for open-domain question answering. In Anna Rogers,\nJordan Boyd-Graber, and Naoaki Okazaki (eds.), _Findings of the Association for Computational_\n_Linguistics: ACL 2023_, pp. 10680\u201310689, Toronto, Canada, July 2023. Association for Computa[tional Linguistics. doi: 10.18653/v1/2023.findings-acl.679. URL https://aclanthology.](https://aclanthology.org/2023.findings-acl.679)\n\n[org/2023.findings-acl.679.](https://aclanthology.org/2023.findings-acl.679)\n\n\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-document Transformer,\n[2020. URL https://arxiv.org/abs/2004.05150. arXiv preprint arXiv:2004.05150.](https://arxiv.org/abs/2004.05150)\n\n\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_9450", "chunk_text": "aud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.\nImproving language models by retrieving from trillions of tokens. In _International conference on_\n_machine learning_ [, pp. 2206\u20132240. PMLR, 2022. URL https://arxiv.org/abs/2112.](https://arxiv.org/abs/2112.04426)\n[04426.](https://arxiv.org/abs/2112.04426)\n\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel\nZiegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neu-_\n_ral Information Processing Systems_, volume 33, pp. 1877\u20131901. Curran Associates, Inc.,\n\n\n10\n\n\nPublished as a conference paper at ICLR 2024\n\n\n2020. [URL https://proceedings.neurips.cc/paper_files/paper/2020/](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)\n[file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_9900", "chunk_text": "d6bfcb4967418bfb8ac142f64a-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)\n\n\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of Artificial General\nIntelligence: Early Experiments with GPT-4. _arXiv preprint arXiv:2303.12712_, 2023. URL\n[https://arxiv.org/abs/2303.12712.](https://arxiv.org/abs/2303.12712)\n\n\nShuyang Cao and Lu Wang. HIBRIDS: Attention with hierarchical biases for structure-aware long\ndocument summarization. In _Proceedings of the 60th Annual Meeting of the Association for_\n_Computational Linguistics (Volume 1: Long Papers)_, pp. 786\u2013807, Dublin, Ireland, May 2022.\n[Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.58. URL https:](https://aclanthology.org/2022.acl-long.58)\n[//aclanthology.org/2022.acl-long.58.](https://aclanthology.org/2022.acl-long.58)\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer\nOpen-Domain Questions. In _Proceedings of the 55th Annual Meeting of the Association for_\n_Computational Linguistics (Volume 1: Long Papers)_, pp. 1870\u20131879, Vancouver, Canada, July\n[2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https:](https://aclanthology.org/P17-1171)\n[//aclanthology.org/P17-1171.](https://aclanthology.org/P17-1171)\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_10350", "chunk_text": "ology.org/P17-1171.](https://aclanthology.org/P17-1171)\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\nScaling Language Modeling with Pathways. _arXiv preprint arXiv:2204.02311_, 2022. URL\n[https://arxiv.org/abs/2204.02311.](https://arxiv.org/abs/2204.02311)\n\n\nArman Cohan and Nazli Goharian. Contextualizing citations for scientific summarization using\nword embeddings and domain knowledge. In _Proceedings of the 40th International ACM SIGIR_\n_Conference on Research and Development in Information Retrieval_, pp. 1133\u20131136, 2017. URL\n[https://dl.acm.org/doi/abs/10.1145/3077136.3080740.](https://dl.acm.org/doi/abs/10.1145/3077136.3080740)\n\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a fixed-length context. In _Proceedings of the_\n_57th Annual Meeting of the Association for Computational Linguistics_, pp. 2978\u20132988, Florence,\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\n[https://aclanthology.org/P19-1285.](https://aclanthology.org/P19-1285)\n\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. FlashAttention: Fast and\nmemory-efficient exact attention with IO-Awareness. _Advances in Neural Information Processing_\n_Systems_ [, 35:16344\u201316359, 2022. URL https://arxiv.org/abs/2205.14135.](https://arxiv.org/abs/2205.14135)\n\n\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A.", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_10800", "chunk_text": " https://arxiv.org/abs/2205.14135.](https://arxiv.org/abs/2205.14135)\n\n\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A Dataset\n\n - f Information-Seeking Questions and Answers Anchored in Research Papers. In _Proceed-_\n_ings of the 2021 Conference of the North American Chapter of the Association for Computa-_\n_tional Linguistics: Human Language Technologies_, pp. 4599\u20134610, Online, June 2021. Asso[ciation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https:](https://aclanthology.org/2021.naacl-main.365)\n[//aclanthology.org/2021.naacl-main.365.](https://aclanthology.org/2021.naacl-main.365)\n\n\nMengxing Dong, Bowei Zou, Yanling Li, and Yu Hong. CoLISA: Inner Interaction via Contrastive\nLearning for Multi-choice Reading Comprehension. In _Advances in Information Retrieval: 45th_\n_European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2\u20136, 2023,_\n_Proceedings, Part I_ [, pp. 264\u2013278. Springer, 2023a. URL https://link.springer.com/](https://link.springer.com/chapter/10.1007/978-3-031-28244-7_17)\n[chapter/10.1007/978-3-031-28244-7_17.](https://link.springer.com/chapter/10.1007/978-3-031-28244-7_17)\n\n\nZican Dong, Tianyi Tang, Lunyi Li, and Wayne Xin Zhao. A survey on long text modeling with\ntransformers. _arXiv preprint arXiv:2302.14502_ [, 2023b. URL https://arxiv.org/abs/](https://arxiv.org/abs/2302.14502)\n[2302.14502.](https://arxiv.org/abs/2302.14502)\n\n\nTianyu Gao, Howard Yen, J", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_11250", "chunk_text": "abs/](https://arxiv.org/abs/2302.14502)\n[2302.14502.](https://arxiv.org/abs/2302.14502)\n\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate\ntext with citations. _arXiv preprint arXiv:2305.14627_ [, 2023. URL https://arxiv.org/](https://arxiv.org/abs/2305.14627)\n[abs/2305.14627.](https://arxiv.org/abs/2305.14627)\n\n\n11\n\n\nPublished as a conference paper at ICLR 2024\n\n\nMandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and\nYinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In _Findings of the_\n_Association for Computational Linguistics: NAACL 2022_, pp. 724\u2013736, Seattle, United States,\nJuly 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55.\n[URL https://aclanthology.org/2022.findings-naacl.55.](https://aclanthology.org/2022.findings-naacl.55)\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval Augmented\nLanguage Model Pre-Training. In _International conference on machine learning_, pp. 3929\u20133938.\n[PMLR, 2020. URL https://doi.org/10.48550/arXiv.2002.08909.](https://doi.org/10.48550/arXiv.2002.08909)\n\n\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022. URL\n[https://arxiv.org/abs/2203.15556.](https://", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_11700", "chunk_text": " al.\nTraining compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022. URL\n[https://arxiv.org/abs/2203.15556.](https://arxiv.org/abs/2203.15556)\n\n\nGautier Izacard and Edouard Grave. Distilling Knowledge from Reader to Retriever for Question Answering, 2022. [URL https://arxiv.org/abs/2012.04584.](https://arxiv.org/abs/2012.04584) arXiv preprint\narXiv:2012.04584.\n\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. _arXiv preprint arXiv:2208.03299_ [, 2022. URL https:](https://arxiv.org/abs/2208.03299)\n[//arxiv.org/abs/2208.03299.](https://arxiv.org/abs/2208.03299)\n\n\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? _Transactions of the Association for Computational Linguistics_, 8:423\u2013438, 2020.\n[URL https://arxiv.org/abs/1911.12543.](https://arxiv.org/abs/1911.12543)\n\n\nJeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou. Billion-Scale Similarity Search with GPUs. _IEEE_\n_Transactions on Big Data_ [, 7(3):535\u2013547, 2019. URL https://arxiv.org/abs/1702.](https://arxiv.org/abs/1702.08734)\n[08734.](https://arxiv.org/abs/1702.08734)\n\n\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language\nModels struggle to learn Long-Tail Knowledge. In _International Conference on Machine Learn-_\n_ing_ [, pp.", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_12150", "chunk_text": "Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large Language\nModels struggle to learn Long-Tail Knowledge. In _International Conference on Machine Learn-_\n_ing_ [, pp. 15696\u201315707. PMLR, 2023. URL https://proceedings.mlr.press/v202/](https://proceedings.mlr.press/v202/kandpal23a/kandpal23a.pdf)\n[kandpal23a/kandpal23a.pdf.](https://proceedings.mlr.press/v202/kandpal23a/kandpal23a.pdf)\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In\n_Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_\n_(EMNLP)_, pp. 6769\u20136781, Online, November 2020. Association for Computational Linguis[tics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.](https://aclanthology.org/2020.emnlp-main.550)\n[emnlp-main.550.](https://aclanthology.org/2020.emnlp-main.550)\n\n\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\nHannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system.\nIn _Findings of the Association for Computational Linguistics: EMNLP 2020_, pp. 1896\u20131907,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nfindings-emnlp.171. [URL https://aclanthology.org/2020.findings-emnlp.](https://aclanthology.org/2020.findings-emnlp.171)\n[171.](https://aclanthology.org/2020.findings-emnlp.171)\n\n\nOmar Khattab and Matei Zaharia. ColBERT:", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_12600", "chunk_text": "aclanthology.org/2020.findings-emnlp.171)\n[171.](https://aclanthology.org/2020.findings-emnlp.171)\n\n\nOmar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via contextualized late interaction over bert. In _Proceedings of the 43rd International ACM SIGIR_\n_conference on research and development in Information Retrieval_, pp. 39\u201348, 2020. URL\n[https://arxiv.org/abs/2004.12832.](https://arxiv.org/abs/2004.12832)\n\n\nTom\u00b4a\u02c7s Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00b4abor Melis,\nand Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. _Transactions_\n\n_of the Association for Computational Linguistics_ [, 6:317\u2013328, 2018. URL https://arxiv.](https://arxiv.org/abs/1712.07040)\n\n[org/abs/1712.07040.](https://arxiv.org/abs/1712.07040)\n\n\n12\n\n\nPublished as a conference paper at ICLR 2024\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. _Advances in Neural Information Processing Systems_,\n[33:9459\u20139474, 2020. URL https://doi.org/10.48550/arXiv.2005.11401.](https://doi.org/10.48550/arXiv.2005.11401)\n\n\n[Jerry Liu. LlamaIndex, 2022. URL https://github.com/jerryjliu/llama_index.](https://github.com/jerryjliu/llama_index)\n\n\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. Lost in the middle: How language models use long contexts. _arXiv pre", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_13500", "chunk_text": "_Methods in Natural Language Processing_, pp. 6997\u20137008, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\n[emnlp-main.560. URL https://aclanthology.org/2021.emnlp-main.560.](https://aclanthology.org/2021.emnlp-main.560)\n\n\nSewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, and Luke\nZettlemoyer. Nonparametric masked language modeling. In _Findings of the Association for_\n_Computational Linguistics: ACL 2023_, pp. 2097\u20132118, Toronto, Canada, July 2023. Associ[ation for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.132. URL https:](https://aclanthology.org/2023.findings-acl.132)\n[//aclanthology.org/2023.findings-acl.132.](https://aclanthology.org/2023.findings-acl.132)\n\n\nEric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn.\nMemory-based model editing at scale. In _International Conference on Machine Learning_,\npp. 15817\u201315831. PMLR, 2022. [URL https://proceedings.mlr.press/v162/](https://proceedings.mlr.press/v162/mitchell22a/mitchell22a.pdf)\n[mitchell22a/mitchell22a.pdf.](https://proceedings.mlr.press/v162/mitchell22a/mitchell22a.pdf)\n\n\nXiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui\nSu. Frustratingly hard evidence retrieval for QA over books. In _Proceedings of the First Joint_\n_Workshop on Narrative Understanding, Storylines, and Events_, pp. 108\u2013113, Online, July 2020.\n[Association for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https:](https://aclanthology.org", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_13950", "chunk_text": " pp. 108\u2013113, Online, July 2020.\n[Association for Computational Linguistics. doi: 10.18653/v1/2020.nuse-1.13. URL https:](https://aclanthology.org/2020.nuse-1.13)\n[//aclanthology.org/2020.nuse-1.13.](https://aclanthology.org/2020.nuse-1.13)\n\n\nInderjeet Nair, Aparna Garimella, Balaji Vasan Srinivasan, Natwar Modani, Niyati Chhaya, Srikrishna Karanam, and Sumit Shekhar. A neural CRF-based hierarchical approach for linear text segmentation. In _Findings of the Association for Computational Linguistics: EACL_\n_2023_, pp. 883\u2013893, Dubrovnik, Croatia, May 2023. Association for Computational Linguis[tics. doi: 10.18653/v1/2023.findings-eacl.65. URL https://aclanthology.org/2023.](https://aclanthology.org/2023.findings-eacl.65)\n[findings-eacl.65.](https://aclanthology.org/2023.findings-eacl.65)\n\n\nBenjamin Newman, Luca Soldaini, Raymond Fok, Arman Cohan, and Kyle Lo. A controllable qabased framework for decontextualization. _arXiv preprint arXiv:2305.14772_ [, 2023. URL https:](https://arxiv.org/pdf/2305.14772.pdf)\n[//arxiv.org/pdf/2305.14772.pdf.](https://arxiv.org/pdf/2305.14772.pdf)\n\n\nOpenAI. GPT-4 Technical Report. _ArXiv_ [, abs/2303.08774, 2023. URL https://arxiv.org/](https://arxiv.org/abs/2303.08774)\n[abs/2303.08774.](https://arxiv.org/abs/2303.08774)\n\n\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,\nVishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. Qu", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_14400", "chunk_text": " Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,\nVishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:\n\n\n13\n\n\nPublished as a conference paper at ICLR 2024\n\n\nQuestion Answering with Long Input Texts, Yes! In _Proceedings of the 2022 Conference of_\n_the North American Chapter of the Association for Computational Linguistics: Human Language_\n_Technologies_, pp. 5336\u20135358, Seattle, United States, July 2022. Association for Computational\n[Linguistics. URL https://aclanthology.org/2022.naacl-main.391.](https://aclanthology.org/2022.naacl-main.391)\n\n\nFabio Petroni, Tim Rockt\u00a8aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as knowledge bases? _arXiv preprint arXiv:1909.01066_,\n[2019. URL https://arxiv.org/abs/1909.01066.](https://arxiv.org/abs/1909.01066)\n\n\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\nMethods, Analysis & Insights from Training Gopher. _arXiv preprint arXiv:2112.11446_, 2021.\n[URL https://arxiv.org/abs/2112.11446.](https://arxiv.org/abs/2112.11446)\n\n\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin LeytonBrown, and Yoav Shoham. In-context retrieval-augmented language models. _arXiv preprint_\n_arXiv:2302.00083_ [, 2023. URL https://arxiv.org/abs/2302.00083.](https://arxiv.org/abs/2302.00083)\n\n\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks.", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_15300", "chunk_text": " Special Publication Sp_ [, 109:109, 1995. URL https://www.](https://www.microsoft.com/en-us/research/publication/okapi-at-trec-3/)\n[microsoft.com/en-us/research/publication/okapi-at-trec-3/.](https://www.microsoft.com/en-us/research/publication/okapi-at-trec-3/)\n\n\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\nZaheer. Questions are all you need to train a dense passage retriever. _Transactions of the As-_\n_sociation for Computational Linguistics_, 11:600\u2013616, 2023. doi: 10.1162/tacl ~~a 0~~ 0564. URL\n[https://aclanthology.org/2023.tacl-1.35.](https://aclanthology.org/2023.tacl-1.35)\n\n\nGideon Schwarz. Estimating the Dimension of a Model. _The annals of statistics_, pp. 461\u2013464,\n[1978. URL https://projecteuclid.org/journals/annals-of-statistics/](https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full)\n[volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/](https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full)\n[aos/1176344136.full.](https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full)\n\n\nKaren Sp\u00a8arck Jones. A Statistical Interpretation of Term Specificity and its Application in Retrieval. _Journal of documentation_ [, 28(1):11\u201321, 1972. URL https://doi.org/10.1108/](https://doi.org/10.1108/eb026526)\n[eb026526", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_15750", "chunk_text": ". _Journal of documentation_ [, 28(1):11\u201321, 1972. URL https://doi.org/10.1108/](https://doi.org/10.1108/eb026526)\n[eb026526.](https://doi.org/10.1108/eb026526)\n\n\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language\nmodels actually use long-range context? In Marie-Francine Moens, Xuanjing Huang, Lucia\nSpecia, and Scott Wen-tau Yih (eds.), _Proceedings of the 2021 Conference on Empirical Methods_\n_in Natural Language Processing_, pp. 807\u2013822, Online and Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n[62. URL https://aclanthology.org/2021.emnlp-main.62.](https://aclanthology.org/2021.emnlp-main.62)\n\n\nZhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language\nmodels. _arXiv preprint arXiv:2210.01296_ [, 2022. URL https://arxiv.org/abs/2210.](https://arxiv.org/abs/2210.01296)\n[01296.](https://arxiv.org/abs/2210.01296)\n\n\n14\n\n\nPublished as a conference paper at ICLR 2024\n\n\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. oLMpics\u2013 on what language\nmodel pre-training captures. _Transactions of the Association for Computational Linguistics_, 8:\n[743\u2013758, 2020. URL https://arxiv.org/abs/1912.13283.](https://arxiv.org/abs/1912.13283)\n\n\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\nwith retrieval? a comprehensive study. _arXiv preprint arXiv:", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_16200", "chunk_text": ", Yi Dong,\nOleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models\nwith retrieval? a comprehensive study. _arXiv preprint arXiv:2304.06762_ [, 2023. URL https:](https://arxiv.org/abs/2304.06762)\n[//arxiv.org/abs/2304.06762.](https://arxiv.org/abs/2304.06762)\n\n\nJeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\nChristiano. Recursively Summarizing Books with Human Feedback, 2021. [URL https:](https://arxiv.org/abs/2109.10862)\n[//arxiv.org/abs/2109.10862.](https://arxiv.org/abs/2109.10862)\n\n\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,\nand Quoc V. Le. QANet: Combining Local Convolution with Global Self-Attention for Read[ing Comprehension, 2018. URL https://arxiv.org/abs/1804.09541. arXiv preprint](https://arxiv.org/abs/1804.09541)\narXiv:1804.09541.\n\n\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang\nZhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large Language Models are\n[strong context generators, 2022. URL https://arxiv.org/abs/2209.10063.](https://arxiv.org/abs/2209.10063)\n\n\nShiyue Zhang, David Wan, and Mohit Bansal. Extractive is not faithful: An investigation of\nbroad unfaithfulness problems in extractive summarization. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association_\n_for Computational Linguistics (Volume 1: Long Papers)_, pp. 2153\u20132174, Toronto, Canada, July", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_16650", "chunk_text": " Naoaki Okazaki (eds.), _Proceedings of the 61st Annual Meeting of the Association_\n_for Computational Linguistics (Volume 1: Long Papers)_, pp. 2153\u20132174, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.120. URL\n[https://aclanthology.org/2023.acl-long.120.](https://aclanthology.org/2023.acl-long.120)\n\n\nA SCALABILITY AND COMPUTATIONAL EFFICIENCY OF THE\n\nTREE-BUILDING PROCESS\n\n\nTo assess the computational efficiency and cost-effectiveness of RAPTOR\u2019s tree-building process,\nwe conducted experiments on a consumer-grade laptop, specifically an Apple M1 Mac with 16GB\n\n- f RAM. These experiments aimed to demonstrate the scalability and feasibility of RAPTOR on\ntypical hardware. We varied the context length from 12,500 to 78,000 tokens and measured both the\ntoken expenditure and the time required to complete the tree-building process, from initial splitting\nand embedding to the construction of the final root node.\n\n\nFigure 5: Token cost as a function of document length for QASPER, NarrativeQA, and QuALITY.\nRAPTOR tree construction costs scale linearly with document length for each of the datasets.\n\n\n**Token Expenditure** We empirically investigated the relationship between the initial document\nlength and the total number of tokens expended during the tree-building process, which includes\nboth the prompt and completion tokens. The document lengths varied significantly across the three\n\n\n15\n\n\nPublished as a conference paper at ICLR 2024\n\n\ndatasets examined: QuALITY, QASPER, and NarrativeQA. Figure 5 illustrates a clear linear correlation between the initial document length and the total token expenditure, emphasizing that RAPTOR maintains a linear token scaling regardless of document complexity or length.\n\n\nFigure 6: Build time as a function of document length for documents of up to 80,000 tokens. RAPTOR tree construction time scales linearly with document length for each of the datasets.\n\n\n**Build Time** We also empirically observed a consistent linear trend between the document length\nand the build time, as shown in Figure 6. This suggests that RAPTOR scales linearly in terms of\ntime, making it a viable solution for efficiently processing large corpora of varying lengths.\n\n\n**", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_17100", "chunk_text": " linear trend between the document length\nand the build time, as shown in Figure 6. This suggests that RAPTOR scales linearly in terms of\ntime, making it a viable solution for efficiently processing large corpora of varying lengths.\n\n\n**Conclusion** Overall, our empirical results indicate that RAPTOR scales both in terms of tokens\nexpended and build time. Even as the complexity and volume of the input text grow, the cost of\nconstructing the tree scales predictably and linearly. This demonstrates that RAPTOR is computationally efficient and well-suited for processing large and diverse corpora.\n\n\nB ABLATION STUDY ON CLUSTERING MECHANISM IN RAPTOR\n\n\nTo assess the effectiveness of the clustering mechanism in our RAPTOR approach, we conducted\nan ablation study on the QuALITY dataset. This study compares RAPTOR\u2019s performance with a\nbalanced tree-style encoding and summarization of contiguous chunks, in contrast to our standard\nclustering method.\n\n\nB.1 METHODOLOGY\n\n\nBoth configurations in this ablation study utilized SBERT embeddings and UnifiedQA to maintain\nconsistency in retrieval. For RAPTOR, we employed our typical clustering and summarization\nprocess. In contrast, the alternative setup involved creating a balanced tree by recursively encoding\nand summarizing contiguous text chunks. We determined the window size for this setup based on\nthe average cluster size observed in RAPTOR, which is approximately 6.7 nodes. Hence, we chose\na window size of 7 nodes. The collapsed tree approach was applied for retrieval in both models.\n\n\nB.2 RESULTS & DISCUSSION\n\n\nThe results of the ablation study are presented in table 9. The results from this ablation study clearly\nindicate an improvement in accuracy when employing RAPTOR\u2019s clustering mechanism over the\nrecency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in\nRAPTOR is more effective in capturing homogeneous content for summarization, thereby enhancing\nthe overall retrieval performance.\n\n\n16\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTable 9: Ablation study results comparing RAPTOR with a recency-based tree approach\n\n\n**Configuration** **Accuracy**\n\n\n**RAPTOR + SBERT embeddings + UnifiedQA** **56.6%**\nRecency-based tree + SBERT embeddings + UnifiedQA 55.8%\n\n\nC DATASET STATISTICS AND COMPRESSION RATIOS\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_17550", "chunk_text": "** **Accuracy**\n\n\n**RAPTOR + SBERT embeddings + UnifiedQA** **56.6%**\nRecency-based tree + SBERT embeddings + UnifiedQA 55.8%\n\n\nC DATASET STATISTICS AND COMPRESSION RATIOS\n\n\nThe average ratio of the summary length to the sum of child node lengths across all datasets is 0.28,\nindicating a 72% compression rate. On average, the summary length is 131 tokens, and the average\nchild node length is 86 tokens. Below are the detailed statistics for all three datasets:\n\n\nTable 10: Statistics of Average Summary Length and Child Node Length Across Datasets\n\n\n\n**Avg.**\n**Compression**\n**Ratio (%)**\n\n\n\n**Avg. # of**\n**Child Nodes**\n\n**Per Parent**\n\n\n\n**Dataset** **Avg.**\n**Summary**\n**Length**\n**(tokens)**\n\n\n\n**Avg. Child**\n**Node Text**\n**Length**\n**(tokens)**\n\n\n\nAll Datasets 131 85.6 6.7 .28\nQuALITY 124.4 87.9 5.7 .28\nNarrativeQA 129.7 85.5 6.8 .27\nQASPER 145.9 86.2 5.7 .35\n\n\nD SUMMARIZATION PROMPT\n\n\nTable 11 shows the prompt used for summarization.\n\n\nTable 11: Prompt for Summarization\n\n|Role|Content|\n|---|---|\n|system|You are a Summarizing Text Portal|\n|user|Write a summary of the following, including as many key details as<br>possible: _{_context_}_:|\n\n\n\nE HALLUCINATION ANALYSIS\n\n\nTo assess the quality and accuracy of the summarizations within our RAPTOR model, we conducted\nan analysis focusing on hallucinations in the generated summaries. The summaries were generated\nby gpt-3.5-turbo and subsequently annotated to quantify the rates of hallucinations, to examine\nwhether such inaccuracies propagate to parent nodes, and to evaluate their impact on questionanswering (QA) tasks.\n\n\nE.1 METHODOLOGY\n\n\nWe randomly sampled 150 nodes across 40 stories and evaluated them for hallucinations. This\nsampling strategy provides a broad view of the model\u2019s performance across different contexts. Each\nnode was annotated by hand, and determined if it contained a hallucination.\n\n\nE.2 FINDINGS\n\n\nOut of the ", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_18000", "chunk_text": " evaluated them for hallucinations. This\nsampling strategy provides a broad view of the model\u2019s performance across different contexts. Each\nnode was annotated by hand, and determined if it contained a hallucination.\n\n\nE.2 FINDINGS\n\n\nOut of the 150 nodes sampled, 4% (6 nodes) contained some form of hallucination. Most commonly,\nthese hallucinations originated from the model adding minor information possibly from its training\ndata that was not present in the text being summarized, or from incorrectly extrapolating some\ninformation when creating the summary.\n\n\n17\n\n\nPublished as a conference paper at ICLR 2024\n\n\n**Example:**\n\n\n_Text of the child nodes:_\n\n\n\u201dAnd you will come with me to my people? We may live here among them, and\nyou will be a great warrior\u2013oh, when Jor dies you may even be chief, for there is\nnone so mighty as my warrior...\u201dBut your father will not permit it\u2013Jor, my father,\nHigh Chief of the Galus, will not permit it, for like me you are cos-ata-lo. Oh, CoTan, if we but could!... Bradley noticed that she spoke in English\u2013broken English\nlike Co-Tan\u2019s but equally appealing.\n\n\n_Summary found in the parent of that node:_\n\n\nThe protagonist, Bradley, is being asked by Co-Tan to stay with her people and\nbecome a great warrior, but he refuses and must return to his own country. Tom\nBillings of Santa Monica arrives and tells them he came to search for a man named\nBowen J. Tyler, Jr. Ajor, Co-Tan\u2019s sister, is excited about the possibility of going\nto Tom\u2019s country to see strange and wonderful things...\n\n\nThe hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not\nexplicitly mention or imply this.\n\n\nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers.\nGenerally, the hallucinations were minor and did not alter the thematic interpretation of the text.\n\n\nE.3 IMPACT ON QA TASKS\n\n\nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This suggests that hallucination is not a major concerns for the summarization component in our RAPTOR\narchitecture.\n\n\nF PSEUDOCODE FOR RETRIEVAL METHODS\n\n\n**Algorithm 1** Tree Traversal Algorithm\n\n\n**", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_18450", "chunk_text": " QA tasks. This suggests that hallucination is not a major concerns for the summarization component in our RAPTOR\narchitecture.\n\n\nF PSEUDOCODE FOR RETRIEVAL METHODS\n\n\n**Algorithm 1** Tree Traversal Algorithm\n\n\n**function** TRAVERSETREE(tree _,_ query _, k_ )\n\n_S_ current _\u2190_ tree.layer[0]\n**for** layer in range(tree.num ~~l~~ ayers) **do**\n\n_topk \u2190_ []\n**for** node in _S_ current **do**\n\n_score \u2190_ dot ~~p~~ roduct(query, node)\ntop k.append((node, score))\n**end for**\n_S_ layer _\u2190_ sorted(top ~~k~~ )[:k].nodes\n_S_ current _\u2190_ _S_ layer\n**end for**\n**return** _S_ 0 _\u222a_ _S_ 1 _\u222a_ _S_ 2 _\u222a_ _. . . \u222a_ _Sk_\n**end function**\n\n\nG QUALITATIVE ANALYSIS\n\n\nTo qualitatively examine RAPTOR\u2019s retrieval process, we test it on thematic, multi-hop questions\nabout a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAPTOR with the context retrieved by Dense Passage Retrieval (DPR). Figure 4 in the main paper details\nthe retrieval process within RAPTOR\u2019s tree structure for two questions. The nodes that RAPTOR\nselects for each question are highlighted, while the leaf nodes that DPR selects for the same question\nare indicated with arrows. This comparison illustrates the advantage of RAPTOR\u2019s tree structure.\nRAPTOR selects nodes from different layers depending on the level of granularity required by the\n\n\n18\n\n\nPublished as a conference paper at ICLR 2024\n\n\n**Algorithm 2** Collapsed Tree Algorithm\n\n\n**function** COLLAPSEDTREE(tree _,_ query _, k,_ max ~~t~~  - kens)\n\ntree _\u2190_ flatten(tree) _\u25b7_ Flatten tree into 1D\ntop nodes _\u2190_ []\n**for** node in tree **do**\n\ntop nodes.append((node, dot ~~p~~ roduct(query, node))\n**end for**\ntop nodes _\u2190_ sorted(top ~~n~~    - des)\nresult _\u2190_ []\ntotal ~~t~~    - kens _\u2190_ 0\n**for** node", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_18900", "chunk_text": " ~~p~~ roduct(query, node))\n**end for**\ntop nodes _\u2190_ sorted(top ~~n~~    - des)\nresult _\u2190_ []\ntotal ~~t~~    - kens _\u2190_ 0\n**for** node in top ~~n~~    - des **do**\n\n**if** total ~~t~~       - kens + node.token ~~s~~ ize _<_ max ~~t~~       - kens **then**\n\nresult.append(node)\n**end if**\n\ntotal ~~t~~      - kens _\u2190_ total ~~t~~      - kens + node.token ~~s~~ ize\n**end for**\n\n**return** result\n\n**end function**\n\n|Question:Whatisthecentralthemeofthestory?|Col2|\n|---|---|\n|**RAPTOR**|Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella<br>impresses the Prince at the ball. . . she loses track of time and has to run home alone in<br>the darkness. The Prince is unable to fnd Cinderella and goes in search of her . . . She<br>forgave her sisters, and treated them always very kindly, and the Prince had great cause<br>to beglad that he had found theglass slipper.|\n|**DPR**|Two mice were turned into footmen; four grasshoppers into white horses. Next, the<br>Fairy touched Cinderella\u2019s rags, and they became rich satin robes, trimmed with point<br>lace. ... Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and<br>the rat and mice ran quickly away when they saw her; while all her fne dress turned to<br>rags, and she had to run home alone. . . They told her a beautiful Princess had been at<br>the ball, with whom the Prince was delighted. They did not know it was Cinderella.|\n|**Question: How does Cinderella fnd a happy ending?**|**Question: How does Cinderella fnd a happy ending?**|\n|**RAPTOR**|The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin<br>into a grand coach with her wand and allows Cinderella to attend the ball. However,<br>Cinderella must return home before the clock strikes eleven or her dress", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_19350", "chunk_text": " of Cinderella involves her godmother, a fairy, who transforms a pumpkin<br>into a grand coach with her wand and allows Cinderella to attend the ball. However,<br>Cinderella must return home before the clock strikes eleven or her dress will turn back<br>into rags... Cinderella impresses the Prince at the ball but leaves before he can fnd<br>out who she is... The Prince searched for the owner of a lost glass slipper and found it<br>belonged to Cinderella. She forgave her sisters and the Prince was glad to have found<br>her.|\n|**DPR**|the clock had struck Eleven. . . The Prince was very much surprised when he missed<br>Cinderella again, and leaving the ball, went in search of her. . .<br>Fairy touched Cin-<br>derella\u2019s rags, and they became rich satin robes, trimmed with point lace... Her old<br>shoes became a charming pair of glass slippers, which shone like diamonds. \u201cNow go<br>to the ball, my love,\u201d she said, \u201cand enjoy yourself. But remember, you must leave the<br>room before the clock strikes eleven. If you do not your dress will return to its original<br>rags.\u201d|\n\n\n\nTable 12: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\nfairytale Cinderella.\n\n\nquestion at hand. Further, the information that would be retrieved by DPR is more often than not\nincluded in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part of a\nsummary from a higher layer.\n\n\n\u201dThe first question we examine is \u201cHow does Cinderella find a happy ending?\u201d, a multi-hop question\nbest answered by synthesizing information from various text segments. To control for the language\nmodel\u2019s potential familiarity with the Cinderella story, we instructed it to rely solely on the retrieved\ninformation for its answers. Table 13 shows the text retrieved by both RAPTOR and DPR for this\nquestion. RAPTOR\u2019s context succinctly describes Cinderella\u2019s journey to happiness, while DPR\u2019s\nleaf nodes primarily focus on her initial transformation. The difference in retrieved information\n\n\n19\n\n\nPublished as a conference paper at ICLR 2024\n\n\nsignificantly impacts downstream tasks. When GPT-4 is provided with RAPTOR\u2019s context, it generates a detailed", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_19800", "chunk_text": " focus on her initial transformation. The difference in retrieved information\n\n\n19\n\n\nPublished as a conference paper at ICLR 2024\n\n\nsignificantly impacts downstream tasks. When GPT-4 is provided with RAPTOR\u2019s context, it generates a detailed answer: \u201cCinderella finds a happy ending when the Prince searches for the owner\n\n- f the lost glass slipper and discovers it belongs to Cinderella. They eventually marry, transforming Cinderella\u2019s life for the better.\u201d In contrast, using DPR\u2019s context, GPT-4 states: \u201cBased on the\ngiven context, it is not possible to determine how Cinderella finds a happy ending, as the text lacks\ninformation about the story\u2019s conclusion.\u201d\n\n\nThe second question we examine is \u201cWhat is the central theme of the story?\u201d, a thematic question\nthat requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for\nthis question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of\nall the major parts of the story, whereas the text retrieved by DPR contains detailed descriptions of\na narrow subset of the story. Again, the difference in retrieval mechanisms affects the performance\n\n- f GPT-4 when answering the question. Given DPR\u2019s context, it outputs \u201cThe central theme of\nthe story is transformation and the power of inner beauty, as Cinderella, a kind and humble girl, is\nmagically transformed into a beautiful princess, capturing the attention and admiration of the Prince\nand others at the ball.\u201d This answer only takes into account the first portion of the story, up until\nCinderella first meets the prince. In contrast, given RAPTOR\u2019s context, GPT-4 outputs \u201cThe central\ntheme of the story is transformation and overcoming adversity, as Cinderella, with the help of her\nFairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident\nyoung woman who ultimately finds happiness and love with the Prince.\u201d This is a more complete\nanswer, demonstrating a comprehensive understanding of the story.\n\n\nThis qualitative analysis indicates that RAPTOR outperforms prior retrieval mechanisms because\nthe information that it retrieves is more relevant and exhaustive, allowing for better performance on\ndownstream tasks.\n\n\nWe also created a 2600-word story along with questions about its narrative and theme. An excerpt\n[from the story is present below and the full PDF of this story is linked here", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_20250", "chunk_text": " and exhaustive, allowing for better performance on\ndownstream tasks.\n\n\nWe also created a 2600-word story along with questions about its narrative and theme. An excerpt\n[from the story is present below and the full PDF of this story is linked here. For questions like \u201cWhat](https://raptor-appendix-g.tiiny.site)\nis the central theme of the story?\u201d, an upper-level node is retrieved which includes the sentence:\n\u201cThis story is about the power of human connection... inspiring and uplifting each other as they\npursued their passions.\u201d This summary, not explicitly present in the original text, almost directly\nanswers the question.\n\n\n**Excerpt from \u201dThe Eager Writer\u201d:**\n\n\n\u201dEthan\u2019s passion for writing had always been a part of him. As a child, he would\n\n     - ften scribble stories and poems in his notebook, and as he grew older, his love\nfor writing only intensified. His evenings were often spent in the dim light of his\nroom, typing away at his laptop. He had recently taken a job as a content writer\nfor an online marketing firm to pay the bills, but his heart still longed for the\nworld of storytelling. However, like many aspiring writers, he struggled to find a\nfoothold in the industry. He took a job as a content writer for an online marketing\nfirm, but it was growing increasingly evident to him that this was not the path he\nwanted to pursue. It was during this time that he stumbled upon the Pathways\napp. The app offered a platform for people in similar professions to connect and\nshare knowledge, and he saw it as an opportunity to finally connect with others\nwho shared his passion for writing. Ethan saw an opportunity to meet others who\nshared his passion and could offer guidance and mentorship. He quickly signed\nup and was surprised by the number of writers he found on the platform, from\nwell establish professionals to beginners just starting out in the business.\u201d\n\n\nH NARRATIVEQA EVALUATION SCRIPT\n\n\nWe made several modifications to AllenNLP\u2019s evaluation script [3] to better fit our evaluation needs:\n\n\n    - **Added Smoothing:** Smoothing was incorporated to handle cases where BLEU score is\nzero, due to no n-gram matches occurring in the reference text. A BLEU score of zero\nskews the results, leading to an overly harsh evaluation for rare or novel phrases. By adding\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_20700", "chunk_text": " handle cases where BLEU score is\nzero, due to no n-gram matches occurring in the reference text. A BLEU score of zero\nskews the results, leading to an overly harsh evaluation for rare or novel phrases. By adding\n\n\n3docs.allennlp.org/models/main/models/rc/tools/narrativeqa/\n\n\n20\n\n\nPublished as a conference paper at ICLR 2024\n\n\na smoothing function, we prevent the BLEU scores from dropping to zero, providing a more\nfair evaluation.\n\n    - **Modified BLEU-4 Weighting:** The original script applied a weight of 1 to the highest\n\n     - rder n-gram (4-gram) and 0 to the rest in its BLEU-4 calculation (i.e., weights=(0, 0,\n0, 1)). This approach may overly focus on 4-gram matches while neglecting lower-order\nmatches. To provide a more balanced evaluation, we evenly distributed the weight across\nall n-gram levels, changing the weights for the BLEU-4 calculation to (0.25, 0.25, 0.25,\n0.25).\n\n    - **Tokenization before Mapping in METEOR Calculation:** The original script utilized a\nsimple split and map method for METEOR calculation. We fixed this by first tokenizing the\ntext and then mapping the tokens. This amendment improves the accuracy of the METEOR\ncalculation by taking into account the correct linguistic boundaries of words.\n\n|Question:Whatisthecentralthemeofthestory?|Col2|\n|---|---|\n|**RAPTOR**|Fairy Godmother helps Cinderella attend a ball by transforming her rags. . . Cinderella<br>impresses the Prince at the ball. . . she loses track of time and has to run home alone in<br>the darkness. The Prince is unable to fnd Cinderella and goes in search of her . . . She<br>forgave her sisters, and treated them always very kindly, and the Prince had great cause<br>to beglad that he had found theglass slipper.|\n|**DPR**|Two mice were turned into footmen; four grasshoppers into white horses. Next, the<br>Fairy touched Cinderella\u2019s rags, and they became rich satin robes, trimmed with point<br>lace. ... Alas! the clock had struck Eleven. She found", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_21150", "chunk_text": " footmen; four grasshoppers into white horses. Next, the<br>Fairy touched Cinderella\u2019s rags, and they became rich satin robes, trimmed with point<br>lace. ... Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and<br>the rat and mice ran quickly away when they saw her; while all her fne dress turned to<br>rags, and she had to run home alone. . . They told her a beautiful Princess had been at<br>the ball, with whom the Prince was delighted. They did not know it was Cinderella.|\n|**Question: How does Cinderella fnd a happy ending?**|**Question: How does Cinderella fnd a happy ending?**|\n|**RAPTOR**|The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin<br>into a grand coach with her wand and allows Cinderella to attend the ball. However,<br>Cinderella must return home before the clock strikes eleven or her dress will turn back<br>into rags... Cinderella impresses the Prince at the ball but leaves before he can fnd<br>out who she is... The Prince searched for the owner of a lost glass slipper and found it<br>belonged to Cinderella. She forgave her sisters and the Prince was glad to have found<br>her.|\n|**DPR**|the clock had struck Eleven. . . The Prince was very much surprised when he missed<br>Cinderella again, and leaving the ball, went in search of her. . .<br>Fairy touched Cin-<br>derella\u2019s rags, and they became rich satin robes, trimmed with point lace... Her old<br>shoes became a charming pair of glass slippers, which shone like diamonds. \u201cNow go<br>to the ball, my love,\u201d she said, \u201cand enjoy yourself. But remember, you must leave the<br>room before the clock strikes eleven. If you do not your dress will return to its original<br>rags.\u201d|\n\n\n\nTable 13: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\nfairytale Cinderella.\n\n\nI ANALYSIS OF DIFFERENT LAYERS ON RAPTOR\u2019S PERFORMANCE\n\n\nI.1 HOW DO DIFFERENT LAYERS IMPACT PERFORMANCE ?\n\n\nIn this section, we present a detailed", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_21600", "chunk_text": " on the\nfairytale Cinderella.\n\n\nI ANALYSIS OF DIFFERENT LAYERS ON RAPTOR\u2019S PERFORMANCE\n\n\nI.1 HOW DO DIFFERENT LAYERS IMPACT PERFORMANCE ?\n\n\nIn this section, we present a detailed breakdown of RAPTOR\u2019s retrieval performance when querying\ndifferent layers of the hierarchical tree structure for various stories. These tables validate the utility\n\n- f RAPTOR\u2019s multi-layered structure for diverse query requirements.\n\n\nTable 14: Performance of RAPTOR when querying different layers of the tree for Story 2.\n\n\n**Layers Queried / Start Layer** **Layer 0 (Leaf Nodes)** **Layer 1** **Layer 2**\n\n\n1 layer 58.8 47.1 41.1\n2 layers        - **64.7** 52.9\n3 layers        -        - 47.1\n\n\n21\n\n\nPublished as a conference paper at ICLR 2024\n\n\nFigure 7: Histogram showing the percentage of nodes retrieved from different layers of the RAPTOR\ntree across three datasets (NarrativeQA, Quality, and Qasper) using three retrievers (SBERT, BM25,\nand DPR). The data indicate that a substantial portion of the nodes contributing to the final retrieval\ncomes from non-leaf layers, with a notable percentage from the first and second layers, highlighting\nthe importance of RAPTOR\u2019s hierarchical summarization in the retrieval process.\n\n\nTable 15: Performance of RAPTOR when querying different layers of the tree for Story 3.\n\n\n**Layers Queried / Start Layer** **Layer 0 (Leaf Nodes)** **Layer 1** **Layer 2**\n\n\n1 layer 66.6 61.1 61.1\n2 layers        - 66.6 66.6\n3 layers        -        - **83.3**\n\n\nTable 16: Performance of RAPTOR when querying different layers of the tree for Story 4.\n\n\n**Layers Queried / Start Layer** **Layer 0 (Leaf Nodes)** **Layer 1**\n\n\n1 layer **94.7** 84.2\n2 layers           - 89.4\n\n\nTable 17: Performance of RAPTOR when querying different layers of the tree for Story 5.\n\n\n**Layers Queried / Start Layer** **Layer 0 (Leaf Nodes)** **Layer 1**\n\n\n1 layer 57.9", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_22050", "chunk_text": "Table 17: Performance of RAPTOR when querying different layers of the tree for Story 5.\n\n\n**Layers Queried / Start Layer** **Layer 0 (Leaf Nodes)** **Layer 1**\n\n\n1 layer 57.9 47.3\n2 layers           - **68.4**\n\n\nI.2 WHICH LAYERS DO RETRIEVED NODES COME FROM ?\n\n\nWe further conduct an ablation study across all three datasets and across three different retrievers\nwith RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes\n\n- riginate. We observe that between 18.5% to 57% of the retrieved nodes come from non-leaf nodes.\nAs illustrated in Figure 7, the retrieval pattern across layers reveals the importance of RAPTOR\u2019s\nmulti-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR\nusing the DPR retriever for the NarrativeQA dataset come from the first and second layers of the\ntree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers,\nalbeit with varying percentages.\n\n\nTable 18: Percentage of nodes from non-leaf nodes across different datasets and retrievers\n\n\n**Dataset** **DPR** **SBERT** **BM25**\n\n\nNarrativeQA 57.36% 36.78% 34.96%\nQuality 32.28% 24.41% 32.36%\nQasper 22.93% 18.49% 22.76%\n\n\n22\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTable 19: Percentage of nodes from different layers with DPR as the retriever\n\n\n**Layer** **NarrativeQA** **Quality** **Qasper**\n\n\n0 42.64% 67.71% 77.07%\n\n1 45.00% 29.43% 21.88%\n\n2 10.57% 2.85% 1.05%\n\n3 1.78%           -           \n4 0.003%           -           \n\nTable 20: Percentage of nodes from different layers with SBERT as the retriever\n\n\n**Layer** **NarrativeQA** **Quality** **Qasper**\n\n\n0 63.22% 75.59% 81.51%\n\n1 31.51% 22.78%", "token_count": 500, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2401.18059_raptor_sarthi:chunk_22500", "chunk_text": " SBERT as the retriever\n\n\n**Layer** **NarrativeQA** **Quality** **Qasper**\n\n\n0 63.22% 75.59% 81.51%\n\n1 31.51% 22.78% 17.84%\n\n2 4.85% 1.63% 0.65%\n\n3 0.42%           -           \n\nTable 21: Percentage of nodes from different layers with BM25 as the retriever\n\n\n**Layer** **NarrativeQA** **Quality** **Qasper**\n\n\n0 65.04% 67.64% 77.24%\n\n1 28.79% 28.85% 21.57%\n\n2 5.36% 3.51% 1.19%\n\n3 0.81%           -           \n\n23\n\n\n", "token_count": 175, "metadata": {"arxiv_id": "2401.18059", "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval", "authors": ["Parth Sarthi", "Salman Abdullah", "Aditi Tuli", "Shubh Khanna", "Anna Goldie", "Christopher D. Manning"], "year": 2024, "url": "https://arxiv.org/pdf/2401.18059v1"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_0", "chunk_text": "## **ARES: An Automated Evaluation Framework for Retrieval-Augmented** **Generation Systems**\n\n\n\n**Jon Saad-Falcon**\nStanford University _[\u2217]_\n\njonsaadfalcon@stanford.edu\n\n\n**Christopher Potts**\nStanford University\ncgpotts@stanford.edu\n\n\n**Abstract**\n\n\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an _Automated RAG Evaluation_\n_System_, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By creating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or documents used in the evaluated RAG systems. We\nmake our code and datasets publicly available\n\n[on Github.](https://github.com/stanford-futuredata/ARES)\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to generate a response. This formulation admits a multitude\n\n- f choices: what retrieval model to use, how to di\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_450", "chunk_text": " LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab@stanford.edu\n\n\n**Matei Zaharia**\n\nDatabricks and UC Berkeley\nmatei@databricks.com\n\n\nThe best design for a RAG system is not necessarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand annotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alternatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strategy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the _relevance_ - f retrieved information and the _faithfulness_ and _accuracy_ - f generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accurately, we propose ARES, the **A** utomated **R** AG\n**E** valuation **S** ystem. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, leading to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RAGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered inference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response generated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\n\n\nRAG system finds relevant contexts and generates\nanswers that are both faithful and relevant", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_900", "chunk_text": " (is the response generated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\n\n\nRAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\n\nMany existing RAG evaluation frameworks require substantial human annotations for scoring.\nARES significantly improves data efficiency during evaluation by only requiring three inputs: an indomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data generation.\n\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question\u2013answer\npairs, derived from the passages in the corpus. Sec\n- nd, it defines three separate judge models to perform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores\nthe different RAG systems being assessed using\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals\nfor RAG scoring. PPI utilizes a small set of human\nannotated datapoints for computing its confidence\nintervals; we designate this annotated set as our _hu-_\n_man preference validation set_, which is composed\n\n- f approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an\nswer relevance.\n\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating existing automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occurrences in the AIS attribution dataset (Rashkin et al.,\n2022), predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach.", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_1350", "chunk_text": " predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach. We also find that ARES consistently\ndistinguishes competitive RAG systems that are\n\n- nly a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop\n\n\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\n[available on Github.](https://github.com/stanford-futuredata/ARES)\n\n\n**2** **Related Work**\n\n\nRAG (Guu et al., 2020; Lewis et al., 2020; Khattab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combining them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual information (Shuster et al., 2021; Huo et al., 2023), and\n\n- ffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essential for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\n\n- ut of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. However, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_1800", "chunk_text": " 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RAGAS (James and Es, 2023). To evaluate RAG systems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several associated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\n\n- ffer little adaptability to new RAG evaluation set\n\ntings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n\n\n**3** **ARES**\n\n\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\n\n- r more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query\u2013passage\u2013answer\ntriples to train LLM judges. Subsequently, we apply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to estimate a confidence interval for the quality of each\nRAG system.\n\n\n**3.1** **LLM Generation of Synthetic Dataset**\n\n\nWe generate synthetic queries and answers from\nthe corpus", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_2250", "chunk_text": " inference (PPI)\nwith our human preference validation set to estimate a confidence interval for the quality of each\nRAG system.\n\n\n**3.1** **LLM Generation of Synthetic Dataset**\n\n\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative\nexamples of query\u2013passage\u2013answer triples (e.g.,\nrelevant/irrelevant passages and correct/incorrect\nanswers). For generation, the LLM uses our input set of few-shot examples with in-domain passages mapped to in-domain queries and answers;\nthe model then generates a synthetic question and\nanswer from a given in-domain passage, allowing\nus to create both positive and negative training examples. We include example prompts for generating synthetic queries and answers in A.6.\nFor creating our synthetic data, we primarily use\n\n- n FLAN-T5 XXL (discussed in subsection 4.1).\nARES works well with this model (see section 5)\nbut our system can ultimately use another highquality model for generating synthetic queries and\nanswers. We then filter out low-quality queries\nby testing if a given query can retrieve its original\npassage as the top result using its retriever. This\nfiltering approach has been used in previous work\nto isolate high-quality synthetic queries (Dai et al.,\n2022; Saad-Falcon et al., 2023).\nTo generate negatives for fine-tuning our LLM\njudges, we rely on two novel strategies, generating\nthe same number of negatives with each strategy:\n\n\n1. **Weak Negative Generation** : For context rel\n\n\nevance negatives, we randomly sample indomain passages unrelated to a given synthetic query. For answer faithfulness and\nanswer relevance negatives, we randomly\nsample synthetically-generated answers from\n\n   - ther passages, which were created using\nFLAN-T5 XXL.\n\n\n2. **Strong Negative Generation** : For context\nrelevance negatives, we randomly sample indomain passages from the same document as\nthe gold passage. For datasets in which multiple passages are not available for the same\ndocument, we use BM25 to retrieve the top10 passages similar to the passage and sample\nfrom them for our context relevance strong\nnegatives. For answer faithfulness and answer relevance negatives, we prompt FLANT5 XXL (subsection 4.1) to generate a contradictory answer using the few-shot prompt in\nsubsection A.", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_2700", "chunk_text": "\nfrom them for our context relevance strong\nnegatives. For answer faithfulness and answer relevance negatives, we prompt FLANT5 XXL (subsection 4.1) to generate a contradictory answer using the few-shot prompt in\nsubsection A.5.\n\n\nIn total, the number of negatives generated\nequals the number of positives generated for evaluating context relevance and answer relevance.\n\n\n**3.2** **Preparing LLM Judges**\n\n\nTo prepare our RAG evaluation judges, we use\n\n- ur synthetic dataset to fine-tune DeBERTa-v3Large judges (discussed in subsection 4.1) to evaluate three different capabilities (Chen et al., 2023;\nJames and Es, 2023):\n\n\n1. **Context Relevance** : Is the passage returned\nrelevant for answering the given query?\n\n\n2. **Answer Faithfulness** : Is the answer generated faithful to the retrieved passage, or does\nit contain hallucinated or extrapolated statements beyond the passage?\n\n\n3. **Answer Relevance** : Is the answer generated\nrelevant given the query and retrieved passage?\n\n\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated querydocument-answer, a single LLM judge must classify the triple as positive or negative for that judge\u2019s\nmetric. To fine-tune these judges, we use our human preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).\n\n\nFigure 1: **Overview of ARES** : As inputs, the ARES pipeline requires an in-domain passage set, a human preference\nvalidation set of 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (five or\nmore), which are used for prompting LLMs in synthetic data generation. To prepare our LLM judges for evaluation,\nwe first generate synthetic queries and answers from the corpus passages. Using our generated training triples and a\nconstrastive learning framework, we fine-tune an LLM to classify query\u2013passage\u2013answer triples in three different\ncriteria: context relevance, answer faithfulness, and answer relevance. Finally, we use the LLM judges to score\nRAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.\n\n\n\n**3.3** **Ranking R", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_3150", "chunk_text": " relevance, answer faithfulness, and answer relevance. Finally, we use the LLM judges to score\nRAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.\n\n\n\n**3.3** **Ranking RAG Systems with Confidence**\n\n**Intervals**\n\n\nOnce we have prepared our LLM judges, we need\nto use them to score and rank the competing RAG\nsystems. To do this, ARES samples the in-domain\nquery-document-answer triples produced by each\nRAG approach, and the judges label each triple,\npredicting their context relevance, answer faithfulness, and answer relevance. By averaging the individual predicted labels for each in-domain triple,\nwe calculate the RAG system performance across\neach of the three metrics.\n\nIn principle, we could simply report these average scores as quality metrics for each RAG system.\nHowever, these scores reflect entirely unlabeled\ndata with predictions from a synthetically-trained\nLLM judge, and hence they may not be entirely\naccurate. As an extreme alternative, we could use\njust the small human preference validation set discussed previously for evaluation, reporting the extent to which each RAG system agrees with (or\ndeviates from) the human annotations. However,\nan annotation-based evaluation approach would require labeling substantially more generative outputs from each RAG systems separately, which can\nbe costly both in terms of time and financing.\n\nTo combine the benefits of both, and hence\nboost the precision of the evaluation, ARES uses\n_prediction-powered inference_ (PPI; Angelopoulos\net al. 2023) to predict the system scores. PPI is\na recent statistical method that provides tighter\nconfidence intervals on a small set of annotated\ndatapoints (i.e., our validation set) by leveraging\npredictions on a much larger set of non-annotated\ndatapoints. PPI can leverage both the labeled dat\n\n\napoints and the ARES judge predictions on the\nnon-annotated datapoints to construct confidence\nintervals for our RAG system\u2019s performance.\n\n\nTo do this, PPI uses the LLM judges on the human preference validation set to learn a _rectifier_\n_function_ for constructing a confidence set of the ML\nmodel\u2019s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence\ninterval for the performance of the evaluated RAG\nsystem (", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_3600", "chunk_text": " confidence set of the ML\nmodel\u2019s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence\ninterval for the performance of the evaluated RAG\nsystem (e.g. its context relevance, answer faithfulness, or answer relevance accuracy individually)\ncompared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the human preference validation set with the much larger\nset of datapoints with ML predictions, PPI can develop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\n\n\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confidence bounds for the success and failure rates of the\n\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (probability) for our confidence interval.\n\n\nWith the accuracy confidence interval for each\ncomponent of the RAG, we find the midpoint of\neach confidence interval and use the midpoints to\nrank the RAG systems. With our ranking, we can\ncompare different RAG systems, as well as different configurations of the same RAG system, to find\nthe best-performing approach for a given domain.\n\n\n**4** **Experiments**\n\n\n**4.1** **Models**\n\n\nFor our fine-tuned judges, ARES relies on generating cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use OpenAI\u2019s _gpt-3.5-turbo-16k_, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similarity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand Open", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_4050", "chunk_text": "23, (Brown\net al., 2020) in a zero/few-shot setting. For similarity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI\u2019s _text-embedding-ada-002_ for generating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n\n\n**4.2** **Datasets**\n\n\nOur core experimental goal is to provide a rich\npicture of where ARES can be applied effectively.\nTo test across multiple types of queries, documents,\nand answers, we selected all the datasets from the\nwidely-used KILT and SuperGLUE benchmarks\nfor which RAG is appropriate.\nFrom KILT (Petroni et al., 2021), we use Natural\nQuestions (NQ), HotpotQA, FEVER, and Wizards\n\n- f Wikipedia (WoW) (Kwiatkowski et al., 2019;\nYang et al., 2018; Akhtar et al., 2023; Dinan et al.,\n\n2018). Each dataset uses Wikipedia passages but\nthe queries and answers offer a range of applications. Both NQ and HotpotQA feature direct questions and expect short answers, but NQ uses single\npassages for reasoning while HotpotQA requires\nmultiple passages for reasoning. Furthermore,\nFEVER focuses on fact-verification, determining if\na passage supports or refutes a given statement, and\nexpects an output of \u201cSUPPORTS\u201d or \u201cREFUTES\u201d.\nWoW seeks to evaluate dialogue agents by mapping\nuser dialogue to relevant Wikipedia passages before a chatbot generates a paragraph-length chat\nresponse incorporating passage knowledge.\nFrom SuperGLUE (Wang et al., 2019), we use\nMultiRC and ReCoRD (Khashabi et al., 2018;\nZhang et al., 2018). MultiRC focuses on direct questions for seven different domains (News,\n\n\n\nWikipedia articles, articles on society/law/justice,\narticles on history/anthropology, elementary school\nscience textbooks, 9/11 reports, and fiction).\nReCoRD focuses on determining the placeholder\nentity in a statement, focusing on news articles", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_4500", "chunk_text": ", articles on society/law/justice,\narticles on history/anthropology, elementary school\nscience textbooks, 9/11 reports, and fiction).\nReCoRD focuses on determining the placeholder\nentity in a statement, focusing on news articles\nfrom CNN and the Daily Mail. For MultiRC and\nReCoRD, we create open-domain versions of their\ntasks. For MultiRC, we perform retrieval over its\nseven sets of domain passages. For ReCoRD, we\nperform retrieval over its news article passages.\nThe efficacy of ARES relies on its ability to rank\ndifferent RAG systems while only using a human\npreference validation set and domain-targeted LLM\njudges. To test the limits of ARES, we need to simulate the existence of many RAG systems that are\nseparated by small accuracy margins on our evaluation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative examples of the mock RAG system. We generate\nthese mock splits of the given datasets by selecting (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for answer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any alteration. For gathering negative query-passage\npairs and query-passage-answer triples, we randomly sample passages and answers from either:\nthe same Wikipedia document or an entirely random Wikipedia document. This sampling allows\nus to artificially create mock RAG systems for testing ARES. By sampling both related and unrelated\ndocuments/answers, we hope to better gauge the\nefficacy of ARES in judging RAG outputs.\nWe do not evaluate answer faithfulness for KILT\n\nand SuperGLUE datasets since we do not have\nhuman-annotated hallucinated answers to use for\n\nevaluation. However, we do test the ARES frame\nwork on real attribution datasets in Section 5.2.\n\nUsing the validation subsets for each KILT\nand SuperGLUE dataset, we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\n\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_4950", "chunk_text": " we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\n\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70.0%, 72.5%, 75.0%, ..., 90.0%).\nEach split also represents a different mock RAG\nsystem. Since we know the success percentages of\neach dataset split, we know the appropriate ranking of each mock RAG system. This allows us to\n\n\ntest ARES success at both scoring and ranking the\nmock RAG systems appropriately across the three\nevaluation criteria.\n\n\n**4.3** **Metrics**\n\n\nTo calculate the correlation between the correct\n\nranking and the ARES ranking, we use the Kendall\nrank correlation coefficient or Kendall\u2019s _\u03c4_ :\n\n\n_\u03c4_ = [(#][ of concordant pairs][)] _[ \u2212]_ [(#][ of discordant pairs][)]\n\n# of pairs total\n\n\nConcordant pairs are defined as two ordinal values in the ranking where the earlier value in the\nsequence is lower than the later value in the sequence. Discordant pairs are defined as two ordinal\nvalues in the ranking where the earlier value in the\nsequence is greater than or equal to the later value\nin the sequence. A Kendall\u2019s _\u03c4_ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document preprocessing. We want to make sure that ARES has satisfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall\u2019s _\u03c4_ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pairwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in\ninformation retrieval, allowing developers to evaluate ranking systems empirically. Therefore, we\nbelieve Kendall\u2019s tau and prediction accuracy provide meaningful metrics for testing the efficacy of\nARES as a RAG evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_5400", "chunk_text": " evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges). We compare against RAGAS (version 0.0.18) and a baseline few-shot prompted GPT3.5 judge ( _gpt-3.5-turbo-16k_ ). For the few-shot\nGPT-3.5 judge, we provide few-shot examples for\nguiding predictions; the prompts are included in\nAppendices A.2, A.3, and A.4. For both ARES\nand the GPT-3.5 judge baseline, we augment the\nLLM with PPI, using a 300-datapoint human preference validation set to rectify the ML predictions\nand produce confidence intervals.\n\n\n\nAcross almost all settings across the datasets\nfrom KILT and SuperGLUE, ARES provides a\nmore accurate ranking of RAG systems than RAGAS. ARES averages a Kendall\u2019s _\u03c4 0.065 higher_\n_for context relevance_ and _0.132 higher for answer_\n_relevance than RAGAS_ . Additionally, the LLMjudge is substantially more accurate than RAGAS\nat predicting context relevance and answer relevance of a query-passage-answer triple. For context relevance, ARES with a fine-tuned LLM-judge\nis _59.9 percentage points higher than RAGAS_ while\nfor answer relevance, our system is _14.4 percent-_\n_age points higher than RAGAS_ . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompting and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall\u2019s _\u03c4_ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall\u2019s _\u03c4_ for ARES is\n0.08 higher", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_5850", "chunk_text": " configuration, in which we\nsampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall\u2019s _\u03c4_ for ARES is\n0.08 higher on average, across both context and answer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides\na more accurate ranking of the RAG systems than\nthe GPT-3.5 judge, averaging a Kendall\u2019s tau 0.06\nhigher over both context relevance and answer relevance. Between the judge configurations, the finetuned LLM judge of ARES can more precisely distinguish between RAG systems and guide configuration decisions surrounding document splitting, retriever selection, and generative LLM choice. However, while the fine-tuned LLM judge had a higher\nKendall\u2019s tau on average, the GPT-3.5 judge is\nmore readily deployable and does not require any\nadditional fine-tuning. The GPT-3.5 judge does\ncome with its own querying costs, which can vary\nbased on the date of querying as well as the total\ntokens used in evaluation.\n\nWe also wanted to better understand the importance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we\n\n\n**ARES Ranking of Pseudo RAG Systems**\n\n\nNQ HotpotQA WoW FEVER MultiRC ReCoRD\n\n\nC.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R.\n\n\nKendall\u2019s Tau for\n0.83 0.89 0.78 0.78 0.78 0.83 **0.89** **0.89** 0.83 0.83 0.72 0.94\nSampled Annotations\n\n\nKendall\u2019s Tau\n0.89 0.89 **0.94** 0.89 0.94 0.94 0.72 0.61 0.83 **0.94** **0.89** 0.44\nfor RAGAS\n\n\nKendall\u2019s Tau\n", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_6300", "chunk_text": " 0.89 0.94 0.94 0.72 0.61 0.83 **0.94** **0.89** 0.44\nfor RAGAS\n\n\nKendall\u2019s Tau\n0.89 0.94 0.67 **0.94** 0.94 0.89 0.78 0.78 0.83 0.89 0.83 **0.94**\nfor GPT-3.5 Judge\n\n\nKendall\u2019s Tau for\n0.89 **1.0** 0.89 **0.94** 0.94 **1.0** 0.83 0.72 **0.94** 0.83 0.78 0.83\nARES LLM Judge\n\n\nKendall\u2019s Tau\n**0.94** **1.0** **0.94** **0.94** **1.0** **1.0** **0.89** 0.78 **0.94** 0.89 0.83 0.89\nfor ARES\n\n\nRAGAS Accuracy 31.4% 71.2% 17.2% 76.0% 36.4% 77.8% 23.7% 69.2% 16.1% 75.0% 15.0% 72.8%\n\n\nGPT-3.5 Judge Accuracy 73.8% 95.5% 75.3% 71.6% 84.3% 85.2% 60.4% 59.6% 72.4% 60.3% 81.0% 65.8%\n\n\nARES Accuracy 79.3% 97.2% 92.3% 81.3% 85.7% 96.1% 88.4% 78.5% 85.8% 82.7% 67.8% 92.3%\n\n\nTable 1: **ARES Ranking with Fine-tuned LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge** :\nFor scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES\nwith our", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_6750", "chunk_text": " LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge** :\nFor scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES\nwith our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.\nFor our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels\nto score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for\neach evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and\nGPT-3.5 across all the explored datasets. The Kendall\u2019s tau for ARES was _0.065 higher on average for scoring_\n_context relevance_ and _0.132 higher on average for scoring answer relevance_ than RAGAS. Additionally, we include\nthe Kendall\u2019s taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of\nthe judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run.\nFor PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation\nset. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\n\n\n\nused ARES with human annotation sets ranging\nin size from 25 to 400 and found that 150 is the\n\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).\n\n\n**5.2** **ARES Performance on AIS**\n\n\n**WoW** **CNN / DM**\n\n\nARES Split Prediction 0.478 0.835\nCorrect Positive/Negative Split 0.458 0.859\nARES Judge Accuracy 62.5% 84.0%\nEvaluation Set Size 707 510\n\nHuman Preference Data Size 200 200\n\n\nTable 2: ARES Results on the AIS benchmark", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_7200", "chunk_text": "0.458 0.859\nARES Judge Accuracy 62.5% 84.0%\nEvaluation Set Size 707 510\n\nHuman Preference Data Size 200 200\n\n\nTable 2: ARES Results on the AIS benchmark\n\n\nTo evaluate whether ARES can effectively gauge\nanswer faithfulness in real RAG systems, we tested\nARES on the AIS attribution benchmark (Rashkin\net al., 2022). In AIS, we selected the Wizards\n\n- f Wikipedia (WoW) and CNN/DM datasets; the\n\n\n\n\n- ther benchmark datasets involve either table rea\nsoning (ToTTo) or focus on passage summarization (QRECC) so we excluded them. In WoW\nand CNN/DM, each evaluation example includes\na query, a retrieved passage, and a generated answer (which is either faithful or non-attributed to\nthe retrieved passage).\n\nTable 2 summarizes our AIS results. We found\n\nthat ARES can effectively score the AIS datasets,\ngetting within 2.5 accuracy points of the correct\nscores. Furthermore, for scoring each system,\nwe only use 200 annotated datapoints for our human preference validation set. Our results on AIS\ndemonstrate the ability of ARES to reliably distinguish faithful and hallucinated answers in realworld RAG systems.\n\n\n**5.3** **ARES Ranking of Existing RAG Systems**\n\n\nWe also wanted to evaluate whether ARES can\n\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen\n\nerations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Additionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-tosequence model (Lewis et al., 2019). During retrieval, each R", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_7650", "chunk_text": "\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-tosequence model (Lewis et al., 2019). During retrieval, each RAG system only retrieves one passage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applications, averaging a Kendall\u2019s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com\npared to RAGAS, ARES is 0.16 higher for context\nrelevance and 0.15 higher for answer relevance, on\naverage. ARES also provided accurate confidence\nbounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on average, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n\n\n**5.4** **Strengths and Limits of Cross-Domain**\n**Applications**\n\n\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are difficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in _query type_ from\ntraining to test (e.g. NQ to FEVER), change in\n_document type_ from training to test (e.g. NQ to\nMultiRC), and change in both _query and document_\n_type_ (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in crossdomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generalizability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Furthermore, we found that even when the LLM judge\u2019s accuracy suffered in cross-domain applications, PPI\nhelped", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_8100", "chunk_text": "RES had strong generalizability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Furthermore, we found that even when the LLM judge\u2019s accuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\n\n\n\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\n\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when\nmaking more drastic shifts in domain, such as:\nswitching languages (e.g. English to Spanish, German, and other languages), switching from text to\ncode (e.g. questions + passages to coding functions\n+ documentation), and switching from retrieving\ntext to extraction of entities, webpages, or citations.\nTo test cross-lingual transfer, we used the\nXGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall\u2019s tau of 0.33\n\n- ver both context relevance and answer relevance\n\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall\u2019s tau\n\n- f 0.28 over both context relevance and answer\n\nrelevance scoring for CodeSearchNet. To test extraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall\u2019s\ntau of 0.38 over both context relevance and answer\n\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n\n\n**6** **Conclusion**\n\n\nIn this work, we present ARES, a novel automated\nevaluation framework for retrieval-augmented generation (RAG). ARES offers a novel training\npipeline for fine-tuning lightweight LLM judges\n\n- n synthetically generated queries and answers.\nARES can evaluate each component of a RAG system separately to help improve system understanding and create targeted solutions, and it requires\n\n- nly minimal human annotations. For the eight different datasets", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_8550", "chunk_text": "LM judges\n\n- n synthetically generated queries and answers.\nARES can evaluate each component of a RAG system separately to help improve system understanding and create targeted solutions, and it requires\n\n- nly minimal human annotations. For the eight different datasets in KILT, SuperGLUE, and AIS requiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\n\n- n context relevance, answer faithfulness, and answer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\n\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing\n\n\nlogits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n\n\n**7** **Limitations**\n\n\nARES relies on a small set of annotations in the\n\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\n\n- ften require an annotator familiar with the RAG\nsystem\u2019s domain application. While these annotations can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our evaluation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\n\n- ther languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\n\n\n**References**\n\n\nMubashara Ak", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_9000", "chunk_text": "- ther languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\n\n\n**References**\n\n\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. _[Proceedings of](https://aclanthology.org/2023.fever-1.0)_\n_[the Sixth Fact Extraction and VERification Workshop](https://aclanthology.org/2023.fever-1.0)_\n_[(FEVER)](https://aclanthology.org/2023.fever-1.0)_ . Association for Computational Linguistics,\nDubrovnik, Croatia.\n\n\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\n[Prediction-powered inference.](http://arxiv.org/abs/2301.09633)\n\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\n\n\n\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n[2020. Language models are few-shot learners.](http://arxiv.org/abs/2005.14165)\n\n\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. _arXiv preprint_\n_arXiv:2309.01431_ .\n\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Z", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_9450", "chunk_text": " Benchmarking large language models in\nretrieval-augmented generation. _arXiv preprint_\n_arXiv:2309.01431_ .\n\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\n_arXiv preprint arXiv:2210.11416_ .\n\n\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B\nHall, and Ming-Wei Chang. 2022. Promptagator:\nFew-shot dense retrieval from 8 examples. _arXiv_\n_preprint arXiv:2209.11755_ .\n\n\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\n\n - f wikipedia: Knowledge-powered conversational\nagents. _arXiv preprint arXiv:1811.01241_ .\n\n\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-rex: A large scale\nalignment of natural language with knowledge base\ntriples. In _Proceedings of the Eleventh International_\n_Conference on Language Resources and Evaluation_\n_(LREC 2018)_ .\n\n\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. _arXiv_\n_preprint arXiv:2302.04166_ .\n\n\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n[2023. Enabling large language models to generate](http://arxiv.org/abs/2305.14627)\n[text with citations.](http://arxiv.org/abs/2305.14627)\n\n\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\n[Elkind, and Idan Szpektor. 2023. Truete", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_9900", "chunk_text": "arxiv.org/abs/2305.14627)\n\n\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\n[Elkind, and Idan Szpektor. 2023. Trueteacher: Learn-](http://arxiv.org/abs/2305.11171)\n[ing factual consistency evaluation with large lan-](http://arxiv.org/abs/2305.11171)\n[guage models.](http://arxiv.org/abs/2305.11171)\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In _International confer-_\n_ence on machine learning_, pages 3929\u20133938. PMLR.\n\n\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. _arXiv preprint arXiv:2111.09543_ .\n\n\n[Jeremy Howard and Sebastian Ruder. 2018. Universal](https://doi.org/10.18653/v1/P18-1031)\n[language model fine-tuning for text classification.](https://doi.org/10.18653/v1/P18-1031)\nIn _Proceedings of the 56th Annual Meeting of the_\n_Association for Computational Linguistics (Volume 1:_\n_Long Papers)_, pages 328\u2013339, Melbourne, Australia.\nAssociation for Computational Linguistics.\n\n\nSiqing Huo, Negar Arabzadeh, and Charles LA Clarke.\n2023. Retrieving supporting evidence for llms generated answers. _arXiv preprint arXiv:2306.13781_ .\n\n\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. CodeSearchNet challenge: Evaluating the state of semantic code search. _arXiv preprint arXiv:1909.09436_ .\n\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_10350", "chunk_text": "Xiv:1909.09436_ .\n\n\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with retrieval augmented language models. _arXiv preprint_\n_arXiv:2208.03299_ .\n\n\n[Jithin James and Shahul Es. 2023. Ragas: Evaluation](https://github.com/explodinggradients/ragas)\n[framework for your retrieval augmented generation](https://github.com/explodinggradients/ragas)\n[(rag) pipelines.](https://github.com/explodinggradients/ragas)\n\n\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019.\nBillion-scale similarity search with GPUs. _IEEE_\n_Transactions on Big Data_, 7(3):535\u2013547.\n\n\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\n[Thakur, and Jimmy Lin. 2023. Hagrid: A human-](http://arxiv.org/abs/2307.16883)\n[llm collaborative dataset for generative information-](http://arxiv.org/abs/2307.16883)\n[seeking with attribution.](http://arxiv.org/abs/2307.16883)\n\n\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n[Wen tau Yih. 2020. Dense passage retrieval for open-](http://arxiv.org/abs/2004.04906)\n[domain question answering.](http://arxiv.org/abs/2004.04906)\n\n\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading comprehension over multiple sentences. In _Proceedings_\n\n_of the 2018 Conference of the North American Chap-_\n_ter of the Association for Computational Linguistics:_\n_Human Language Technologies, Volume 1 (Long Pa-_\n_p", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_10800", "chunk_text": " set for reading comprehension over multiple sentences. In _Proceedings_\n\n_of the 2018 Conference of the North American Chap-_\n_ter of the Association for Computational Linguistics:_\n_Human Language Technologies, Volume 1 (Long Pa-_\n_pers)_, pages 252\u2013262.\n\n\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Relevance-guided supervision for openqa with\ncolbert. _Transactions of the association for computa-_\n_tional linguistics_, 9:929\u2013944.\n\n\n[Diederik P. Kingma and Jimmy Ba. 2017. Adam: A](http://arxiv.org/abs/1412.6980)\n[method for stochastic optimization.](http://arxiv.org/abs/1412.6980)\n\n\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. _arXiv preprint arXiv:2302.14520_ .\n\n\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit\nIyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.\n[2023. LongEval: Guidelines for human evaluation of](https://aclanthology.org/2023.eacl-main.121)\n[faithfulness in long-form summarization. In](https://aclanthology.org/2023.eacl-main.121) _Proceed-_\n_ings of the 17th Conference of the European Chap-_\n_ter of the Association for Computational Linguistics_,\npages 1650\u20131669, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. _Transactions of the_\n_Association for Computational Linguistics_, 7:453\u2013\n466.\n\n\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\n[Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-](http://arxiv.org/abs/191", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_11700", "chunk_text": "hen Huang, Furu Wei, Weiwei Deng,\nFeng Sun, and Qi Zhang. 2023b. Calibrating llmbased evaluator. _arXiv preprint arXiv:2309.13308_ .\n\n\nGr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, Edouard Grave, Yann LeCun, and\n[Thomas Scialom. 2023. Augmented language mod-](http://arxiv.org/abs/2302.07842)\n[els: a survey.](http://arxiv.org/abs/2302.07842)\n\n\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\n[Factscore: Fine-grained atomic evaluation of factual](http://arxiv.org/abs/2305.14251)\n[precision in long form text generation.](http://arxiv.org/abs/2305.14251)\n\n\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian\n[Riedel. 2021. KILT: a benchmark for knowledge](https://doi.org/10.18653/v1/2021.naacl-main.200)\n[intensive language tasks. In](https://doi.org/10.18653/v1/2021.naacl-main.200) _Proceedings of the 2021_\n_Conference of the North American Chapter of the_\n_Association for Computational Linguistics: Human_\n_Language Technologies_, pages 2523\u20132544, Online.\nAssociation for Computational Linguistics.\n\n\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov,", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_12150", "chunk_text": " 2523\u20132544, Online.\nAssociation for Computational Linguistics.\n\n\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\n[Reitter. 2022. Measuring attribution in natural lan-](http://arxiv.org/abs/2112.12870)\n[guage generation models.](http://arxiv.org/abs/2112.12870)\n\n\nJon Saad-Falcon, Omar Khattab, Keshav Santhanam,\nRadu Florian, Martin Franz, Salim Roukos, Avirup\nSil, Md Arafat Sultan, and Christopher Potts. 2023.\n\n\nUdapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. _arXiv_\n_preprint arXiv:2303.00807_ .\n\n\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In _DESIRES_,\npages 136\u2013146.\n\n\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\n[Christopher Potts, and Matei Zaharia. 2022. Col-](https://doi.org/10.18653/v1/2022.naacl-main.272)\nBERTv2: [Effective and efficient retrieval via](https://doi.org/10.18653/v1/2022.naacl-main.272)\n[lightweight late interaction. In](https://doi.org/10.18653/v1/2022.naacl-main.272) _Proceedings of the_\n_2022 Conference of the North American Chapter of_\n_the Association for Computational Linguistics: Hu-_\n_man Language Technologies_, pages 3715\u20133734, Seattle, United States. Association for Computational\nLinguistics.\n\n\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\n[and Jason Weston. 2021. Retrieval augmentation](http://arxiv.org/abs/2104.07567)\n[reduces hallucination in conversation.](http://arxiv.org/abs/2104.07567)\n\n\nMosaic", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_13050", "chunk_text": ".\n\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2023.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. _arXiv preprint arXiv:2306.05685_ .\n\n\n**A** **Appendix**\n\n\n**A.1** **Fine-tuning Configuration for LLM**\n**Judges**\n\n\nFor our loss function used in LLM judge training, we selected cross-entropy loss using Adam\n\n\n\n(Kingma and Ba, 2017). For our classification head,\nwe use a single linear classification layer and apply a 0.1 dropout to the input, which is the final\nhidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\n\n\n**A.2** **GPT Prompting for Context Relevance**\n**Scoring**\n\n\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the following prompt to score context relevance:\n\n\n  - Given the following question and document,\nyou must analyze the provided document and\ndetermine whether it is sufficient for answering the question. In your evaluation, you\nshould consider the content of the document\n\nand how it relates to the provided question.\nOutput your final verdict by strictly following\nthis format: \"[[Yes]]\" if the document is sufficient and \"[[No]]\" if the document provided is\nnot sufficient. Do not provide any additional\nexplanation for your decision.\n\n\nQuestion: < _few-shot example here_   \n\nDocument: < _few-shot example here_   \n\nFor FEVER, we use the following prompt to\nscore context relevance:\n\n\n  - You are an expert fact-checking agent. Given\nthe following statement and document, you\nmust analyze the provided document and determine whether it is sufficient for determining\nthe statement\u2019s factuality. In your evaluation,\nyou should consider the content of the document and how it relates to the provided statement\u2019s factuality. Output your final verdict\nby strictly", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_13500", "chunk_text": " analyze the provided document and determine whether it is sufficient for determining\nthe statement\u2019s factuality. In your evaluation,\nyou should consider the content of the document and how it relates to the provided statement\u2019s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\n\n\nStatement: < _few-shot example here_   \n\nDocument: < _few-shot example here_   \n\nFor WoW, we use the following prompt to score\ncontext relevance:\n\n\n  - You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must\n\n\nanalyze the provided document and determine\nwhether it is relevant for responding to the\ndialogue. In your evaluation, you should consider the content of the document and how\n\nit relates to the provided dialogue. Output\nyour final verdict by strictly following this\nformat: \"[[Yes]]\" if the document is relevant\nand \"[[No]]\" if the document provided is not\nrelevant. Do not provide any additional explanation for your decision.\n\n\nDialogue: < _few-shot example here_   \n\nDocument: < _few-shot example here_   \n\n**A.3** **GPT Prompting for Answer Faithfulness**\n**Scoring**\n\n\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the following prompt to score answer faithfulness:\n\n\n  - Given the following question, document, and\nanswer, you must analyze the provided answer\nand determine whether it is faithful to the con\ntents of the document. The answer must not\n\n   - ffer new information beyond the context provided in the document. The answer also must\n\nnot contradict information provided in the document. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis faithful to the document and \"[[No]]\" if the\n\nanswer is not faithful to the document. Do not\n\nprovide any additional explanation for your\ndecision.\n\n\nQuestion: < _few-shot example here_   \n\nDocument: < _few-shot example here_   \n\nAnswer: < _few-shot example here_   \n\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_13950", "chunk_text": "   \n\nAnswer: < _few-shot example here_   \n\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\n\n\n**A.4** **GPT Prompting for Answer Relevance**\n**Scoring**\n\n\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the following prompt to score answer relevance:\n\n\n  - Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided question. In your evaluation, you should consider\n\n\n\nwhether the answer addresses all aspects of\nthe question and provides only correct information from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given question. Do not provide any additional explanation for your decision.\n\n\nQuestion: < _few-shot example here_   \n\nDocument: < _few-shot example here_   \n\nAnswer: < _few-shot example here_   \n\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\n\n\n**A.5** **Prompting for Generation of Synthetic**\n**Queries and Answers**\n\n\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and provide 5 few-shot examples:\n\n\n  - Example N\n\n\nQuestion: < _few-shot example here_   \n\nDocument: < _few-shot example here_   \n\nAnswer: < _few-shot example here_   \n\nWe use the same prompting structure for generating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\n\n\n**A.6** **Synthetic Query and Answer Generation**\n\n\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n\n\n  - Example #1\n\n\nDocument: < _few-shot example here_   \n\nQuery: < _few-shot example here_   \n\nExample #2\n\n\nDocument: < _few-shot example here_   \n\nQuery: < _few-shot example here_   \n\nExample #3\n\n\nDocument: < _few-shot", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_14400", "chunk_text": "few-shot example here_   \n\nQuery: < _few-shot example here_   \n\nExample #2\n\n\nDocument: < _few-shot example here_   \n\nQuery: < _few-shot example here_   \n\nExample #3\n\n\nDocument: < _few-shot example here_   \n\nQuery: < _few-shot example here_   \n\nExample #4\n\n\nDocument: < _in-domain passage_   \n\nQuery:\n\n\nFor generating our synthetic answers, we use the\nfollowing prompt for FLAN-T5 XXL:\n\n\n  - Example #1\n\n\nQuery: < _few-shot example here_   \n\nDocument: < _few-shot example here_   \n\nAnswer: < _few-shot example here_   \n\nExample #2\n\n\nQuery: < _few-shot example here_   \n\nDocument: < _few-shot example here_   \n\nAnswer: < _few-shot example here_   \n\nExample #3\n\n\nQuery: < _few-shot example here_   \n\nDocument: < _few-shot example here_   \n\nAnswer: < _few-shot example here_   \n\nExample #4\n\n\nQuery: < _synthetic query here_   \n\nDocument: < _in-domain passage here_   \n\nAnswer:\n\n\nFigure 2: **RAG Systems Evaluation on NQ - Context Relevance**\n\n\nFigure 3: **RAG Systems Evaluation on NQ - Answer Relevance**\n\n\n**Kendall\u2019s Tau by Dataset**\n\n\nNQ MultiRC ReCoRD\n\n\n**PPI Labeled**\nC.R. A.R. C.R. A.R. C.R. A.R.\n**Count**\n\n\n400 1.0 1.0 0.89 0.94 0.89 0.94\n\n300 0.89 1.0 0.94 0.89 0.83 0.89\n\n200 0.83 1.0 0.83 0.94 0.83 0.83\n\n150 0.72 1.0 0.83 0.89 0.72 0.83\n\n100 0.44 1.0 0.67 0.67 0.67 0.83\n\n50 0.44 0.94 0.61 0.44 0.56 0.67\n\n25 0.44 0.89 0.56 0.44 0.44 0.56\n\n\nTable 3: **Analysis of", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_14850", "chunk_text": " 0.61 0.44 0.56 0.67\n\n25 0.44 0.89 0.56 0.44 0.44 0.56\n\n\nTable 3: **Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall\u2019s Tau** : The Kendall\u2019s tau values represent\nthe correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same\nexperimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human\npreference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their\naccuracies in context relevance and answer relevance (C.R. and A.R., respectively).\n\n\n**ARES Ranking of Pseudo RAG Systems using GPT-4 Labels**\n\n\nNQ ReCoRD MultiRC\n\n\nContext Answer Context Answer Context Answer\n\nRelevance Relevance Relevance Relevance Relevance Relevance\n\n\nKendall\u2019s Tau 0.78 1.0 0.78 0.72 0.89 0.78\n\n\nKendall\u2019s Tau of\n0.94 1.0 0.83 0.89 0.94 0.89\nHuman Labeled Approach\n\n\nAverage PPI Range 9.2% 6.8% 8.2% 9.0% 7.7% 8.3%\n\n\nAccuracy on\n79.3% 96.7% 88.4% 78.3% 85.8% 82.5%\nRAG Evaluation Sets\n\n\nTable 4: **GPT-4 Labels vs. Human Labels** : We wanted to explore the practicality of using GPT-4 generated\nlabels instead of human annotations for our human preference validation set in ARES. In the experiments, we\ngenerated 500 GPT-4 labels as replacements for human labeling using few-shot prompts (see Sections A.2, A.3,\nand A.4). While GPT-4 generated labels decreased Kendall\u2019s tau in most settings by 0.05 to 0.30, the ability to\ncheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of\nannotations to less than ten for few-shot prompts. Additionally, the efficacy of P", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_15300", "chunk_text": ".05 to 0.30, the ability to\ncheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of\nannotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we\ngenerate more GPT-4 generated labels. In the table, we define PPI range as the number of percentage points from\nthe lower number to the upper number of the PPI confidence bounding. Additionally, we use the fine-tuned LLM\njudge (DeBERTa-v3-Large) for evaluation.\n\n\n**ARES Ranking of Real RAG Systems**\n\n\nNQ WoW FEVER\n\n\nC.R. A.R. C.R. A.R. C.R. A.R.\n\n\nKendall\u2019s Tau for\n0.73 0.78 0.73 0.73 0.73 0.82\nSampled Annotations\n\nKendall\u2019s Tau\n0.82 0.82 0.73 0.82 0.73 0.87\nfor RAGAS\n\n\nKendall\u2019s Tau\n0.82 0.87 0.82 0.82 0.64 0.87\nfor GPT-3.5 Judge\n\nKendall\u2019s Tau\n0.91 **0.96** **0.91** **1.0** 0.73 0.87\nfor ARES LLM Judge\n\nKendall\u2019s Tau\n**1.0** **0.96** **0.91** **1.0** **0.82** **1.0**\nfor ARES\n\n\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\n\n\nTable 5: **ARES Ranking on Real-World RAG Systems** : For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_15750", "chunk_text": " 5: **ARES Ranking on Real-World RAG Systems** : For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall\u2019s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\n\n\n**ARES Cross-Domain Ranking of Pseudo RAG Systems**\n\n\nNQ to FEVER to NQ to MultiRC to NQ to ReCoRD to\nFEVER NQ MultiRC NQ ReCoRD NQ\n\n\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\n\n\nKendall\u2019s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\n\n\nKendall\u2019s Tau of\n0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nIn-Domain LLM Judge\n\n\nAverage PPI Range 8.7% 7.2%", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_16200", "chunk_text": " 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nIn-Domain LLM Judge\n\n\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\n\n\nAccuracy on\n92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nRAG Evaluation Sets\n\n\nTable 6: **Cross-Domain Usage of Fine-tuned LLM Judges** : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge\u2019s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.\n\n\n|Query|Passage|Answer|Context<br>Relevance|Answer<br>Relevance|\n|---|---|---|---|---|\n|How can a ball that is not<br>moving possess energy<br>of position?|Mechanical energy is a combination of the energy of motion or position.<br>This type of energy describes objects that are moving or could move.<br>A moving ball can have energy from motion. An arrow can also have<br>the energy of motion. Both are types of mechanical", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_16650", "chunk_text": " combination of the energy of motion or position.<br>This type of energy describes objects that are moving or could move.<br>A moving ball can have energy from motion. An arrow can also have<br>the energy of motion. Both are types of mechanical energy.|The ball holds<br>mechanical energy|1|1|\n|Who has a Jimmy<br>Stewart-like quality<br>of quiet trust?|One look at Fred Rooney, and you just know he\u2019s the good guy.<br>A trace of childish innocence in his face gives the lanky<br>Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.<br>In black jeans and button-down shirt, he\u2019s a kind of folk hero<br>in the south Bethlehem melting pot where he\u2019s crafted a law<br>practice catering to working-class families - mostly Latino -<br>in the shadow of the hulkish remnants of Bethlehem Steel.|Fred Rooney|1|1|\n|Before he murder the<br>doctor and Ralph Smith,<br>where did the stepfather<br>reside?|Surviving being shot and stabbed at the end of the previous flm,<br>the stepfather has been institutionalized in Puget Sound, Washington since,<br>spending his time building model houses in the workshop.<br>Assigned a new doctor named Joseph Danvers the stepfather<br>begins confding in him to gain his trust, ultimately murdering<br>the doctor during a session by stabbing him in the neck with a<br>blade smuggled out of the workshop . After killing Danvers the stepfather<br>beats a suspicious guard named Ralph Smith to death with his own nightstick<br>with only two strikes and takes his uniform, successfully<br>sneaking out of the sanitarium . Checking into a hotel after robbing and<br>murdering a traveling salesman the stepfather alters his appearance,<br>takes the name Doctor Gene F. Clifford from the newspaper obituaries<br>and travels to Palm Meadows, Los Angeles after seeing an ad for it on<br>an episode of Dream House .|Los Angeles|1|0|\n|What was the name of the<br>2006 flm about Pushkin\u2019s death,<br>and who portrayed Pushkin?|After arriving in New York City, Einstein was taken to various places and<br>events, including Chinatown, a lunch with the editors of the New York<br>Times, and a performance", "token_count": 500, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2311.09476_ares_saad_falcon:chunk_17100", "chunk_text": "\u2019s death,<br>and who portrayed Pushkin?|After arriving in New York City, Einstein was taken to various places and<br>events, including Chinatown, a lunch with the editors of the New York<br>Times, and a performance of Carmen at the Metropolitan Opera,<br>where he was cheered by the audience on his arrival.<br>During the days following, he was given the keys to the city by Mayor<br>Jimmy Walker and met the president of Columbia University, who<br>described Einstein as \"The ruling monarch of the mind.\" Harry<br>Emerson Fosdick, pastor at New York\u2019s Riverside Church, gave<br>Einstein a tour of the church and showed him a full-size statue that<br>the church made of Einstein, standing at the entrance.|Vasily Szaitsev portrayed<br>Pushkin in the flm<br>Pushkin Returns|0|0|\n\n\n\nTable 7: Positive and Negatives Evaluation Examples\n\n\n", "token_count": 199, "metadata": {"arxiv_id": "2311.09476", "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems", "authors": ["Jon Saad-Falcon", "Omar Khattab", "Christopher Potts", "Matei Zaharia"], "year": 2023, "url": "https://arxiv.org/pdf/2311.09476v2"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_0", "chunk_text": "1\n\n\n# A Survey on Knowledge Distillation of Large Language Models\n\nXiaohan Xu [1], Ming Li [2], Chongyang Tao [3], Tao Shen [4], Reynold Cheng [1], Jinyang Li [1],\nCan Xu [5], Dacheng Tao [6], Tianyi Zhou [2]\n\n\n1The University of Hong Kong 2University of Maryland 3Microsoft\n4University of Technology Sydney 5Peking University 6The University of Sydney\n_{_ shawnxxh,chongyangtao,hishentao _}_ @gmail.com _{_ minglii,tianyi _}_ @umd.edu\nckcheng@cs.hku.hk jl0725@connect.hku.hk\n\n\n**Abstract** - In the era of Large Language Models (LLMs), Knowledge Distillation (KD) emerges as a pivotal methodology for transferring\nadvanced capabilities from leading proprietary LLMs, such as GPT-4, to their open-source counterparts like LLaMA and Mistral.\nAdditionally, as open-source LLMs flourish, KD plays a crucial role in both compressing these models, and facilitating their selfimprovement by employing themselves as teachers. This paper presents a comprehensive survey of KD\u2019s role within the realm of\nLLM, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and selfimprovement. Our survey is meticulously structured around three foundational pillars: _algorithm_, _skill_, and _verticalization_ - providing a\ncomprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across\ndiverse fields. Crucially, the survey navigates the interaction between data augmentation (DA) and KD, illustrating how DA emerges\nas a powerful paradigm within the KD framework to bolster LLMs\u2019 performance. By leveraging DA to generate context-rich, skillspecific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness,\nethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful\nguide for researchers and practitioners, offering a detailed overview of current methodologies in knowledge distillation and proposing\nfuture research directions. By bridging the gap between proprietary and open-source LLMs, this survey underscores the potential\nfor more accessible, efficient, and powerful AI solutions. Most importantly, we firmly advocate for compliance with the legal terms\nthat regulate the use of L", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_450", "chunk_text": " By bridging the gap between proprietary and open-source LLMs, this survey underscores the potential\nfor more accessible, efficient, and powerful AI solutions. Most importantly, we firmly advocate for compliance with the legal terms\nthat regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at\n[https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.](https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs)\n\n\n**Index Terms** - Large language models, knowledge distillation, data augmentation, skill distillation, supervised fine-tuning\n\n\n\u2726\n\n\n\n**1** **INTRODUCTION**\n\n\nIn the evolving landscape of artificial intelligence (AI),\nproprietary [1] Large Language Models (LLMs) such as GPT3.5 (Ouyang et al., 2022), GPT-4 (OpenAI et al., 2023),\nGemini (Team et al., 2023) and Claude [2] have emerged as\ngroundbreaking technologies, reshaping our understanding\n\n- f natural language processing (NLP). These models, characterized by their vast scale and complexity, have unlocked\nnew realms of possibility, from generating human-like text\nto offering sophisticated problem-solving capabilities. The\ncore significance of these LLMs lies in their emergent abilities (Wei et al., 2022a,b; Xu et al., 2024a), a phenomenon\nwhere the models display capabilities beyond their explicit\ntraining objectives, enabling them to tackle a diverse array\n\n- f tasks with remarkable proficiency. These models excel\nin understanding and generation, driving applications from\ncreative generation to complex problem-solving (OpenAI\net al., 2023; Liang et al., 2022). The potential of these models\n\n\n1. For simplicity, we use \u2018proprietary\u2019 to represent both versatile yet\nclose-source LLMs like GPT-4 and open-source yet huge LLMs like\nLLaMA-2-70B, which encapsulate rich knowledge with a large number\n\n- f parameters.\n2. https://www.anthropic.com/claude-in-slack\n\n\n\nextends far beyond current applications, promising to revolutionize industries, augment human creativity, and redefine\n\n- ur interaction with technology.\nDespite the remarkable capabilities of proprietary LLMs\nlike GPT-", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_900", "chunk_text": ".anthropic.com/claude-in-slack\n\n\n\nextends far beyond current applications, promising to revolutionize industries, augment human creativity, and redefine\n\n- ur interaction with technology.\nDespite the remarkable capabilities of proprietary LLMs\nlike GPT-4 and Gemini, they are not without their shortcomings, particularly when viewed in light of the advantages\n\n- ffered by open-source models. A significant drawback is\ntheir limited accessibility and higher cost (OpenAI et al.,\n2023). These proprietary models often come with substantial\nusage fees and restricted access, making them less attainable for individuals and smaller organizations. In terms of\ndata privacy and security (Wu et al., 2023a), using these\nproprietary LLMs frequently entails sending sensitive data\nto external servers, which raises concerns about data privacy and security. This aspect is especially critical for users\nhandling confidential information. Moreover, the generalpurpose design of proprietary LLMs, while powerful, may\nnot always align with the specific needs of niche applications. The constraints of accessibility, cost, and adaptability\nthus present significant challenges in leveraging the full\npotential of proprietary LLMs.\nIn contrast to proprietary LLMs, open-source models\nlike LLaMA (Touvron et al., 2023) and Mistral (Jiang et al.,\n2023a) bring several notable advantages. One of the primary\n\n\nbenefits of open-source models is their accessibility and\nadaptability. Without the constraints of licensing fees or\nrestrictive usage policies, these models are more readily\navailable to a broader range of users, from individual researchers to smaller organizations. This openness fosters a\nmore collaborative and inclusive AI research environment,\nencouraging innovation and diverse applications. Additionally, the customizable nature of open-source LLMs allows\nfor more tailored solutions, addressing specific needs that\ngeneric, large-scale models may not meet.\nHowever, the open-source LLMs also have their own\nset of drawbacks, primarily stemming from their relatively\nlimited scale and resources compared to their proprietary\ncounterparts. One of the most significant limitations is\nthe smaller model scale, which often results in lower performance on real-world tasks with a bunch of instructions (Zheng et al., 2023a). These models, with fewer parameters, may struggle to capture the depth and breadth\n\n- f knowledge embodied in larger models like GPT-4. Additionally, the pre-training investment in these", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_1350", "chunk_text": " of instructions (Zheng et al., 2023a). These models, with fewer parameters, may struggle to capture the depth and breadth\n\n- f knowledge embodied in larger models like GPT-4. Additionally, the pre-training investment in these open-source\nmodels is typically less substantial. This reduced investment\ncan lead to a narrower range of pre-training data, potentially limiting the models\u2019 understanding and handling of\ndiverse or specialized topics (Liang et al., 2022; Sun et al.,\n2024a). Moreover, open-source models often undergo fewer\nfine-tuning steps due to resource constraints. Fine-tuning\nis crucial for optimizing a model\u2019s performance for specific tasks or industries, and the lack thereof can hinder\nthe model\u2019s effectiveness in specialized applications. This\nlimitation becomes particularly evident when these models\nare compared to the highly fine-tuned proprietary LLMs,\nwhich are often tailored to excel in a wide array of complex\nscenarios (OpenAI et al., 2023).\nPrimarily, recognizing the disparities between proprietary and open-source LLMs, KD techniques have surged\nas a means to bridge the performance gap between these\nmodels (Gou et al., 2021; Gupta and Agrawal, 2022). Knowledge distillation, in this context, involves leveraging the\nmore advanced capabilities of leading proprietary models\nlike GPT-4 or Gemini as a guiding framework to enhance\nthe competencies of open-source LLMs. This process is\nakin to transferring the \u2018knowledge\u2019 of a highly skilled\nteacher to a student, wherein the student (e.g., open-source\nLLM) learns to mimic the performance characteristics of\nthe teacher (e.g., proprietary LLM). Compared to traditional\nknowledge distillation algorithms (Gou et al., 2021), data\naugmentation (DA) (Feng et al., 2021) has emerged as a\nprevalent paradigm to achieve knowledge distillation of\nLLMs, where a small seed of knowledge is used to prompt\nthe LLM to generate more data with respect to a specific\nskill or domain (Taori et al., 2023). Secondly, KD still retains\nits fundamental role in compressing LLMs, making them\nmore efficient without significant loss in performance. (Gu\net al., 2024; Agarwal et al., 2024). More recently, the strategy\n\n", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_1800", "chunk_text": " still retains\nits fundamental role in compressing LLMs, making them\nmore efficient without significant loss in performance. (Gu\net al., 2024; Agarwal et al., 2024). More recently, the strategy\n\n- f employing open-source LLMs as teachers for their own\nself-improvement has emerged as a promising approach,\nenhancing their capabilities significantly (Yuan et al., 2024a;\nChen et al., 2024a). Figure 1 provides an illustration of these\nthree key roles played by KD in the context of LLMs.\nA key aspect of the knowledge distillation is the enhancement of skills such as advanced context following\n\n\n\n2\n\n\n\u2462\n\n\nSelf-Improvement\n\n\n\u2460 \u2461\n\nAdvance Compress\n\n\nClosed-Source LLMs Open-Source LLMs Smaller LMs\n\n\nDirection of KD\n\n\nFig. 1: KD plays three key roles in LLMs: 1) Primarily\nenhancing capabilities, 2) offering traditional compression\nfor efficiency, and 3) an emerging trend of self-improvement\nvia self-generated knowledge.\n\n\n(e.g., in-context learning (Huang et al., 2022a) and instruction following (Taori et al., 2023)), improved alignment with user intents (e.g., human values/principles (Cui\net al., 2023a), and thinking patterns like chain-of-thought\n(CoT) (Mukherjee et al., 2023)), and NLP task specialization\n(e.g., semantic understanding (Ding et al., 2023a), and code\ngeneration (Chaudhary, 2023)). These skills are crucial for\nthe wide array of applications that LLMs are expected\nto perform, ranging from casual conversations to complex problem-solving in specialized domains. For instance,\nin vertical domains like healthcare (Wang et al., 2023a),\nlaw (LAW, 2023), or science (Zhang et al., 2024), where\naccuracy and context-specific knowledge are paramount,\nknowledge distillation allows open-source models to significantly improve their performance by learning from the\nproprietary models that have been extensively trained and\nfine-tuned in these areas.\nThe benefits of knowledge distillation in the era of\nLLMs are multifaceted and transformative (Gu et al., 2024).\nThrough", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_2250", "chunk_text": " learning from the\nproprietary models that have been extensively trained and\nfine-tuned in these areas.\nThe benefits of knowledge distillation in the era of\nLLMs are multifaceted and transformative (Gu et al., 2024).\nThrough a suite of distillation techniques, the gap between\nproprietary and open-source models is significantly narrowed (Chiang et al., 2023; Xu et al., 2023a) and even\nfilled (Zhao et al., 2023a). This process not only streamlines\ncomputational requirements but also enhances the environmental sustainability of AI operations, as open-source models become more proficient with lesser computational overhead. Furthermore, knowledge distillation fosters a more\naccessible and equitable AI landscape, where smaller entities and individual researchers gain access to state-of-the-art\ncapabilities, encouraging wider participation and diversity\nin AI advancements. This democratization of technology\nleads to more robust, versatile, and accessible AI solutions,\ncatalyzing innovation and growth across various industries\nand research domains.\n\nThe escalating need for a comprehensive survey on the\nknowledge distillation of LLMs stems from the rapidly\nevolving landscape of AI (OpenAI et al., 2023; Team et al.,\n2023) and the increasing complexity of these models. As AI\ncontinues to penetrate various sectors, the ability to efficiently and effectively distill knowledge from proprietary\nLLMs to open-source ones becomes not just a technical\naspiration but a practical necessity. This need is driven by\nthe growing demand for more accessible, cost-effective, and\nadaptable AI solutions that can cater to a diverse range\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 2: An overview of this survey on knowledge distillation of large language models. Note that \u2018Section\u2019 is abbreviated\nas \u2018Sec.\u2019 in this figure. RM _S_ ( _\u00b7_ ) denotes the student reward model. 1 _\u20dd\u20dd_ 2 _\u20dd_ 3 _\u20dd_ 4 denote the steps in KD of LLMs.\n\n\n\n\n- f applications and users. A survey in this field is vital\nfor synthesizing the current methodologies, challenges, and\nbreakthroughs in knowledge distillation. It may serve as a\nbeacon for researchers and practitioners alike, guiding them\nto distill complex AI capabilities into more manageable and\naccessible forms. Moreover, such a survey can illuminate the\npath forward, identifying gaps", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_2700", "chunk_text": " in knowledge distillation. It may serve as a\nbeacon for researchers and practitioners alike, guiding them\nto distill complex AI capabilities into more manageable and\naccessible forms. Moreover, such a survey can illuminate the\npath forward, identifying gaps in current techniques and\nproposing directions for future research.\n\n\n_**Survey Organization.**_ The remainder of this survey is organized into several comprehensive sections, each designed to\n\n- ffer a deep dive into the multifaceted aspects of knowledge\ndistillation within the realm ofLLMs. Following this introduction, \u00a72 provides a foundational overview of knowledge\ndistillation, comparing traditional techniques with those\nemerging in the era of LLMs and highlighting the role of\ndata augmentation (DA) in this context. \u00a73 delves into the\napproaches to elicit knowledge from teacher LLMs and core\ndistillation algorithms, examining methods from supervised\nfine-tuning to more complex strategies involving divergence\nand similarity, reinforcement learning, and ranking optimization. Then, \u00a74 focuses on skill distillation, exploring\nhow student models can be enhanced to improve context\nunderstanding, alignment with user intentions, and performance across a variety of NLP tasks. This includes discussions on natural language understanding (NLU), generation (NLG), information retrieval, recommendation systems,\nand the evaluation of text generation. In \u00a75, we venture\ninto domain-specific vertical distillation, showcasing how\nknowledge distillation techniques are applied within specialized fields such as law, healthcare, finance, and science,\n\n\n\nillustrating the practical implications and transformative\nimpact of these approaches. The survey suggests open\nproblems in \u00a76, identifying current challenges and gaps in\nknowledge distillation research that offer opportunities for\nfuture work. Finally, the conclusion and discussion in \u00a77\nsynthesize the insights gained, reflecting on the implications for the broader AI and NLP research community and\nproposing directions for future research. Figure 2 shows an\n\n- verview of this survey.\n\n\n**2** **OVERVIEW**\n\n\n**2.1** **Comparing Traditional Recipe**\n\n\nThe concept of knowledge distillation in the field of AI\nand deep learning (DL) refers to the process of transferring\nknowledge from a large, complex model (teacher) to a\nsmaller, more efficient model (student) (Gou et al., 2021).\nThis technique is pivotal in mitigating the challenges posed\nby the computational demands and resource constraints of\n", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_3150", "chunk_text": " a large, complex model (teacher) to a\nsmaller, more efficient model (student) (Gou et al., 2021).\nThis technique is pivotal in mitigating the challenges posed\nby the computational demands and resource constraints of\ndeploying large-scale models in practical applications.\nHistorically, knowledge distillation techniques, prior to\nthe era of LLMs, primarily concentrated on transferring\nknowledge from complex, often cumbersome neural networks to more compact and efficient architectures (Sanh\net al., 2019; Kim and Rush, 2016). This process was largely\ndriven by the need to deploy machine learning models in\nresource-constrained environments, such as mobile devices\n\n- r edge computing platforms, where the computational\npower and memory are limited. The focus was predominantly on ad-hoc neural architecture selection and training\n\n- bjectives tailored for single tasks. These earlier methods\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 3: Taxonomy of Knowledge Distillation of Large Language Models. The detailed taxonomy of Verticalization\nDistillation is shown in Figure 7.\n\n\ninvolved training a smaller student network to mimic the\n\n- utput of a larger teacher network, often through techniques\nlike soft target training, where the student learns from\nthe softened softmax output of the teacher. Please refer to\nthe survey (Gou et al., 2021) for more details on general\nknowledge distillation techniques in AI and DL.\nIn contrast, the advent of LLMs has revolutionized\nthe knowledge distillation landscape. The current era of\nknowledge distillation in LLMs shifts the focus from mere\narchitecture compression to knowledge elicitation and transfer (Taori et al., 2023; Chaudhary, 2023; Tunstall et al., 2023).\nThis paradigm change is largely due to the expansive and\ndeep-seated knowledge that LLMs like GPT-4 and Gemini\npossess. And the inaccessible parameters of LLMs make it\nhard to compress them by using pruning (Han et al., 2016) or\nquantization (Liu et al., 2023a) techniques. Unlike the earlier\nera, where the goal was to replicate the output behavior of\nthe teacher model or reduce the model size, the current focus\nin LLM-based knowledge distillation is to elicit the specific\nknowledge these models have.\nThe key to this modern approach lies in", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_3600", "chunk_text": " the goal was to replicate the output behavior of\nthe teacher model or reduce the model size, the current focus\nin LLM-based knowledge distillation is to elicit the specific\nknowledge these models have.\nThe key to this modern approach lies in heuristic and\ncarefully designed prompts, which are used to elicit specific\nknowledge (Ding et al., 2023b) or capabilities (Chaudhary,\n2023) from the LLMs. These prompts are crafted to tap\ninto the LLM\u2019s understanding and capabilities in various\ndomains, ranging from natural language understanding (He\net al., 2023a) to more complex cognitive tasks like reasoning (Hsieh et al., 2023) and problem-solving (Qiao et al.,\n2024). The use of prompts as a means of knowledge elicitation offers a more flexible and dynamic approach to distillation. It allows for a more targeted extraction of knowledge, focusing on specific skills or domains of interest. This\nmethod is particularly effective in harnessing the emergent\nabilities of LLMs, where the models exhibit capabilities\nbeyond their explicit training objectives.\nFurthermore, this era of knowledge distillation also emphasizes the transfer of more abstract qualities such as\nreasoning patterns (Mitra et al., 2023), preference alignment (Cui et al., 2023a), and value alignment (Sun et al.,\n2024b). This is in stark contrast to the earlier focus on output\nreplication (Taori et al., 2023), indicating a shift towards\na more holistic and comprehensive transfer of cognitive\ncapabilities. The current techniques involve not just the\nreplication of outputs, but also the emulation of the thought\nprocesses (Mitra et al., 2023) and decision-making (Asai\net al., 2023) patterns of the teacher model. This involves\ncomplex strategies like chain-of-thought prompting, where\nthe student model is trained to learn the reasoning process\n\n- f the teacher, thereby enhancing its problem-solving and\ndecision-making capabilities.\n\n\n**2.2** **Relation to Data Augmentation (DA)**\n\n\nIn the era of LLMs, Data Augmentation (DA) (Wang et al.,\n2022a; Ye et al., 2022) emerges as a critical paradigm integral\nto the process of knowledge distillation. Unlike traditional\nDA techniques such as paraphrasing (Gangal", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_4050", "chunk_text": "mentation (DA) (Wang et al.,\n2022a; Ye et al., 2022) emerges as a critical paradigm integral\nto the process of knowledge distillation. Unlike traditional\nDA techniques such as paraphrasing (Gangal et al., 2022) or\nback-translation (Longpre et al., 2019), which primarily aim\nat expanding the training dataset in a somewhat mechanical\nmanner, DA within the context of LLMs focuses on the\ngeneration of novel, context-rich training data tailored to\nspecific domains and skills.\n\n\n\n5\n\n\nThe relationship between DA and KD in LLMs is both\nsymbiotic and foundational. By leveraging a set of seed\nknowledge, KD employs DA to prompt LLMs to produce\nexplicit data that encapsulates specific skills or domain\nexpertise (Chaudhary, 2023; West et al., 2022). This method\nstands out as a potent mechanism for bridging the knowledge and capability gap between proprietary and opensource models. Through DA, LLMs are prompted to create\ntargeted, high-quality datasets that are not merely larger in\nvolume but are also rich in diversity and specificity. This\napproach enables the distillation process to be more effective, ensuring that the distilled models not only replicate\nthe teacher model\u2019s output behavior but also embody its\ndeep-seated understanding and cognitive strategies.\nDA acts as a force multiplier, enabling the distilled models to acquire and refine capabilities that would otherwise\nrequire exponentially larger datasets and computational resources. It facilitates a more effective transfer of knowledge,\nfocusing on the qualitative aspects of learning rather than\nquantitative expansion. This strategic use of DA within\nKD processes underscores a pivotal shift towards a more\nefficient, sustainable, and accessible approach to harnessing\nthe power of LLMs. It empowers open-source models with\nthe ability to approximate the contextual adeptness, ethical\nalignment, and deep semantic insights characteristic of their\nproprietary counterparts, thereby democratizing access to\nadvanced AI capabilities and fostering innovation across a\nbroader spectrum of applications and users.\n\n\n**2.3** **Survey Scope**\n\n\nBuilding on the discussions introduced earlier, this survey\naims to comprehensively explore the landscape of knowledge distillation within the context of LLMs, following\na meticulously structured taxonomy as in Figure 3. The\nsurvey\u2019s scope is delineated through three primary facets:\nKD Algorithms, Skill Dist", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_4500", "chunk_text": "\naims to comprehensively explore the landscape of knowledge distillation within the context of LLMs, following\na meticulously structured taxonomy as in Figure 3. The\nsurvey\u2019s scope is delineated through three primary facets:\nKD Algorithms, Skill Distillation, and Verticalization Distillation. Each facet encapsulates a range of subtopics and\nmethodologies. It\u2019s important to note that KD algorithms\nprovide the technical foundations for skill distillation and\nverticalization distillation.\n\n\n_**KD Algorithms.**_ This segment focuses on the technical\nfoundations and methodologies of knowledge distillation. It\nincludes an in-depth exploration of the processes involved\nin constructing knowledge from teacher models (e.g., proprietary LLMs) and integrating this knowledge into student\nmodels (e.g., open-source LLMs). Under the umbrella of\n\u2018 _knowledge_ \u2019, we delve into strategies such as labeling (Hsieh\net al., 2023), expansion (Taori et al., 2023), curation (Gunasekar et al., 2023), feature understanding (Agarwal et al.,\n2024), feedback mechanisms (Tunstall et al., 2023), and selfknowledge generation (Wang et al., 2022a). This exploration\nseeks to uncover the various ways in which knowledge\ncan be identified, expanded, and curated for effective distillation. The \u2018 _distillation_ \u2019 subsection examines learning approaches like supervised fine-tuning (SFT) (Wang et al.,\n2022a), divergence minimization (Agarwal et al., 2024),\nreinforcement learning techniques (Cui et al., 2023a), and\nrank optimization strategies (Tunstall et al., 2023). Together,\nthese techniques demonstrate how KD enables open-source\nmodels to obtain knowledge from proprietary ones.\n\n\n_**Skill Distillation.**_ This facet examines the specific competencies and capabilities enhanced through KD. It encompasses detailed discussions on context following (Taori et al.,\n2023; Luo et al., 2023c), with subtopics like instruction\nfollowing and retrieval-augmented generation (RAG) Capability. In the realm of alignment (Mitra et al., 2023; Tunstall et al., 2023), the survey investigates thinking patterns,\npersona/preference modeling, and value alignment. The\n\u2018agent\u2019 category delves into skills such as Tool", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_4950", "chunk_text": " of alignment (Mitra et al., 2023; Tunstall et al., 2023), the survey investigates thinking patterns,\npersona/preference modeling, and value alignment. The\n\u2018agent\u2019 category delves into skills such as Tool Using and\nPlanning. NLP task specialization (Dai et al., 2023a; Jung\net al., 2023; Chaudhary, 2023) is scrutinized through lenses\nlike natural language understanding (NLU), natural language generation (NLG), information retrieval, recommendation systems, text generation evaluation, and code generation. Finally, the survey addresses multi-modality (Liu\net al., 2023e; Zhao et al., 2023b), exploring how KD enhances\nLLMs\u2019 ability to integrate multiple forms of input.\n\n\n_**Verticalization Distillation.**_ This section assesses the application of KD across diverse vertical domains, offering\ninsights into how distilled LLMs can be tailored for specialized fields such as Law (LAW, 2023), Medical & Healthcare (Wang et al., 2023a), Finance (Zhang and Yang, 2023),\nScience (Zhang et al., 2024), among others. This exploration\nnot only showcases the practical implications of KD techniques but also highlights their transformative impact on\ndomain-specific AI solutions.\nThrough these facets, this survey provides a comprehensive analysis of KD in LLMs, guiding researchers and\npractitioners through methodologies, challenges, and opportunities in this rapidly evolving domain.\n\n\n_**Declaration.**_ This survey represents our earnest effort to\nprovide a comprehensive and insightful overview of knowledge distillation techniques applied to LLMs, focusing on\nalgorithms, skill enhancement, and domain-specific applications. Given the vast and rapidly evolving nature of\nthis field, especially with the prevalent practice of eliciting knowledge from training data across academia, we\nacknowledge that this manuscript may not encompass every\npertinent study or development. Nonetheless, it endeavors\nto introduce the foundational paradigms of knowledge distillation, highlighting key methodologies and their impacts\nacross a range of applications.\n\n\n**2.4** **Distillation Pipeline in LLM Era**\n\n\n\n\n\n\n\n\n|Skill/Domain OL be ja er cn tin ivg<br>e<br>steer train<br>drive Generated<br>Seed Knowledge<br>Knowledge<br>Teacher LLM Student Model|Col2|Col3|Col", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_5400", "chunk_text": " LLM Era**\n\n\n\n\n\n\n\n\n|Skill/Domain OL be ja er cn tin ivg<br>e<br>steer train<br>drive Generated<br>Seed Knowledge<br>Knowledge<br>Teacher LLM Student Model|Col2|Col3|Col4|\n|---|---|---|---|\n|Seed<br>Knowledge<br>Skill/Domain<br>Teacher LLM<br>Student Model<br>steer<br>drive<br>Generated<br>Knowledge<br>Learning<br>Objective<br>train|Teacher LLM<br>steer<br>drive<br>G<br>Kn|Teacher LLM<br>steer<br>drive<br>G<br>Kn|Student Model<br>d<br>e<br>train|\n|Seed<br>Knowledge<br>Skill/Domain<br>Teacher LLM<br>Student Model<br>steer<br>drive<br>Generated<br>Knowledge<br>Learning<br>Objective<br>train|Teacher LLM<br>steer<br>drive<br>G<br>Kn|owledg|owledg|\n\n\n\nKnowledge Elicitation\n\n\n\nDistillation Algorithm\n\n\n\nFig. 4: An illustration of a general pipeline to distill knowledge from a large language model to a student model.\n\n\nThe general distillation pipeline of LLMs is a structured\nand methodical process aimed at transferring knowledge\n\n\n\n6\n\n\nfrom a sophisticated teacher model to a less complex student\nmodel. This pipeline is integral for leveraging the advanced\ncapabilities of models like GPT-4 or Gemini in more accessible and efficient open-source counterparts. The outline of\nthis pipeline can be broadly categorized into four distinct\nstages, each playing a crucial role in the successful distillation of knowledge. An illustration is shown in Figure 4. The\ndetailed pipeline could also be seen in Figure 2.\n**I. Target Skill or Domain Steering Teacher LLM.** The\nfirst stage involves directing the teacher LLM towards a\nspecific target skill or domain. This is achieved through carefully crafted instructions or templates that guide the LLM\u2019s\nfocus. These instructions are designed to elicit responses\nthat demonstrate the LLM\u2019s proficiency in a particular area,\nbe it a specialized domain like healthcare or law, or a skill\nsuch as reasoning or language understanding.\n**II. Seed Knowledge as Input.** Once the target area is\ndefined, the next step is to feed the teacher LLM with\nseed knowledge. This seed knowledge typically comprises\na small dataset or specific data", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_5850", "chunk_text": " as reasoning or language understanding.\n**II. Seed Knowledge as Input.** Once the target area is\ndefined, the next step is to feed the teacher LLM with\nseed knowledge. This seed knowledge typically comprises\na small dataset or specific data clues relevant to the elicit\nskill or domain knowledge from the teacher LLM. It acts\nas a catalyst, prompting the teacher LLM to generate more\nelaborate and detailed outputs based on this initial information. The seed knowledge is crucial as it provides a\nfoundation upon which the teacher model can build and\nexpand, thereby creating more comprehensive and in-depth\nknowledge examples.\n**III. Generation of Distillation Knowledge.** In response\nto the seed knowledge and steering instructions, the teacher\nLLM generates knowledge examples. These examples are\npredominantly in the form of question-and-answer (QA)\ndialogues or narrative explanations, aligning with the natural language processing/understanding capabilities of the\nLLM. In certain specialized cases, the outputs may also include logits or hidden features, although this is less common\ndue to the complexity and specific requirements of such\ndata forms. The generated knowledge examples constitute\nthe core of the distillation knowledge, encapsulating the\nadvanced understanding and skills of the teacher LLM.\n**IV. Training the Student Model with a Specific Learn-**\n**ing Objective.** The final stage involves the utilization of\nthe generated knowledge examples to train the student\nmodel. This training is guided by a loss function that aligns\nwith the learning objectives. The loss function quantifies\nthe student model\u2019s performance in replicating or adapting\nthe knowledge from the teacher model. By minimizing this\nloss, the student model learns to emulate the target skills or\ndomain knowledge of the teacher, thereby acquiring similar\ncapabilities. The process involves iteratively adjusting the\nstudent model\u2019s parameters to reduce the discrepancy between its outputs and those of the teacher model, ensuring\nthe effective transfer of knowledge.\nIn essential, the above four stages can be abstracted\nas two formulations. The first formulation represents the\nprocess of eliciting knowledge:\n\n\n_DI_ [(kd)] = _{_ Parse( _o, s_ ) _|o \u223c_ _pT_ (o _|I \u2295_ _s_ ) _, \u2200s \u223cS},_ (1)\n\n\nwhere _\u2295_ denotes fusing two pieces of text, _I_ denotes an\ninstruction or a template", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_6300", "chunk_text": " _pT_ (o _|I \u2295_ _s_ ) _, \u2200s \u223cS},_ (1)\n\n\nwhere _\u2295_ denotes fusing two pieces of text, _I_ denotes an\ninstruction or a template for a task, skill, or domain to\nsteer the LLM and elicit knowledge, _s \u223cS_ denotes an\n\n\nexample of the seed knowledge, upon which the LLM can\nexplore to generate novel knowledge, Parse( _o, s_ ) stands for\nto parse the distillation example ( e.g., ( _x, y_ )) from the\nteacher LLM\u2019s output _o_ (plus the input _s_ in some cases),\nand _pT_ represents the teacher LLM with parameters _\u03b8T_ .\nGiven the datasets _DI_ [(kd)] built for distillation, we then define\na learning objective as\n\n\n\n7\n\n\ntasks leverage LLMs to label evaluated results (Li et al.,\n2024b; Wang et al., 2023b), and reasoning tasks utilize LLMs\nfor labeling Chains of Thought (CoT) explanations (Hsieh\net al., 2023; Li et al., 2022; Ho et al., 2023; Magister et al.,\n2023; Fu et al., 2023; Ramnath et al., 2023; Li et al., 2023d;\nLiu et al., 2023g), among others. Rather than concentrating\n\n- n specific tasks, many current works focus on labeling\n\n- utputs based on instructions, thereby teaching student\nmodels to solve tasks in a more flexible way by following instructions. Collections of various NLP tasks, complemented\nby instructional templates, serve as valuable input sources\nfor _x_ . For instance, FLAN-v2 collections (Longpre et al.,\n2023) offers extensive publicly available sets of tasks with\ninstructions, which are labeled with responses generated\nby teacher LLMs in Orca (Mukherjee et al., 2023; Mitra\net al., 2023). The instructions from these NLP tasks are\nbuilt from predefined templates, which lack diversity and\nmay have gaps between human\u2019s natural query. The real\nconversations between humans and chat models provide\nlarge-scale data with real queries and generations labeled\nby powerful LLMs, like Share", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_6750", "chunk_text": "\nbuilt from predefined templates, which lack diversity and\nmay have gaps between human\u2019s natural query. The real\nconversations between humans and chat models provide\nlarge-scale data with real queries and generations labeled\nby powerful LLMs, like ShareGPT. Additionally, Xu et al.\n(2023b) and Anand et al. (2023) label the real questions\nsampled from forums like Quora and Stack Overflow.\nMoreover, the process of labeling could be guided by\ninstructions _I_ - r demonstrations _c_ . A commonly used instruction type for guiding labeling is chain-of-thought (CoT)\nprompt (Hsieh et al., 2023; Fu et al., 2023; Magister et al.,\n2023). Mukherjee et al. (2023) add multiple system messages\n(e.g. \u201cYou must generate a detailed and long answer.\u201d or\n\u201cexplain like I\u2019m five, think step-by-step\u201d) to elicit rich\nsignals. Yue et al. (2023a) and Chenglin et al. (2023) label a hybrid of knowledge of chain-of-thought (CoT) and\nprogram-of-thought (PoT) rationales. Xu et al. (2023b) propose a self-chat technique that two teacher LLMs simulate\nthe real conversational to generate multi-turn dialogues for\na question from Quora and Stack Overflow.\n\n\n_3.1.2_ _Expansion_\n\n\nWhile the labeling approach is simple and effective, it faces\ncertain limitations. Primarily, it is constrained by the scale\nand variety of the input data. In real-world applications,\nespecially those involving user conversations, there are also\nconcerns regarding the privacy of the data involved. To\naddress these limitations, various expansion methods have\nbeen proposed (Wang et al., 2022a; Taori et al., 2023; Chaudhary, 2023; Si et al., 2023; Ji et al., 2023a; Luo et al., 2023b,a;\nWu et al., 2023c; Sun et al., 2024b; Xu et al., 2023a; Guo\net al., 2023c; Rozi`ere et al., 2023; West et al., 2022). These\nmethods take the demonstrations as seed", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_7200", "chunk_text": "4b; Xu et al., 2023a; Guo\net al., 2023c; Rozi`ere et al., 2023; West et al., 2022). These\nmethods take the demonstrations as seed knowledge and\naim to expand a large scale and various data by in-context\nlearning.\nA key characteristic of these expansion methods is the\nutilization of the in-context learning ability of LLMs to generate data similar to the provided demonstrations _c_ . Unlike\nin the labeling approach, where the input _x_ is sampled\nfrom the existing dataset, in the expansion approach, both _x_\nand _y_ are generated by teacher LLMs. This process can be\nformulated as follows:\n\n\n_D_ [(exp)] = _{_ ( _x, y_ ) _|x \u223c_ _pT_ ( _x|I \u2295_ _c_ ) _, y \u223c_ _pT_ ( _y|I \u2295_ _x_ ) _}._ (4)\n\n\n\n_L_ = \n\n\n_I_ _[L][I]_ [(] _[D]_ _I_ [(kd)] ; _\u03b8S_ ) _,_ (2)\n\n\n\nwhere [\ufffd] _I_ [denotes there could be multiple tasks or skills]\nbeing distilled into one student model, _LI_ ( _\u00b7_ ; _\u00b7_ ) stands for a\nspecific learning objective, and _\u03b8S_ parameterizes the student\nmodel.\nFollowing our exploration of the distillation pipeline and\nthe foundational concepts underlying knowledge distillation in the LLM era, we now turn our focus to the specific\nalgorithms that have gained prominence in this era.\n\n\n**3** **KNOWLEDGE DISTILLATION ALGORITHMS**\n\n\nThis section navigates through the process of knowledge\ndistillation. According to Section 2.4, it is categorized into\ntwo principal steps: \u2018Knowledge,\u2019 focusing on eliciting\nknowledge from teacher LLMs (Eq.1), and \u2018Distillation,\u2019\ncentered on injecting this knowledge into student models\n(Eq.2). We will elaborate on these two processes in the\nsubsequent sections.\n\n\n**3.1** **Knowledge**\n\n\nThis section focuses on the approaches to elicit knowledge\nfrom teacher LLMs. According to the manners to acquire\nknowledge, we divided them into _Labeling_, _Expansion_, _", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_7650", "chunk_text": "sequent sections.\n\n\n**3.1** **Knowledge**\n\n\nThis section focuses on the approaches to elicit knowledge\nfrom teacher LLMs. According to the manners to acquire\nknowledge, we divided them into _Labeling_, _Expansion_, _Data_\n_Curation_, _Feature_, _Feedback_, and _Self-Knowledge_ . Figure 5\nshows an illustration of these knowledge elicitation meth\n- ds.\n\n\n_3.1.1_ _Labeling_\nLabeling knowledge refers to using a teacher LLM to label\nthe output _y_ for a given input _x_ as the seed knowledge,\naccording to the instruction _I_ - r demonstrations _c_, where\n_c_ = ( _x_ 1 _, y_ 1) _, . . .,_ ( _xn, yn_ ). This method of eliciting knowledge from teacher LLMs is straightforward yet effective and\nhas been widely applied across various tasks and applications. It requires only the collection of an input dataset\nand feeding it into LLMs to obtain the desired generations.\nMoreover, the generation of _y_ is controllable through the\npredefined _I_ and _c_ . This process can be formulated as\nfollows:\n\n\n_D_ [(lab)] = _{x, y|x \u223cX_ _, y \u223c_ _pT_ ( _y|I \u2295_ _c \u2295_ _x_ ) _}._ (3)\n\n\nInput _x_ could be sourced from existing NLP task\ndatasets, which serve as typical reservoirs for distillation\nefforts. Numerous works have sought to harness the capabilities of powerful LLMs as teachers for annotating dataset\nsamples across a range of tasks. For instance, efforts in\nnatural language understanding involve using LLMs to categorize text (Gilardi et al., 2023; Ding et al., 2023a; He et al.,\n2023a), while in natural language generation, LLMs assist\nin generating sequences for outputs (Hsieh et al., 2023; Jung\net al., 2023; Wang et al., 2021b). Text generation evaluation\n\n\n8\n\n\n\nInput Set\n\n\n\n\n\n\n\n\n\n\n|et|Col2|Col3|\n|---|---|---|\n|t|||\n|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\ud835\udc65<br>\ud835\udc3c<br", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_8100", "chunk_text": "\n\n\n\nInput Set\n\n\n\n\n\n\n\n\n\n\n|et|Col2|Col3|\n|---|---|---|\n|t|||\n|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\ud835\udc65<br>\ud835\udc3c<br> <br>\ud835\udc50|\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|xtract|||\n|xtract|||\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 5: An illustration of different knowledge elicitation methods from teacher LLMs. _Labeling_ : The teacher generates\nthe output from the input; _Expansion_ : The teacher generates samples similar to the given demonstrations through incontext learning; _Data Curation_ : The teacher synthesizes data according to meta-information, such as a topic or an entity;\n_Feature_ : Feed the data into the teacher and extract its internal knowledge, such as logits and features; _Feedback_ : The teacher\nprovides feedback on the student\u2019s generations, such as preferences, corrections, expansions of challenging samples, etc;\n_Self-Knowledge_ : The student first generates outputs, which is then filtered for high quality or evaluated by the student itself.\n\n\n\nIn this formulation, _x_ and _y_ represent the new input\n- utput pairs generated by the teacher LLM. The input _x_\nis generated based on a set of input-output demonstrations\n_c_ . The output _y_ is then generated in response to the new\ninput _x_ under the guidance of an instruction _I_ . Note that\nthe demonstrations could be predefined or dynamically\nupdated by adding the newly generated samples.\nExpansion techniques have been widely utilized to\nextract extensive instruction-following knowledge from\nteacher LLMs. Wang et al. (2022a) first introduces an iterative bootstrapping method, Self-Instruct, to utilize LLMs\nto generate a wide array of instructions based on several demonstrations sampled from 175 manually-written instructions. The newly generated instructions are then added\nback to the initial pool, benefiting subsequent expansion\niterations. Subsequently, Taori et al. (2023) applies this expansion method to a more powerful teacher LLM, textdavinci-003, to distill 52K high-quality data. To improve\nthe diversity and coverage during expansion, Wu et al.\n(2023c) and (Sun et", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_8550", "chunk_text": " expansion method to a more powerful teacher LLM, textdavinci-003, to distill 52K high-quality data. To improve\nthe diversity and coverage during expansion, Wu et al.\n(2023c) and (Sun et al., 2024b) prompt the teacher LLM to\ngenerate instructions corresponding to some specific topics.\nXu et al. (2023a) propose an Evol-Instruct method to expand the instructions from two dimensions: difficulty (e.g.\nrewriting the question to be more complex) and diversity\n(e.g. generating more long-tailed instructions). This EvolInstruct method is domain-agnostic and has been used to\nexpand the distillation of coding (Luo et al., 2023a) and\nmath (Luo et al., 2023b). Additionally, expansion methods\ncan significantly augment NLP task datasets with similar\nsamples, thereby enhancing task performance. For instance,\nAugGPT (Dai et al., 2023a) leverages a teacher LLM to\nrephrase each sentence in the training samples into multiple conceptually similar, but semantically varied, samples\nto improve classification performance. Similarly, TDG (He\n\n\n\net al., 2023b) proposes the Targeted Data Generation (TDG)\nframework, which automatically identifies challenging subgroups within data and generates new samples for these\nsubgroups using LLMs through in-context learning.\nIn summary, the expansion method leverages the incontext learning strengths of LLMs to produce more varied and extensive datasets with both inputs and outputs.\nHowever, the quality and diversity of the generated data\nare heavily reliant on the teacher LLMs and the initial seed\ndemonstrations. This dependence can lead to a dataset with\ninherent bias from LLMs (Yu et al., 2023a; Wei et al., 2023)\nand a homogeneity issue where the generations may be\nprone to similarity ultimately, limiting the diversity this\nmethod seeks to achieve (Ding et al., 2023b). Moreover, the\nexpansion process may inadvertently amplify any biases\npresent in the seed data.\n\n\n_3.1.3_ _Data Curation_\n\nThe pursuit of high-quality and scalable data generation in\nknowledge distillation from LLMs has led to the emergence\n\n- f the Data Curation approach. This method arises in response to the limitations observed", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_9000", "chunk_text": "1.3_ _Data Curation_\n\nThe pursuit of high-quality and scalable data generation in\nknowledge distillation from LLMs has led to the emergence\n\n- f the Data Curation approach. This method arises in response to the limitations observed in both the Labeling and\nExpansion approaches. These methods often yield data of\nvariable quality and face constraints in quantity. In Labeling,\nthe seed knowledge is sourced from task datasets, leading\nto potential noise and dirty data. Meanwhile, in Expansion,\nthe input _x_ is derived from seed demonstrations, which\ncan result in homogeneous data when generated in large\nquantities. To overcome these challenges, the Data Curation\nmethod curates high-quality or large-scale data by extensive\nmeta-information as seed knowledge (Ding et al., 2023b;\nGunasekar et al., 2023; Li et al., 2023a; Mar, 2023; Liu et al.,\n2023d; Wei et al., 2023; Yu et al., 2024; Ye et al., 2022; Gao\net al., 2023a; Yang and Nicolai, 2023).\n\n\nA distinct feature of Data Curation is its approach\nto synthesize data from scratch. Numerous diverse metainformation, such as topics or knowledge points, could be\nincorporated into this process to generate controllable _x_\nand _y_ . Thus, this process can be meticulously controlled\nto yield datasets that are not only large in scale but also\n\n- f high quality. The formulation for Data Curation can be\nrepresented as:\n\n\n_D_ [(cur)] = _{_ ( _x, y_ ) _|x \u223c_ _pT_ ( _x|I \u2295_ _m_ ) _, y \u223c_ _pT_ ( _y|I \u2295_ _x_ ) _}._ (5)\n\n\nIn this formulation, _m_ represents the diverse metainformation used to guide the synthesis of _x_, and _I_ is the\ninstruction guiding teacher LLMs to generate _x_ - r _y_ .\nDifferent studies primarily vary in their source and\nmethod of leveraging meta-information. UltraChat (Ding\net al., 2023b) effectively demonstrates the process of curating\nboth high-quality and diverse data by distilled knowledge.\nThey collect extensive meta-information across three domains: _", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_9450", "chunk_text": " and\nmethod of leveraging meta-information. UltraChat (Ding\net al., 2023b) effectively demonstrates the process of curating\nboth high-quality and diverse data by distilled knowledge.\nThey collect extensive meta-information across three domains: _Questions about the World, Creation and Generation_,\nand _Assistance on Existing Materials_ . For example, under\n_Questions about the World_, they explore 30 meta-topics like\n\u201dTechnology\u201d and \u201dFood and Drink.\u201d the teacher LLMs\nthen use this meta-information to distill a broad array\n\n- f instructions and conversations, achieving a substantial\nscale of 1.5 million instances. UltraChat stands out with its\nlexical and topical diversity. The UltraLLaMA model, finetuned on this data, consistently surpasses other open-source\nmodels. Another notable series, **phi** (Gunasekar et al., 2023;\nLi et al., 2023a; Mar, 2023), focuses on distilling smaller,\nhigh-quality datasets akin to \u201dtextbooks.\u201d **Phi-1** (Gunasekar\net al., 2023) experiments with synthesizing \u201dtextbook quality\u201d data in the coding domain. Their approach involves\ndistilling clear, self-contained, instructive, and balanced content from LLMs, guided by random topics or function names\nto enhance diversity. The distilled data is a synthesis of 1\nbillion tokens of Python textbooks, complete with natural\nlanguage explanations and code snippets, as well as 180 million tokens of Python exercises with solutions. Remarkably,\nthe **phi-1** model, despite its smaller size, outperforms nearly\nall open-source models on coding benchmarks like HumanEval and MBPP while being 10 times smaller in model\nsize and 100 times smaller in dataset size. MFTCoder (Liu\net al., 2023d) utilizes hundreds of Python knowledge points\nas meta-information to create a CodeExercise Dataset. In\ncontrast, Magicoder (Wei et al., 2023) and WaveCoder (Yu\net al., 2024) get raw code collections from open-source\ncode datasets, using this as meta-information for generating\ninstructional data. In the context of NLU tasks, certain\nstudies (Ye et al., 2022; Gao et al., 2023a; Wang et al., 2021a)\n", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_9900", "chunk_text": " as meta-information for generating\ninstructional data. In the context of NLU tasks, certain\nstudies (Ye et al., 2022; Gao et al., 2023a; Wang et al., 2021a)\nexplore the use of labels as meta-information to synthesize\ncorresponding samples for data augmentation. Similarly, in\ninformation retrieval tasks, there are efforts to utilize documents as meta-information for generating potential queries,\nthereby constructing large-scale retrieval pairs (Bonifacio\net al., 2022; Meng et al., 2023).\nIn conclusion, Data Curation through teacher LLMs has\nemerged as a promising technique for synthesizing datasets\nthat are not only high-quality and diverse but also large\nin scale. The success of models like **phi-1** in specialized\ndomains underscores the efficacy of this method. The ability\n\n\n\n9\n\n\nto create synthetic datasets will become a crucial technical\nskill and a key area of focus in AI (Li et al., 2023a).\n\n\n_3.1.4_ _Feature_\n\nThe previously discussed knowledge elicitation methods\nare typically applied to powerful black-box models, which\nare expensive and somewhat unreproducible due to calling\nAPI. In contrast, white-box distillation offers a more transparent and accessible approach for researchers. It involves\nleveraging the output distributions, intermediate features,\n\n- r activations from teacher LLMs, which we collectively\nrefer to as _Feature_ knowledge. White-box KD approaches\nhave predominantly been studied for smaller encoder-based\nLMs, typically those with fewer than 1 billion parameters\n(cf. Gou et al. (2021) for detail). However, recent research\nhas begun to explore white-box distillation in the context of\ngenerative LLMs (Timiryasov and Tastet, 2023; Liang et al.,\n2023a; Gu et al., 2024; Agarwal et al., 2024; Liu et al., 2023a;\nWen et al., 2023; Wan et al., 2024a; Zhao and Zhu, 2023; Qin\net al., 2023b; Boizard et al., 2024; Zhong et al., 2024).\nThe typical method for acquiring this feature knowledge\ninvolves teacher LLMs annotating the output sequence _y_\nwith its", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_10350", "chunk_text": "., 2023b; Boizard et al., 2024; Zhong et al., 2024).\nThe typical method for acquiring this feature knowledge\ninvolves teacher LLMs annotating the output sequence _y_\nwith its internal representations. These annotations are then\ndistilled into the student model using methods such as\nKullback-Leibler Divergence (KLD). The process of eliciting\nfeature knowledge can be formulated as follows:\n\n\n_D_ [(feat)] = _{_ ( _x, y, \u03d5_ feat( _x, y_ ; _\u03b8T_ )) _| x \u223cX_ _, y \u223cY}._ (6)\n\n\nIn this formulation, _Y_ is the output set, which can be\ngenerated by teacher LLMs, the student model, or directly\nsourced from the dataset. _\u03d5_ feat( _\u00b7_ ; _\u03b8T_ ) represents the operation of extracting feature knowledge (such as output distribution) from the teacher LLM.\nThe most straightforward method to elicit feature knowledge of teacher is to label a fixed dataset of sequences with\ntoken-level probability distributions (Sanh et al., 2019; Wen\net al., 2023). To leverage the rich semantic and syntactic\nknowledge in intermediate layers of the teacher model,\nTED (Liang et al., 2023a) designs task-aware layer-wise\ndistillation. They align the student\u2019s hidden representations\nwith those of the teacher at each layer, selectively extracting\nknowledge pertinent to the target task. Gu et al. (2024) and\nAgarwal et al. (2024) introduce a novel approach where\nthe student model first generates sequences, termed \u2018selfgenerated sequences.\u2019 The student then learns by using\nfeedback (i.e. output distribution) from teacher on these\nsequences. This method is particularly beneficial when the\nstudent model lacks the capacity to mimic teacher\u2019s distribution. Moreover, various LLM-quantization methods with\ndistilling feature knowledge from teacher LLMs have been\nproposed (Tao et al., 2022a; Liu et al., 2023a; Kim et al.,\n2023b). These methods aim to preserve the original output\ndistribution when quantizing the LLMs, ensuring minimal\nloss of performance. Additionally, feature knowledge could\nserve as a potent source for multi-teacher knowledge distillation. Tim", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_10800", "chunk_text": ".,\n2023b). These methods aim to preserve the original output\ndistribution when quantizing the LLMs, ensuring minimal\nloss of performance. Additionally, feature knowledge could\nserve as a potent source for multi-teacher knowledge distillation. Timiryasov and Tastet (2023) leverages an ensemble\n\n- f GPT-2 and LLaMA as teacher models to extract output\ndistributions. Similarly, FuseLLM (Wan et al., 2024a) innovatively combines the capabilities of various LLMs through\na weighted fusion of their output distributions, integrating\nthem into a singular LLM. This approach has the potential\n\n\nto significantly enhance the student model\u2019s capabilities,\nsurpassing those of any individual teacher LLM.\nIn summary, feature knowledge offers a more transparent alternative to black-box methods, allowing for deeper\ninsight into and control over the distillation process. By\nutilizing feature knowledge from teacher LLMs, such as output distributions and intermediate layer features, white-box\napproaches enable richer knowledge transfer. While showing promise, especially in smaller models, its application\nis not suitable for black-box LLMs where internal parameters are inaccessible. Furthermore, student models distilled\nfrom white-box LLMs may underperform compared to their\nblack-box counterparts, as the black-box teacher LLMs (e.g.\nGPT-4) tend to be more powerful.\n\n\n_3.1.5_ _Feedback_\n\nMost previous works predominantly focus on one-way\nknowledge transfer from the teacher to the student for\nimitation, without considering feedback from the teacher\n\n- n the student\u2019s generation. The feedback from the teacher\ntypically offers guidance on student-generated outputs by\nproviding preferences, assessments, or corrective information. For example, a common form of feedback involves\nteacher ranking the student\u2019s generations and distilling this\npreference into the student model through Reinforcement\nLearning from AI Feedback (RLAIF) (Bai et al., 2022a).\nHere is a generalized formulation for eliciting feedback\nknowledge:\n\n_D_ [(][fb][)] = _{_ ( _x, y, \u03d5_ fb( _x, y_ ; _\u03b8T_ )) _|x \u223cX_ _, y \u223c_ _pS_ ( _y|x_ ) _},_ (7)\n\n\nwhere _y_ denotes the output generated by the student\nmodel in response", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_11250", "chunk_text": " y_ ; _\u03b8T_ )) _|x \u223cX_ _, y \u223c_ _pS_ ( _y|x_ ) _},_ (7)\n\n\nwhere _y_ denotes the output generated by the student\nmodel in response to _x_, and _\u03d5_ fb( _\u00b7_ ; _\u03b8T_ )) represents providing\nfeedback from teacher LLMs. This operation evaluates the\nstudent\u2019s output _y_ given the input _x_, by offering assessment, corrective information, or other forms of guidance.\nThis feedback knowledge can not only be distilled into\nthe student to also generate feedback (such as creating a\nstudent preference model) but, more importantly, enable\nthe student to refine its responses based on the feedback.\nVarious methods have been explored to elicit this advanced\nknowledge (Bai et al., 2022a; Luo et al., 2023b; Cui et al.,\n2023a; Kwon et al., 2023; Jiang et al., 2023b; Chen et al.,\n2023a; Gu et al., 2024; Agarwal et al., 2024; Chen et al., 2024b;\nGuo et al., 2024; Ye et al., 2023; Hong et al., 2023; Lee et al.,\n2023a).\nPreference, as previously discussed, represents a notable\nform of feedback knowledge from teacher models. Various\nknowledge of preferences could be distilled from teachers\nby prompting it with specific criteria. Bai et al. (2022a) introduce RLAIF for distilling harmlessness preferences from\nLLMs. This involves using an SFT-trained LLM to generate\nresponse pairs for each prompt, then ranking them for\nharmlessness to create a preference dataset. This dataset is\ndistilled into a Preference Model (PM), which then guides\nthe RL training of a more harmless LLM policy. WizardMath (Luo et al., 2023b) places emphasis on mathematical\nreasoning. They employ ChatGPT as teacher to directly\nprovide process supervision and evaluate the correctness\n\n- f each step in the generated solutions. To scale up highquality distilled preference data, Cui et al. (2023a) develop a\nlarge-scale preference dataset for distilling better preference\n\n\n\n10\n\n\nmodels, UltraFeedback. It compiles various instructions and\nmodels to produce comparative data", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_11700", "chunk_text": " scale up highquality distilled preference data, Cui et al. (2023a) develop a\nlarge-scale preference dataset for distilling better preference\n\n\n\n10\n\n\nmodels, UltraFeedback. It compiles various instructions and\nmodels to produce comparative data. Then, GPT-4 is used\nto score candidates from various aspects of preference,\nincluding instruction-following, truthfulness, honesty and\nhelpfulness.\nBeyond merely assessing student generations, teachers\ncan also furnish extensive feedback on instances where\nstudents underperform. In Lion (Jiang et al., 2023b), teacher\nmodel pinpoints instructions that pose challenges to the\nstudent model, generating new, more difficult instructions\naimed at bolstering the student\u2019s abilities. PERsD (Chen\net al., 2023a) showcases a method where teacher offers\ntailored refinement feedback on incorrect code snippets generated by students, guided by the specific execution errors\nencountered. Similarly, SelFee (Ye et al., 2023) leverages\nChatGPT to generate feedback and revise the student\u2019s\nanswer based on the feedback. In contrast, FIGA (Guo et al.,\n2024) revises the student\u2019s response by comparing it to\nthe ground-truth response. Furthermore, teacher model\u2019s\ndistribution over the student\u2019s generations can itself act\nas a form of feedback. MiniLLM (Gu et al., 2024) and\nGKD (Agarwal et al., 2024) present an innovative strategy\nwherein the student model initially generates sequences,\nfollowed by teacher model producing an output distribution\nas feedback. This method leverages the teacher\u2019s insight\nto directly inform and refine the student model\u2019s learning\n\nprocess.\n\n\n_3.1.6_ _Self-Knowledge_\nThe knowledge could also be elicited from the student itself,\nwhich we refer to as _Self-Knowledge_ . In this setting, the same\nmodel acts both as the teacher and the student, iteratively\nimproving itself by distilling and refining its own previously\ngenerated outputs. This knowledge uniquely circumvents\nthe need for an external, potentially proprietary, powerful\nteacher model, such as GPT-series LLMs. Furthermore, it\nallows the model to surpass the limitations or \u201cceiling\u201d\ninherent in traditional teacher-student methods. Eliciting\nself-knowledge could be formulated as:\n\n_D_ [(][sk][", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_12150", "chunk_text": " GPT-series LLMs. Furthermore, it\nallows the model to surpass the limitations or \u201cceiling\u201d\ninherent in traditional teacher-student methods. Eliciting\nself-knowledge could be formulated as:\n\n_D_ [(][sk][)] = _{_ ( _x, y, \u03d5_ sk( _x, y_ )) _|x \u223cS, y \u223c_ _pS_ ( _y|I \u2295_ _x_ ) _},_ (8)\n\n\nwhere _\u03d5_ sk( _\u00b7_ ) is a generalized function that represents an\nadditional process to the self-generated outputs _y_, which\ncould include but is not limited to filtering, rewarding, or\nany other mechanisms for enhancing or evaluating _y_ . It\ncould be governed by external tools or the student itself _\u03b8S_ .\nRecent research in this area has proposed various innovative\nmethodologies to elicit self-knowledge, demonstrating its\npotential for creating more efficient and autonomous learning systems. (Allen-Zhu and Li, 2020; Wang et al., 2022a;\nSun et al., 2024b; Yang et al., 2024; Jung et al., 2023; Huang\net al., 2023a; Gulcehre et al., 2023; Yuan et al., 2024a; Xu\net al., 2023b; Zelikman et al., 2022; Chen et al., 2024a; Zheng\net al., 2024; Li et al., 2024c; Zhao et al., 2024; Singh et al.,\n2023; Chen et al., 2024c; Hosseini et al., 2024)\nA notable example  - f this methodology is SelfInstruct (Wang et al., 2022a), which utilizes GPT-3 for\ndata augmentation through the _Expansion_ approach, generating additional data samples to enhance the dataset.\nThis enriched dataset subsequently fine-tunes the original\nmodel. Other methods aim to elicit targeted knowledge\n\n\nfrom student models by modifying prompts, and leveraging\nthese data for further refinement. In Self-Align (Sun et al.,\n2024b), they find that models fine-tuned by Self-Instruct\ndata tend to generate short or indirect responses. They\nprompt this model with verbose instruction to produce indepth and detailed responses.", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_12600", "chunk_text": " In Self-Align (Sun et al.,\n2024b), they find that models fine-tuned by Self-Instruct\ndata tend to generate short or indirect responses. They\nprompt this model with verbose instruction to produce indepth and detailed responses. Then, they employ contextdistillation (Askell et al., 2021) to distill these responses\npaired with non-verbose instructions back to the model.\nSimilarly, RLCD (Yang et al., 2024) introduces the use of\ncontrasting prompts to generate preference pairs from an\nunaligned LLM, encompassing both superior and inferior\nexamples. A preference model trained on these pairs then\nguides the enhancement of the unaligned model through\nreinforcement learning. Several other approaches employ\nfiltering methods to refine self-generated data. For example, Impossible Distillation (Jung et al., 2023) targets sentence summarization tasks, implementing filters based on\nentailment, length, and diversity to screen self-generated\nsummaries. LMSI (Huang et al., 2023a) generates multiple\nCoT reasoning paths and answers for each question, and\nthen retains only those paths that lead to the most consistent\n\nanswer.\nNote that refined self-knowledge can be iteratively acquired as the student model continuously improves, further\nenhancing the student\u2019s capabilities. This is Gulcehre et al.\n(2023) introduces a Reinforced Self-Training (ReST) framework that cyclically alternates between Grow and Improve\nstages to progressively obtain better self-knowledge and\nrefine the student model. During the Grow stage, the student\nmodel generates multiple output predictions. Then, in the\nImprove stage, these self-generated outputs are ranked\nand filtered using a scoring function. Subsequently, the language model undergoes fine-tuning on this curated dataset,\nemploying an offline RL objective. Self-Play (Chen et al.,\n2024a) introduces a framework resembling iterative DPO,\nwhere the language model is fine-tuned to differentiate the\nself-generated responses from the human-annotated data.\nThese self-generated responses could be seen as \u201cnegative\nknowledge\u201d to promote the student to better align with\nthe target distribution. Self-Rewarding (Yuan et al., 2024a)\nexplores a novel and promising approach by utilizing the\nlanguage model itself as a reward model. It employs LLMas-a", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_13050", "chunk_text": " student to better align with\nthe target distribution. Self-Rewarding (Yuan et al., 2024a)\nexplores a novel and promising approach by utilizing the\nlanguage model itself as a reward model. It employs LLMas-a-Judge prompting to autonomously assign rewards for\nthe self-generated responses. The entire process can then\nbe iterated, improving instruction following and reward\nmodeling capabilities.\n\n\n**3.2** **Distillation**\n\n\nThis section focuses on the methodologies for effectively\ntransferring the elicited knowledge from teacher LLMs into\nstudent models. We explore a range of distillation techniques, from the strategies that enhance imitation by _Su-_\n_pervised Fine-Tuning_, _Divergence_ and _Similarity_, to advanced\nmethods like _Reinforcement Learning_ and _Rank Optimization_,\nas shown in Figure 3.\n\n\n_3.2.1_ _Supervised Fine-Tuning_\n\nSupervised Fine-Tuning (SFT), or called Sequence-Level KD\n(SeqKD) (Kim and Rush, 2016), is the simplest and one of\nthe most effective methods for distilling powerful black-box\n\n\n\n11\n\n\n**Divergence Type** _D_ ( _p, q_ ) **Function**\n\nForward KLD ~~\ufffd~~ _p_ ( _t_ ) log _pq_ (( _tt_ ))\n\nReverse KLD ~~\ufffd~~ _q_ ( _t_ ) log _pq_ (( _tt_ ))\n\nJS Divergence 12 ~~\ufffd~~   - _p_ ( _t_ ) log _p_ ( _t_ 2)+ _p_ ( _tq_ )( _t_ ) [+] [\ufffd] _[q]_ [(] _[t]_ [) log] _p_ ( _t_ 2)+ _q_ ( _tq_ )( _t_ ) ~~\ufffd~~\n\n\nTABLE 1: Functional forms of _D_ for various divergence\ntypes. _p_ : reference\n\n\n**Similarity Function** _LF_ **Expression**\n\n\nL2-Norm Distance _\u2225_ \u03a6 _T_ ( _fT_ ( _x, y_ )) _\u2212_ \u03a6 _S_ ( _fS_ ( _x, y_ )) _\u2225_ 2\n\n\nL1-Norm Distance _\u2225_ \u03a6 _T_ ( _", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_13500", "chunk_text": " ( _x, y_ )) _\u2212_ \u03a6 _S_ ( _fS_ ( _x, y_ )) _\u2225_ 2\n\n\nL1-Norm Distance _\u2225_ \u03a6 _T_ ( _fT_ ( _x, y_ )) _\u2212_ \u03a6 _S_ ( _fS_ ( _x, y_ )) _\u2225_ 1\n\n\nCross-Entropy Loss _\u2212_ ~~[\ufffd]~~ \u03a6 _T_ ( _fT_ ( _x, y_ )) log(\u03a6 _S_ ( _fS_ ( _x, y_ )))\n\n\nMaximum Mean Discrepancy MMD(\u03a6 _T_ ( _fT_ ( _x, y_ )) _,_ \u03a6 _S_ ( _fS_ ( _x, y_ )))\n\n\nTABLE 2: Summary of similarity functions in knowledge\ndistillation.\n\n\nLLMs. SFT finetunes student model by maximizing the likelihood of sequences generated by the teacher LLMs, aligning\nthe student\u2019s predictions with those of the teacher. This\nprocess can be mathematically formulated as minimizing\nthe objective function:\n\n\n_L_ SFT = E _x\u223cX_ _,y\u223cpT_ ( _y|x_ ) [ _\u2212_ log _pS_ ( _y|x_ )] _,_ (9)\n\n\nwhere _y_ is the output sequence produced by the teacher\nmodel. This simple yet highly effective technique forms\nthe basis of numerous studies in the field. Numerous researchers have successfully employed SFT to train student\nmodels using sequences generated by teacher LLMs (Taori\net al., 2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al.,\n2023a; Luo et al., 2023b). Additionally, SFT has been explored in many self-distillation works (Wang et al., 2022a;\nHuang et al., 2023c; Xu et al., 2023b; Zelikman et al., 2022).\nDue to the large number of KD works applying SFT, we\n\n- nly list representative ones here. More detailed works can\nbe found in \u00a74.\n\n\n_3.2.2_ _Divergence and Similarity_\n\nThis section mainly concentrates on", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_13950", "chunk_text": " large number of KD works applying SFT, we\n\n- nly list representative ones here. More detailed works can\nbe found in \u00a74.\n\n\n_3.2.2_ _Divergence and Similarity_\n\nThis section mainly concentrates on algorithms designed for\ndistilling feature knowledge from white-box teacher LLMs,\nincluding distributions and hidden state features. These\nalgorithms can be broadly categorized into two groups:\nthose minimizing divergence in probability distributions\nand those aimed at enhancing the similarity of hidden\nstates.\n\n\n_**Divergence.**_ Divergence-based methods minimize divergence between the probability distributions of the teacher\nand student models, represented by a general divergence\nfunction _D_ :\n\n\n_L_ Div = E (10)\n_x\u223cX_ _,y\u223cY_ [[] _[D]_ [ (] _[p][T]_ [ (] _[y][|][x]_ [)] _[, p][S]_ [(] _[y][|][x]_ [))]] _[,]_\n\n\nThe specific form of _D_ varies depending on the type of\ndivergence employed. Table 1 outlines the functional forms\n\n- f _D_ for different divergence measures. The commonly-used\nstandard KD objectives essentially minimize the approximated forward Kullback-Leibler divergence (KLD) between\nthe teacher and the student distribution (Sanh et al., 2019;\n\n\nFig. 6: **Comparison of Forward and Reverse KL Diver-**\n**gences in Approximating a Target Distribution** . Forward\nKL divergence approach tends to cover all modes of the\ntarget distribution but is less precise, i.e. \u201cmode-covering\u201d\nbehavior. Reverse KL divergence method focuses predominantly on the most prominent mode, thereby exhibiting a\n\u201cmode-seeking\u201d behavior.\n\n\nWen et al., 2023; Timiryasov and Tastet, 2023; Liang et al.,\n2023a; Chen et al., 2024d), which forces _pS_ to cover all the\nmodes of _pT_ . However, when a student model is unable\nto learn all modes of a highly complex teacher, the resultant \u201cmode-covering\u201d behavior might cause the student\nto assign probability mass to tokens with low probability\nunder the teacher\u2019s distribution (cf. Figure 6 blue curve).\nThis mode-covering phenomenon can potentially lead to\nhall", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_14400", "chunk_text": " teacher, the resultant \u201cmode-covering\u201d behavior might cause the student\nto assign probability mass to tokens with low probability\nunder the teacher\u2019s distribution (cf. Figure 6 blue curve).\nThis mode-covering phenomenon can potentially lead to\nhallucinations and low-quality generations. Alternatively,\nmode-seeking divergences like reverse KL prioritize tokens\nwhere the teacher assigns high probabilities (cf. Figure 6\ngreen curve). This approach can mitigate the risk of lowquality outputs, fostering more accurate generations. However, it often does so at the cost of reduced diversity.\nGu et al. (2024) adopt reverse KL divergence to prevent\nstudents from overestimating low-probability regions of the\nteacher\u2019s distribution, employing Policy Gradient methods\nfor optimization. Both Agarwal et al. (2024) and Sason and\nVerd\u00b4u (2016) assess the effect of different divergence functions in LLM distillation, finding the optimal divergence to\nbe task-dependent. For instance, forward KL divergence is\nmore suitable for tasks like Machine Translation, where the\n\n- utput has fewer modes or variations, while reverse KL\ndivergence is preferable for tasks like dialogue generation\nand instruction tuning, which involve multiple modes and\na wider range of potential responses. Thus, the nature of the\ntask significantly influences the selection of the divergence\nfunction for optimal performance.\n\n\n_**Similarity.**_ Similarity-based methods in knowledge distillation aim to align the hidden states or features of the student\nmodel with those of the teacher. These methods use various\nsimilarity metrics to measure and optimize the congruence\n\n- f internal representations between the two models. The\n\n- bjective is to ensure that the student model not only\nproduces similar outputs to the teacher but also processes\ninformation in a comparable manner. The formulation for a\nsimilarity-based objective might look like this:\n\n\n_L_ Sim = E\n_x\u223cX_ _,y\u223cY_ [[] _[L][F]_ [ (\u03a6] _[T]_ [ (] _[f][T]_ [ (] _[x, y]_ [))] _[,]_ [ \u03a6] _[S]_ [ (] _[f][S]_ [(] _[x, y]_ [)))]] _[,]_ [ (11)]\n\n\nwhere _fT_ ( _x, y_ )", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_14850", "chunk_text": " \u03a6] _[S]_ [ (] _[f][S]_ [(] _[x, y]_ [)))]] _[,]_ [ (11)]\n\n\nwhere _fT_ ( _x, y_ ) and _fS_ ( _x, y_ ) are the feature maps of the\nteacher and student models, respectively. The transforma\n\n\n12\n\n\ntion functions \u03a6 _T_ and \u03a6 _S_ are applied to these feature maps\nto ensure they are in the same shape, facilitating direct\ncomparison. The similarity function _LF_ is used to match\nthese transformed feature maps. Table 2 shows common\nchoices for _LF_ . Few works have employed similarity-based\nmethods in the KD of LLMs. Among them, Liang et al.\n(2023a) propose Task-Aware Layer-Wise Distillation (TED),\na method that utilizes task-aware filters. These filters are\ndesigned to selectively capture the most pertinent information for a specific task from the teacher model. The key\n\n- bjective is to minimize the discrepancy between the filtered\nrepresentations in both teacher and student models. While\nsimilarity-based approaches are common in encoder-based\nLMs (Sun et al., 2019, 2020; Jiao et al., 2020; Hou et al.,\n2020; Zuo et al., 2022; Liang et al., 2021), their application in\nLLM knowledge distillation is not as widespread. However,\nconsidering their effectiveness, we anticipate an increase in\nresearch exploring these methods for LLM distillation in the\nnear future.\n\n\n_3.2.3_ _Reinforcement Learning_\n\n\nThis section explores advanced methods of distilling knowledge into student models using reinforcement learning (RL).\nThis approach is especially relevant for leveraging the feedback from teacher to train student models (Bai et al., 2022a;\nCui et al., 2023a; Luo et al., 2023b; Agarwal et al., 2024; Chen\net al., 2024b; Ma et al., 2023a; Pang et al., 2023; Du et al.,\n2023a). The RL-based distillation process typically involves\ntwo main stages:\n\n\n_**Distilled Reward Model Training.**_ The first stage involves\ntraining a reward model _r\u03d5", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_15300", "chunk_text": " al., 2023; Du et al.,\n2023a). The RL-based distillation process typically involves\ntwo main stages:\n\n\n_**Distilled Reward Model Training.**_ The first stage involves\ntraining a reward model _r\u03d5_ using the feedback data _D_ [(][fd][)]\n\ngenerated by teacher LLMs. Preference data, as one of the\ntypical feedback, is employed to train the student reward\nmodel (Bai et al., 2022a; Cui et al., 2023a; Lee et al., 2023a;\nKim et al., 2023a). They usually consist of input-output\npairs ( _x, yw, yl_ ). Here, _yw_ and _yl_ represent \u201cwinning\u201d and\n\u201closing\u201d outputs relative to the teacher\u2019s preferences. The\nloss function for the reward model is defined as:\n\n\n_L_ RM( _r\u03d5, D_ [(fd)] ) = _\u2212_ E\n( _x,yw,yl_ ) _\u223cD_ [(fd)][ [log] _[ \u03c3]_ [ (] _[r][\u03d5]_ [ (] _[x, y][w]_ [)] _[ \u2212]_ _[r][\u03d5]_ [ (] _[x, y][l]_ [))]]\n\n(12)\n\n\nThis formulation guides the reward model to correctly\ndistinguish between more and less preferable outputs based\n\n- n the teacher\u2019s criteria. Instead of learning the instancelevel rewards, RLMEC (Chen et al., 2024b) adopts a different approach by training a generative reward model. It\nis trained on an erroneous solution rewriting data distilled\nfrom a teacher LLM. This distilled reward model can produce token-level rewards for RL training.\n\n\n_**Reinforcement Learning Optimization.**_ In the second stage,\nthe student model, represented by a policy _\u03c0\u03b8_, is optimized\nto maximize the expected reward as per the trained reward\nmodel. Simultaneously, it minimizes the divergence from\na reference policy _\u03c0_ ref, typically the initial policy of the\nstudent model trained by SFT, controlled by a factor _\u03b2_ . The\nRL objective is given by:\n\n\nmax E\n_\u03c0\u03b8_ _x\u223cX,y\u223c\u03c0\u03b8_ ( _y|x_ ) [[] _[", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_15750", "chunk_text": "student model trained by SFT, controlled by a factor _\u03b2_ . The\nRL objective is given by:\n\n\nmax E\n_\u03c0\u03b8_ _x\u223cX,y\u223c\u03c0\u03b8_ ( _y|x_ ) [[] _[r][\u03d5]_ [(] _[x, y]_ [)]] _[ \u2212]_ _[\u03b2D]_ [KL][ [] _[\u03c0][\u03b8]_ [(] _[y][ |][ x]_ [)] _[\u2225][\u03c0]_ [ref] [(] _[y][ |][ x]_ [)]]\n\n(13)\n\n\nThis RL framework not only ensures that the student model\nlearns the explicit content from the teacher but also effectively adopts the teacher\u2019s preference patterns. The use of\nRL, particularly with the PPO (Schulman et al., 2017) algorithm, offers a robust mechanism for aligning the student\nmodel\u2019s outputs with the teacher. Alternatively, the teacher\nLLM can also serve as the reward model to directly assign\nrewards during RL, circumventing the need for training a\nreward model (Lee et al., 2023a; Kwon et al., 2023). While\nthis approach may exhibit superior performance, it comes\nat a higher computational cost compared to employing a\nsmaller distilled reward model.\n\n\n_3.2.4_ _Ranking Optimization_\n\nRanking optimization presents a stable and computationally\nefficient alternative to RL for injecting preference feedback\ninto language models (Rafailov et al., 2023; Song et al.,\n2023a; Yuan et al., 2023b). This method, diverging from\ntraditional RL approaches, directly incorporates ranking\ninformation into language models from a fixed preference\ndataset during fine-tuning. Intuitively, it directly updates\npolicy to increase the relative likelihood of preferred over\nless favored responses. This direct optimization of preferences, without the need for sampling outputs, makes the\nprocess more stable and efficient. Recently, some works have\nbeen proposed to explore using ranking optimization to\ndistill teacher\u2019s preferences into student models (Tunstall\net al., 2023; Hong et al., 2023; Yuan et al., 2024a).\nZephyr (Tunstall et al., 2023) utilizes Direct Preference\nOptimization (DPO) (Rafail", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_16200", "chunk_text": " 2023; Hong et al., 2023; Yuan et al., 2024a).\nZephyr (Tunstall et al., 2023) utilizes Direct Preference\nOptimization (DPO) (Rafailov et al., 2023) to distill the\npreference alignment in teacher LLMs. DPO streamlines\nthe objective of reinforcement learning (as in Eq. 13),\nwhich involves reward maximization with a KL-divergence\nconstraint, into a single-stage policy training. Specifically,\nDPO\u2019s training goal is to maximize the following expectation:\n\n\n\n13\n\n\ncomparison to handle preference rankings of any length. For\na given instruction _x_ and a sequence of responses ordered by\nteacher preference as _y_ 1 _\u227b_ _y_ 2 _\u227b_ _... \u227b_ _yn_, the RPO training\n\n- bjective is:\n\n\n\n_L_ PRO = _\u2212_\n\n\n\n_n\u2212_ 1\n\nexp ( _pk_ )\n\n- log ~~_n_~~ (16)\n\n_k_ =1 ~~\ufffd~~ _i_ = _k_ [exp (] _[p][i]_ [)] _[,]_\n\n\n\n_\u03c0_ ref ( _yl|x_ )\n\n\n\nwhere _pk_ represents the conditional log probabilities for\n_yk_ under the student policy _\u03c0\u03b8_ . By iteratively contrasting\nthe likelihood of generating responses, PRO optimizes the\nstudent LM to prioritize the most preferred response while\nprogressively ranking the rest in the order of diminishing\npreference.\n\n\n**4** **SKILL DISTILLATION**\n\n\nBuilding upon the foundation laid out in Section 3 about\neliciting knowledge and distillation algorithms, we shift our\nfocus to how these techniques facilitate the distillation of\nspecific skills in LLMs. Our exploration will encompass\na diverse range of skills exhibited by LLMs, including\n_Context Following_, _Alignment_, _Agent_, _NLP Task Specializa-_\n_tion_ and _Multi-Modality_ . _Context Following_ focuses on the\nstudent\u2019s ability to comprehend and respond effectively\nto input information. _Alignment_ delves into the student\u2019s\ncapability to align its output with the teacher\u2019s responses.\nMoving forward, _Agent_ underscores the autonomous nature\n\n- f language models. _NLP Task Specialization_ highlights the\nLLM\u2019s versatility in specializing across", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_16650", "chunk_text": "ves into the student\u2019s\ncapability to align its output with the teacher\u2019s responses.\nMoving forward, _Agent_ underscores the autonomous nature\n\n- f language models. _NLP Task Specialization_ highlights the\nLLM\u2019s versatility in specializing across various Natural\nLanguage Processing tasks, demonstrating its adaptability.\nFinally, _Multi-Modality_ encompasses the knowledge transfer from teacher LLMs to multi-modal models. Table 3\nsummarizes the representative works, encompassing details\nsuch as the skills involved, seed knowledge, teacher LLM,\nstudent model, knowledge elicitation method, and training\n\n- bjectives.\n\n\n**4.1** **Context Following**\n\n\nThis part concentrates on the distillation of context following skills from LLMs. This process involves transferring the\nability of LLMs to handle a variety of complex contexts \u2014\nsuch as few-shot demonstrations, intricate instructions, dialogue history, and retrieval-augmented information \u2014 into\nsmaller models. Many research efforts in this domain aim\nto imbue smaller models with these sophisticated, contextfollowing capabilities. Our discussion here will dissect this\nfacet of skill distillation, categorizing it based on different\ntypes of context and elaborating on how each is distilled\nand incorporated into smaller, efficient models.\n\n\n_4.1.1_ _Instruction Following_\nInstruction-following capacity enables LLMs to understand\nand follow user-given instructions. This ability significantly\nenhances human-AI interaction, allowing for seamless understanding and execution of tasks as directed by users. A\nprimary method for acquiring this skill involves constructing instruction-like prompt-response pairs and employing\nSupervised Fine Tuning (SFT) for model training. Data for\nthis purpose can be manually curated by human experts\n\n- r transformed from existing NLP tasks into instructional\n\n\n\nE\n( _x,yw,yl_ ) _\u223cD_ [(fd)]\n\n\n\nlog _\u03c3_ _\u03b2_ log _[\u03c0][\u03b8]_ [(] _[y][w][|][x]_ [)]\n\n- - _\u03c0_ ref ( _yw|x_\n\n\n\n\n_[\u03c0][\u03b8]_ [(] _[y][w][|][x]_ [)] _[\u03c0][\u03b8]_ [(] _[y][l][|][x]_ [)]\n\n_\u03c0_ ref ( _yw|x_ ) _[\u2212]_ _[\u03b2]_ [ log] _\u03c0_ ref (", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_17100", "chunk_text": " _[\u03c0][\u03b8]_ [(] _[y][l][|][x]_ [)]\n\n_\u03c0_ ref ( _yw|x_ ) _[\u2212]_ _[\u03b2]_ [ log] _\u03c0_ ref ( _yl|x_ )\n\n\n\n_,_\n\n- \ufffd\n\n(14)\n\n\n\nwhere _yw_ is preferred over _yl_ according to the teacher\nLLM. Hong et al. (2023) (Hong et al., 2023) adopt two\nranking-based optimization objectives, Rank Responses to\nalign Human Feedback (RRHF) (Yuan et al., 2023b) and\nPreference Ranking Optimization (PRO) (Song et al., 2023a),\nfor preference distillation. RRHF (Yuan et al., 2023b) focuses\n\n- n a ranking loss defined as:\n\n\n_L_ RRHF =     - max(0 _, pi \u2212_ _pj_ ) _,_ (15)\n\n_ri<rj_\n\n\nwhere _ri_ and _rj_ are the reward scores assigned by the\nteacher LLM for responses _yi_ and _yj_, respectively, and _pi_, _pj_\nare their corresponding conditional log probabilities under\nthe policy _\u03c0\u03b8_ . This approach emphasizes direct comparison\nand ranking of responses based on the teacher\u2019s preferences.\nPRO (Song et al., 2023a) expands the concept of pairwise\n\n\n14\n\n\n**Methods** **Skill** **Seed Knowledge** **Teacher LLM** **Student Model** **Knowledge Elicitation** **Objective**\n\n\n_**Context Following**_\n\n\nSelf-Instruct (Wang et al., 2022a) IF 175 human-curated tasks GPT3 GPT3 Expansion + Self-Knowledge SFT\nAlpaca (Taori et al., 2023) IF 175 human-curated tasks GPT3 LLaMA Expansion + Self-Knowledge SFT\n\nLaMini-LM (Wu et al., 2023c) IF 3.5K Wikipedia Categories +Mixed Dataset ChatGPT Various Models Expansion SFT\n\nWizardLM (Xu et al., 2023a) IF Alpaca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryas", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_17550", "chunk_text": "aca Data ChatGPT LLaMA Expansion SFT\nLion (Jiang et al., 2023b) IF Alpaca Cata ChatGPT LLaMA Labeling + Expansion + Feedback   BabyLlama (Timiryasov and Tastet, 2023) IF 10M-word BabyLM dataset GPT-2 + small LLaMA 58M-parameter LLaMA Feature D&S\nMiniLLM (Gu et al., 2024) IF Dolly Dataset GPT2 + OPT + LLaMA GPT2 + OPT + LLaMA Feature D&S\nSelf-Align (Sun et al., 2024b) IF Human-written Principles LLaMA LLaMA Expansion + Self-Knowledge SFT\nSelf-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL\nSTaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT\nLlama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT\nReflection-Tuning (Li et al., 2023e) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nSelective Reflection-Tuning (Li et al., 2024d) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT\nVicuna (Chiang et al., 2023) IF/MD Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT\nKoala (Geng et al., 2023) IF/MD Human Conversation ChatGPT LLaMA Labeling SFT\nBaize (Xu et al., 2023b) IF/MD Quora + Stack Overflow ChatGPT LLaMA Expansion + Self-Knowledge SFT\nUltraChat (Ding et al., 2023b) IF/MD Wikidata + Text Material + C4 ChatGPT LLaMA Curation SFT\nOrca (Mukherjee et al., 2023) IF/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_18000", "chunk_text": "MA Curation SFT\nOrca (Mukherjee et al., 2023) IF/TP FLAN-v2 ChatGPT + GPT4 LLaMA Labeling SFT\nOrca2 (Mitra et al., 2023) IF/TP FLAN-v2 + Few-Shot/Math/Synthetic GPT4 LLaMA Labeling SFT\nSelFee (Ye et al., 2023) IF/TP Human Conv, Flan/Code/Math Collection ChatGPT LLaMA Labeling SFT\nCoT-Distill (Hsieh et al., 2023) IF/TP e-SNLI + ANLI + CQA + SVAMP PaLM T5 Labeling SFT\nKnowPAT (Zhang et al., 2023a) IF/TP CPKG + QA Data ChatGPT + ChatGLM + Vicuna-7B LLaMA Labeling SFT\nDEBATunE (Li et al., 2024e) IF/TP Controversial Topics ChatGPT LLaMA Labeling SFT\nPhi-1 (Gunasekar et al., 2023) IF/Code   - GPT3.5 phi-1 Curation SFT\nPhi-1.5 (Li et al., 2023a) IF/Code 20k Topics from Web GPT3.5 phi-1 Curation + Labeling SFT\nSAIL (Luo et al., 2023c) IF/RAG Alpaca Data + Web Content GPT4 LLaMA Label SFT\nKARD (Kang et al., 2023b) IF/RAG MedQAUSMLE ChatGPT T5 + OPT Label SFT + D&S\nSelf-RAG (Asai et al., 2023) IF/RAG Open-Instruct GPT4 LLaMA Labeling SFT\n\n\n_**Alignment**_\n\n\nOpenChat (Wang et al., 2023c) IF/Preference Human Conversation ChatGPT + GPT4 LLaMA Labeling SFT + RL\nZephyr (Tunstall et al., 2023) IF/Preference Mixed Datasets GPT4 Mistral Labeling + Feedback SFT + RO\nALMoST (Kim et al., 2023a) IF/Preference Human-written Prompts LLaMA LLa", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_18450", "chunk_text": "., 2023) IF/Preference Mixed Datasets GPT4 Mistral Labeling + Feedback SFT + RO\nALMoST (Kim et al., 2023a) IF/Preference Human-written Prompts LLaMA LLaMA Expansion + Labeling SFT + RL\nRLCD (Yang et al., 2024) IF/Preference Human-written Prompts LLaMA LLaMA Labeling SFT + RL\nRLAIF (Lee et al., 2023a) IF/Preference Human-written Prompts PaLM 2 PaLM 2 Labeling + Feedback RL\nGPT3 Reward (Kwon et al., 2023) Preference Human-written Prompts GPT3 GPT3 Labeling RL\nILF (Scheurer et al., 2023) Preference Task-specific Datasets GPT3 + FeedME GPT3 Labeling RL\nULTRAFEEDBACK (Cui et al., 2023a) Preference Mixed Datasets GPT4 LLaMA Labeling RL\nConstitutional AI (Bai et al., 2022a) Preference/Value Human-written Prompts Self-defined Student Model Self-defined Model Labeling + Expansion + Feedback SFT + RL\n\nSANDBOX (Liu et al., 2023b) Value Simulation text-davinci-002/-003 +GPT4 + ChatGPT LLaMA Data Curation SFT + RL\n\n\n_**Agent**_\n\n\nToolformer (Schick et al., 2023) Tool CCNet GPT-J GPT-J Labeling SFT\nGraph-ToolFormer (Zhang, 2023) Tool Mixed Graph Dataset ChatGPT GPT-J + LLaMA Labeling SFT\nGorilla (Patil et al., 2023) Tool Online API Documentation GPT4 LLaMA Expansion SFT\nGPT4Tools (Yang et al., 2023b) Tool Image Content ChatGPT LLaMA Curation + Expansion SFT\nToolAlpaca (Tang et al., 2023a) Tool Public-apis Repository ChatGPT LLaMA Curation SFT\nToolLLM (Qin et al., 2023a) Tool Real-world APIs ChatGPT LLaMA Curation SFT\nMLLM-Tool (Wang et al., 2024) Tool HuggingFace Model Cards GPT", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_18900", "chunk_text": "LLM (Qin et al., 2023a) Tool Real-world APIs ChatGPT LLaMA Curation SFT\nMLLM-Tool (Wang et al., 2024) Tool HuggingFace Model Cards GPT4 LLaMA Curation SFT\nFireAct (Chen et al., 2023b) Planning Mixed QA Dataset GPT4 LLaMA Labeling SFT\nAgentTuning (Zeng et al., 2023a) Planning 6 Agent Tasks GPT4 + ChatGPT LLaMA Labeling + Expansion SFT\nLumos (Yin et al., 2023a) Planning Mixed Interactive Tasks GPT4 LLaMA Labeling SFT\nAUTOACT (Qiao et al., 2024) Planning Mixed QA Tasks LLaMA LLaMA Labeling SFT\n\n\n_**NLP Task Specialization**_\n\n\nAugGPT (Dai et al., 2023a) NLU Amazon/Symptoms/PubMed20k Dataset ChatGPT BERT Label SFT\nTDG (He et al., 2023b) NLU SST + QQP + MNLI GPT3 BERT Expansion SFT\nSunGen (Gao et al., 2023a) NLU Text Classification Tasks GPT2 DistilBERT Curation SFT\nUDG (Wang et al., 2021a) NLU NLU Tasks GPT3 BERT Expansion SFT\nInheritSumm (Xu et al., 2023c) NLG Pile + ArXiv + CNN/DM + WikiHow GPT3.5 ZCode++ Label SFT\nDIMSUM+ (Jung et al., 2023) NLG None GPT2 + CTRL + BioGPT T5 Curation + Self-Knowledge SFT\nGenie (Yehudai et al., 2024) NLG ELI5 + ASQA + NQ + CNN/DM Falcon + LLaMA FLAN + LLaMA Label SFT\nGKD (Agarwal et al., 2024) NLG/NLU/IF XSum+WMT14 en-de+GSM8K+FLAN2021 T5-XL T5 Feature + Feedback D&S + RL\nQUILL (Srinivasan et al., 2022) IR IR Datasets", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_19350", "chunk_text": "IF XSum+WMT14 en-de+GSM8K+FLAN2021 T5-XL T5 Feature + Feedback D&S + RL\nQUILL (Srinivasan et al., 2022) IR IR Datasets T5 4-layer Transformer Internal Knowledge D&S\nRankVicuna (Pradeep et al., 2023a) IR IR Datasets ChatGPT LLaMA Labeling SFT\nRankZephyr (Pradeep et al., 2023b) IR IR Datasets ChatGPT + GPT4 Mistral Labeling SFT\nNDR (Mysore et al., 2023) Recommendation Recommendation Datasets GPT3 MPnet-110M Labeling SFT\nInstrcutRec (Zhang et al., 2023b) Recommendation 39 instruction templates ChatGPT Flan-T5 Expansion + Self-Knowledge SFT\nONCE (Liu et al., 2023c) Recommendation Recommendation Dataset ChatGPT LLaMA Labeling SFT\nPandaLM (Wang et al., 2023b) Evaluation Alpaca Data ChatGPT LLaMA Labeling SFT\nPrometheus (Kim et al., 2024) Evaluation 50 Seed Rubrics GPT4 LLaMA Labeling SFT\nInstructScore (Xu et al., 2023d) Evaluation Mixed Dataset GPT4 LLaMA Labeling SFT\nWizardMath (Luo et al., 2023b) Math GSM8k + MATH ChatGPT LLaMA Expansion + Feedback SFT + RL\nMammoth (Yue et al., 2023a) Math/TP Mixed Math Dataset GPT4 LLaMA Labeling SFT\nMixed Distill (Chenglin et al., 2023) Math/TP SVAMP + GSM8K + ASDIV + StrategyQA ChatGPT LLaMa Labeling SFT\nWizardCoder (Luo et al., 2023a) Code Code Alpaca Data ChatGPT StarCoder Expansion SFT\nMagicoder (Wei et al., 2023) Code Existing Source Codes ChatGPT LLaMa Curation SFT\nWaveCoder (Yu et al., 2024) Code Existing Source Codes GPT4 LLaMa Curation SFT\nCode Alpaca (Chaudhary, 202", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_19800", "chunk_text": " Codes ChatGPT LLaMa Curation SFT\nWaveCoder (Yu et al., 2024) Code Existing Source Codes GPT4 LLaMa Curation SFT\nCode Alpaca (Chaudhary, 2023) Code Code Instructions ChatGPT LLaMA Expansion + Self-Knowledge SFT\nCode Llama (Rozi`ere et al., 2023) Code Human-written Instructions LLaMA LLaMA Expansion + Self-Knowledge SFT\nCode Clean (Jain et al., 2023) Code Code Datasets ChatGPT LLaMA Labeling SFT\n\n\n_**Multi-Modality**_\n\n\nLLaVA (Liu et al., 2023e) Vision-Language COCO GPT4 LLaMA Labeling SFT\nSVIT (Zhao et al., 2023b) Vision-Language Visual Genome + COCO GPT4 LLaMA Labeling SFT\nLVIS-Instruct4V (Wang et al., 2023e) Vision-Language LVIS GPT4V LLaMA Labeling SFT\nLLaVAR (Zhang et al., 2023d) Vision-Language LAION GPT4 LLaMA Labeling SFT\nMacaw-LLM (Lyu et al., 2023) Multiple Modalities Image/Video with Caption ChatGPT LLaMA Labeling SFT\nMIMIC-IT (Li et al., 2023f) Multiple Modalities Image/Video Dataset ChatGPT LLaMA Labeling SFT\nChatBridge (Zhao et al., 2023d) Multiple Modalities Task-Specific/Multimodal-Chat Data GPT4 + ChatGPT LLaMA Labeling SFT\n\n\nTABLE 3: A summary of skill distillation works. IF: Instruction Following, MD: Multi-turn Dialoue, TP: Think Pattern,\nRAG: Retrieval-Augmented Generation, NLU: Natural Language Understanding, NLG: Natural Language Generation, IR:\nInformation Retrieval, SFT: Supervised Fine-Tuning, D&S: Divergence and Similarity, RL: Reinforcement Learning, RO:\nRanking Optimization.\n\n\n\nformats with templates, such as prefacing machine translation data with _\u201dTranslate this sentence to Spanish:\u201d_ . However,\nthese approaches have limitations. Manual data creation is\nlabor-intensive, while template-based", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_20250", "chunk_text": " Learning, RO:\nRanking Optimization.\n\n\n\nformats with templates, such as prefacing machine translation data with _\u201dTranslate this sentence to Spanish:\u201d_ . However,\nthese approaches have limitations. Manual data creation is\nlabor-intensive, while template-based transformation lacks\ndiversity in instructions and may not align well with natural\nhuman input. LLMs like GPT-4 offer an efficient alternative\nfor creating diverse and controlled SFT data by their capabilities of in-context learning and instruction following. Most\n\n\n\nrelevant works use OpenAI\u2019s GPT series models to generate\nprompt-response data pairs and then train the student LLMs\nby supervised fine-tuning (Wang et al., 2022a; Taori et al.,\n2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a;\nMukherjee et al., 2023; Mitra et al., 2023; Luo et al., 2023b;\nPeng et al., 2023a).\n\n\n_**Basic Instructions.**_ Self-Instruct (Wang et al., 2022a) leverages the in-context learning capability of GPT-3 to expand\n\n\na seed pool of 175 tasks to 52K task-agnostic instructions,\nensuring a broad spectrum of general instructions. Additionally, a filtering and post-processing stage is introduced\nto eliminate redundant or similar instructions. Notably,\nthrough training with this enriched dataset, GPT-3 acquires\nthe ability to follow instructions, enabling it to perform\ncomparably to InstructGPT in zero-shot instruction tasks\nand when provided with expert-written instructions for\nnovel tasks. Based on the self-instruct method, Taori et al.\n(2023) train an Alpaca model using the Llama 7B model\n\n- n 52K instruction-following demonstrations, generated in\na similar style as self-instruct but utilizing the more robust\ntext-davinci-003 model. To enhance the diversity of instructional data, Wu et al. (2023c) introduce a technique known\nas _Topic-Guided Instruction Generation_ . This method involves\ngathering 3.5K common topics from Wikipedia to serve as\nguidance during the generation process.\n\n\n_**Complex Instructions.**_ Some works promote students to\nsolve more complex instructions (Xu et al.,", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_20700", "chunk_text": " . This method involves\ngathering 3.5K common topics from Wikipedia to serve as\nguidance during the generation process.\n\n\n_**Complex Instructions.**_ Some works promote students to\nsolve more complex instructions (Xu et al., 2023a; Luo et al.,\n2023b,a; Guo et al., 2023c). According to Xu et al. (2023a), instruction datasets derived from human-written seeds often\nexhibit low to moderate complexity. To enhance the complex instruction-following capabilities of smaller models,\nWizardLM (Xu et al., 2023a) introduces _Evol-Instruct_ . This\nmethod gradually transforms instructions into more complex forms through a multi-step evolution process, focusing\n\n- n both increasing difficulty levels and expanding the diversity of topics. They conducted four rounds of evolution\nusing the OpenAI ChatGPT API, resulting in a dataset of\n250k complex instructions. Subsequently, they trained the\nLLaMA 7B model, referred to as WizardLM, on this dataset.\nIn the high-difficulty section of test instructions, WizardLM\neven outperformed ChatGPT, achieving a win rate 7.9%\nhigher than ChatGPT. Zhao et al. (2023e) further conduct\npreliminary studies revealing the effectiveness of increasing\ninstruction complexity. Instruction Fusion (Guo et al., 2023c)\nfurther uses teacher LLMs to increase the complexity by\nfusing two distinct evolved instructions. Furthermore, this\nconcept of \u201cevolving\u201d instructions has been extended to\ndistill specific skills such as coding (Luo et al., 2023a) and\nmathematics (Luo et al., 2023b).\n\n\n_**Human Instructions.**_ In contrast to works that rely on generating instructions from ChatGPT, which may lack diversity\nand have gaps with real human instructions, Vicuna (Chiang\net al., 2023) and Koala (Geng et al., 2023) showcase impressive performance by using human conversations and natural instructions from community-contributed conversations.\nThese conversations, found in platforms like ShareGPT, provide a forum for users to share their interactions with ChatGPT. It\u2019s important to note, however, that models trained\n\n- n such natural conversations might mimic the style but\nmay not fully capture the reasoning process of the original\nteacher (G", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_21150", "chunk_text": " a forum for users to share their interactions with ChatGPT. It\u2019s important to note, however, that models trained\n\n- n such natural conversations might mimic the style but\nmay not fully capture the reasoning process of the original\nteacher (Gudibande et al., 2023; Mukherjee et al., 2023).\n\n\n_**System Instructions.**_ To encourage student models to learn\nthe reasoning process, Orca and Orca 2 (Mukherjee et al.,\n2023; Mitra et al., 2023) enhance the _prompt, response_ data\npairs by introducing a _system message_ (e.g., \u201dexplain like\nI\u2019m five, think step-by-step\u201d) to encourage student models to grasp the reasoning process. This _system message_\n\n\n\n15\n\n\nprompts GPT-4 to provide explanation traces that elucidate the teacher\u2019s reasoning process. Orca 2 (Mitra et al.,\n2023) further trains the student model to identify the most\neffective solution strategy for each task, guided by Orca\u2019s\nperformance. This approach significantly improves the ability of smaller models to follow instructions that involve\nreasoning.\n\n\n_**High-Quality Instructions.**_ As demonstrated in Zhou et al.\n(2023a) and (Li et al., 2024f), the data quality is crucial\nfor instruction following training. UltraChat (Ding et al.,\n2023b) distills large-scale data with high-quality and diverse instructions from teacher LLMs by various metainformation. The UltraLLaMA model, fine-tuned on this\ndata, consistently surpasses other open-source models. The\nPhi series models (Gunasekar et al., 2023; Li et al., 2023a;\nMar, 2023) prioritize data quality and employ synthetic\nmethods to generate data of \u201ctextbook quality\u201d to enhance\nthe learning experience for smaller models. Notably, Phi\nexhibits the ability to follow instructions effectively even\nwithout specific instruction fine-tuning. What\u2019s particularly\nremarkable is that Phi-2, with just 2.7 billion parameters,\n\n- utperforms Mistral and Llama-2 models with 7B and 13B\nparameters across various benchmark evaluations.\n\n\n_**Improved Instructions.**_ Another line of work focuses on\nimproving the quality of existing instruction data, including\nboth the improvement", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_21600", "chunk_text": " and Llama-2 models with 7B and 13B\nparameters across various benchmark evaluations.\n\n\n_**Improved Instructions.**_ Another line of work focuses on\nimproving the quality of existing instruction data, including\nboth the improvement of instruction and corresponding\nresponse. SelFee (Ye et al., 2023) utilizes the ChatGPT to iteratively improve the quality of responses. ExpertLLaMA (Xu\net al., 2023f) improves the quality of responses by augmenting vanilla instructions with specialized Expert Identity\ndescriptions. Reflection-Tuning (Li et al., 2023e) improves\nboth the instruction and response sequentially by reflecting\n\n- n specific criteria. DEITA (Liu et al., 2023h) proposes to\nenhance and score instructions in three directions including complexity, quality, and diversity to get high-quality\ndistillation data. MUFFIN (Lou et al., 2023) proposes to\nscale the instruction according to the input by diversifying\nthese tasks with various input facets. Selective ReflectionTuning (Li et al., 2024d) first involves the student model\nin the data improvement pipeline with a novel studentselection module, in which the student model is able to\ndecide the data learn from.\n\n\nIn summary, distilling instruction data from teachers\npresents a promising avenue for training cheap and reproducible instruction-following language models. Current small models have made strides in enhancing various aspects of instruction-following ability, like diversity, complexity and explanation. However, student models trained on instruction data expanded by ChatGPT often mimic ChatGPT\u2019s style without replicating its factual\naccuracy (Gudibande et al., 2023). Achieving a more capable instruction-following capability requires a stronger\nteacher LLM (Gudibande et al., 2023) and access to diverse, high-quality instruction data, such as the one used\nin Orca (Mukherjee et al., 2023; Mitra et al., 2023), which\nincorporates extensive task instructions from the Flan 2022\nCollection (Longpre et al., 2023).\n\n\n_4.1.2_ _Multi-turn Dialogue_\n\nWhile instruction following focuses on single-instance command execution, multi-turn dialogue extends this to comprehend and maintain context through ongoing interactions.\nThis skill is vital for models to engage meaningfully in\nhuman", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_22050", "chunk_text": ").\n\n\n_4.1.2_ _Multi-turn Dialogue_\n\nWhile instruction following focuses on single-instance command execution, multi-turn dialogue extends this to comprehend and maintain context through ongoing interactions.\nThis skill is vital for models to engage meaningfully in\nhuman-like conversations and respond coherently over successive dialogue turns. Some works have been dedicated\nto train to small chat models by distilling multi-turn knowledge from teacher LLMs (Chiang et al., 2023; Xu et al., 2023b;\nDing et al., 2023b; Li et al., 2023b; Wang et al., 2023c; Tunstall\net al., 2023).\nShareGPT serves as a platform for users to share their\nconversations with ChatGPT, offering a vast repository of\nmulti-turn conversations readily available. Some small chat\nmodels are trained using this data to acquire the capability\nfor engaging in multi-turn dialogues (Chiang et al., 2023; Ye\net al., 2023; Wang et al., 2023c). For example, Vicuna (Chiang\net al., 2023) is a chat model exclusively trained on ShareGPT\ndata. Despite its sole training source being ShareGPT, Vicuna achieves a high MT-Bench (Zheng et al., 2023a) score\nassigned by GPT-4 [3] . In the study conducted by Wang et al.\n(2023c), GPT-3.5 and GPT-4 are employed to generate mixed\nresponses using ShareGPT data. They assign higher rewards\nto responses generated by GPT-4, aiming to incentivize\nstudent models to produce high-quality responses. Additionally, Ye et al. (2023) enhance the quality of multi-turn\ndata from ShareGPT by generating self-feedback on model\nresponses and iteratively refining the responses based on\nthe received feedback.\nTo enhance the multi-turn capabilities of student models,\nanother line of research focuses on expanding conversational datasets through self-chat and using them to train\nsmaller models (Xu et al., 2023b; Ding et al., 2023b; Tunstall\net al., 2023). For instance, Xu et al. (2023b) initiate their work\nby using questions sourced from Quora and Stack Overflow\nas seeds, resulting in the collection", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_22500", "chunk_text": " 2023b; Tunstall\net al., 2023). For instance, Xu et al. (2023b) initiate their work\nby using questions sourced from Quora and Stack Overflow\nas seeds, resulting in the collection of 111.5k dialogues\nthrough self-chat. Subsequently, they employ parameterefficient tuning to train a chat model named Baize. Ding\net al. (2023b) first construct a significantly larger dataset\ncalled UltraChat, comprising 1.5 million high-quality multiturn dialogues. They achieve this by distilling instructions\nand dialogues from ChatGPT. Notably, UltraChat encompasses a wide range of topics and instructions. Building\nupon the UltraChat dataset, they fine-tune a LLaMA model,\nresulting in the creation of a powerful chat model known as\nUltraLLaMA. UltraLLaMA consistently outperforms other\n\n- pen-source chat models, including Vicuna and Baize. Furthermore, UltraChat is employed in conjunction with an\nAI preference-aligned chat model named Zephyr (Tunstall\net al., 2023). Zephyr enhances intent alignment through\nthe application of distilled direct preference optimization\n(dDPO).\n\n\n_4.1.3_ _RAG Capbility_\n\nLLMs are known to lack the ability to utilize up-to-date\nknowledge, and often produce responses containing factual\ninaccuracies due to their sole reliance on the parametric\nknowledge. Retrieval-Augmented Generation (RAG) is a\n\n\n3. MT-Bench: a multi-turn question set, where the generations of\nmodels are evaluated by LLM, like GPT-4.\n\n\n\n16\n\n\npromising technique to decrease this issue. Handling the\naugmented context of retrieved information is also a nontrivial skill of LLMs. Several approaches to distill RAG\ncapabilities have been proposed (Kang et al., 2023a; Luo\net al., 2023c; Asai et al., 2023).\nSAIL (Luo et al., 2023c) starts by retrieving search results\nfor each training case using search APIs, creating searchaugmented instructions that include both the instruction\nand grounding information. To encourage the language\nmodel to prioritize informative retrieval results, they input\neach retrieved passage along with the ground truth response\ninto the entailment model to label", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_22950", "chunk_text": " search APIs, creating searchaugmented instructions that include both the instruction\nand grounding information. To encourage the language\nmodel to prioritize informative retrieval results, they input\neach retrieved passage along with the ground truth response\ninto the entailment model to label each retrieval result for\nrelevance. Subsequently, the search-augmented instructions\nand relevance labels are fed into teacher LLMs (like GPT4) for generating responses. Following fine-tuning on this\ntraining set, the student model becomes proficient at denoising search results and generating accurate responses.\nKARD (Kang et al., 2023b) distills rationales _r_ from the\nteacher LLM in response to questions _x_ . These rationales\nare then utilized to train two models: a student LM and a\nReranker. For training the student LM, the rationales serve\nas a means to retrieve relevant knowledge _d_, and the student\nLM is subsequently fine-tuned using the rationales alongside questions and knowledge. However, during inference,\n\n- nly questions are available. To address this, the Reranker\nis trained to mimic how the retriever scores passages with\nthe rationale by minimizing the KL divergence between\n_Retriever_ ( _d|r_ ) and _Reranker_ ( _d|x_ ). However, the integration of a fixed number of passages in language models,\nwithout considering their necessity or relevance, can reduce\nversatility and lead to the generation of unhelpful responses.\nTo equip student LMs with adaptive RAG capabilities, SelfRag (Asai et al., 2023) distills this adaptive ability from\nteacher LLMs into a small critic model. This critic model\ndetermines whether retrieval is necessary and evaluates the\nquality of the retrieved results by generating \u2018reflection tokens.\u2019 For instance, Self-Rag initiates the retrieval operation\nwhen generating the reflection token _Retrieve_ . To distill\nthis critic data, GPT-4 is prompted to assess the need for\nretrieval using few-shot demonstrations _I_, the task input\n_x_, and output _y_ to predict a reflection token _r_ as follows:\n_p_ ( _r|I, x, y_ ).\n\n\n**4.2** **Alignment**\n\n\n_4.2.1_ _Thinking Pattern_\n\nMost existing methods mainly focus on directly aligning the\ndirect responses of the", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_23400", "chunk_text": ":\n_p_ ( _r|I, x, y_ ).\n\n\n**4.2** **Alignment**\n\n\n_4.2.1_ _Thinking Pattern_\n\nMost existing methods mainly focus on directly aligning the\ndirect responses of the student models to the responses of\nteacher models (Taori et al., 2023). Though effective, these\nmodels might suffer the problems that they tend to learn to\nimitate the response style of the teacher models, but not the\nreasoning process (Mukherjee et al., 2023). Thus in order to\nbetter distill from the teacher models, methods are proposed\nthat not only imitate the pure responses but some novel\nthinking patterns (Ye et al., 2023; Mukherjee et al., 2023;\nMitra et al., 2023; Wang et al., 2023d; Cheng et al., 2023;\nZhang et al., 2023a).\nMotivated by the effectiveness of LLMs in generating their own feedback without relying on external models (Schick et al., 2022; Madaan et al., 2023; Saunders\net al., 2022), SelFee (Ye et al., 2023) proposes to train a\n\n\nmodel that has been fine-tuned to continuously revise its\n\n- wn answer until it provides a high-quality response in a\nsingle inference. During training, it utilizes both the final\nresponse and feedback chain as the fitting target. This pattern, response with the revision process, shows a promising\nperformance gain. Following SelFee, Reflection-Tuning (Li\net al., 2023e, 2024d) also utilizes the reflection process as the\nlearning pattern. Noticing the lack of reasoning imitation\n\n- f the previous methods, Orca (Mukherjee et al., 2023)\nfirst proposes Explanation tuning, which aims to learn the\nreasoning steps, including explanation traces, step-by-step\nthought processes, and other complex instructions, from the\nteacher model, rather than just the vanilla styles. Extensive\nexperiments verify the effectiveness of distilling with this\nthinking pattern. The following Orca2 (Mitra et al., 2023)\nfurther presents to equip the student models with the ability\nto utilize different solution strategies for different tasks, motivated by the capability discrepancies between the smaller\nand larger models. By", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_23850", "chunk_text": " The following Orca2 (Mitra et al., 2023)\nfurther presents to equip the student models with the ability\nto utilize different solution strategies for different tasks, motivated by the capability discrepancies between the smaller\nand larger models. By employing this training pattern, the\nstudent models are able to gain a better reasoning ability. Besides learning with the corresponding revision or reflection\nprocess, another thinking pattern that recently appeared is\ngenerating both responses and preferences. Zhang et al.\n(2023a) propose to learn both the knowledge and corresponding preference for domain-specific QA with LLMs.\nRecently, DEBATunE (Li et al., 2024e) proposes to improve\nthe controllability of LLMs in generating statements on\ncontroversial topics. By engaging two agents in a structured\nmulti-round debate on controversial topics, salient and indepth statements can be obtained and further distilled into\nthe student models.\n\n\n_4.2.2_ _Preference_\n\nThe previously mentioned methods primarily focus on the\nbasic capability of student models to produce outcomes\nthat are strictly accurate but may not align with human\npreferences, reaching alignment at this level enables these\nmodels to aid in various tasks without meeting higher-level\ndemands. Early methods mainly utilize human feedback for\nthe alignment of human preferences (Ziegler et al., 2019;\nStiennon et al., 2020; Wu et al., 2021; Ouyang et al., 2022; Bai\net al., 2022b; K\u00a8opf et al., 2023; Yuan et al., 2023b). However,\n\n- btaining human feedback is costly and labor-intensive,\nthus methods that learn from AI feedback are also proposed\nto align with human preferences (Bai et al., 2022a; Kwon\net al., 2023; Scheurer et al., 2023; Kim et al., 2023a; Roit et al.,\n2023; Yang et al., 2024; Lee et al., 2023a; Tunstall et al., 2023;\nCui et al., 2023a; Wang et al., 2023f).\nThe concept of RLAIF, introduced by Bai et al. (2022a),\ninvolves the integration of preferences labeled by LLMs\nwith those labeled by humans. This approach is designed", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_24300", "chunk_text": " Wang et al., 2023f).\nThe concept of RLAIF, introduced by Bai et al. (2022a),\ninvolves the integration of preferences labeled by LLMs\nwith those labeled by humans. This approach is designed\nto simultaneously optimize two key objectives: ensuring\nthe helpfulness of the output and minimizing any potential\nharm, making the responses of LLMs more aligned with\nHuman preferences. Kwon et al. (2023) develop a proxy\nreward function using LLMs like GPT-3, which is created by\nfirst providing the LLM with a description of the behaviors\ndesired by the user, along with a small number of examples.\nThe LLM then produces rewards by evaluating how closely\nthe outputs of a model align with the provided descriptions, essentially measuring their relevance to the estab\n\n\n17\n\n\nlished ground truth. Scheurer et al. (2023) propose Imitation\nLearning from Language Feedback, in which a language\nmodel is utilized to improve various outputs generated by\na model. This refinement is based on a reference provided\nby a human. Following this process, the most effectively\nrefined output is chosen to be used in further supervised\nfine-tuning. As outlined by Kim et al. (2023a), ALMoST involves condensing human preferences into a set of heuristic\nguidelines. An example of such a rule is the idea that larger\nLLMs that utilize more comprehensive and higher-quality\nprompts are likely to yield superior responses. Based on\nthese established guidelines, comparison data is generated\nusing responses from LLMs of different sizes and with\nvarying prompts. This data is then used to train a reward\nmodel. Yang et al. (2024) propose Reinforcement Learning\nfrom Contrast Distillation, which aims to align language\nmodels without relying on human feedback. This approach\ninvolves training a preference model using simulated pairs\n\n- f preferences, including both high-quality and low-quality\nexamples which are generated through contrasting prompts,\npositive and negative.\n\nLee et al. (2023a) further highlight the effectiveness of\nRLAIF. This work proposes that RLAIF not only matches but\nin some cases surpasses RLHF, and interestingly, RLAIF can\nalso enhance the performance of Supervised Fine-Tuning.\nAnother notable discovery is that directly prompting the\nLLM for reward scores during reinforcement learning", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_24750", "chunk_text": " not only matches but\nin some cases surpasses RLHF, and interestingly, RLAIF can\nalso enhance the performance of Supervised Fine-Tuning.\nAnother notable discovery is that directly prompting the\nLLM for reward scores during reinforcement learning can\nbe more effective than the conventional approach of training\na reward model based on LLM preferences. Wang et al.\n(2023f) propose Conditioned-RLFT, which treats different\ndata sources as coarse-grained reward labels and develops\na class-conditioned policy to effectively utilize the varying\nqualities of data, which is a Reinforcement Learning-free\nsupervised learning approach. Cui et al. (2023a) propose a\nlarge-scale, high-quality, and diversified preference dataset\nlabeled by GPT4 for comprehensive feedback. Tunstall et al.\n(2023), by proposing distilled Direct Preference Optimization (Rafailov et al., 2023) on UltraFeedback, obtaining a\nsmall by powerful LLM.\n\n\n_4.2.3_ _Value_\n\nAttaining alignment with human preferences allows large\nmodels to optimize human satisfaction by operating in a\nmanner that aligns with human preferences. However, to\nestablish trustworthy LLMs, the notion of \u2019aligning LLMs\nwith human values\u2019 is proposed and the key principles of\nalignment are often summarized as the \u201cHHH\u201d criteria:\nhelpful, harmless, honest (Weidinger et al., 2021; Askell\net al., 2021). Numerous methods have been undertaken for\nbuilding trustworthy LLMs. However, due to the intrinsic\ndifficulty of this aim, which is still an unsolved problem\nfor proprietary models (Sun et al., 2024a), most existing\nmethods rely on constructing high-quality human preference datasets (Ji et al., 2023b; Solaiman and Dennison, 2021;\nBai et al., 2022b; Qiu et al., 2022; Kiesel et al., 2022; Liu et al.,\n2022a), utilizing human-written rules as constrains (Glaese\net al., 2022; Sun et al., 2023b, 2024b), etc. For detailed\nprogress on trustworthy LLMs, please further refer to Yao\net al. (2023a); Liu et al. (2023i); Sun et", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_25200", "chunk_text": " et al., 2023b, 2024b), etc. For detailed\nprogress on trustworthy LLMs, please further refer to Yao\net al. (2023a); Liu et al. (2023i); Sun et al. (2024a).\nThough slightly under-explored, aligning LLMs with\nhuman values by distilling is still possible (Bai et al., 2022a;\n\n\nCui et al., 2023a; Yang et al., 2024; Sun et al., 2024b). For\ninstance, Bai et al. (2022a) propose RLAIF, utilizing AIgenerated labels to interactively improve both helpfulness\nand harmlessness. Sun et al. (2024b) prompt the student\nmodel with 16 principles as guidelines for generating helpful, ethical, and reliable responses. Similarly, both harmless\nand harmful generations could be elicited by modifying\nthe prompts, and then are used to train the preference\nmodel (Yang et al., 2024). Cui et al. (2023a) utilize GPT4 to rank generations regarding helpfulness, truthfulness,\nand honesty. Liu et al. (2023b) advance the alignment of\nLLMs with societal values by incorporating simulated social\ninteractions into the training process. This approach encompasses a range of elements, including demonstrations that\nare both in alignment and in conflict with social norms, as\nwell as collective ratings, in-depth feedback, and responses\nthat are revised iteratively.\n\n\n**4.3** **Agent**\n\n\n_4.3.1_ _Tool Using_\n\nWhile recent LLMs have shown proficiency in solving various tasks, they still tend to make mistakes when handling\nlarge numerical values or executing intricate mathematical\ncalculations (Qian et al., 2022; She et al., 2023; Manikandan\net al., 2023; Liang et al., 2023b; Mialon et al., 2023). Thus\nequipping LLM agents with the capability to utilize tools\nhas been increasingly focused on. Commonly used methods\nmainly relied on human-curated data for training (Parisi\net al., 2022; Nakano et al., 2022; Qin et al., 2023c; Song\net al., 2023b) or prompt designing(Cai et", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_25650", "chunk_text": "-curated data for training (Parisi\net al., 2022; Nakano et al., 2022; Qin et al., 2023c; Song\net al., 2023b) or prompt designing(Cai et al., 2023; Shen\net al., 2023a; Hao et al., 2024). Recently, distillation-based\nmethods are also proposed (Schick et al., 2023; Zhang, 2023;\nPatil et al., 2023; Tang et al., 2023a; Qin et al., 2023a; Yuan\net al., 2023a; Gao et al., 2023b; Wang et al., 2024; Shen et al.,\n2024; Yuan et al., 2024b).\nToolformer (Schick et al., 2023) utilizes a self-supervised\nmanner, avoiding large human annotations, to obtain the\nmost required APIs to use and further distill this capability\nto the model itself. The performance of the GPT-J-based\nToolformer surpasses OPT (66B) (Zhang et al., 2022) and\nGPT3 (175B) (Brown et al., 2020) greatly. Graph-ToolFormer\n(Zhang, 2023) aims to equip LLMs with the ability to process\nand reason over complex graph data, which is designed\nto enhance LLMs with graph reasoning skills using external graph reasoning API tools by adopting ChatGPT to\nannotate and augment a larger graph reasoning statement\ndataset for training. Gorilla (Patil et al., 2023) addresses the\nlimitations of current LLMs in generating accurate input\narguments and reduces the problem of \u201dhallucination\u201d or\ngenerating incorrect API usage and it collects thousands of\nmodels from platforms like HuggingFace and Torch Hub\nas the API calls and utilizes GPT4 to generate synthetic\ninstruction data for training. GPT4Tools (Yang et al., 2023b)\nintroduces to enable open-source LLMs like LLaMA and\nOPT to use multimodal tools, a capability previously limited\nto advanced proprietary models like ChatGPT and GPT-4.\nThe approach involves generating an instruction-following\ndataset by prompting an advanced teacher model with multimodal contexts, using the Low-Rank Adaptation optimization", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_26100", "chunk_text": " tools, a capability previously limited\nto advanced proprietary models like ChatGPT and GPT-4.\nThe approach involves generating an instruction-following\ndataset by prompting an advanced teacher model with multimodal contexts, using the Low-Rank Adaptation optimization. ToolAlpaca (Tang et al., 2023a) proposes a framework\n\n\n\n18\n\n\naimed at enhancing the tool-use capabilities of compact\nlanguage models for embodied intelligence. It creates a\ndataset with 3938 instances from over 400 real-world tool\nAPIs across 50 categories and utilizes ChatGPT to generate\ndocumentation for each prompt for later training. ToolLLM\n(Qin et al., 2023a) proposes a comprehensive framework for\nenhancing LLMs with tool-use proficiency, focusing on data\ncreation, model training, and evaluation by distilling from\nchatGPT. Their ToolLLaMA shows impressive performance\nin executing complex instructions and handling new APIs,\nrivaling ChatGPT. CRAFT (Yuan et al., 2023a) builds a\ngeneral tool creation and retrieval framework, which utilizes GPT4 to generate code snippets as the created tools.\nDuring the inference, other small LLMs could select and\nretrieve from the generated code snippets to execute or\ngenerate other methods conditioned on the given snippets.\nConfucius (Gao et al., 2023b) introduces a tiered training\nstrategy for LLMs to master tool usage through a graduated\ncurriculum and an innovative method called Iterative Selfinstruction from Introspective Feedback (ISIF) for dynamic\ndataset enhancement to handle complex tools. MLLM-Tool\n(Wang et al., 2024) is a multi-modal tool agent capable\n\n- f interpreting instructions embedded in visual or audio\ncontent through the integration of multi-modal encoders\nwith open-source large language models. As a trainable\nmethod, the initial instruction-answer pairs are generated\nby utilizing GPT4. Shen et al. (2024) demonstrate that small\nLLMs are weak tool learners and proposes a multi-LLM\nframework that decomposes the tool-use ability of a single\nmodel into a planner, caller, and summarizer for the tool\nusing, leading to a supreme performance. The two-stage\ntraining strategy introduced by this work is powered by\nChatGPT and GPT4 for collecting execution trajectories for\nthe training set. Yuan et al", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_26550", "chunk_text": ", and summarizer for the tool\nusing, leading to a supreme performance. The two-stage\ntraining strategy introduced by this work is powered by\nChatGPT and GPT4 for collecting execution trajectories for\nthe training set. Yuan et al. (2024b) notice the potential\nissue of the current lengthy tool documentation, which\nhinders LLMs from understanding how to utilize a tool,\nthus proposing EASYTOOL to purify the important information from extensive documentation. The ground truth\nsummarization of the training documents is obtained by\nusing ChatGPT.\n\n\n_4.3.2_ _Planning_\nAnother important aspect for LLM agents is the ability to\ndecompose high-level tasks to a chosen set of actionable\nsteps (Huang et al., 2022b), which is especially useful when\nacting in interactive environments. Huang et al. (2022b) first\ndemonstrate that LLMs can generate plausible goal-driven\naction plans without training, introduces non-invasive tools\nto enhance model executability, and assesses these methods\nthrough human evaluation to balance executability and\nsemantic accuracy. Most existing methods utilize prompting\nstrategies for task planning (Singh et al., 2022; Zhou et al.,\n2023b; Song et al., 2023c; Wang et al., 2023g; Yao et al.,\n2023b; Liu et al., 2023j; Hao et al., 2023; Hu et al., 2023a), or\nbuilding human-curated data for training (Lin et al., 2023a;\nValmeekam et al., 2023). Recently, there have also been some\ndistilling methods emerging (Chen et al., 2023b; Zeng et al.,\n2023a; Yin et al., 2023a; Qiao et al., 2024; Kong et al., 2023).\nFireAct (Chen et al., 2023b) introduces an innovative approach for refining LLMs. This method involves fine-tuning\nsmaller-scale LLMs using agent trajectories that are derived\n\n\nfrom a variety of tasks and prompting techniques. Applying\nthis method with trajectories generated by GPT4 has been\nshown to consistently enhance performance. AgentTuning\n(Zeng et al., 2023a) aims to enhance the performance of\nLLMs in", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_27000", "chunk_text": " of tasks and prompting techniques. Applying\nthis method with trajectories generated by GPT4 has been\nshown to consistently enhance performance. AgentTuning\n(Zeng et al., 2023a) aims to enhance the performance of\nLLMs in executing agent tasks without sacrificing their\nwide-ranging capabilities. By utilizing a new dataset called\nAgentInstruct, which includes high-quality interaction trajectories, it applies a hybrid instruction-tuning approach\nthat merges these trajectories with general domain instructions. Lumos (Yin et al., 2023a) pertains to a novel framework designed to train agents using a unified data format\nand modular architecture based on open-source LLMs. This\nsystem comprises three key modules: planning, grounding,\nand execution, enabling the decomposition of tasks into\nsubgoals and actionable steps. TPTU-v2 (Kong et al., 2023)\nfocuses on improving the task planning and tool usage abilities of LLMs in real-world scenarios, by utilizing data generated by human experts or LLMs. It introduces a framework\ncomprising three components: an API Retriever, an LLM\nFinetuner, and a Demo Selector. AUTOACT (Qiao et al.,\n2024) proposes an agent learning framework that does not\nrequire large-scale annotated data or synthetic trajectories\nfrom high-resource models like GPT-4. Instead, it uses a selfinstruct method to generate its own planning trajectories\nwith limited initial data. It then applies a division-of-labor\nstrategy, creating sub-agents specialized in different aspects\n\n- f the task completion process.\nDistillation also works out for the training of embodied\nmulti-modal agents (Sumers et al., 2023; Yang et al., 2023c;\nMa et al., 2023a; Du et al., 2023a; Sumers et al., 2023). For\ninstance, Sumers et al. (2023) aim to enhance the ability of\nAI agents to follow instructions by using pretrained visionlanguage models to provide supervision for understanding\nand acting upon language within their operational environment, leveraging model distillation and hindsight experience replay to teach them contextually relevant interactions\nin a simulated 3D setting. Emma (Yang et al., 2023c) evaluates the challenges and inefficiency of training an embodied\nagent in a noisy visual world without expert guidance, and\nproposes to train them in a simulated", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_27450", "chunk_text": "in a simulated 3D setting. Emma (Yang et al., 2023c) evaluates the challenges and inefficiency of training an embodied\nagent in a noisy visual world without expert guidance, and\nproposes to train them in a simulated environment using\nimitation learning, guided by an expert Language Model\n(like ChatGPT), which operates in a corresponding textbased simulation, focusing on the same tasks.\n\n\n**4.4** **NLP Task Specialization**\n\n\nNLP tasks often grapple with challenges like data scarcity,\ninterpretability issues, privacy concerns, and noisy data.\nThe \u201cKnowledge\u201d section of our survey illustrates various\nmethods for distilling knowledge from LLMs, effectively\nsetting the stage for student models to adapt to a range\n\n- f NLP tasks. This knowledge provides supervision for\nthe training of student models through information augmentation (e.g., CoT and explanation), data augmentation,\nand semantic representation. By transferring the distilled\nknowledge from LLMs, student models can better handle\ndiverse NLP challenges, improving task performance and\naddressing data limitations more robustly.\n\n\n_4.4.1_ _Natural Language Understanding_\nNatural Language Understanding (NLU) is a fundamental NLP task that involves comprehending and interpret\n\n\n19\n\n\ning human language. The knowledge distilled from LLMs,\nsuch as through data labeling or augmentation, is typically transferred into encoder-based language models like\nBERT (Vaswani et al., 2017) and RoBERTa (Liu et al., 2019).\nRegarding the task of classification, certain studies have\nbeen noteworthy (Dai et al., 2023a; Gilardi et al., 2023; He\net al., 2023b; Gao et al., 2023a; Chenglin et al., 2023; Li\net al., 2023g). AugGPT (Dai et al., 2023a) focuses on both\ngeneral and clinical domain text classification. To address\nthe limitations of small-scale clinical datasets, which often\nlack expert annotation and are subject to stringent privacy\nregulations, AugGPT utilizes knowledge from teacher LLMs\nto rephrase each sentence in the training samples. This\nprocess creates multiple conceptually similar but semantically distinct samples, enhancing the dataset\u2019s richness\nand diversity. Another approach is demonstrated by Gilardi\net al. (", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_27900", "chunk_text": " teacher LLMs\nto rephrase each sentence in the training samples. This\nprocess creates multiple conceptually similar but semantically distinct samples, enhancing the dataset\u2019s richness\nand diversity. Another approach is demonstrated by Gilardi\net al. (2023), who employ ChatGPT as an annotator to categorize inputs. This method has been shown to outperform\ncrowd-workers in several tasks, including relevance, stance,\ntopics, and frame detection. Furthermore, He et al. (2023b)\npropose _Targeted Data Generation_ (TDG), a novel approach\nfor identifying challenging subgroups within a dataset. TDG\nleverages LLMs, along with human-in-the-loop, to generate\nnew data specifically tailored for these subgroups, thereby\nenriching the dataset and improving model performance\nin sentiment analysis and natural language inference tasks.\nTo facilitate the clinical information extraction task, Tang\net al. (2023b) elicit diverse samples from LLMs by providing\nexamples and different seeds of clinical entities, i.e. the\n_Curation_ manner.\n\nSeveral studies have also focused on multiple NLU\ntasks (Ding et al., 2023a; He et al., 2023a; Wang et al.,\n2021a; He et al., 2022; Ye et al., 2022; Meng et al., 2022).\nFor example, He et al. (2023a) utilize the knowledge in\nGPT-3.5 to annotate inputs with labels and explanations\nfor various NLU tasks, including user input and keyword\nrelevance assessment, BoolQ, and WiC. Wang et al. (2021a)\nemploy few-shot prompts to expand high-quality training\ndata using GPT-3, i.e. the _Expansion_ manner. Beyond merely\nemploying a single approach to elicit NLP task knowledge,\nDing et al. (2023a) explore a combination of _Labeling_, _Ex-_\n_pansion_, and _Curation_ methods to extract knowledge from\nGPT-3 for distilling data for both sequence- and token-level\nNLP tasks.\n\n\n_4.4.2_ _Natural Language Generation_\n\n\nNatural Language Generation (NLG) is a key aspect of evaluating the capabilities of LLMs, encompassing tasks such as\nsummarization, machine translation, and other open-ended", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_28350", "chunk_text": "_4.4.2_ _Natural Language Generation_\n\n\nNatural Language Generation (NLG) is a key aspect of evaluating the capabilities of LLMs, encompassing tasks such as\nsummarization, machine translation, and other open-ended\ntext generation tasks. Known for their potent generative\nabilities and creativity, LLMs excel in these areas, making\nthem prime sources for distilling knowledge into student\nmodels tailored for NLG tasks (Xu et al., 2023c, 2024b;\nRamnath et al., 2023; Agarwal et al., 2024). Additionally,\nthe knowledge distilled from LLMs can be effectively used\nfor NLG task-specific data augmentation (Jung et al., 2023;\nWang et al., 2021b; Guo et al., 2023a; Yang and Nicolai,\n2023; Wang et al., 2023h; Yang et al., 2023d). While the\nprevious sections have focused on the works about openended generation and multi-turn dialogue, this part will\n\n\nspecifically highlight the distillation techniques relevant to\n\n- ther NLG tasks.\nAlthough automatic metrics often favor smaller, finetuned models in summarization tasks, human evaluators\ntend to prefer the summaries generated by LLMs. Addressing this discrepancy, Xu et al. (2023c) develop a student summarization model by distilling a GPTSUMM dataset, which\ncomprises over 4 million paragraph-summary pairs generated by querying GPT-3.5. In a different approach, Jung et al.\n(2023) introduce \u2018Impossible Distillation,\u2019 a method that\ncreates high-quality summarization-specific dataset from\nweak teacher LLMs. This method involves training a student model on the generated dataset and enhancing its\ncapabilities through Self-Knowledge. Turning to the task of\nmachine translation, where creating parallel corpora is traditionally expensive and time-consuming, Yang and Nicolai\n(2023) propose a three-step distillation process. This process\ninvolves generating seeds of verbs and nouns, forming sentences, and then translating these sentences. Their findings\nsuggest that while the distilled dataset may lack diversity,\nit effectively improves the translation signal for training\nstudent translation models. To distill high-quality contentgrounded data automatically, Genie (Yehudai et al., 2024)\nproposes a", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_28800", "chunk_text": " that while the distilled dataset may lack diversity,\nit effectively improves the translation signal for training\nstudent translation models. To distill high-quality contentgrounded data automatically, Genie (Yehudai et al., 2024)\nproposes a general methodology containing three key steps:\n(a) preparation of the content, (b) distillation of responses\nfrom a teacher LLM corresponding to the content, and (c)\nfiltering mechanism to ensure the quality and faithfulness of\nthe generated data. Genie demonstrates that student models\ntrained through this distilled data can match or even surpass\nmodels trained on human-generated data.\n\n\n_4.4.3_ _Information Retrieval_\n\n\nInformation Retrieval (IR) represents a crucial branch of\ncomputer science, focused on efficiently retrieving information relevant to user queries from extensive repositories (Cai et al., 2022; Liu et al., 2022b; Feng et al., 2023;\nShen et al., 2023b). A typical IR system encompasses three\nmain components: the query rewriter, the retriever, and\nthe reranker. Recent studies have highlighted the effectiveness of employing LLMs in IR systems, e.g. in enhancing\nthe reranking stage through both point-wise and list-wise\nranking methods (Ma et al., 2023b; Sun et al., 2023a; Qin\net al., 2023d). However, the practical application of LLMs in\nIR systems faces challenges, primarily due to their slower\ngeneration speed, which conflicts with the low-latency requirements of IR tasks (Sun et al., 2023a). As a result,\nthe KD of LLMs emerges as a more promising approach\nfor IR, offering a way to infuse the distilled knowledge\nfrom LLMs into various stages of the IR pipeline without\ncompromising on speed. There has been a significant body\n\n- f work demonstrating how knowledge distilled from LLMs\ncan benefit each component of the IR system, including the\n_Query Rewriter_ (Srinivasan et al., 2022; Ma et al., 2023c), the\n_Retriever_ (Dai et al., 2023b; Sachan et al., 2022, 2023; Schick\nand Sch\u00a8utze, 2021; Meng et al., 2023; Peng et al., ", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_29250", "chunk_text": " (Dai et al., 2023b; Sachan et al., 2022, 2023; Schick\nand Sch\u00a8utze, 2021; Meng et al., 2023; Peng et al., 2023b), and\nthe _Reranker_ (Bonifacio et al., 2022; Sun et al., 2023a; Pradeep\net al., 2023a,b; Saad-Falcon et al., 2023; Ferraretto et al., 2023;\nJeronymo et al., 2023; Sun et al., 2023c).\n\n\n_**Query Rewriter.**_ The Query Rewriter (QR) is a pivotal component in IR systems, tasked with enhancing the precision\n\n\n\n20\n\n\nand expressiveness of user queries by refining or modifying\nthe initial query to more accurately align with the user\u2019s\ninformation needs. One notable approach is QUILL (Srinivasan et al., 2022), which introduces a two-stage distillation\nmethod for query intent understanding. Initially, a retrievalaugmented LLM, serving as the \u2018professor,\u2019 is distilled into\na non-retrieval augmented teacher LLM, aiming to bolster\nits understanding capabilities. Subsequently, this enhanced\nteacher LLM is distilled into a final student model using a\nlarge dataset, further refining the process. Incorporating the\nQR into IR systems, Ma et al. (2023c) develop a \u2019RewriteRetrieve-Read\u2019 framework. This process begins with an\nLLM rewriting the queries via prompting, followed by a\nretrieval-augmented reading stage. To integrate the rewritten queries effectively into the IR system, the knowledge\ngleaned from the LLM is distilled into a compact student\nrewriter. This rewriter is then fine-tuned using feedback\nfrom the LLM reader through reinforcement learning.\n\n\n_**Retriever and Reranker.**_ In IR systems, the Retriever is\ndesigned to efficiently locate the top-k relevant texts from\na large corpus. It encodes both queries and documents into\nvector representations and performs retrieval by computing\nthe dot product between these vectors. The Reranker further\nrefines the order of the retrieved documents to improve\nthe overall quality of the output. This is achieved in two\nprimary ways, including _Pointwise Reranker_ and", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_29700", "chunk_text": "\nthe dot product between these vectors. The Reranker further\nrefines the order of the retrieved documents to improve\nthe overall quality of the output. This is achieved in two\nprimary ways, including _Pointwise Reranker_ and _Listwise_\n_Reranker_ . Pointwise Reranker takes both the query and a\nsingle candidate document as input to directly generate a\nrelevance score. Listwise Reranker directly reorders a list of\ninput documents in terms of their relevance.\n\n_Retriever and Pointwise Reranker._ For the retriever and\npointwise reranker, a common application of KD from LLMs\nis the generation of pseudo-queries for given documents.\nThis approach aims to expand the pairwise data, enhancing\nthe training of dense retrievers or rerankers. For example,\nInPars (Bonifacio et al., 2022) utilizes GPT-3 to generate\nmultiple pseudo-queries for an unlabeled document. To\nensure the relevance of these queries, the system filters\nthem based on the highest log probabilities of generating a\nquery conditioned on the documents. Subsequently, InPars\nfine-tunes a reranker based on monoT5 (Raffel et al., 2020).\nAnother similar approach, Promptagator (Dai et al., 2023b),\nintroduces a few-shot dense retrieval method that leverages\na small number of demonstrations from the target domain\nfor pseudo-query generation. Diverging from the reliance\n\n- n unlabeled documents, Sachan et al. (2022) distill knowledge from GPT-4 to curate diverse synthetic data for text\nembedding tasks across nearly 100 languages. They finetune powerful decoder-only LLMs, such as Mistral-7b (Jiang\net al., 2023a), on this synthetic data using standard contrastive loss. Remarkably, this method demonstrates strong\nperformance on text embedding and multilingual retrieval\nbenchmarks without any labeled data. Beyond generating\npseudo-queries, teacher LLMs can also be employed to generate relevance scores as soft labels. These scores are used\nto train the retriever by minimizing the KL-divergence loss\nbetween the teacher and student distributions, as explored\nby Sachan et al. (2023).\n\n_Listwise Reranker._ A distinct set of studies focuses on\nlistwise reranking", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_30150", "chunk_text": " the retriever by minimizing the KL-divergence loss\nbetween the teacher and student distributions, as explored\nby Sachan et al. (2023).\n\n_Listwise Reranker._ A distinct set of studies focuses on\nlistwise reranking, where its advantage lies in compar\n\ning multiple documents simultaneously to determine the\n\n- ptimal reorder. RankGPT (Sun et al., 2023a) leverages\nGPT-4 to generate permutations for a group of candidate\npassages. To distill this listwise ranking knowledge into a\npointwise student reranker, various training loss functions\nare employed, such as Listwise Cross-Entropy (Bruch et al.,\n2019), RankNet (Burges et al., 2005), and LambdaLoss (Wang\net al., 2018). Building upon RankGPT\u2019s framework, RankVicuna (Pradeep et al., 2023a) and RankZephyr (Pradeep\net al., 2023b) further refine this approach by directly finetuning a listwise reranker using teacher-generated textual\npermutations. This enables the student reranker to produce\nsequences of ranked results directly, bypassing the intermediate step of calculating individual relevance scores.\n\n\n_4.4.4_ _Recommendation_\n\nRecommender systems are integral to enhancing user experience in various online services, providing personalized\ncontent based on user preferences and behaviors. Many\nworks have demonstrated that LLMs could be directly used\nas recommenders without fine-tuning (Wang et al., 2023i;\nDai et al., 2023c) or generate auxiliary textual features to\nbenefit recommender systems (Xi et al., 2023; Ren et al.,\n2023; Wei et al., 2024). (Wang et al., 2023j; Ren et al., 2023;\nWei et al., 2024). However, the real-time nature of online rec\n- mmender systems demands rapid response times, posing\na challenge with the inherent inference latency associated\nwith LLMs. To address this, several studies have explored\nways to distill and integrate the knowledge from LLMs into\nrecommender systems, thereby leveraging their advanced\ncapabilities while mitigating latency issues for efficient realtime recommendations (Mysore et al., 2023; Zhang et al.,\n2023b; Liu", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_30600", "chunk_text": " and integrate the knowledge from LLMs into\nrecommender systems, thereby leveraging their advanced\ncapabilities while mitigating latency issues for efficient realtime recommendations (Mysore et al., 2023; Zhang et al.,\n2023b; Liu et al., 2023c).\n\nMysore et al. (2023) tackle data scarcity in narrativedriven recommendation (NDR), where users provide detailed descriptions of their preferences. They utilize GPT-3\nto create synthetic narrative queries from user-item interactions via few-shot prompting, then distill this data into retrieval models for NDR. Similarly, GENRE (Liu et al., 2023c)\nemploys GPT-3.5 to augment datasets with new knowledge\nabout news summarization, user profiles, and personalized\ncontent, aiding the training of content-based recommendation models. To bridge the gap between language models\nand recommender systems, some research views behavior\nmodeling as an extension of language modeling (Cui et al.,\n2022; Liu et al., 2023k). InstructRec (Zhang et al., 2023b),\nfor instance, interprets recommendation as instruction following. They use ChatGPT to distill a wealth of userpersonalized instruction data reflecting diverse preferences\nand intentions based on real historical interactions. This\ndata is then used to fine-tune a 3B student language model\nspecifically for recommendation purposes.\n\n\n_4.4.5_ _Text Generation Evaluation_\n\nText generation evaluation, i.e. NLG evaluation, focuses on\nassessing the quality of generated content. Unlike traditional NLG evaluation metrics like BLEU (Papineni et al.,\n2002) or ROUGE (Lin, 2004), which primarily rely on\nsurface-level text comparisons, LLMs, trained on extensive\ncorpora and refined through techniques like RLHF, offer a\nmore human-aligned assessment. This sophistication has led\n\n\n\n21\n\n\nto the increasing use of LLMs in NLG evaluation (detailed\nfurther in (Li et al., 2024b)). Through KD of LLMs, student\nevaluators could enhance inference efficiency and achieve\nmore flexible and highly customized evaluation (Wang et al.,\n2023b; Kim et al., 2024; Xu et al., 2023d; Jiang et al., 2023c; Li\net al., ", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_31050", "chunk_text": " achieve\nmore flexible and highly customized evaluation (Wang et al.,\n2023b; Kim et al., 2024; Xu et al., 2023d; Jiang et al., 2023c; Li\net al., 2024a).\nPandaLM (Wang et al., 2023b) concentrates on a pairwise\nevaluator designed to compare two pieces of generated\ncontent. It utilizes a teacher LLM (GPT-3.5) to judge which\nresponse is better for a given instruction and input, providing reasons for its decision. Addressing the need for customized and flexible criteria to meet realistic user demands,\nPrometheus (Kim et al., 2024) distills GPT-4 to construct a\ntraining dataset that includes reference answers and a variety of customized scoring rubrics. This dataset is then used\nto tune LLaMA for evaluating model-generated responses.\nInstructscore (Xu et al., 2023d) takes a more fine-grained approach by using GPT-4 to create detailed analysis data. This\ndata is employed to tune LLaMA, enabling it to perform\nerror analysis on generated texts compared to reference\ntexts. The system further refines its evaluation capabilities\nthrough self-training with real model-generated responsereference pairs. For reference-free evaluation across diverse\ndomains, TigerScore (Jiang et al., 2023c) samples data from\na variety of text generation datasets, such as summarization, translation, and data-to-text. It distills error analysis\nknowledge from GPT-4 and uses this to fine-tune LLaMA.\nLastly, to adapt evaluation to real-world scenarios beyond\nconventional NLP tasks, Auto-J (Li et al., 2024a) collects\nreal-world user queries and their evaluations from a teacher\nLLM. This massive dataset of real-world scenarios is then\nused to distill evaluation knowledge into LLaMA through\nfine-tuning, enhancing its practical applicability.\n\n\n_4.4.6_ _Code_\n\nLLMs, trained on extensive corpora containing code, are\nhighlighted for their proficiency in code-related tasks. Their\ncapabilities extend beyond direct code generation to include\nthe provision of external knowledge and data, which is\ncrucial in distilling their expertise into smaller, more efficient models. Several works have successfully distilled code\nknowledge from LLMs into those compact and specialized", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_31500", "chunk_text": " beyond direct code generation to include\nthe provision of external knowledge and data, which is\ncrucial in distilling their expertise into smaller, more efficient models. Several works have successfully distilled code\nknowledge from LLMs into those compact and specialized\ncode models (Chaudhary, 2023; Rozi`ere et al., 2023; Gunasekar et al., 2023; Wei et al., 2023; Chen et al., 2023a;\nLiu et al., 2023d; Yu et al., 2024; Jain et al., 2023; Su and\nMcMillan, 2023; Guo et al., 2023d).\nA primary focus in these student code models is on\ncode generation, a task of both common utility and practical\nsignificance. For instance, Code Alpaca (Chaudhary, 2023)\nfine-tunes Llama using self-instruct with ChatGPT-distilled\ninstructions specifically for code generation tasks. Similarly,\nCode Llama-instruct (Rozi`ere et al., 2023) is fine-tuned via\nself-instruct, prompting Llama-2 (Touvron et al., 2023) with\ncoding problems, and further refined with unit tests. Phi1 (Gunasekar et al., 2023) aims to enhance the quality of distilled code data by extracting \u201ctextbook quality\u201d data from\na teacher LLM, incorporating Python textbook and exercise\ndata. Magicoder (Wei et al., 2023) addresses potential biases\nin teacher LLMs by referencing a wealth of open-source\ncode, yielding more diverse and grounded data for code\ngeneration. To consider the capability of the student model\n\n\nand leverage the feedback of the teacher, PERsD (Chen et al.,\n2023a) introduces a Personalized Distillation method where\nthe teacher LLM refines the student\u2019s generated code based\n\n- n the execution feedback of the executor.\nHowever, these models primarily target the code generation task, lacking generalizability across a broader range\n\n- f code-related tasks. To address this issue, MFTCoder (Liu\net al., 2023d) utilizes self-instruct to distill diverse code data\nfrom teacher LLMs for various tasks, such as code completion and text-to-code generation, training a student model\nvia", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_31950", "chunk_text": "FTCoder (Liu\net al., 2023d) utilizes self-instruct to distill diverse code data\nfrom teacher LLMs for various tasks, such as code completion and text-to-code generation, training a student model\nvia multi-task learning. WaveCoder (Yu et al., 2024), in\ncontrast, creates a comprehensive instruction tuning dataset\ncovering four universal code-related tasks distilled from\nGPT-3.5-turbo. WaveCoder first selects a diverse coreset of\nraw data using the KCenterGreedy (Sener and Savarese,\n2018) clustering method, then employs the teacher LLM\nfor generating task definitions and outputs. The teacher\nmodel also plays a role in evaluating and filtering this data.\nNotably, WaveCoder demonstrates superior generalization\nacross different code-related tasks compared to other opensource models.\n\n\n**4.5** **Multi-Modality**\n\n\nMultimodal Large Language Models (MLLMs) surpass traditional language-only LLMs by understanding and processing information across multiple modalities, more closely\nmirroring human perception and enabling a broader range\n\n- f real-world applications. There is a growing trend towards\ndeveloping MLLMs that follow multimodal instructions,\nfacilitating tasks with enhanced levels of interactivity. To address the scarcity of multimodal instruction-following data\nand to harness the commonsense and world knowledge\nembedded in teacher LLMs, numerous studies have focused\n\n- n multimodal knowledge distillation from LLMs (Liu et al.,\n2023e; Zhao et al., 2023b; Wang et al., 2023e; Chen et al.,\n2023c; Park et al., 2023; Pi et al., 2023; Zhao et al., 2023c; Liu\net al., 2023f; Wu et al., 2023b; Luo et al., 2023d; Jiang et al.,\n2023d; Li et al., 2023c; Xu et al., 2023e).\n\n\n_**Vision-Language.**_ In the vision-language domain,\nLLaVA (Liu et al., 2023e) pioneers the extension of the\nSelf-Instruct approach from the language to the multimodal\nfield. It translates images into textual descriptions,\nincluding captions and bounding boxes, and distills\nGPT-4 for generating new data in the", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_32400", "chunk_text": "3e) pioneers the extension of the\nSelf-Instruct approach from the language to the multimodal\nfield. It translates images into textual descriptions,\nincluding captions and bounding boxes, and distills\nGPT-4 for generating new data in the context of seed\nexamples. This approach creates a LLaVA-Instruct-150k\ndataset, which serves as the foundation for further\ndevelopments like LLaVA-1.5 (Liu et al., 2023l) and\nGPT4ROI (Zhang et al., 2023e), enhancing the instructionfollowing capabilities of MLLMs. To expand the dataset\u2019s\nscale, SVIT (Zhao et al., 2023b) introduces a 4.2 million\nimage dataset, distilled from GPT-4 by leveraging manual\nimage annotations. It employs a novel data recipe to select\nan informative, diverse, and balanced subset of training\ndata. LVIS-Instruct4V (Wang et al., 2023e) leverages GPT4V (OpenAI, 2023), a powerful large multimodal model,\nas a teacher to distill a more accurate and context-aware\ninstruction-following dataset, focusing - n fine-grained\nunderstanding. Further advancements include integrating\nspecific region referencing in image-based instruction\nfollowing. For instance, Shikra (Chen et al., 2023c) uses\n\n\n\n22\n\n\nGPT-4 to distill referential question-answer pairs from\nthe Flickr30K (Plummer et al., 2015) dataset, enhancing\nthe understanding of referential regions within images.\nLSKD (Park et al., 2023) introduces localized references\nto specific image regions, prompting the teacher LLM\nto generate commonsense inferences about these areas.\nTo enhance the visual instruction tuning pipeline with\ntext-rich images, LLaVAR (Zhang et al., 2023d) employs\nthe text-only GPT-4 as a teacher, using recognized texts\nand image captions to generate 16K conversation pairs for\ntext-rich images. The resultant student MLLM demonstrates\nenhanced interaction skills in content that combines both\ntext and imagery.\n\n\n_**Multiple**_ _**Modalities.**_ To extend knowledge distillation\n\n- f LLMs to encompass more modalities, such as audio\nand video, several innovative approaches have been introduced. These methods", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_32850", "chunk_text": "\ntext and imagery.\n\n\n_**Multiple**_ _**Modalities.**_ To extend knowledge distillation\n\n- f LLMs to encompass more modalities, such as audio\nand video, several innovative approaches have been introduced. These methods typically involve transforming\nthese modalities into a textual format comprehensible to\nteacher LLMs, followed by the distillation of the teacher.\nMacaw-LLM (Lyu et al., 2023) leverages GPT-4 to generate\ninstruction-response pairs corresponding to the content of\nimages or videos. MIMIC-IT (Li et al., 2023f) aims to broaden\nthe scope to language, image, and video understanding,\ncreating a substantial dataset with 2.8 million multimodal\ninstruction-response pairs distilled from ChatGPT. ChatBridge (Zhao et al., 2023d), on the other hand, represents\na novel approach in multimodal language modeling. It\ntranslates various non-textual modalities into text, combining fine-grained and global descriptions. This information\nis then used to distill responses from ChatGPT or GPT-4\nthrough an in-context learning process, effectively bridging\nthe gap between different modalities.\n\n\n_**Others.**_ Beyond distilling instruction-following data, several methods have emerged that concentrate on harnessing\ndifferent aspects of knowledge from LLMs. For instance,\nEMMA (Yang et al., 2023c) trains an MLLM to act as\nan embodied reflex agent within a visual environment.\nIt achieves this by distilling GPT-4\u2019s skills in a parallel\ntextual world, generating actions and providing reflective\nfeedback. Silkie (Li et al., 2023h) takes a unique approach by\ndistilling preferences from GPT-4V, focusing on criteria like\nhelpfulness and visual faithfulness. Ha et al. (2023) represent\nanother innovative direction, where it generates, labels,\nand distills diverse robot-centric exploration experiences by\nLLMs into a multi-task visuo-linguo-motor policy.\n\n\n**5** **DOMAIN-SPECIFIED VERTICAL DISTILLATION**\n\n\nThis section shifts from skill distillation to examine KD of\nLLMs in various vertical domains, including Law, Medical\n& Healthcare, Finance, and Science, etc. It delves into customizing distilled LLMs for these fields, showing its", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_33300", "chunk_text": "This section shifts from skill distillation to examine KD of\nLLMs in various vertical domains, including Law, Medical\n& Healthcare, Finance, and Science, etc. It delves into customizing distilled LLMs for these fields, showing its significant role in enhancing domain-specific AI applications. The\ntaxonomy of these works is shown in Figure 7.\n\n\n**5.1** **Law**\n\n\nLaw holds a crucial position in molding societies, overseeing human interactions, and ensuring justice prevails.\nInformed decision-making, legal interpretation, and the provision of legal advice by professionals hinge on precise\n\n\n23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 7: Taxonomy of Verticalization Distillation.\n\n\n\nand current information. Legal intelligent applications in\ndifferent scenarios usually require combinations of multiple\nfundamental capabilities of legal text retrieval, understanding, reasoning and generating (Zhang et al., 2023g; Sun,\n2023; Lai et al., 2023). To address challenges like legal terminology, subtle interpretations, and the constant evolution\n\n- f legislation presents distinctive challenges that demand\ncustomized resolutions. To handle the above challenges,\nseveral studies have investigated the customization of LLMs\nfor intelligent legal services (Cui et al., 2023b; Yue et al.,\n2023b; Huang et al., 2023b; Wu et al., 2023d). This involves\na continued pre-training process on extensive legal corpora,\nfollowed by fine-tuning with self-constructed instructions or\naugmented data using advanced LLMs.\n\n\nHuang et al. (2023b) have unveiled a Chinese legal\nlarge model named LawyerLLaMA. The model undergoes\nan initial pre-training phase on an extensive legal corpus,\nsystematically assimilating knowledge of the Chinese legal\nsystem. Subsequently, fine-tuning occurs through the analysis of objective questions from the Chinese National Judicial\nExamination (Zhong et al., 2020) and the gathering of responses to legal consultations using ChatGPT. This process\nequips the model with the ability to apply legal knowledge\nto specific scenarios. Cui et al. (2023b) present LawGPT,\nbuilt upon the foundation of OpenLLAMA. The model is\ntrained using a construction process that incorporates realworld legal text, legal regulations, judicial interpretations,\nand actual legal consultation data. Additionally, the authors\nutilize the ChatGPT API for assisted construction, enabling\n", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_33750", "chunk_text": " OpenLLAMA. The model is\ntrained using a construction process that incorporates realworld legal text, legal regulations, judicial interpretations,\nand actual legal consultation data. Additionally, the authors\nutilize the ChatGPT API for assisted construction, enabling\nthe generation of supplementary data derived from the\nexisting dataset. Wu et al. (2023d) have developed a largescale Chinese legal model (named Fuzi) with ChatGLM\nas its foundation. This model undergoes training on an\nextensive Chinese legal corpus, which incorporates unsupervised judicial language data, including diverse judgment\ndocuments and legal regulations. Additionally, it undergoes\nsupervised judicial fine-tuning with data encompassing legal QA and case retrieval. Fuzi\u2019s training also involves both\ngeneral instruction fine-tuning datasets, such as Alpaca,\nand domain-specific instruction fine-tuning datasets from\nLawyerLLaMA (Huang et al., 2023b) and LawGPT (Cui\net al., 2023b).\n\n\n\n**5.2** **Medical and Healthcare**\n\n\nThe integration of LLMs holds great potential for transforming medicine and healthcare. Extensive research has\nfocused on adapting general-purpose LLMs to the medical\ndomain (Singhal et al., 2023), such as electronic health\nrecords, and healthcare applications like patient care (Zhu\net al., 2023). Recent work has focused on enhancing medical instruction-following data with advanced teacher LLMs\nto better align with complex user instructions. Given the\nabundance of medical data, most studies combine realworld data with distilled instruction data from teacher\nLLMs (Zhang et al., 2023c; Xiong et al., 2023; Zhang et al.,\n2023f; Wang et al., 2023a; Li et al., 2023i; Han et al., 2023; Wu\net al., 2023f; Bao et al., 2023a; Chen et al., 2023d).\nWhile existing studies predominantly concentrate on\ntraining using dedicated medical dialogue datasets comprising medical textbooks (Wu et al., 2023e), biomedical\npapers (Luo et al., 2023e) medical knowledge-graphs (Bao\net al., 2023b), or authentic doctor-patient interactions (Bao\net al., 2023b),", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_34200", "chunk_text": "e), biomedical\npapers (Luo et al., 2023e) medical knowledge-graphs (Bao\net al., 2023b), or authentic doctor-patient interactions (Bao\net al., 2023b), an expanding body of research is delving into the augmentation of medical instruction-following\ndata with advanced LLMs to enhance the alignment with\npractical user instructions. Zhang et al. (2023c) introduce\nHuatuoGPT specifically tailored for medical consultations.\nThe model leverages both _distilled data from ChatGPT_ and\n_real-world data from doctors_ during the supervised finetuning stage. In a parallel effort, Xiong et al. (2023) construct a dataset of medical dialogues in Chinese, employing ChatGPT\u2019s assistance. Their methodology encompassed various techniques to train DoctorGLM, an easily\ndeployable LLM designed for tasks such as diagnoses,\ndrug recommendations, and other medical advice. Zhang\net al. (2023f) fine-tune LLaMA-series models using 52k\ndiverse, machine-generated, medical instruction-following\ndata named MedInstruct-52k. This effort resulted in the\ndevelopment of AlpaCare, a model demonstrating robust\nmedical proficiency and generalizability across both general\nand medical-specific domain free-form instruction evaluations. In a different vein, Wang et al. (2023a) propose\nHuaTuo, a LLaMA-based model that undergoes supervised\nfine-tuning with generated QA instances. This refinement\nprocess enhances the model\u2019s possession of more reliable\nmedical knowledge. Li et al. (2023i) introduce ChatDoctor,\nwhich was first trained as a generic conversation model\nbased on LLaMA. It utilized 52K instruction-following data\n\n\nfrom Stanford University\u2019s Alpaca project (Taori et al.,\n2023). Subsequently, the conversation model underwent\nfine-tuning on a dataset of 100K patient-physician conversations collected from an online medical consultation website. This two-step training process underscores the model\u2019s\nadaptability to diverse conversational contexts, particularly\nthose specific to patient-physician interactions.\nBuilt upon existing datasets, MedAlpaca (Han et al.,\n2023) proposes to reconstruct the data with GPT-3.5-Turbo,\nwhich is then used to fine-tune LLMs for effective", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_34650", "chunk_text": "physician interactions.\nBuilt upon existing datasets, MedAlpaca (Han et al.,\n2023) proposes to reconstruct the data with GPT-3.5-Turbo,\nwhich is then used to fine-tune LLMs for effective medical\napplications. Furthermore, PMC-LLaMA (Wu et al., 2023f)\nproposes a training framework (i.e., continual pre-training\nand domain-specific multi-task supervised fine-tuning) to\nadapt a general LLM to the medicine domain, where GPT4 is leveraged to write synonymous sentences for data\naugmentation in the SFT. To adapt LLMs to real-world\nmedical consultation, DISC-MedLLM (Bao et al., 2023a)\nleverages GPT-3.5 to 1) construct 50K QA pairs in a fewshot manner and 2) re-generate the 420k dialogues based\n\n- n real cases, which are then used to train LLMs in a\nsupervised fine-tuning manner. More recently, HuatuoGPTII (Chen et al., 2023d) proposes a one-stage training with\ninstruction-formatting unification of domain data collection\nfor medical adaption upon LLMs, where GPT-4 is used to\nformulate medical questions to fine-tuning instructions.\nThese diverse studies collectively contribute to the advancing field of the medical domain, facilitated by knowledge distillation from advanced LLMs. Through the exploration of various methodologies, these approaches provide valuable insights into the challenges and potential\nbreakthroughs at the intersection of cutting-edge language\nmodels and medical applications.\n\n\n**5.3** **Finance**\n\n\nThe application of LLMs to the finance domain (Xue et al.,\n2023) significantly transforms how financial data is analyzed, decisions are made, and customer interactions are\nmanaged. In finance, LLMs offer unprecedented capabilities in understanding complex financial documents, predicting market trends, and automating risk assessment,\nthus enabling more informed and faster decision-making\nprocesses. By processing and analyzing vast amounts of\nunstructured financial data, such as news articles, reports,\nand real-time market feeds, LLMs can identify patterns\nand insights that were previously inaccessible, leading to\nmore accurate forecasts and strategic financial planning.\nFurthermore, LLMs enhance customer experiences through\npersonalized financial advice, automated customer service,\nand sophisticated chat", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_35100", "chunk_text": " market feeds, LLMs can identify patterns\nand insights that were previously inaccessible, leading to\nmore accurate forecasts and strategic financial planning.\nFurthermore, LLMs enhance customer experiences through\npersonalized financial advice, automated customer service,\nand sophisticated chatbots that can handle complex queries.\nThis level of automation and insight has the potential to\nincrease efficiency, reduce operational costs, and improve\ncompliance and risk management practices in financial\ninstitutions, making LLMs a transformative force in the\nfinance sector. Knowledge distillation from a proprietary\nLLM is still under-explored, and most existing works focus\n\n- n adapting LLMs to finance applications by continual pretraining on finance-specific corpora (Wu et al., 2023g; Lu\net al., 2023) or fine-tuning in a supervised manner on multitask finance-specific instructions (Yang et al., 2023e; Xie\net al., 2023b; Wang et al., 2023k).\n\n\n\n24\n\n\nSpecifically, XuanYuan (Zhang and Yang, 2023) leverages self-instruct over seed data and self-QA over structured/unstructured data to generate instruction data in the\nfinance domain, which is used to train a finance LLM.\n\n\n**5.4** **Science**\n\n\nThe integration of LLMs into the science domain (Taylor\net al., 2022; Yin et al., 2023b) represents a paradigm shift\nin research, knowledge discovery, and the dissemination\n\n- f scientific information. In science, LLMs are leveraged to\ndigest and synthesize vast amounts of literature, aiding in\nthe identification of new research opportunities and the acceleration of scientific breakthroughs. They facilitate the understanding of complex scientific concepts by summarizing\nresearch papers, generating hypotheses, and even drafting\nresearch proposals and manuscripts, thus significantly reducing the time researchers spend on literature review and\nenabling them to focus more on experimental work. LLMs\nalso democratize access to scientific knowledge by providing layperson summaries of complex research findings,\nmaking science more accessible to non-experts and fostering\na broader public understanding of scientific advancements.\nBy enhancing the efficiency of research workflows and\nfostering interdisciplinary collaborations, LLMs are poised\nto accelerate the pace of scientific discovery and innovation\nacross various fields. To distill knowledge from an LLM,\nDARWIN Series (Xie et", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_35550", "chunk_text": " efficiency of research workflows and\nfostering interdisciplinary collaborations, LLMs are poised\nto accelerate the pace of scientific discovery and innovation\nacross various fields. To distill knowledge from an LLM,\nDARWIN Series (Xie et al., 2023a) utilizes a semi selfinstruct for instruction generation for science papers, which\nis then used to fine-tune an LLM. SciGLM (Zhang et al.,\n2024) proposes to train a scientific LLM, which prompts a\nteacher LLM to generate detailed answers for unlabelled\nscientific questions, as well as a self-reflective critic-andrevise to improve data quality. Besides the above knowledge\ndistillation methods to adapt LLMs to science, we will also\ndelve into how the distillation happens in sub-domains, e.g.,\nmathematics, astronautics, chemistry, etc.\n\n\n_**Mathematics.**_ The application of LLMs within the subdomain of mathematics heralds a transformative era in\nmathematical research, education, and problem-solving\n(Azerbayev et al., 2023; Yu et al., 2023b). LLMs in mathematics facilitate the exploration and understanding of complex\nmathematical theories and problems by providing intuitive\nexplanations, proofs, and solutions that can bridge the\ngap between advanced mathematical concepts and learners at various levels. These models have shown potential\nin conjecturing new mathematical theorems and patterns,\nthus opening new avenues for research and discovery that\nmight not have been readily accessible to humans alone.\nIn education, they serve as personalized tutors, offering\nstudents step-by-step guidance through mathematical problems and adapting explanations to the learner\u2019s level of understanding. This democratizes access to high-quality mathematical education and fosters a deeper appreciation and\nunderstanding of mathematics among a broader audience.\nBy enhancing collaborative efforts through the generation\n\n- f new ideas and the simplification of complex concepts,\nLLMs are poised to significantly advance the field of mathematics, making it more accessible, efficient, and innovative. WizardMath (Luo et al., 2023b) enhances the mathematical reasoning capabilities of Llama-2 by applying the\n\n\nnovel Reinforcement Learning from Evol-Instruct Feedback\n(RLEIF) method, significantly outperforming other opensource LLMs on the GSM8k and MATH benchmarks, as\nwell as surpassing several closed-source LLM", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_36000", "chunk_text": "\n\n\nnovel Reinforcement Learning from Evol-Instruct Feedback\n(RLEIF) method, significantly outperforming other opensource LLMs on the GSM8k and MATH benchmarks, as\nwell as surpassing several closed-source LLMs including\nChatGPT-3.5 and Minerva. MAmmoTH (Yue et al., 2023a) is\na series of open-source LLMS specifically developed for general math problem-solving, achieving superior performance\n\n- n nine mathematical reasoning datasets. Utilizing a novel\ninstruction tuning dataset called MathInstruct, which combines chain-of-thought and program-of-thought rationales,\nMAmmoTH models demonstrate substantial improvements\n\n- ver existing models. TORA (Gou et al., 2024), a series of\nTool-integrated Reasoning Agents, significantly advances\nmathematical problem-solving by combining natural language reasoning with the use of external computational\ntools. It markedly outperforms existing open-source models\n\n- n 10 mathematical reasoning datasets, showcasing notable\nimprovements over both rationale-based and programbased approaches, and introduces innovative training techniques such as output space shaping to enhance model reasoning capabilities. G-LLaVA (Gao et al., 2023c) introduces\na significant advancement in geometric problem-solving for\nLLMs by leveraging a multimodal approach that combines\ntext and image data. This model, utilizing the Geo170K\ndataset comprising over 170,000 geometric image-caption\nand question-answer pairs, demonstrates remarkable improvements over GPT-4V on the MathVista benchmark.\n\n\n_**Astronautics.**_ The application - f LLMs in astronautics (Nguyen et al., 2023) propels the field forward.\nAstroLLaMA-Chat (Perkowski et al., 2024) is an advancement of the AstroLLaMA model, leveraging a 7Bparameter LLaMA-2 model and targeted continual pretraining on a curated astronomy corpus to enhance performance in astronomy-focused question-answering. This\nmodel demonstrates significant improvements in specialized topic comprehension and introduces a chat-enabled\nversion for the astronomy community, highlighting the\neffectiveness of domain-specific knowledge distillation in\nachieving superior performance on specialized topics.\n\n\n_**Chemistry and Materials Science.**_ The integration of LLMs\ninto Chemistry and Materials Science has revolutionized\nthe way researchers approach the discovery and development of new", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_36450", "chunk_text": "-specific knowledge distillation in\nachieving superior performance on specialized topics.\n\n\n_**Chemistry and Materials Science.**_ The integration of LLMs\ninto Chemistry and Materials Science has revolutionized\nthe way researchers approach the discovery and development of new compounds and materials. By analyzing vast\ndatasets and scientific literature, LLMs can predict the properties and behaviors of substances, significantly accelerating\nthe innovation cycle.\nGIMLET (Zhao et al., 2023f), Graph Instruction based\nMolecuLe zEro-shoT learning, is a novel approach to\nmolecule property prediction that integrates graph and text\ndata within a single language model framework, aiming\nto improve instruction-based zero-shot learning for molecular tasks. By leveraging a transformer mechanism with\ngeneralized position embedding and decoupled attention,\nGIMLET significantly outperforms traditional molecule-text\nbaselines in zero-shot learning scenarios, demonstrating\nthe model\u2019s effectiveness in generalizing from instructions\nto a broad range of molecule-related tasks without prior\nexplicit task-specific training. LLM-Prop (Rubungo et al.,\n2023), leveraging the T5 model, showcases how LLMs can\n\n- utperform SoTA graph neural networks in predicting the\n\n\n\n25\n\n\nphysical and electronic properties of crystalline solids from\ntext descriptions. This approach underscores the potential of\ntext-based methods in materials science, offering significant\nimprovements in prediction accuracy while also contributing a benchmark dataset, TextEdge, to foster further research in this emerging field. InstructMol (Cao et al., 2023a)\nintegrates multi-modal data, aligning molecular structures\nwith natural language instructions for drug discovery tasks.\nThrough a novel two-stage instruction-tuning approach,\nit significantly enhances performance in molecule-related\ntasks, establishing a reliable molecular assistant that outperforms existing LLMs and reduces the performance gap with\nspecialized models. This demonstrates the value of multimodal integration in developing versatile tools for complex\ndomains like drug discovery.\n\n\n_**Biology.**_ In the field of Biology, particularly in the study\n\n- f proteins, DNA, and RNA, LLMs are revolutionizing our\nunderstanding of the fundamental molecules of life. By analyzing vast datasets of biological sequences and structures,\nLLMs can predict the three-dimensional shapes of proteins,\npotential functions, and interactions at a scale and speed\nbeyond traditional computational methods. This capability\nis critical for unraveling the complexities", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_36900", "chunk_text": " life. By analyzing vast datasets of biological sequences and structures,\nLLMs can predict the three-dimensional shapes of proteins,\npotential functions, and interactions at a scale and speed\nbeyond traditional computational methods. This capability\nis critical for unraveling the complexities of biological systems, advancing drug discovery by identifying targets and\ndesigning molecules with high precision, and understanding genetic diseases through the interpretation of genomic\nvariations.\n\nProt2Text (Abdine et al., 2023) introduces a novel multimodal framework for generating protein function descriptions in free text by combining GNNs and LLMs. This\napproach, which integrates structural and sequential protein\ninformation, highlights the transformative impact of knowledge distillation through the fusion of GNNs and LLMs\nfor accurate protein function prediction, potentially revolutionizing research in bioinformatics and biological sciences.\nBioMedGPT (Luo et al., 2023e) introduces a multimodal\ngenerative pre-trained transformer specifically designed for\nthe biomedicine domain, emphasizing the significance of\naligning molecular, protein, and natural language modalities to enhance biomedical question-answering, molecule,\nand protein QA tasks. This framework showcases the critical\nrole of knowledge distillation in bridging the gap between\ncomplex biological data and human language, thereby facilitating groundbreaking advancements in drug discovery\nand therapeutic target identification. xTrimoPGLM (Chen\net al., 2024e), a unified 100B-scale pre-trained transformer\nmodel, addresses both protein understanding and generation tasks by integrating autoencoding and autoregressive\npre-training objectives. Its significant advancements over\nexisting models in 18 protein understanding benchmarks\nand its capability in de novo protein sequence generation\nhighlight the model\u2019s importance in advancing the field of\nprotein science through knowledge distillation.\n\n\n_**Geography, Geology, and Environmental Science.**_ The integration of LLMs into Geography, Geology, and Environmental Science is revolutionizing these fields by enhancing data\nanalysis, predictive modeling, and interdisciplinary research\n(Roberts et al., 2023; Lin et al., 2023b; Wang et al., 2023l).\nK2 (Deng et al., 2023), the first-ever LLM specialized in\nthe geoscience domain, demonstrates the significant impact\n\n\n- f knowledge distillation in vertical domain specialization.\nBy adapting the general-domain LLaMA-7B model with a\n5", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_37350", "chunk_text": "., 2023), the first-ever LLM specialized in\nthe geoscience domain, demonstrates the significant impact\n\n\n- f knowledge distillation in vertical domain specialization.\nBy adapting the general-domain LLaMA-7B model with a\n5.5B token geoscience corpus and introducing the GeoSignal\ninstruction tuning dataset, K2 showcases enhanced performance in geoscience knowledge understanding and utilization. The model\u2019s development highlights a novel approach to efficiently gather domain-specific data and align\nmodel responses to specialized user queries. OceanGPT (Bi\net al., 2023), introduced as the first LLM for ocean science tasks, underscores the vital role of knowledge distillation in the vertical domain of oceanography. It leverages\nDOINSTRUCT, a novel framework for generating domainspecific instruction data through multi-agent collaboration,\nand establishes OCEANBENCH, a benchmark for evaluating LLMs in the ocean domain. MarineGPT (Zheng et al.,\n2023b) showcases the transformative potential of knowledge distillation in the marine domain by leveraging a\nnovel vision-language model tailored for marine science.\nUtilizing the Marine-5M dataset, which includes over 5\nmillion marine image-text pairs, MarineGPT excels in providing detailed, accurate, and domain-specific responses.\nGeoGalactica (Lin et al., 2024) represents a pioneering step\nin specializing LLMs for geoscience, leveraging a 30 billion\nparameter model pre-trained on a vast geoscience corpus.\nThis model is notable for being the largest of its kind within\nthe geoscience domain.\n\n\n**5.5** **Miscellaneous**\n\n\nKnowledge distillation of LLMs has vast potential across\nvarious verticals beyond the ones previously discussed,\nhighlighting their versatility and transformative impact\nacross different industries. For instance, in the education\nsector, EduChat (Dan et al., 2023) exemplifies a chatbot\nsystem that provides tailored support to teachers, students,\nand parents. KD is central to its design, leveraging pretraining on educational data followed by fine-tuning with\ncustom instructions to deliver capabilities such as essay\nevaluation and emotional support. Similarly, Owl (Guo\net al., 2023b), an LLM designed for IT operations, boosts\n\n- perational efficiency using the Owl-Instruct dataset, which\nis distilled from ChatGPT. By applying a mixture-of-adapter\n", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_37800", "chunk_text": " (Guo\net al., 2023b), an LLM designed for IT operations, boosts\n\n- perational efficiency using the Owl-Instruct dataset, which\nis distilled from ChatGPT. By applying a mixture-of-adapter\nstrategy for domain-specific tuning, it enhances analysis and\nperformance in IT-related tasks.\n\n\n**6** **OPEN PROBLEMS**\n\n\n_**Further Data Selection**_ How much data is required for LLM\ndistillation and how to filter out the low-quality data remain\n\n- pen-domain questions. In the field of instruction tuning,\n\n- ne of the most commonly used methods for distillation,\nZhou et al. (2023a) propose that only 1000 human-curated\nhigh-quality data is enough for the alignment of LLMs,\nhypothesizing that LLMs have learned the required knowledge from pretraining and only a small amount of data is\nrequired for the alignment. Its finding further raises a new\nquestion, how to automatically select the data for better\ndistillation? Chen et al. (2023e) directly apply ChatGPT to\nrate each data sample together with explanations, and then\nthe data is selected based on the rating. Cao et al. (2023b)\nsplit the existing instruction-tuning datasets and trains a\n\n\n\n26\n\n\nlinear function to select the most effective data based on\ntheir statistical properties. Li et al. (2023j) propose a data\nselection pipeline similar to self-distillation, in which the\nLLM firstly learns from a small subset of the data to get the\nbasic ability, and then further uses this learned model to rate\nfor the original dataset. Du et al. (2023b) propose to consider\nthree aspects including quality, coverage, and necessity for\nthe filtering process. Li et al. (2023k) select instruction data\nby evaluating their one-shot improvement on a hold-out\nset. Li et al. (2024f) recently propose Superfiltering, which is\nable to utilize small language models like GPT2 to filter out\nthe high-quality subset from a given high-quality dataset.\nDespite the emergence of these works working on data filtering, How to efficiently select the optimal distillation data\nfor LLMs, and How much data is required for distillation\nare still unsolved.\n\n\n_**Reduce the Distillation Cost (Lightweight Methods)**_ Despite the remarkable", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_38250", "chunk_text": " on data filtering, How to efficiently select the optimal distillation data\nfor LLMs, and How much data is required for distillation\nare still unsolved.\n\n\n_**Reduce the Distillation Cost (Lightweight Methods)**_ Despite the remarkable abilities of the latest LLMs, their significant resource requirements underscore the urgent need\nto find efficient solutions to overcome these challenges.\nCommon ways to further reduce the distillation cost include\nModel Compression and Efficient Fine-Tuning. In the realm\n\n- f Model Compression, Quantization (Frantar et al., 2023;\nDettmers et al., 2022; Kim et al., 2023c; Tao et al., 2022b; Yao\net al., 2022; Xiao et al., 2023), Parameter Pruning (Ma et al.,\n2023d; Zhang et al., 2023h; Frantar and Alistarh, 2023), and\nLow-Rank Approximation (Xu et al., 2023g; Li et al., 2023l)\nare commonly utilized. In the realm of Efficient Fine-Tuning,\nParameter Efficient Fine-Tuning (Hu et al., 2023b; Liu et al.,\n2022c; Wang et al., 2022b; Hu et al., 2021; Li and Liang,\n2021; Liu et al., 2022d), and Memory Efficient Fine-Tuning\n(Dettmers et al., 2023; Kim et al., 2023d; Malladi et al., 2024)\nare utilized. A detailed survey on Efficient Large Language\nModels can be found here in Wan et al. (2024b). The problem\nthat remains is how can we further compress the model and\nbuild effective distillation algorithms.\n\n\n_**Multi-Teacher Distillation**_ Most of the existing distilled\nmodels are distilled from a single teacher model, however, it is widely accepted that models trained with different sources of data have various capabilities. Thus a\nquestion arises: Is it possible to distill knowledge from\ndifferent teacher models into one student model? BabyLlama (Timiryasov and Tastet, 2023) proposes to distill the\nknowledge from both the GPT2 and LLaMA into the smallsize student models. Ensemble-Instruct (Lee et al., 2023b)\ntries to generate both instructions and responses en", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_38700", "chunk_text": ", 2023) proposes to distill the\nknowledge from both the GPT2 and LLaMA into the smallsize student models. Ensemble-Instruct (Lee et al., 2023b)\ntries to generate both instructions and responses ensembled\nfrom several different LLMs with RougeL as the indicator.\nFUSELLM (Wan et al., 2024a) externalizes the collective\nknowledge and unique strengths by leveraging the generative distributions of different LLMs aiming to train a student\nmodel beyond those of any individual source LLM. Despite\nthe recent progress in this topic, it still remains an underexplored topic.\n\n\n_**Explore Richer Knowledge from Teacher LLMs**_ As indicated\nin Table 3, the majority of teacher LLMs are closed-source\ndue to their advanced capabilities. Consequently, current\nmethodologies primarily focus on using the generations\nfrom these models as hard labels, training student models\nthrough simple supervised fine-tuning. However, beyond\n\n\nthe straightforward imitation of output behaviors via hard\nlabels, there is a growing interest in harnessing richer\nknowledge from teacher LLMs, including feedback and\nfeature knowledge, as well as exploring diverse combinations of knowledge elicitation methods. As highlighted in\nthe _Feedback_ section, teachers can provide various types of\nfeedback based on the student\u2019s outputs (Lee et al., 2023a;\nJiang et al., 2023b; Chen et al., 2023a). Similarly, the _Feature_\nsection discusses how knowledge based on features, such\nas logits serving as soft labels, can offer deeper, intrinsic\ninsights into the teacher model (Gu et al., 2024; Agarwal\net al., 2024). These explorations have demonstrated promising outcomes, suggesting that access to a broader spectrum\n\n- f knowledge can significantly enhance student model performance beyond what is achievable through simple SFT\ndistillation alone. This highlights the critical need for further\nresearch into varied knowledge extraction methods from\nteacher LLMs to augment the effectiveness of KD processes.\n\n\n_**Overcoming Catastrophic Forgetting During Distillation**_\nPrevious research has delved into the fine-tuning of LLMs\nto acquire the ability to follow instructions or transfer\nknowledge for forthcoming tasks, skills, or domains, leveraging advancements in LLM technology. Nevertheless, investigations have revealed that the continual fine-tuning of\nLLMs on", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_39150", "chunk_text": "-tuning of LLMs\nto acquire the ability to follow instructions or transfer\nknowledge for forthcoming tasks, skills, or domains, leveraging advancements in LLM technology. Nevertheless, investigations have revealed that the continual fine-tuning of\nLLMs on particular datasets (skills, domains) can lead to\na phenomenon known as catastrophic forgetting, wherein\npreviously acquired knowledge and problem-solving abilities for earlier tasks are compromised (Chen et al., 2023f;\nKotha et al., 2023; Koloski et al., 2023; Wu et al., 2024;\nLuo et al., 2023f). Earlier studies in machine learning and\ndeep learning have investigated various techniques to help\nmitigate forgetting during the fine-tuning or continue learning process, such as rehearsal, which entails periodically\nrevisiting and training on past data (Kirkpatrick et al., 2017;\nRostami et al., 2019; Rolnick et al., 2019), as well as regularization methods like elastic weight consolidation (Lee\net al., 2017), or dynamic architecture methods (Mallya et al.,\n2018; Wang et al., 2022c; Hu et al., 2023c; Chen et al., 2023f).\nTo address the challenges of catastrophic forgetting and to\nenhance the diversity of generated instructions in knowledge distillation for LLMs, Jiang et al. (2023b) randomly\nsample an instruction from the easy instructions and also\nprompt the generator to generate a new instruction that\nbelongs to the same domain as the sampled one. In a similar\nvein, Li et al. (2023m) study the problem of instructiontuning in multi-modal LLMs knowledge distillation and\nintroduce a competitive distillation framework. The model\ntries to produce new instructions that differ in content but\nare similar in difficulty to the original pictures in the multimodal augmentation phase, so as to alleviate catastrophic\nforgetting of the model and enhance the diversity of the\ninstruction tuning pool. Chen et al. (2023f) propose the\nLifelong-MoE (Mixture-of Experts) architecture based on\ngeneral language models, which dynamically adds model\ncapacity via adding experts with regularized pretraining.\nAdditionally, the model also introduces implicit regularization via distillation of the knowledge from old experts and\ngatings to effectively preserve old knowledge", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_39600", "chunk_text": ") architecture based on\ngeneral language models, which dynamically adds model\ncapacity via adding experts with regularized pretraining.\nAdditionally, the model also introduces implicit regularization via distillation of the knowledge from old experts and\ngatings to effectively preserve old knowledge. Zeng et al.\n(2023b) propose a new generative-based rehearsal method\nas Dirichlet Continual Learning (DCL). This method com\n\n\n27\n\n\nbines task distribution modeling and knowledge distillation\nto mitigate catastrophic forgetting without requiring access\nto the old data. To evaluate the effectiveness of instruction\ntuning in the context of continuous learning tasks, Zhang\net al. (2023i) introduce a more challenging yet practical\nproblem called Continual Instruction Tuning (CIT) and also\nestablish a benchmark suite consisting of learning and evaluation protocols. Although current research has explored\nsome simple methods to alleviate knowledge forgetting during model fine-tuning or knowledge distillation processes,\neffectively avoiding catastrophic forgetting across domains\nand skills remains a challenging issue. How to retain the\n\n- riginal model\u2019s capabilities effectively during knowledge\ndistillation or transfer processes is still a challenging problem.\n\n\n_**Trustworthy Knowledge Distillation**_ Trustworthiness in\nLLMs is paramount, encompassing attributes such as truthfulness, safety, fairness, robustness, privacy, and adherence\nto machine ethics (Sun et al., 2024a). The rapid advancement\n\n- f LLMs brings to the forefront concerns regarding their\ntrustworthiness, stemming from their complex outputs, the\nbiases present in vast training datasets, and the potential\ninclusion of private information. Current efforts in KD\n\n- f LLMs primarily focus on distilling various skills from\nLLMs, with relatively little attention paid to trustworthiness\naspects. Existing studies tend to concentrate on a subset of\ntrustworthiness aspects, such as helpfulness, honesty, and\nharmlessness (Bai et al., 2022a; Yang et al., 2024; Cui et al.,\n2023a). Consequently, in the distillation process, student\nmodels may inherit issues related to trustworthiness from\ntheir teacher LLMs. As assessed in Sun et al. (2024a), smaller\n\n- pen-source LLMs generally fall short of their proprietary\ncounterparts in trustworthiness metrics. Therefore, considering trustworthiness alongside the distillation of capabilities into student models", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_40050", "chunk_text": " As assessed in Sun et al. (2024a), smaller\n\n- pen-source LLMs generally fall short of their proprietary\ncounterparts in trustworthiness metrics. Therefore, considering trustworthiness alongside the distillation of capabilities into student models is crucial. It is imperative that\nfuture research on KD not only enhances the capabilities\n\n- f student models but also ensures that broader aspects of\ntrustworthiness are meticulously addressed.\n\n\n_**Weak-to-strong Distillation.**_ The concept of \u201cweak-tostrong generalization\u201d in LLMs (Burns et al., 2023) emphasizes the potential to leverage weak supervision to elicit\nthe advanced capabilities of more powerful models. This\napproach challenges the traditional distillation paradigm by\nsuggesting that even with limited or imperfect supervision,\nit is possible to enhance the performance of LLMs significantly. This necessitates exploring innovative strategies\nthat enable weaker models to guide the learning process\n\n- f stronger ones effectively, highlighting the importance\n\n- f developing methods that can bridge the gap between\nthese models. Such research could unlock new avenues\nfor improving LLMs\u2019 efficiency and effectiveness, making\nthe pursuit of \u201cweak-to-strong distillation\u201d a crucial area\nfor future investigations in this LLM era. Initially, Burns\net al. (2023) investigates whether weak model supervision\ncan unlock the full capabilities of much stronger models.\nThrough experiments with pre-trained language models in\nthe GPT-4 family across NLP, chess, and reward modeling\ntasks, it finds that finetuning strong models on weak labels\nleads to better performance than their weak supervisors,\ndemonstrating weak-to-strong generalization. Then, Li et al.\n\n\n(2024g) introduce Superfiltering, a method that employs\nsmaller, weaker models like GPT-2 to select high-quality\ndata for fine-tuning larger, more capable models such as\nLLaMA2. This approach is rooted in discovering a strong\nconsistency in evaluating instruction tuning data difficulty\nacross models of varying sizes. More recently, Ji et al. (2024)\nintroduce Aligner, a novel approach for aligning LLMs with\nhuman values and intentions by utilizing weak supervisory\nsignals from smaller models to improve the performance\n\n- f larger models. However, Burns et al. (2023) find that\nachieving the full capabilities of strong models requires\nmore than naive fin", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_40500", "chunk_text": " values and intentions by utilizing weak supervisory\nsignals from smaller models to improve the performance\n\n- f larger models. However, Burns et al. (2023) find that\nachieving the full capabilities of strong models requires\nmore than naive finetuning, suggesting the need for further\nresearch in this area. Therefore, open questions still remain\nabout 1) What are the theoretical and practical limits of\nweak-to-strong distillation? Can weak supervision reliably\nextract and enhance the full spectrum of capabilities in\nstronger models across all domains, or are there inherent\nlimitations based on model architecture or task specificity?\n2) How do we identify or design the optimal weak supervisors for distilling knowledge into stronger models? Is\nthere a framework or criteria to predict which weak models\nwould be most effective in guiding the learning process of\nmore complex models for specific tasks? 3) To what extent\nare weak-to-strong distillation techniques transferable and\nscalable across different sizes and types of models? How\ncan these methods be adapted to ensure efficacy and efficiency in distilling knowledge from very large models to\nsignificantly smaller ones, especially in resource-constrained\nenvironments?\n\n\n_**Self-Alignment.**_ Aligning LLMs traditionally relies heavily\n\n- n human or teacher LLMs to supply extensive preference\ndata. Consequently, the alignment of the student model\nis limited by the quantity of distilled preference data and\nthe teacher\u2019s capabilities. Self-alignment offers a promising\nalternative, aiming to enhance alignment beyond the constraints of teacher-provided preferences. In self-alignment,\nthe student model endeavors to autonomously improve\nand align its responses with desired behaviors, including\ngenerating model-written feedback, critiques, and explanations. Several studies have explored utilizing the student\nmodel\u2019s inherent capabilities to generate knowledge for\nalignment (Bai et al., 2022a; Sun et al., 2024b; Li et al., 2024c;\nYuan et al., 2024a). Beyond merely producing improved\nresponses (Bai et al., 2022a; Sun et al., 2024b), implementations of self-alignment include employing the student as\nits reward model to offer feedback (Yuan et al., 2024a), a\nstrategy that merges _Self-Knowledge_ with _Feedback_ methods\n\n- f eliciting knowledge. We advocate for increasingly leveraging the student model itself to provide feedback,", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_40950", "chunk_text": " model to offer feedback (Yuan et al., 2024a), a\nstrategy that merges _Self-Knowledge_ with _Feedback_ methods\n\n- f eliciting knowledge. We advocate for increasingly leveraging the student model itself to provide feedback, thereby\nenhancing self-alignment capabilities. This approach not\n\n- nly facilitates moving beyond traditional human/teacher\npreference-based rewards but also opens avenues for continual self-improvement and alignment.\n\n\n**7** **CONCLUSION AND DISCUSSION**\n\n\nThis survey has explored the diverse landscape of knowledge distillation for LLMs, highlighting key techniques,\napplications, and challenges. KD plays a crucial role in\ndemocratizing access to advanced LLM capabilities, providing cutting-edge advancements without the high costs\n\n\n\n28\n\n\n- f training and deployment. Our review emphasizes vari\n- us KD approaches, from algorithmic innovations to skill\nenhancement and vertical distillation. Notably, data augmentation and synthesis within KD emerge as vital tools\nfor improving distillation, revealing the powerful synergy\nbetween enriched training data and effective model distillation. As the AI landscape evolves, rapid advancements\nin model architectures and training methods present both\nchallenges and research opportunities for KD of LLMs.\nFuture innovation will need to focus on achieving efficiency,\ntransparency, and ethics while maintaining model trustworthiness. Furthermore, promising areas such as weakto-strong generalization, self-alignment, and multi-modal\nLLMs offer the potential to enhance the capabilities of\ndistilled models. In conclusion, the KD of LLMs is set to play\na pivotal role in the future of AI research. As highlighted\nin this survey, sustained research efforts will be critical in\ndeveloping accessible, efficient, and responsible AI for all.\nImportantly, when conducting KD of LLMs like ChatGPT\n\n- r Llama, it\u2019s essential to comply with the model providers\u2019\nterms [4], such as the restrictions on developing competitive\nproducts.\n\n\n**REFERENCES**\n\n\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray\n_et al._, \u201cTraining language models to follow instructions\nwith human feedback,\u201d _Advances in Neural Information_\n_Processing Systems_, vol. 35, pp. 27 730\u201327 744,", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_41400", "chunk_text": " Slama, A. Ray\n_et al._, \u201cTraining language models to follow instructions\nwith human feedback,\u201d _Advances in Neural Information_\n_Processing Systems_, vol. 35, pp. 27 730\u201327 744, 2022.\nOpenAI, :, J. Achiam, S. Adler, S. Agarwal, L. Ahmad,\nI. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,\nS. Altman, S. Anadkat, R. Avila, I. Babuschkin, S. Balaji,\nV. Balcom, P. Baltescu, H. Bao, M. Bavarian, J. Belgum,\nI. Bello, J. Berdine, G. Bernadett-Shapiro, C. Berner, L. Bogdonoff, O. Boiko, M. Boyd, A.-L. Brakman, G. Brockman,\nT. Brooks, M. Brundage, K. Button, T. Cai, R. Campbell,\nA. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan,\nC. Chang, F. Chantzis, D. Chen, S. Chen, R. Chen, J. Chen,\nM. Chen, B. Chess, C. Cho, C. Chu, H. W. Chung,\nD. Cummings, J. Currier, Y. Dai, C. Decareaux, T. Degry,\nN. Deutsch, D. Deville, A. Dhar, D. Dohan, S. Dowling,\nS. Dunning, A. Ecoffet, A. Eleti, T. Eloundou, D. Farhi,\nL. Fedus, N. Felix, S. P. Fishman, J. Forte, I. Fulford,\nL. Gao, E. Georges, C. Gibson, V. Goel, T. Gogineni,\nG. Goh, R. Gontijo-Lopes, J. Gordon, M. Grafstein, S. Gray,\nR. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han,\nJ. Harris, Y. He, M. Heaton, J. Heidecke, C", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_41850", "chunk_text": " Gray,\nR. Greene, J. Gross, S. S. Gu, Y. Guo, C. Hallacy, J. Han,\nJ. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse,\nA. Hickey, W. Hickey, P. Hoeschele, B. Houghton, K. Hsu,\nS. Hu, X. Hu, J. Huizinga, S. Jain, S. Jain, J. Jang, A. Jiang,\nR. Jiang, H. Jin, D. Jin, S. Jomoto, B. Jonn, H. Jun, T. Kaftan, \u0141ukasz Kaiser, A. Kamali, I. Kanitscheider, N. S.\nKeskar, T. Khan, L. Kilpatrick, J. W. Kim, C. Kim, Y. Kim,\nH. Kirchner, J. Kiros, M. Knight, D. Kokotajlo, \u0141ukasz\nKondraciuk, A. Kondrich, A. Konstantinidis, K. Kosic,\nG. Krueger, V. Kuo, M. Lampe, I. Lan, T. Lee, J. Leike,\nJ. Leung, D. Levy, C. M. Li, R. Lim, M. Lin, S. Lin,\nM. Litwin, T. Lopez, R. Lowe, P. Lue, A. Makanju, K. Malfacini, S. Manning, T. Markov, Y. Markovski, B. Mar\n\n[4. OpenAI Business Terms: https://openai.com/policies/business-](https://openai.com/policies/business-terms)\n[terms](https://openai.com/policies/business-terms)\n\n\ntin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney, C. McLeavey, P. McMillan, J. McNeil, D. Medina,\nA. Mehta, J. Menick, L. Metz, A. Mishchenko, P. Mishkin,\nV. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati,\nO. Murk, D. M\u00b4ely, A", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_42300", "chunk_text": " Metz, A. Mishchenko, P. Mishkin,\nV. Monaco, E. Morikawa, D. Mossing, T. Mu, M. Murati,\nO. Murk, D. M\u00b4ely, A. Nair, R. Nakano, R. Nayak, A. Neelakantan, R. Ngo, H. Noh, L. Ouyang, C. O\u2019Keefe, J. Pachocki, A. Paino, J. Palermo, A. Pantuliano, G. Parascandolo, J. Parish, E. Parparita, A. Passos, M. Pavlov, A. Peng,\nA. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P.\nde Oliveira Pinto, Michael, Pokorny, M. Pokrass, V. Pong,\nT. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach,\nC. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli,\nT. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnurr,\nJ. Schulman, D. Selsam, K. Sheppard, T. Sherbakov,\nJ. Shieh, S. Shoker, P. Shyam, S. Sidor, E. Sigler, M. Simens,\nJ. Sitkin, K. Slama, I. Sohl, B. Sokolowsky, Y. Song,\nN. Staudacher, F. P. Such, N. Summers, I. Sutskever,\nJ. Tang, N. Tezak, M. Thompson, P. Tillet, A. Tootoonchian,\nE. Tseng, P. Tuggle, N. Turley, J. Tworek, J. F. C. Uribe,\nA. Vallone, A. Vijayvergiya, C. Voss, C. Wainwright, J. J.\nWang, A. Wang, B. Wang, J. Ward, J. Wei", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_43200", "chunk_text": "mans, M. Bosma, F. Xia, E. Chi,\nQ. V. Le, D. Zhou _et al._, \u201cChain-of-thought prompting\nelicits reasoning in large language models,\u201d _Advances in_\n_Neural Information Processing Systems_, vol. 35, pp. 24 824\u2013\n24 837, 2022.\nX. Xu, C. Tao, T. Shen, C. Xu, H. Xu, G. Long, and J. guang\nLou, \u201cRe-reading improves reasoning in large language\nmodels,\u201d 2024.\nP. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu,\nM. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar,\nB. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove,\nC. D. Manning, C. R\u00b4e, D. Acosta-Navas, D. A. Hudson,\nE. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren,\nH. Yao, J. Wang, K. Santhanam, L. J. Orr, L. Zheng,\nM. Y\u00a8uksekg\u00a8on\u00a8ul, M. Suzgun, N. Kim, N. Guha, N. S.\nChatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi,\nS. M. Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard,\nT. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang,\nand Y. Koreeda, \u201cHolistic evaluation  - f language\nmodels,\u201d _CoRR_, vol. abs/2211.09110, 2022. [Online].\n[Available: https://doi.org/10.48550/arXiv.2211.09110](https://doi.org/10.48550/arXiv.2211.09110)\nX. Wu, R. Duan, and J. Ni, \u201cUnveiling security, privacy,\nand ethical concerns of chatgpt,\u201d _Journal of Information and_\n\n\n\n29\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_43650", "chunk_text": "50/arXiv.2211.09110)\nX. Wu, R. Duan, and J. Ni, \u201cUnveiling security, privacy,\nand ethical concerns of chatgpt,\u201d _Journal of Information and_\n\n\n\n29\n\n\n_Intelligence_, 2023.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,\nY. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale,\nD. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull,\nD. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao,\nV. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou,\nH. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann,\nA. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee,\nD. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov,\nP. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor,\nA. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang,\nA. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic,\nS. Edunov, and T. Scialom, \u201cLlama 2: Open foundation\nand fine-tuned chat models,\u201d 2023.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S.\nChaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saul", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_45450", "chunk_text": ", \u201cOrca: Progressive learning from\ncomplex explanation traces of gpt-4,\u201d _arXiv preprint_\n_arXiv:2306.02707_, 2023.\nB. Ding, C. Qin, L. Liu, Y. K. Chia, B. Li, S. Joty, and L. Bing,\n\u201cIs GPT-3 a good data annotator?\u201d in _ACL (1)_ . Association for Computational Linguistics, 2023, pp. 11 173\u2013\n11 195.\nS. Chaudhary, \u201cCode alpaca: An instruction-following\n[llama model for code generation,\u201d https://github.com/](https://github.com/sahil280114/codealpaca)\n[sahil280114/codealpaca, 2023.](https://github.com/sahil280114/codealpaca)\nH. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, and\nT. Liu, \u201cHuatuo: Tuning llama model with chinese medical knowledge,\u201d _arXiv preprint arXiv:2304.06975_, 2023.\n_LawGPT_ . GitHub, 2023.\nD. Zhang, Z. Hu, S. Zhoubian, Z. Du, K. Yang, Z. Wang,\nY. Yue, Y. Dong, and J. Tang, \u201cSciglm: Training\nscientific language models with self-reflective instruction\nannotation and tuning,\u201d _CoRR_, vol. abs/2401.07950, 2024.\n\n[[Online]. Available: https://doi.org/10.48550/arXiv.2401.](https://doi.org/10.48550/arXiv.2401.07950)\n[07950](https://doi.org/10.48550/arXiv.2401.07950)\nW.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,\nL. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez,\nI. Stoica, and E. P. Xing, \u201cVicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality,\u201d March 202", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_45900", "chunk_text": " J. E. Gonzalez,\nI. Stoica, and E. P. Xing, \u201cVicuna: An open-source chatbot\nimpressing gpt-4 with 90%* chatgpt quality,\u201d March 2023.\n\n[[Online]. Available: https://lmsys.org/blog/2023-03-30-](https://lmsys.org/blog/2023-03-30-vicuna/)\n[vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)\nC. Xu, Q. Sun, K. Zheng, X. Geng, P. Zhao, J. Feng, C. Tao,\nand D. Jiang, \u201cWizardlm: Empowering large language\nmodels to follow complex instructions,\u201d _arXiv preprint_\n_arXiv:2304.12244_, 2023.\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\nB. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen,\nZ. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y.\nNie, and J.-R. Wen, \u201cA survey of large language models,\u201d\n2023.\nX. He, Z. Lin, Y. Gong, A. Jin, H. Zhang, C. Lin, J. Jiao, S. M.\nYiu, N. Duan, W. Chen _et al._, \u201cAnnollm: Making large\nlanguage models to be better crowdsourced annotators,\u201d\n_arXiv preprint arXiv:2303.16854_, 2023.\nY. Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang,\nR. Xie, J. Wang, X. Xie, W. Ye, S. Zhang, and Y. Zhang,\n\u201cPandalm: An automatic evaluation benchmark for llm\ninstruction tuning optimization,\u201d 2023.\nC. Hsieh, C. Li, C. Yeh, H. Nakhost, Y. Fujii, A. Ratner,\nR. Krishna, C. Lee, and T. Pfister, \u201c", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_47250", "chunk_text": " Jul. 2023, pp. 8506\u20138520. [Online]. Available:\n[https://aclanthology.org/2023.acl-long.474](https://aclanthology.org/2023.acl-long.474)\nN. Ding, Y. Chen, B. Xu, Y. Qin, S. Hu, Z. Liu, M. Sun,\nand B. Zhou, \u201cEnhancing chat language models by scaling\nhigh-quality instructional conversations,\u201d in _EMNLP_ . Association for Computational Linguistics, 2023, pp. 3029\u2013\n3051.\nS. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D.\nGiorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa,\nO. Saarikivi, A. Salim, S. Shah, H. S. Behl, X. Wang,\nS. Bubeck, R. Eldan, A. T. Kalai, Y. T. Lee, and Y. Li,\n\u201cTextbooks are all you need,\u201d 2023.\nY. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and\nY. T. Lee, \u201cTextbooks are all you need ii: phi-1.5 technical\nreport,\u201d _arXiv preprint arXiv:2309.05463_, 2023.\n_Phi-2:_ _The_ _surprising_ _power_ _of_ _small_ _lan-_\n\n\n_guage_ _models_, December 2023. [Online]. Avail[able: https://www.microsoft.com/en-us/research/blog/](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)\n[phi-2-the-surprising-power-of-small-language-models/](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)\nY. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang, \u201cMagicoder:\nSource code is all you need,\u201d 2023.\nZ. Yu, X. Zhang, N. Shang, Y. Huang, C. Xu, Y", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_48150", "chunk_text": " with no performance penalty,\u201d in\n_Proceedings of the BabyLM Challenge at the 27th Conference_\n\n_on Computational Natural Language Learning_, A. Warstadt,\nA. Mueller, L. Choshen, E. Wilcox, C. Zhuang, J. Ciro,\nR. Mosquera, B. Paranjabe, A. Williams, T. Linzen,\nand R. Cotterell, Eds. Singapore: Association for\nComputational Linguistics, Dec. 2023, pp. 279\u2013289.\n\n[[Online]. Available: https://aclanthology.org/2023.conll-](https://aclanthology.org/2023.conll-babylm.24)\n[babylm.24](https://aclanthology.org/2023.conll-babylm.24)\nC. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu,\nP. Luo, and N. Wong, \u201cCompression of generative pretrained language models via quantization,\u201d _arXiv preprint_\n_arXiv:2203.10705_, 2022.\nZ. Liu, B. Oguz, C. Zhao, E. Chang, P. Stock, Y. Mehdad,\nY. Shi, R. Krishnamoorthi, and V. Chandra, \u201cLlm-qat:\nData-free quantization aware training for large language\nmodels,\u201d _arXiv preprint arXiv:2305.17888_, 2023.\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones,\nA. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, C. Chen,\nC. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-Johnson, E. Perez, J. Kerr, J. Mueller,\nJ. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt,\nM. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer, S. Johnston,\nS. Kr", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_49500", "chunk_text": " K. Konyushkova,\nL. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang,\nC. Gu, W. Macherey, A. Doucet, O. Firat, and N. de Freitas,\n\u201cReinforced self-training (rest) for language modeling,\u201d\n2023.\n\nE. Zelikman, Y. Wu, J. Mu, and N. D. Goodman, \u201cStar: Bootstrapping reasoning with reasoning,\u201d in _NeurIPS_, 2022.\nV. Sanh, L. Debut, J. Chaumond, and T. Wolf, \u201cDistilbert,\na distilled version of bert: smaller, faster, cheaper and\nlighter,\u201d _arXiv preprint arXiv:1910.01108_, 2019.\nY. Wen, Z. Li, W. Du, and L. Mou, \u201cf-divergence\nminimization for sequence-level knowledge distillation,\u201d\nin _Proceedings of the 61st Annual Meeting of the Association_\n_for Computational Linguistics (Volume 1: Long Papers)_,\nA. Rogers, J. Boyd-Graber, and N. Okazaki, Eds. Toronto,\nCanada: Association for Computational Linguistics, Jul.\n2023, pp. 10 817\u201310 834. [Online]. Available: [https:](https://aclanthology.org/2023.acl-long.605)\n[//aclanthology.org/2023.acl-long.605](https://aclanthology.org/2023.acl-long.605)\nC. Liang, S. Zuo, Q. Zhang, P. He, W. Chen, and T. Zhao,\n\u201cLess is more: Task-aware layer-wise distillation for language model compression,\u201d in _International Conference on_\n_Machine Learning_ . PMLR, 2023, pp. 20 852\u201320 867.\nM. Kwon, S. M. Xie, K. Bullard, and D. Sadigh, \u201cReward design with language models,\u201d in _ICLR_ . OpenReview.net,\n2023.\nB. Peng, C. Li, P. He, M. Galley, and J. Gao, \u201cInstruction\ntuning with gpt-4,\u201d 2023.\nG. Li", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_50850", "chunk_text": ". Keller,\nN. Momchev, S. Ramos Garea, P. Stanczyk, N. Vieillard,\nO. Bachem, G. Elidan, A. Hassidim, O. Pietquin,\nand I. Szpektor, \u201cFactually consistent summarization\nvia reinforcement learning with textual entailment\nfeedback,\u201d in _Proceedings of the 61st Annual Meeting of_\n_the Association for Computational Linguistics (Volume 1:_\n_Long Papers)_, A. Rogers, J. Boyd-Graber, and N. Okazaki,\nEds. Toronto, Canada: Association for Computational\nLinguistics, Jul. 2023, pp. 6252\u20136272. [Online]. Available:\n[https://aclanthology.org/2023.acl-long.344](https://aclanthology.org/2023.acl-long.344)\nY. Yang, E. Chern, X. Qiu, G. Neubig, and P. Liu, \u201cAlignment\nfor honesty,\u201d _arXiv preprint arXiv:2312.07000_, 2023.\nR. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai,\nD. Yang, and S. Vosoughi, \u201cTraining socially aligned language models on simulated social interactions,\u201d 2023.\nT. Schick, J. Dwivedi-Yu, R. Dess`\u0131, R. Raileanu, M. Lomeli,\nL. Zettlemoyer, N. Cancedda, and T. Scialom, \u201cToolformer: Language models can teach themselves to use\ntools,\u201d 2023.\nJ. Zhang, \u201cGraph-toolformer: To empower llms with graph\nreasoning ability via prompt augmented by chatgpt,\u201d\n\n\n\n32\n\n\n_arXiv preprint arXiv:2304.11116_, 2023.\nS. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, \u201cGorilla:\nLarge language model connected with massive apis,\u201d\n2023.\nQ. Tang, Z. Deng, H. Lin, X. Han, Q. Liang, B. Cao, and\nL. Sun, \u201cToolalpaca: Generalized tool learning for language models with 3000 simulated cases,\u201d ", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_52200", "chunk_text": "PT,\u201d in\n_Findings of the Association for Computational Linguistics:_\n_EMNLP 2023_, H. Bouamor, J. Pino, and K. Bali, Eds.\nSingapore: Association for Computational Linguistics,\n[Dec. 2023, pp. 13 879\u201313 892. [Online]. Available: https:](https://aclanthology.org/2023.findings-emnlp.927)\n[//aclanthology.org/2023.findings-emnlp.927](https://aclanthology.org/2023.findings-emnlp.927)\nF. Xu, W. Shi, and E. Choi, \u201cRECOMP: Improving retrievalaugmented LMs with context compression and selective\naugmentation,\u201d in _The Twelfth International Conference_\n\n_on Learning Representations_, 2024. [Online]. Available:\n\n\n[https://openreview.net/forum?id=mlJLVigNHp](https://openreview.net/forum?id=mlJLVigNHp)\nS. Ramnath, B. Joshi, S. Hallinan, X. Lu, L. H. Li, A. Chan,\nJ. Hessel, Y. Choi, and X. Ren, \u201cTailoring self-rationalizers\nwith multi-reward distillation,\u201d 2023.\nS. Wang, Y. Liu, Y. Xu, C. Zhu, and M. Zeng,\n\u201cWant to reduce labeling cost? GPT-3 can help,\u201d in\n_Findings of the Association for Computational Linguistics:_\n_EMNLP_ _2021_, M.-F. Moens, X. Huang, L. Specia,\nand S. W.-t. Yih, Eds. Punta Cana, Dominican\nRepublic: Association for Computational Linguistics,\n[Nov. 2021, pp. 4195\u20134205. [Online]. Available: https:](https://aclanthology.org/2021.findings-emnlp.354)\n[//aclanthology.org/2021.findings-emnlp.354](https://aclanthology.org/2021.findings-emnlp.354)\nZ. Guo, P. Wang, Y. Wang, and S. Yu, \u201cImproving small\nlanguage models on pubmedqa via generative data augmentation,\u201d 2023.\nW. Yang and G. Nicolai, \u201c", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_52650", "chunk_text": ")\nZ. Guo, P. Wang, Y. Wang, and S. Yu, \u201cImproving small\nlanguage models on pubmedqa via generative data augmentation,\u201d 2023.\nW. Yang and G. Nicolai, \u201cNeural machine translation data\ngeneration and augmentation using chatgpt,\u201d 2023.\nK. Srinivasan, K. Raman, A. Samanta, L. Liao, L. Bertelli,\nand M. Bendersky, \u201cQUILL: Query intent with large\nlanguage models using retrieval augmentation and\nmulti-stage distillation,\u201d in _Proceedings_ _of_ _the_ _2022_\n_Conference on Empirical Methods in Natural Language_\n_Processing:_ _Industry_ _Track_, Y. Li and A. Lazaridou,\nEds. Abu Dhabi, UAE: Association for Computational\nLinguistics, Dec. 2022, pp. 492\u2013501. [Online]. Available:\n[https://aclanthology.org/2022.emnlp-industry.50](https://aclanthology.org/2022.emnlp-industry.50)\nZ. Dai, V. Y. Zhao, J. Ma, Y. Luan, J. Ni,\nJ. Lu, A. Bakalov, K. Guu, K. B. Hall, and\nM. Chang, \u201cPromptagator: Few-shot dense retrieval\nfrom 8 examples,\u201d in _The_ _Eleventh_ _International_\n_Conference on Learning Representations, ICLR 2023, Kigali,_\n_Rwanda,_ _May_ _1-5,_ _2023_, 2023. [Online]. Available:\n[https://openreview.net/pdf?id=gmL46YMpu2J](https://openreview.net/pdf?id=gmL46YMpu2J)\nR. Meng, Y. Liu, S. Yavuz, D. Agarwal, L. Tu, N. Yu, J. Zhang,\nM. Bhat, and Y. Zhou, \u201cAugtriever: Unsupervised dense\nretrieval by scalable data augamentation,\u201d 2023.\nW. Sun, L. Yan, X. Ma, S. Wang, P. Ren, Z. Chen, D. Yin, and\nZ. Ren, \u201cIs chatgpt good at search? investigating large\n", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_53100", "chunk_text": " 2023.\nW. Sun, L. Yan, X. Ma, S. Wang, P. Ren, Z. Chen, D. Yin, and\nZ. Ren, \u201cIs chatgpt good at search? investigating large\nlanguage models as re-ranking agents,\u201d 2023.\nR. Pradeep, S. Sharifymoghaddam, and J. Lin, \u201cRankvicuna:\nZero-shot listwise document reranking with open-source\nlarge language models,\u201d 2023.\n\n- \u2014, \u201cRankzephyr: Effective and robust zero-shot listwise\nreranking is a breeze!\u201d 2023.\nF. Ferraretto, T. Laitz, R. Lotufo, and R. Nogueira,\n\u201cExaranker: Synthetic explanations improve neural\nrankers,\u201d in _Proceedings of the 46th International ACM SIGIR_\n_Conference on Research and Development in Information_\n_Retrieval_, ser. SIGIR \u201923. New York, NY, USA: Association\nfor Computing Machinery, 2023, p. 2409\u20132414. [Online].\n[Available: https://doi.org/10.1145/3539618.3592067](https://doi.org/10.1145/3539618.3592067)\nS. Mysore, A. Mccallum, and H. Zamani, \u201cLarge language\nmodel augmented narrative driven recommendations,\u201d\nin _Proceedings of the 17th ACM Conference on Recommender_\n_Systems_, ser. RecSys \u201923. New York, NY, USA: Association\nfor Computing Machinery, 2023, p. 777\u2013783. [Online].\n[Available: https://doi.org/10.1145/3604915.3608829](https://doi.org/10.1145/3604915.3608829)\nJ. Zhang, R. Xie, Y. Hou, W. X. Zhao, L. Lin, and J.-R.\nWen, \u201cRecommendation as instruction following: A large\nlanguage model empowered recommendation approach,\u201d\n2023.\n\n\n\n33\n\n\nQ. Liu, N. Chen, T. Sakai, and X.-M. Wu, \u201cOnce: Boosting content-based recommendation with both open- and\nclosed-source large language models,\u201d 2023.\nS. Kim, J. Shin, Y", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_54450", "chunk_text": " He, and T. Huang, \u201cSvit: Scaling up\nvisual instruction tuning,\u201d 2023.\nJ. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y.-G. Jiang,\n\u201cTo see is to believe: Prompting gpt-4v for better visual\ninstruction tuning,\u201d 2023.\nK. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao,\n\u201cShikra: Unleashing multimodal llm\u2019s referential dialogue\nmagic,\u201d 2023.\nJ. S. Park, J. Hessel, K. R. Chandu, P. P. Liang, X. Lu,\nP. West, Y. Yu, Q. Huang, J. Gao, A. Farhadi, and Y. Choi,\n\u201cLocalized symbolic knowledge distillation for visual\ncommonsense models,\u201d 2023.\nR. Pi, J. Gao, S. Diao, R. Pan, H. Dong, J. Zhang, L. Yao,\nJ. Han, H. Xu, L. Kong, and T. Zhang, \u201cDetGPT:\nDetect what you need via reasoning,\u201d in _Proceedings_\n\n_of the 2023 Conference on Empirical Methods in Natural_\n_Language Processing_, H. Bouamor, J. Pino, and K. Bali, Eds.\nSingapore: Association for Computational Linguistics,\n[Dec. 2023, pp. 14 172\u201314 189. [Online]. Available: https:](https://aclanthology.org/2023.emnlp-main.876)\n[//aclanthology.org/2023.emnlp-main.876](https://aclanthology.org/2023.emnlp-main.876)\n\n\nL. Zhao, E. Yu, Z. Ge, J. Yang, H. Wei, H. Zhou, J. Sun,\nY. Peng, R. Dong, C. Han, and X. Zhang, \u201cChatspot:\nBootstrapping multimodal llms via precise referring instruction tuning,\u201d 2023.\nF. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang,\n\u201cMitigating hallucination in large multi-modal models via\nrobust instruction tuning,\u201d 2023.\nS. Wu, H", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_54900", "chunk_text": " K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang,\n\u201cMitigating hallucination in large multi-modal models via\nrobust instruction tuning,\u201d 2023.\nS. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, \u201cNext-gpt: Anyto-any multimodal llm,\u201d 2023.\nR. Luo, Z. Zhao, M. Yang, J. Dong, D. Li, P. Lu, T. Wang,\nL. Hu, M. Qiu, and Z. Wei, \u201cValley: Video assistant with\nlarge language model enhanced ability,\u201d 2023.\nY. Jiang, E. Schoop, A. Swearngin, and J. Nichols, \u201cIluvui:\nInstruction-tuned language-vision modeling of uis from\nmachine conversations,\u201d 2023.\nY. Li, C. Zhang, G. Yu, Z. Wang, B. Fu, G. Lin, C. Shen,\nL. Chen, and Y. Wei, \u201cStablellava: Enhanced visual instruction tuning with synthesized image-dialogue data,\u201d\n2023.\nR. Xu, X. Wang, T. Wang, Y. Chen, J. Pang, and D. Lin,\n\u201cPointllm: Empowering large language models to understand point clouds,\u201d 2023.\nQ. Huang, M. Tao, Z. An, C. Zhang, C. Jiang, Z. Chen,\nZ. Wu, and Y. Feng, \u201cLawyer llama technical report,\u201d\n_arXiv preprint arXiv:2305.15062_, 2023.\nJ. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, \u201cChatlaw: Opensource legal large language model with integrated external knowledge bases,\u201d _arXiv preprint arXiv:2306.16092_,\n2023.\nH. Zhang, J. Chen, F. Jiang, F. Yu, Z. Chen, G. Chen,\nJ. Li, X. Wu, Z. Zhiyi, Q. Xiao, X. Wan, B. Wang,\nand H. Li, \u201cHuatuoGPT, towards taming language\nmodel to be a doctor,\u201d in _Findings of the Association_\n_for Computational Linguistics: EMN", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_55800", "chunk_text": "3583780.3615285](https://doi.org/10.1145/3583780.3615285)\nT. Xie, Y. Wan, W. Huang, Z. Yin, Y. Liu, S. Wang,\nQ. Linghu, C. Kit, C. Grazian, W. Zhang, I. Razzak,\nand B. Hoex, \u201cDARWIN series: Domain specific\nlarge language models for natural science,\u201d _CoRR_,\n[vol. abs/2308.13565, 2023. [Online]. Available: https:](https://doi.org/10.48550/arXiv.2308.13565)\n[//doi.org/10.48550/arXiv.2308.13565](https://doi.org/10.48550/arXiv.2308.13565)\nY. Dan, Z. Lei, Y. Gu, Y. Li, J. Yin, J. Lin, L. Ye, Z. Tie,\nY. Zhou, Y. Wang, A. Zhou, Z. Zhou, Q. Chen, J. Zhou,\nL. He, and X. Qiu, \u201cEduchat: A large-scale language\n\n\n\n34\n\n\nmodel-based chatbot system for intelligent education,\u201d\n_CoRR_, vol. abs/2308.02773, 2023. [Online]. Available:\n[https://doi.org/10.48550/arXiv.2308.02773](https://doi.org/10.48550/arXiv.2308.02773)\nH. Guo, J. Yang, J. Liu, L. Yang, L. Chai, J. Bai, J. Peng, X. Hu,\nC. Chen, D. Zhang, X. Shi, T. Zheng, L. Zheng, B. Zhang,\nK. Xu, and Z. Li, \u201cOWL: A large language model for IT\n\n - perations,\u201d _CoRR_, vol. abs/2309.09298, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2309.09298](https://doi.org/10.48550/arXiv.2309.09298)\nY. Kim and A. M. Rush, \u201cSequence-level knowledge distillation,\u201d _arXiv preprint arXiv", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_56700", "chunk_text": "neffe, and I. V. Meza Ruiz, Eds.\nSeattle, United States: Association for Computational\nLinguistics, Jul. 2022, pp. 4602\u20134625. [Online]. Available:\n[https://aclanthology.org/2022.naacl-main.341](https://aclanthology.org/2022.naacl-main.341)\nZ. Li, X. Xu, T. Shen, C. Xu, J.-C. Gu, and C. Tao, \u201cLeveraging\nlarge language models for nlg evaluation: A survey,\u201d 2024.\nS. Li, J. Chen, Y. Shen, Z. Chen, X. Zhang, Z. Li, H. Wang,\nJ. Qian, B. Peng, Y. Mao, W. Chen, and X. Yan, \u201cExplanations from large language models make small reasoners\nbetter,\u201d 2022.\nN. Ho, L. Schmid, and S. Yun, \u201cLarge language models\nare reasoning teachers,\u201d in _ACL (1)_ . Association for\nComputational Linguistics, 2023, pp. 14 852\u201314 882.\nL. C. Magister, J. Mallinson, J. Adamek, E. Malmi,\nand A. Severyn, \u201cTeaching small language models to\nreason,\u201d in _Proceedings of the 61st Annual Meeting of the_\n_Association for Computational Linguistics (Volume 2: Short_\n_Papers)_, A. Rogers, J. Boyd-Graber, and N. Okazaki,\nEds. Toronto, Canada: Association for Computational\nLinguistics, Jul. 2023, pp. 1773\u20131781. [Online]. Available:\n[https://aclanthology.org/2023.acl-short.151](https://aclanthology.org/2023.acl-short.151)\nY. Fu, H. Peng, L. Ou, A. Sabharwal, and T. Khot, \u201cSpecializing smaller language models towards multi-step reasoning,\u201d 2023.\nL. H. Li, J. Hessel, Y. Yu, X. Ren, K.-W. Chang, and Y. Choi,\n\u201cSymbolic chain-of-thought distillation: Small models can\nalso \u201cthink\u201d step-by-step,\u201d in _", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_57600", "chunk_text": "for Computational Linguistics, 2023, pp. 4086\u20134107.\nY. Ji, Y. Deng, Y. Gong, Y. Peng, Q. Niu, L. Zhang, B. Ma, and\nX. Li, \u201cExploring the impact of instruction data scaling on\nlarge language models: An empirical study on real-world\nuse cases,\u201d 2023.\nM. Wu, A. Waheed, C. Zhang, M. Abdul-Mageed, and A. F.\nAji, \u201cLamini-lm: A diverse herd of distilled models from\nlarge-scale instructions,\u201d 2023.\nW. Guo, J. Yang, K. Yang, X. Li, Z. Rao, Y. Xu, and D. Niu,\n\u201cInstruction fusion: Advancing prompt evolution through\nhybridization,\u201d 2023.\nY. Yu, Y. Zhuang, J. Zhang, Y. Meng, A. Ratner, R. Krishna,\nJ. Shen, and C. Zhang, \u201cLarge language model as attributed training data generator: A tale of diversity and\nbias,\u201d 2023.\nF. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi,\n\u201cKnowledge fusion of large language models,\u201d in _The_\n_Twelfth International Conference on Learning Representations_,\n[2024. [Online]. Available: https://openreview.net/forum?](https://openreview.net/forum?id=jiDsk12qcz)\n[id=jiDsk12qcz](https://openreview.net/forum?id=jiDsk12qcz)\nQ. Zhao and B. Zhu, \u201cTowards the fundamental\nlimits  - f knowledge transfer  - ver finite domains,\u201d\nin _NeurIPS 2023 Workshop on Mathematics of Modern_\n_Machine_ _Learning_, 2023. [Online]. Available: [https:](https://openreview.net/forum?id=9qxoXqxa0N)\n[//openreview.net/forum?id=9qxoXqxa0N](https://openreview.net/forum?id=9qxoXqxa0N)\nC. Qin, W. Xia, F. Jiao, and S. Joty, \u201cImproving in-context\nlearning via bidirectional alignment,\u201d 2023.\nN. Boizard, K. El-Hadd", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_58050", "chunk_text": "xa0N)\nC. Qin, W. Xia, F. Jiao, and S. Joty, \u201cImproving in-context\nlearning via bidirectional alignment,\u201d 2023.\nN. Boizard, K. El-Haddad, C. Hudelot, and P. Colombo,\n\u201cTowards cross-tokenizer distillation: the universal logit\ndistillation loss for llms,\u201d _arXiv preprint arXiv:2402.12030_,\n2024.\nQ. Zhong, L. Ding, L. Shen, J. Liu, B. Du, and D. Tao, \u201cRevisiting knowledge distillation for autoregressive language\nmodels,\u201d 2024.\nM. Kim, S. Lee, J. Lee, S. Hong, D.-S. Chang, W. Sung,\nand J. Choi, \u201cToken-scaled logit distillation for ternary\nweight generative language models,\u201d _arXiv_ _preprint_\n_arXiv:2308.06744_, 2023.\nZ. Chen, K. Zhou, W. X. Zhao, J. Wan, F. Zhang, D. Zhang,\nand J.-R. Wen, \u201cImproving large language models via finegrained reinforcement learning with minimum editing\nconstraint,\u201d 2024.\n\n\n\n35\n\n\nG. Guo, R. Zhao, T. Tang, X. Zhao, and J.-R. Wen, \u201cBeyond\nimitation: Leveraging fine-grained quality signals for\nalignment,\u201d in _The_ _Twelfth_ _International_ _Conference_\n\n_on Learning Representations_, 2024. [Online]. Available:\n[https://openreview.net/forum?id=LNLjU5C5dK](https://openreview.net/forum?id=LNLjU5C5dK)\nZ. Allen-Zhu and Y. Li, \u201cTowards understanding ensemble,\nknowledge distillation and self-distillation in deep learning,\u201d _arXiv preprint arXiv:2012.09816_, 2020.\nT. Zheng, S. Guo, X. Qu, J. Guo, W. Zhang, X. Du, C. Lin,\nW. Huang, W. Chen, J. Fu _et al._, \u201cKun: Answer polishment for chinese self-alignment with instruction backtranslation,\u201d _arXiv preprint arXiv:", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_59400", "chunk_text": " and\nJ. Tetreault, Eds. Online: Association for Computational\nLinguistics, Jul. 2020, pp. 2158\u20132170. [Online]. Available:\n[https://aclanthology.org/2020.acl-main.195](https://aclanthology.org/2020.acl-main.195)\nX. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang,\nand Q. Liu, \u201cTinyBERT: Distilling BERT for natural\nlanguage understanding,\u201d in _Findings of the Association for_\n\n\n_Computational Linguistics: EMNLP 2020_, T. Cohn, Y. He,\nand Y. Liu, Eds. Online: Association for Computational\nLinguistics, Nov. 2020, pp. 4163\u20134174. [Online]. Available:\n[https://aclanthology.org/2020.findings-emnlp.372](https://aclanthology.org/2020.findings-emnlp.372)\nL. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, and\nQ. Liu, \u201cDynabert: Dynamic bert with adaptive width and\ndepth,\u201d _Advances in Neural Information Processing Systems_,\nvol. 33, pp. 9782\u20139793, 2020.\nS. Zuo, Q. Zhang, C. Liang, P. He, T. Zhao, and W. Chen,\n\u201cMoebert: from bert to mixture-of-experts via importanceguided adaptation,\u201d _arXiv preprint arXiv:2204.07675_, 2022.\nK. J. Liang, W. Hao, D. Shen, Y. Zhou, W. Chen, C. Chen, and\nL. Carin, \u201cMixkd: Towards efficient distillation of largescale language models,\u201d in _9th International Conference on_\n_Learning Representations, ICLR 2021, Virtual Event, Austria,_\n_May 3-7, 2021_ . OpenReview.net, 2021. [Online]. Available:\n[https://openreview.net/forum?id=UFGEelJkLu5](https://openreview.net/forum?id=UFGEelJkLu5)\nY. J. Ma, W. Liang", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_59850", "chunk_text": "1. [Online]. Available:\n[https://openreview.net/forum?id=UFGEelJkLu5](https://openreview.net/forum?id=UFGEelJkLu5)\nY. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu, L. Fan, and A. Anandkumar, \u201cEureka:\nHuman-level reward design via coding large language\nmodels,\u201d 2023.\nJ.-C. Pang, P. Wang, K. Li, X.-H. Chen, J. Xu, Z. Zhang, and\nY. Yu, \u201cLanguage model self-improvement by reinforcement learning contemplation,\u201d 2023.\nY. Du, O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel,\nA. Gupta, and J. Andreas, \u201cGuiding pretraining in\nreinforcement learning with large language models,\u201d\nin _Proceedings of the 40th International Conference on_\n_Machine Learning_, ser. Proceedings of Machine Learning\nResearch, A. Krause, E. Brunskill, K. Cho, B. Engelhardt,\nS. Sabato, and J. Scarlett, Eds., vol. 202. PMLR,\n23\u201329 Jul 2023, pp. 8657\u20138677. [Online]. Available:\n[https://proceedings.mlr.press/v202/du23f.html](https://proceedings.mlr.press/v202/du23f.html)\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and\nO. Klimov, \u201cProximal policy optimization algorithms,\u201d\n2017.\n\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn, \u201cDirect preference optimization: Your\nlanguage model is secretly a reward model,\u201d 2023.\nF. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang,\n\u201cPreference ranking optimization for human alignment,\u201d\n_arXiv preprint arXiv:2306.17492_, 2023.\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S.", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_60300", "chunk_text": " H. Wang,\n\u201cPreference ranking optimization for human alignment,\u201d\n_arXiv preprint arXiv:2306.17492_, 2023.\nZ. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and\nF. Huang, \u201cRrhf: Rank responses to align language models with human feedback without tears,\u201d _arXiv preprint_\n_arXiv:2304.05302_, 2023.\nM. Li, L. Chen, J. Chen, S. He, and T. Zhou,\n\u201cReflection-tuning: Recycling data for better instructiontuning,\u201d in _NeurIPS 2023 Workshop on Instruction Tuning_\n_and_ _Instruction_ _Following_, 2023. [Online]. Available:\n[https://openreview.net/forum?id=xaqoZZqkPU](https://openreview.net/forum?id=xaqoZZqkPU)\nM. Li, L. Chen, J. Chen, S. He, J. Gu, and T. Zhou, \u201cSelective\nreflection-tuning: Student-selected data recycling for\n[llm instruction-tuning,\u201d 2024. [Online]. Available: https:](https://api.semanticscholar.org/CorpusID:267682220)\n[//api.semanticscholar.org/CorpusID:267682220](https://api.semanticscholar.org/CorpusID:267682220)\nX. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel,\nS. Levine, and D. Song, \u201cKoala: A dialogue model\nfor academic research,\u201d Blog post, April 2023. [Online].\nAvailable: [https://bair.berkeley.edu/blog/2023/04/03/](https://bair.berkeley.edu/blog/2023/04/03/koala/)\n[koala/](https://bair.berkeley.edu/blog/2023/04/03/koala/)\nM. Li, J. Chen, L. Chen, and T. Zhou, \u201cCan llms speak\n\n\n\n36\n\n\nfor diverse people? tuning llms via debate to generate\ncontrollable controversial statements,\u201d 2024.\nM. Kang, S. Lee, J. Baek, K. Kawaguchi, and S. J. Hwang,\n", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_60750", "chunk_text": "36\n\n\nfor diverse people? tuning llms via debate to generate\ncontrollable controversial statements,\u201d 2024.\nM. Kang, S. Lee, J. Baek, K. Kawaguchi, and S. J. Hwang,\n\u201cKnowledge-augmented reasoning distillation for small\nlanguage models in knowledge-intensive tasks,\u201d 2023.\nR. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li, and Y. Shan,\n\u201cGpt4tools: Teaching large language model to use tools\nvia self-instruction,\u201d 2023.\nA. Yehudai, B. Carmeli, Y. Mass, O. Arviv, N. Mills,\nA. Toledo, E. Shnarch, and L. Choshen, \u201cGenie: Achieving\nhuman parity in content-grounded datasets generation,\u201d\n2024.\nY. Zhang, R. Zhang, J. Gu, Y. Zhou, N. Lipka, D. Yang, and\nT. Sun, \u201cLlavar: Enhanced visual instruction tuning for\ntext-rich image understanding,\u201d 2023.\nC. Lyu, M. Wu, L. Wang, X. Huang, B. Liu, Z. Du, S. Shi, and\nZ. Tu, \u201cMacaw-llm: Multi-modal language modeling with\nimage, audio, video, and text integration,\u201d _arXiv preprint_\n_arXiv:2306.09093_, 2023.\nB. Li, Y. Zhang, L. Chen, J. Wang, F. Pu, J. Yang, C. Li,\nand Z. Liu, \u201cMimic-it: Multi-modal in-context instruction\ntuning,\u201d 2023.\nZ. Zhao, L. Guo, T. Yue, S. Chen, S. Shao, X. Zhu, Z. Yuan,\nand J. Liu, \u201cChatbridge: Bridging modalities with large\nlanguage model as a language catalyst,\u201d 2023.\nY. Zhao, B. Yu, B. Hui, H. Yu, F. Huang, Y. Li, and N. L.\nZhang, \u201cA preliminary study of the intrinsic relationship\nbetween complexity and alignment,\u201d 2023.\nA. Gudibande, E. Wallace, C. Snell, X. Geng, H.", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_61650", "chunk_text": ". Xu, Y. Su, and\nW. Yin, \u201cMuffin: Curating multi-faceted instructions for\nimproving instruction-following,\u201d 2023.\nT. Schick, J. Dwivedi-Yu, Z. Jiang, F. Petroni, P. Lewis,\nG. Izacard, Q. You, C. Nalmpantis, E. Grave, and S. Riedel,\n\u201cPeer: A collaborative language model,\u201d 2022.\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,\nS. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang,\nS. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark, \u201cSelf-refine: Iterative refinement\nwith self-feedback,\u201d 2023.\nW. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward,\nand J. Leike, \u201cSelf-critiquing models for assisting human\n\n\nevaluators,\u201d 2022.\nD. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford,\nD. Amodei, P. Christiano, and G. Irving, \u201cFine-tuning\nlanguage models from human preferences,\u201d _arXiv preprint_\n_arXiv:1909.08593_, 2019.\nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss,\nA. Radford, D. Amodei, and P. F. Christiano, \u201cLearning\nto summarize with human feedback,\u201d _Advances in Neu-_\n_ral Information Processing Systems_, vol. 33, pp. 3008\u20133021,\n2020.\nJ. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe,\nJ. Leike, and P. Christiano, \u201cRecursively summarizing\nbooks with human feedback,\u201d 2021.\nY. Bai, A. Jones, K. Ndousse, A. Ask", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_64800", "chunk_text": ", C. Washington, B. M. Sadler, W.-L. Chao,\nand Y. Su, \u201cLlm-planner: Few-shot grounded planning for\nembodied agents with large language models,\u201d in _Proceed-_\n_ings of the IEEE/CVF International Conference on Computer_\n_Vision_, 2023, pp. 2998\u20133009.\nZ. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, \u201cDescribe,\nexplain, plan and select: Interactive planning with large\nlanguage models enables open-world multi-task agents,\u201d\n_arXiv preprint arXiv:2302.01560_, 2023.\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao,\nand K. Narasimhan, \u201cTree of thoughts: Deliberate problem solving with large language models,\u201d _arXiv preprint_\n_arXiv:2305.10601_, 2023.\nB. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas,\nand P. Stone, \u201cLlm+ p: Empowering large language models with optimal planning proficiency,\u201d _arXiv preprint_\n_arXiv:2304.11477_, 2023.\nS. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and\nZ. Hu, \u201cReasoning with language model is planning with\nworld model,\u201d _arXiv preprint arXiv:2305.14992_, 2023.\nM. Hu, Y. Mu, X. Yu, M. Ding, S. Wu, W. Shao, Q. Chen,\nB. Wang, Y. Qiao, and P. Luo, \u201cTree-planner: Efficient\nclose-loop task planning with large language models,\u201d\n_arXiv preprint arXiv:2310.08582_, 2023.\nB. Y. Lin, C. Huang, Q. Liu, W. Gu, S. Sommerer, and\nX. Ren, \u201cOn grounded planning for embodied tasks with\nlanguage models,\u201d in _Proceedings of the AAAI Conference_\n\n_on Artificial Intelligence_, vol. 37, no. 11, 2023, pp", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_66150", "chunk_text": "ea141dc-](http://papers.nips.cc/paper_files/paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc-Abstract-Conference.html)\n[Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc-Abstract-Conference.html)\nJ. Wang, Z. Yao, A. Mitra, S. Osebe, Z. Yang, and H. Yu,\n\u201cUMASS BioNLP at MEDIQA-chat 2023: Can LLMs\ngenerate high-quality synthetic note-oriented doctorpatient conversations?\u201d in _Proceedings of the 5th Clinical_\n_Natural_ _Language_ _Processing_ _Workshop_, T. Naumann,\nA. Ben Abacha, S. Bethard, K. Roberts, and A. Rumshisky,\nEds. Toronto, Canada: Association for Computational\nLinguistics, Jul. 2023, pp. 460\u2013471. [Online]. Available:\n[https://aclanthology.org/2023.clinicalnlp-1.49](https://aclanthology.org/2023.clinicalnlp-1.49)\nZ. Yang, S. Cherian, and S. Vucetic, \u201cData augmentation\nfor radiology report simplification,\u201d in _Findings of the_\n_Association_ _for_ _Computational_ _Linguistics:_ _EACL_ _2023_,\nA. Vlachos and I. Augenstein, Eds. Dubrovnik,\nCroatia: Association for Computational Linguistics,\n[May 2023, pp. 1922\u20131932. [Online]. Available: https:](https://aclanthology.org/2023.findings-eacl.144)\n[//aclanthology.org/2023.findings-eacl.144](https://aclanthology.org/2023.findings-eacl.144)\nZ. Cai, C. Tao, T. Shen, C. Xu, X. Geng, X. A. Lin, L. He, and\nD. Jiang, \u201cHyper: Multitask hyper-prompted training enables large-scale retrieval generalization,\u201d in _The Eleventh_\n\n\n_International Conference on Learning Representations", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_66600", "chunk_text": "eng, X. A. Lin, L. He, and\nD. Jiang, \u201cHyper: Multitask hyper-prompted training enables large-scale retrieval generalization,\u201d in _The Eleventh_\n\n\n_International Conference on Learning Representations_, 2022.\nC. Liu, C. Tao, X. Geng, T. Shen, D. Zhao, C. Xu, B. Jiao,\nand D. Jiang, \u201cAdam: Dense retrieval distillation with\nadaptive dark examples,\u201d _arXiv preprint arXiv:2212.10192_,\n2022.\nJ. Feng, C. Tao, X. Geng, T. Shen, C. Xu, G. Long, D. Zhao,\nand D. Jiang, \u201cKnowledge refinement via interaction between search engines and large language models,\u201d _arXiv_\n_preprint arXiv:2305.07402_, 2023.\nT. Shen, G. Long, X. Geng, C. Tao, T. Zhou, and D. Jiang,\n\u201cLarge language models are strong zero-shot retriever,\u201d\n_arXiv preprint arXiv:2304.14233_, 2023.\nX. Ma, X. Zhang, R. Pradeep, and J. Lin, \u201cZero-shot listwise\ndocument reranking with a large language model,\u201d 2023.\nZ. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, J. Shen,\nT. Liu, J. Liu, D. Metzler, X. Wang, and M. Bendersky,\n\u201cLarge language models are effective text rankers with\npairwise ranking prompting,\u201d 2023.\nX. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, \u201cQuery\nrewriting in retrieval-augmented large language models,\u201d\nin _Proceedings of the 2023 Conference on Empirical Methods_\n_in Natural Language Processing_, H. Bouamor, J. Pino, and\nK. Bali, Eds. Singapore: Association for Computational\nLinguistics, Dec. 2023, pp. 5303\u20135315. [Online]. Available:\n[https://aclanthology.org/2023.emnlp-main.322](https://aclanthology.org/2023.emnlp-main.322)\nD.", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_67500", "chunk_text": "[Nov. 2021, pp. 6943\u20136951. [Online]. Available: https:](https://aclanthology.org/2021.emnlp-main.555)\n[//aclanthology.org/2021.emnlp-main.555](https://aclanthology.org/2021.emnlp-main.555)\nZ. Peng, X. Wu, and Y. Fang, \u201cSoft prompt tuning for\naugmenting dense retrieval with large language models,\u201d\n_arXiv preprint arXiv:2307.08303_, 2023.\nJ. Saad-Falcon, O. Khattab, K. Santhanam, R. Florian,\nM. Franz, S. Roukos, A. Sil, M. A. Sultan, and C. Potts,\n\u201cUDAPDR: unsupervised domain adaptation via LLM\nprompting and distillation of rerankers,\u201d in _Proceedings_\n\n_of the 2023 Conference on Empirical Methods in Natural_\n_Language Processing, EMNLP 2023, Singapore, December_\n_6-10, 2023_, 2023, pp. 11 265\u201311 279. [Online]. Available:\n[https://aclanthology.org/2023.emnlp-main.693](https://aclanthology.org/2023.emnlp-main.693)\nV. Jeronymo, L. Bonifacio, H. Abonizio, M. Fadaee,\nR. Lotufo, J. Zavrel, and R. Nogueira, \u201cInpars-v2: Large\n\n\n\n39\n\n\nlanguage models as efficient dataset generators for information retrieval,\u201d _arXiv preprint arXiv:2301.01820_, 2023.\nW. Sun, Z. Chen, X. Ma, L. Yan, S. Wang, P. Ren, Z. Chen,\nD. Yin, and Z. Ren, \u201cInstruction distillation makes large\nlanguage models efficient zero-shot rankers,\u201d 2023.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring\nthe limits of transfer learning with a unified text-to-text\ntransformer,\u201d _J. Mach. Learn. Res._", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_68400", "chunk_text": ", p. 1313\u20131322. [Online].\n[Available: https://doi.org/10.1145/3269206.3271784](https://doi.org/10.1145/3269206.3271784)\nW. Wang, X. Lin, F. Feng, X. He, and T.-S. Chua, \u201cGenerative\nrecommendation: Towards next-generation recommender\nparadigm,\u201d 2023.\nS. Dai, N. Shao, H. Zhao, W. Yu, Z. Si, C. Xu, Z. Sun,\nX. Zhang, and J. Xu, \u201cUncovering chatgpt\u2019s capabilities\nin recommender systems,\u201d in _Proceedings of the 17th_\n_ACM Conference on Recommender Systems_, ser. RecSys\n\u201923. New York, NY, USA: Association for Computing\nMachinery, 2023, p. 1126\u20131132. [Online]. Available:\n[https://doi.org/10.1145/3604915.3610646](https://doi.org/10.1145/3604915.3610646)\nY. Xi, W. Liu, J. Lin, X. Cai, H. Zhu, J. Zhu, B. Chen,\nR. Tang, W. Zhang, R. Zhang, and Y. Yu, \u201cTowards openworld recommendation with knowledge augmentation\nfrom large language models,\u201d 2023.\nX. Ren, W. Wei, L. Xia, L. Su, S. Cheng, J. Wang, D. Yin, and\nC. Huang, \u201cRepresentation learning with large language\nmodels for recommendation,\u201d 2023.\nW. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng, J. Wang,\nD. Yin, and C. Huang, \u201cLlmrec: Large language models\nwith graph augmentation for recommendation,\u201d 2024.\nL. Wang, S. Zhang, Y. Wang, E.-P. Lim, and Y. Wang,\n\u201cLLM4Vis: Explainable visualization recommendation\nusing ChatGPT,\u201d in _Proceedings of the 2023 Conference_\n\n_on Empirical Methods in Natural Language Processing:_\n_Industry Track_, M. Wang and I. Zitouni, Eds. Singapore:\nAssociation for Computational Linguistics, Dec", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_70200", "chunk_text": "iv:2304.01097_,\n2023.\nX. Zhang, C. Tian, X. Yang, L. Chen, Z. Li, and L. R. Petzold, \u201cAlpacare: Instruction-tuned large language models\nfor medical application,\u201d _arXiv preprint arXiv:2310.14558_,\n\n\n\n40\n\n\n2023.\nY. Li, Z. Li, K. Zhang, R. Dan, S. Jiang, and Y. Zhang,\n\u201cChatdoctor: A medical chat model fine-tuned on a large\nlanguage model meta-ai (llama) using medical domain\nknowledge,\u201d _Cureus_, vol. 15, no. 6, 2023.\nT. Han, L. C. Adams, J. Papaioannou, P. Grundmann,\nT. Oberhauser, A. L\u00a8oser, D. Truhn, and K. K.\nBressem, \u201cMedalpaca - an open-source collection of\nmedical conversational AI models and training data,\u201d\n_CoRR_, vol. abs/2304.08247, 2023. [Online]. Available:\n[https://doi.org/10.48550/arXiv.2304.08247](https://doi.org/10.48550/arXiv.2304.08247)\nC. Wu, W. Lin, X. Zhang, Y. Zhang, Y. Wang, and W. Xie,\n\u201cPmc-llama: Towards building open-source language\nmodels for medicine,\u201d _arXiv preprint arXiv:2305.10415_,\nvol. 6, 2023.\nZ. Bao, W. Chen, S. Xiao, K. Ren, J. Wu, C. Zhong,\nJ. Peng, X. Huang, and Z. Wei, \u201cDisc-medllm: Bridging\ngeneral large language models and real-world medical\nconsultation,\u201d _CoRR_, vol. abs/2308.14346, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2308.14346](https://doi.org/10.48550/arXiv.2308.14346)\nZ. Gou, Z. Shao, Y. Gong, yelong shen, Y. Yang,\nM. Huang", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_70650", "chunk_text": "8.14346](https://doi.org/10.48550/arXiv.2308.14346)\nZ. Gou, Z. Shao, Y. Gong, yelong shen, Y. Yang,\nM. Huang, N. Duan, and W. Chen, \u201cToRA: A toolintegrated reasoning agent for mathematical problem\nsolving,\u201d in _The_ _Twelfth_ _International_ _Conference_ _on_\n_Learning Representations_ [, 2024. [Online]. Available: https:](https://openreview.net/forum?id=Ep0TtjVoap)\n[//openreview.net/forum?id=Ep0TtjVoap](https://openreview.net/forum?id=Ep0TtjVoap)\nE. Perkowski, R. Pan, T. D. Nguyen, Y. Ting, S. Kruk,\nT. Zhang, C. O\u2019Neill, M. Jablonska, Z. Sun, M. J.\nSmith, H. Liu, K. Schawinski, K. Iyer, I. Ciuca,\nand UniverseTBD, \u201cAstrollama-chat: Scaling astrollama\nwith conversational and diverse datasets,\u201d _CoRR_,\n[vol. abs/2401.01916, 2024. [Online]. Available: https:](https://doi.org/10.48550/arXiv.2401.01916)\n[//doi.org/10.48550/arXiv.2401.01916](https://doi.org/10.48550/arXiv.2401.01916)\nJ. Gao, R. Pi, J. Zhang, J. Ye, W. Zhong, Y. Wang, L. Hong,\nJ. Han, H. Xu, Z. Li, and L. Kong, \u201cG-llava: Solving\ngeometric problem with multi-modal large language\nmodel,\u201d _CoRR_, vol. abs/2312.11370, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2312.11370](https://doi.org/10.48550/arXiv.2312.11370)\nH. Zhao, S. Liu, C. Ma, H. Xu, J. Fu, Z.-H. Deng, L. Kong,\nand", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_71100", "chunk_text": "https://doi.org/10.48550/arXiv.2312.11370)\nH. Zhao, S. Liu, C. Ma, H. Xu, J. Fu, Z.-H. Deng, L. Kong,\nand Q. Liu, \u201cGIMLET: A unified graph-text model\nfor instruction-based molecule zero-shot learning,\u201d in\n_Thirty-seventh Conference on Neural Information Processing_\n_Systems_ [, 2023. [Online]. Available: https://openreview.](https://openreview.net/forum?id=Tt6DrRCgJV)\n[net/forum?id=Tt6DrRCgJV](https://openreview.net/forum?id=Tt6DrRCgJV)\nA. N. Rubungo, C. Arnold, B. P. Rand, and A. B. Dieng,\n\u201cLlm-prop: Predicting physical and electronic properties\n\n - f crystalline solids from their text descriptions,\u201d\n_CoRR_, vol. abs/2310.14029, 2023. [Online]. Available:\n[https://doi.org/10.48550/arXiv.2310.14029](https://doi.org/10.48550/arXiv.2310.14029)\nH. Cao, Z. Liu, X. Lu, Y. Yao, and Y. Li, \u201cInstructmol:\nMulti-modal integration for building a versatile and\nreliable molecular assistant in drug discovery,\u201d _CoRR_,\n[vol. abs/2311.16208, 2023. [Online]. Available: https:](https://doi.org/10.48550/arXiv.2311.16208)\n[//doi.org/10.48550/arXiv.2311.16208](https://doi.org/10.48550/arXiv.2311.16208)\nH. Abdine, M. Chatzianastasis, C. Bouyioukos, and\nM. Vazirgiannis, \u201cProt2text: Multimodal protein\u2019s\nfunction generation with GNNs and transformers,\u201d in _Deep_ _Generative_ _Models_ _for_ _Health_\n_Workshop_ _NeurIPS_ _2023_, 2023. [Online]. Available:\n[https://openreview.net/forum?id=EJ7YNgWYFj](https://openreview.net/forum?id=EJ", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_71550", "chunk_text": "Health_\n_Workshop_ _NeurIPS_ _2023_, 2023. [Online]. Available:\n[https://openreview.net/forum?id=EJ7YNgWYFj](https://openreview.net/forum?id=EJ7YNgWYFj)\nY. Luo, J. Zhang, S. Fan, K. Yang, Y. Wu, M. Qiao,\n\n\nand Z. Nie, \u201cBiomedgpt: Open multimodal generative\npre-trained transformer for biomedicine,\u201d _arXiv preprint_\n_arXiv:2308.09442_, 2023.\nB. Chen, X. Cheng, P. Li, Y. Geng, J. Gong, S. Li, Z. Bei,\nX. Tan, B. Wang, X. Zeng, C. Liu, A. Zeng, Y. Dong,\nJ. Tang, and L. Song, \u201cxtrimopglm: Unified 100b-scale\npre-trained transformer for deciphering the language\n\n - f protein,\u201d _CoRR_, vol. abs/2401.06199, 2024. [Online].\n[Available: https://doi.org/10.48550/arXiv.2401.06199](https://doi.org/10.48550/arXiv.2401.06199)\nC. Deng, T. Zhang, Z. He, Y. Xu, Q. Chen, Y. Shi, L. Fu,\nW. Zhang, X. Wang, C. Zhou, Z. Lin, and J. He, \u201cK2:\nA foundation language model for geoscience knowledge\nunderstanding and utilization,\u201d 2023.\nZ. Bi, N. Zhang, Y. Xue, Y. Ou, D. Ji, G. Zheng, and\nH. Chen, \u201cOceangpt: A large language model for ocean\nscience tasks,\u201d _CoRR_, vol. abs/2310.02031, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2310.02031](https://doi.org/10.48550/arXiv.2310.02031)\nZ. Zheng, J. Zhang, T. Vu, S. Diao, Y. H. W. Tim, and\nS. Yeung, \u201cMarinegpt: Unlock", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_72000", "chunk_text": "50/arXiv.2310.02031)\nZ. Zheng, J. Zhang, T. Vu, S. Diao, Y. H. W. Tim, and\nS. Yeung, \u201cMarinegpt: Unlocking secrets of ocean to\nthe public,\u201d _CoRR_, vol. abs/2310.13596, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2310.13596](https://doi.org/10.48550/arXiv.2310.13596)\nZ. Lin, C. Deng, L. Zhou, T. Zhang, Y. Xu, Y. Xu, Z. He,\nY. Shi, B. Dai, Y. Song, B. Zeng, Q. Chen, T. Shi,\nT. Huang, Y. Xu, S. Wang, L. Fu, W. Zhang, J. He,\nC. Ma, Y. Zhu, X. Wang, and C. Zhou, \u201cGeogalactica:\nA scientific large language model in geoscience,\u201d\n_CoRR_, vol. abs/2401.00434, 2024. [Online]. Available:\n[https://doi.org/10.48550/arXiv.2401.00434](https://doi.org/10.48550/arXiv.2401.00434)\nD. Zhang, A. Petrova, D. Trautmann, and F. Schilder, \u201cUnleashing the power of large language models for legal\napplications,\u201d in _Proceedings of the 32nd ACM International_\n_Conference on Information and Knowledge Management_, 2023,\npp. 5257\u20135258.\nZ. Sun, \u201cA short survey of viewing large language models\nin legal aspect,\u201d _arXiv preprint arXiv:2303.09136_, 2023.\nJ. Lai, W. Gan, J. Wu, Z. Qi, and P. S. Yu, \u201cLarge language\nmodels in law: A survey,\u201d _arXiv preprint arXiv:2312.03718_,\n2023.\nS. Yue, W. Chen, S. Wang, B. Li, C. Shen, S. Liu, Y. Zhou,\nY. Xiao, S. Yun, W. Lin _et al._", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_72450", "chunk_text": ".03718_,\n2023.\nS. Yue, W. Chen, S. Wang, B. Li, C. Shen, S. Liu, Y. Zhou,\nY. Xiao, S. Yun, W. Lin _et al._, \u201cDisc-lawllm: Fine-tuning\nlarge language models for intelligent legal services,\u201d _arXiv_\n_preprint arXiv:2309.11325_, 2023.\nH. Zhong, C. Xiao, C. Tu, T. Zhang, Z. Liu, and M. Sun,\n\u201cJec-qa: a legal-domain question answering dataset,\u201d in\n_Proceedings of the AAAI Conference on Artificial Intelligence_,\nvol. 34, no. 05, 2020, pp. 9701\u20139708.\nK. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn,\nL. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal,\nM. Schaekermann, A. Wang, M. Amin, S. Lachgar, P. A.\nMansfield, S. Prakash, B. Green, E. Dominowska, B. A.\ny Arcas, N. Tomasev, Y. Liu, R. Wong, C. Semturs,\nS. S. Mahdavi, J. K. Barral, D. R. Webster, G. S.\nCorrado, Y. Matias, S. Azizi, A. Karthikesalingam, and\nV. Natarajan, \u201cTowards expert-level medical question\nanswering with large language models,\u201d _CoRR_, vol.\n[abs/2305.09617, 2023. [Online]. Available: https://doi.](https://doi.org/10.48550/arXiv.2305.09617)\n\n[org/10.48550/arXiv.2305.09617](https://doi.org/10.48550/arXiv.2305.09617)\nW. Zhu, X. Wang, H. Zheng, M. Chen, and B. Tang,\n\u201cPromptcblue: A chinese prompt tuning benchmark for\nthe medical domain,\u201d _arXiv preprint arXiv:2310.14151_,\n2023.\n\n\n\n41\n\n\nC", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_72900", "chunk_text": ". Zheng, M. Chen, and B. Tang,\n\u201cPromptcblue: A chinese prompt tuning benchmark for\nthe medical domain,\u201d _arXiv preprint arXiv:2310.14151_,\n2023.\n\n\n\n41\n\n\nC. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, \u201cPmcllama: Further finetuning llama on medical papers,\u201d\n_CoRR_, vol. abs/2304.14454, 2023. [Online]. Available:\n[https://doi.org/10.48550/arXiv.2304.14454](https://doi.org/10.48550/arXiv.2304.14454)\nZ. Bao, W. Chen, S. Xiao, K. Ren, J. Wu, C. Zhong, J. Peng,\nX. Huang, and Z. Wei, \u201cDisc-medllm: Bridging general\nlarge language models and real-world medical consultation,\u201d _arXiv preprint arXiv:2308.14346_, 2023.\nS. Xue, F. Zhou, Y. Xu, H. Zhao, S. Xie, Q. Dai,\nC. Jiang, J. Zhang, J. Zhou, D. Xiu, and H. Mei,\n\u201cWeaverbird: Empowering financial decision-making\nwith large language model, knowledge base, and search\nengine,\u201d _CoRR_, vol. abs/2308.05361, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2308.05361](https://doi.org/10.48550/arXiv.2308.05361)\nS. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze,\nS. Gehrmann, P. Kambadur, D. S. Rosenberg, and\nG. Mann, \u201cBloomberggpt: A large language model\nfor finance,\u201d _CoRR_, vol. abs/2303.17564, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2303.17564](https://doi.org/10.48550/arXiv.2303.17564)\nD. Lu, H. Wu, J. Li", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_73350", "chunk_text": ": https://doi.org/10.48550/arXiv.2303.17564](https://doi.org/10.48550/arXiv.2303.17564)\nD. Lu, H. Wu, J. Liang, Y. Xu, Q. He, Y. Geng, M. Han,\nY. Xin, and Y. Xiao, \u201cBbt-fin: Comprehensive construction\n\n - f chinese financial domain pre-trained language model,\ncorpus and benchmark,\u201d _CoRR_, vol. abs/2302.09432, 2023.\n\n[[Online]. Available: https://doi.org/10.48550/arXiv.2302.](https://doi.org/10.48550/arXiv.2302.09432)\n[09432](https://doi.org/10.48550/arXiv.2302.09432)\nY. Yang, Y. Tang, and K. Y. Tam, \u201cInvestlm: A large language\nmodel for investment using financial domain instruction\ntuning,\u201d _CoRR_, vol. abs/2309.13064, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2309.13064](https://doi.org/10.48550/arXiv.2309.13064)\nQ. Xie, W. Han, X. Zhang, Y. Lai, M. Peng, A. LopezLira, and J. Huang, \u201cPIXIU: A large language model,\ninstruction data and evaluation benchmark for finance,\u201d\n_CoRR_, vol. abs/2306.05443, 2023. [Online]. Available:\n[https://doi.org/10.48550/arXiv.2306.05443](https://doi.org/10.48550/arXiv.2306.05443)\nN. Wang, H. Yang, and C. D. Wang, \u201cFingpt: Instruction\ntuning benchmark for open-source large language models\nin financial datasets,\u201d _CoRR_, vol. abs/2310.04793, 2023.\n\n[[Online]. Available: https://doi.org/10.48550/arXiv.2310.](https://doi.org/10.48550/arXiv.2310.04793)\n[04793](https://doi.org/10.48550", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_73800", "chunk_text": ": https://doi.org/10.48550/arXiv.2310.](https://doi.org/10.48550/arXiv.2310.04793)\n[04793](https://doi.org/10.48550/arXiv.2310.04793)\nR. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn,\nE. Saravia, A. Poulton, V. Kerkez, and R. Stojnic,\n\u201cGalactica: A large language model for science,\u201d\n_CoRR_, vol. abs/2211.09085, 2022. [Online]. Available:\n[https://doi.org/10.48550/arXiv.2211.09085](https://doi.org/10.48550/arXiv.2211.09085)\nJ. Yin, S. Dash, F. Wang, and M. Shankar, \u201cFORGE:\npre-training  - pen foundation models for science,\u201d\nin _Proceedings of the International Conference for High_\n_Performance Computing, Networking, Storage and Analysis,_\n_SC_ _2023,_ _Denver,_ _CO,_ _USA,_ _November_ _12-17,_ _2023_,\nD. Arnold, R. M. Badia, and K. M. Mohror, Eds.\n[ACM, 2023, pp. 81:1\u201381:13. [Online]. Available: https:](https://doi.org/10.1145/3581784.3613215)\n[//doi.org/10.1145/3581784.3613215](https://doi.org/10.1145/3581784.3613215)\nZ. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos,\nS. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and\nS. Welleck, \u201cLlemma: An open language model for\nmathematics,\u201d _CoRR_, vol. abs/2310.10631, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2310.10631](https://doi.org/10.48550/arXiv.2310.", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_74250", "chunk_text": "0.10631, 2023. [Online].\n[Available: https://doi.org/10.48550/arXiv.2310.10631](https://doi.org/10.48550/arXiv.2310.10631)\nF. Yu, A. Gao, and B. Wang, \u201cOutcome-supervised\nverifiers for planning in mathematical reasoning,\u201d\n_CoRR_, vol. abs/2311.09724, 2023. [Online]. Available:\n[https://doi.org/10.48550/arXiv.2311.09724](https://doi.org/10.48550/arXiv.2311.09724)\n\n\nT. D. Nguyen, Y. Ting, I. Ciuca, C. O\u2019Neill, Z. Sun,\nM. Jablonska, S. Kruk, E. Perkowski, J. W. Miller,\nJ. Li, J. Peek, K. Iyer, T. R\u00b4ozanski, P. Khetarpal,\nS. Zaman, D. Brodrick, S. J. R. M\u00b4endez, T. Bui,\nA. Goodman, A. Accomazzi, J. P. Naiman, J. Cranney,\nK. Schawinski, and UniverseTBD, \u201cAstrollama: Towards\nspecialized foundation models in astronomy,\u201d _CoRR_,\n[vol. abs/2309.06126, 2023. [Online]. Available: https:](https://doi.org/10.48550/arXiv.2309.06126)\n[//doi.org/10.48550/arXiv.2309.06126](https://doi.org/10.48550/arXiv.2309.06126)\nJ. Roberts, T. L\u00a8uddecke, S. Das, K. Han, and S. Albanie,\n\u201cGpt4geo: How a language model sees the world\u2019s ge\n - graphy,\u201d 2023.\nZ. Lin, C. Deng, L. Zhou, T. Zhang, Y. Xu, Y. Xu, Z. He,\nY. Shi, B. Dai, Y. Song, B. Zeng, Q. Chen, T. Shi, T. Huang,\nY. Xu, S. Wang, L. Fu, W. Zhang,", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_75150", "chunk_text": ",\nJ. Liu, T. Liu, F. Huang, and Y. Li, \u201cOne shot learning as\ninstruction data prospector for large language models,\u201d\n2023.\nE. Frantar, S. P. Singh, and D. Alistarh, \u201cOptimal brain compression: A framework for accurate post-training quantization and pruning,\u201d 2023.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer,\n\u201cGPT3.int8(): 8-bit matrix multiplication for transformers\nat scale,\u201d in _Advances in Neural Information Processing_\n_Systems_, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho,\n[Eds., 2022. [Online]. Available: https://openreview.net/](https://openreview.net/forum?id=dXiGWqBoxaD)\n[forum?id=dXiGWqBoxaD](https://openreview.net/forum?id=dXiGWqBoxaD)\nY. J. Kim, R. Henry, R. Fahim, and H. H. Awadalla,\n\u201cFinequant: Unlocking efficiency with fine-grained\nweight-only quantization for llms,\u201d 2023.\nC. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu, P. Luo,\nand N. Wong, \u201cCompression of generative pre-trained\nlanguage models via quantization,\u201d in _Proceedings of the_\n_60th Annual Meeting of the Association for Computational_\n_Linguistics (Volume 1: Long Papers)_, S. Muresan, P. Nakov,\nand A. Villavicencio, Eds. Dublin, Ireland: Association\nfor Computational Linguistics, May 2022, pp. 4821\u20134836.\n\n[Online]. Available: [https://aclanthology.org/2022.acl-](https://aclanthology.org/2022.acl-long.331)\n[long.331](https://aclanthology.org/2022.acl-long.331)\n\n\n\n42\n\n\nZ. Yao, R. Yazdani Aminabadi, M. Zhang, X. Wu, C. Li, and\nY. He, \u201cZeroquant: Efficient and affordable post-training\nquantization for large-scale transformers,\u201d _Advances in_\n_Neural", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_75600", "chunk_text": "dani Aminabadi, M. Zhang, X. Wu, C. Li, and\nY. He, \u201cZeroquant: Efficient and affordable post-training\nquantization for large-scale transformers,\u201d _Advances in_\n_Neural Information Processing Systems_, vol. 35, pp. 27 168\u2013\n27 183, 2022.\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,\n\u201cSmoothquant: Accurate and efficient post-training quantization for large language models,\u201d 2023.\nX. Ma, G. Fang, and X. Wang, \u201cLlm-pruner: On the structural pruning of large language models,\u201d 2023.\nM. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu,\nand B. Zhuang, \u201cLoraprune: Pruning meets low-rank\nparameter-efficient fine-tuning,\u201d 2023.\nE. Frantar and D. Alistarh, \u201cSparsegpt: Massive language\nmodels can be accurately pruned in one-shot,\u201d 2023.\nM. Xu, Y. L. Xu, and D. P. Mandic, \u201cTensorgpt: Efficient\ncompression of the embedding layer in llms based on the\ntensor-train decomposition,\u201d 2023.\nY. Li, Y. Yu, Q. Zhang, C. Liang, P. He, W. Chen, and\nT. Zhao, \u201cLosparse: Structured compression of large language models based on low-rank and sparse approximation,\u201d 2023.\nZ. Hu, L. Wang, Y. Lan, W. Xu, E.-P. Lim, L. Bing, X. Xu,\nS. Poria, and R. K.-W. Lee, \u201cLlm-adapters: An adapter\nfamily for parameter-efficient fine-tuning of large language models,\u201d 2023.\nH. Liu, D. Tam, M. Mohammed, J. Mohta, T. Huang,\nM. Bansal, and C. Raffel, \u201cFew-shot parameterefficient fine-tuning is better and cheaper than incontext learning,\u201d in _Advances in Neural Information_\n_Processing Systems_, A. H. Oh, A. Agarwal, D. Belgrave,\n[and K.", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_76050", "chunk_text": " parameterefficient fine-tuning is better and cheaper than incontext learning,\u201d in _Advances in Neural Information_\n_Processing Systems_, A. H. Oh, A. Agarwal, D. Belgrave,\n[and K. Cho, Eds., 2022. [Online]. Available: https:](https://openreview.net/forum?id=rBCvMG-JsPd)\n[//openreview.net/forum?id=rBCvMG-JsPd](https://openreview.net/forum?id=rBCvMG-JsPd)\nY. Wang, S. Agarwal, S. Mukherjee, X. Liu, J. Gao,\nA. H. Awadallah, and J. Gao, \u201cAdaMix: Mixture\n - f-adaptations for parameter-efficient model tuning,\u201d\nin _Proceedings_ _of_ _the_ _2022_ _Conference_ _on_ _Empirical_\n_Methods in Natural Language Processing_, Y. Goldberg,\nZ. Kozareva, and Y. Zhang, Eds. Abu Dhabi,\nUnited Arab Emirates: Association for Computational\nLinguistics, Dec. 2022, pp. 5744\u20135760. [Online]. Available:\n[https://aclanthology.org/2022.emnlp-main.388](https://aclanthology.org/2022.emnlp-main.388)\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,\nL. Wang, and W. Chen, \u201cLora: Low-rank adaptation of\nlarge language models,\u201d 2021.\nX. L. Li and P. Liang, \u201cPrefix-tuning: Optimizing continuous\nprompts for generation,\u201d in _Proceedings of the 59th Annual_\n_Meeting of the Association for Computational Linguistics and_\n_the 11th International Joint Conference on Natural Language_\n_Processing (Volume 1: Long Papers)_, C. Zong, F. Xia,\nW. Li, and R. Navigli, Eds. Online: Association for\nComputational Linguistics, Aug. 2021, pp. 4582\u20134597.\n\n[Online]. Available: [https://aclanthology.org/2021.acl-](https://aclanthology.org/2021.acl-long.353)\n", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2402.13116_long_context_rag_xu:chunk_76500", "chunk_text": ", Aug. 2021, pp. 4582\u20134597.\n\n[Online]. Available: [https://aclanthology.org/2021.acl-](https://aclanthology.org/2021.acl-long.353)\n[long.353](https://aclanthology.org/2021.acl-long.353)\nX. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and J. Tang, \u201cPtuning: Prompt tuning can be comparable to fine-tuning\nacross scales and tasks,\u201d in _Proceedings of the 60th Annual_\n_Meeting of the Association for Computational Linguistics_\n_(Volume 2: Short Papers)_, S. Muresan, P. Nakov, and\nA. Villavicencio, Eds. Dublin, Ireland: Association for\nComputational Linguistics, May 2022, pp. 61\u201368. [Online].\n\n\n[Available: https://aclanthology.org/2022.acl-short.8](https://aclanthology.org/2022.acl-short.8)\nT. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer,\n\u201cQlora: Efficient finetuning of quantized llms,\u201d 2023.\nJ. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon,\nand D. Lee, \u201cMemory-efficient fine-tuning of compressed\nlarge language models via sub-4-bit integer quantization,\u201d\n2023.\n\nS. Malladi, T. Gao, E. Nichani, A. Damian, J. D. Lee, D. Chen,\nand S. Arora, \u201cFine-tuning language models with just\nforward passes,\u201d 2024.\nZ. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, J. Liu, Z. Qu,\nS. Yan, Y. Zhu, Q. Zhang, M. Chowdhury, and M. Zhang,\n\u201cEfficient large language models: A survey,\u201d 2024.\nY.-S. Lee, M. Sultan, Y. El-Kurdi, T. Naseem, A. Munawar,\nR. Florian, S. Roukos, and R. Astudillo", "token_count": 500, "metadata": {"arxiv_id": "2402.13116", "title": "A Survey on Knowledge Distillation of Large Language Models", "authors": ["Xiaohan Xu", "Ming Li", "Chongyang Tao", "Tao Shen", "Reynold Cheng", "Jinyang Li", "Can Xu", "Dacheng Tao", "Tianyi Zhou"], "year": 2024, "url": "https://arxiv.org/pdf/2402.13116v4"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_0", "chunk_text": "## **Sources of Hallucination by Large Language Models on Inference Tasks**\n\n**Nick M** **[c]** **Kenna** _[\u2020]_ ***** **Tianyi Li** _[\u2020]_ *****\n**Liang Cheng** _[\u2020]_ **Mohammad Javad Hosseini** _[\u2021]_ **Mark Johnson** _[\u00a7]_ **Mark Steedman** _[\u2020]_\n\n_\u2020_ University of Edinburgh _\u2021_ Google Research _\u00a7_ Macquarie University\n{nick.mckenna, tianyi.li}@ed.ac.uk\n\n\n\n**Abstract**\n\n\nLarge Language Models (LLMs) are claimed\nto be capable of Natural Language Inference\n(NLI), necessary for applied tasks like question\nanswering and summarization. We present a\nseries of behavioral studies on several LLM\n\nfamilies (LLaMA, GPT-3.5, and PaLM) which\nprobe their behavior using controlled experiments. We establish two biases originating\nfrom pretraining which predict much of their\nbehavior, and show that these are major sources\n\n  - f hallucination in generative LLMs. First,\nmemorization at the level of sentences: we\n\nshow that, regardless of the premise, models\nfalsely label NLI test samples as entailing when\nthe hypothesis is attested in training data, and\nthat entities are used as \u201cindices\u201d to access the\n\nmemorized data. Second, statistical patterns\n\n  - f usage learned at the level of corpora: we\nfurther show a similar effect when the premise\npredicate is less frequent than that of the hypothesis in the training data, a bias following\nfrom previous studies. We demonstrate that\nLLMs perform significantly worse on NLI test\nsamples which do not conform to these biases\nthan those which do, and we offer these as valuable controls for future LLM evaluation. [1]\n\n\n**1** **Introduction**\n\n\nLarge Language Models (LLMs) such as LLaMA,\nGPT-3/4, PaLM, and more (Touvron et al., 2023;\nBrown et al., 2020; Chowdhery et al., 2022), have\nbeen trusted by many to perform language understanding in downstream tasks such as summarization, question answering, and fact verification,\namong others (Zhang et al., 2023", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_450", "chunk_text": " 2020; Chowdhery et al., 2022), have\nbeen trusted by many to perform language understanding in downstream tasks such as summarization, question answering, and fact verification,\namong others (Zhang et al., 2023). However, due\nto the large-scale nature of LLM training on vast,\n\n- ften proprietary data, and the inherent opacity\n\n- f LLM parameters, it is difficult to explain their\n\n\n*Equal contribution.\n1Code and LLM  - utputs (LLaMA and GPT-3.5)\n[are available at https://github.com/Teddy-Li/](https://github.com/Teddy-Li/LLM-NLI-Analysis)\n[LLM-NLI-Analysis.](https://github.com/Teddy-Li/LLM-NLI-Analysis)\n\n\n\nbehavior when answering user queries and the corresponding risks in terms of bias and robustness.\nIn particular, one LLM behavior poses a significant challenge: \u201challucination,\u201d the phenomenon\nin which LLMs provide information which is incorrect or inappropriate, presented as fact.\n\nThis paper investigates two biases driving LLM\nperformance in natural language inference, sometimes called _textual entailment_ . This is a basic com\nponent of language understanding which is critical\nin applied tasks, and we offer these two biases as\nexplanations of general false positive hallucination\nin everyday use. We examine broader NLI, and\nespecially _directional entailments_, which hold in\n\n- ne direction, but not both. For example, DEFEAT\n_entails_ PLAY but PLAY _does not entail_ DEFEAT. In\nferring directional entailment is more difficult than\nthat of symmetric paraphrase, so it more deeply\nprobes understanding.\n\nOur approach is a behavioral study of prompted\nLLM decision-making. We alter existing NLI\ndatasets in targeted ways while measuring how predictions change, across several major LLM families\n(LLaMA, GPT-3.5, and PaLM). We demonstrate\ntwo sources of LLM performance on the NLI task,\nwhich we offer as explanations of general false positive hallucination: (1) LLM bias toward affirming\nentailment when the hypothesis may be attested in\nthe training text, including reliance on named entity\nidentifiers; and (2) a corpus-frequency bias, affirming entailment when the premise is less", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_900", "chunk_text": " LLM bias toward affirming\nentailment when the hypothesis may be attested in\nthe training text, including reliance on named entity\nidentifiers; and (2) a corpus-frequency bias, affirming entailment when the premise is less frequent\nthan the hypothesis.\n\nWe establish that these biases originate from\nthe LLM pretraining objective, in which statistical modeling of the natural distribution of humangenerated text leads to (at the level of sentences)\nmemorizing individual statements, and (at the\nlevel of corpora) learning typical patterns of usage. Though they are superficially performant, our\nexperiments show that even powerful LLMs still\nuse unsatisfactory tools instead of robust reasoning.\n\n\nWe present three contributions, the demonstrations of both factors and an analysis of their impact:\n(1) In a prompting scenario, LLMs respond to\nentailment samples according to an _attestation bias_,\naffirming entailments more readily if the hypothesis is attested by the pretraining text. We find\nthat LLaMA-65B, GPT-3.5, and PaLM-540B are\nrespectively 1.9, 2.2, and 2.0 times more likely\nto wrongly predict Entail when the model already asserts the hypothesis is attested, compared\nto when not attested. Further, LLMs recall from\ntheir propositional memory using named entities as\nidentifying \u201cindices,\u201d even though they are irrelevant to the logic of the predicate inference task.\n(2) LLMs also rely on a simple corpus-statistic\nbias using relative term-frequencies, especially\nwhen propositional memory is not available. The\nthree LLMs are 1.6, 1.8 and 2.0 times more likely to\nwrongly affirm entailments if the premise has lower\nterm frequency than the hypothesis, than when not.\n(3) For the NLI test samples consistent with\nthese factors, LLM scores are misleadingly high;\nfor NLI samples adversarial to them, LLM performance is severely degraded. We show that when\nlabels go against the _attestation bias_, LLMs can be\npoor or even near-random classifiers; for the _rela-_\n_tive frequency bias_, we similarly show a substantial\nperformance decrease across all LLMs.\n\n\n**2** **Related Work**\n\n\nAddressing task robustness", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_1350", "chunk_text": " can be\npoor or even near-random classifiers; for the _rela-_\n_tive frequency bias_, we similarly show a substantial\nperformance decrease across all LLMs.\n\n\n**2** **Related Work**\n\n\nAddressing task robustness, Poliak et al. (2018)\nfound a range of NLI datasets contain artifacts\nwhich are learned by supervised models trained\n\n- n only sample hypotheses. In our work we design\na similar hypothesis-only test with LLMs, but we\nuse it to probe model memory without any training.\nBy conditioning on the attestation of hypotheses,\nwe show that LLMs are inherently sensitive to attestation, separate from the statistical idiosyncrasies\n\n- f NLI datasets. [2]\n\n\nAdditionally, Talman and Chatzikyriakidis\n(2019) report generalization failure among many\nmodels supervised for NLI \u2014 models fail to generalize between NLI datasets, even if the task is\nformatted the same. On smaller Language Models\nsuch as RoBERTa (Liu et al., 2019; 355M params),\nLi et al. (2022) also observed a reliance on dataset\n\n\n2We speculate that a similar attestation effect could even\nbe present in the supervised models studied in Poliak et al.\n(2018), which could contribute to those models\u2019 performance.\nWe leave the investigation of this to future work.\n\n\n\nartifacts when performing directional NLI on predicates. We now study the behavior of much larger\nLMs, which have demonstrated more robust performance across NLP tasks.\n\nRecent work has also explored LLM memorization and generalization. Carlini et al. (2023) establish that LLMs are able to memorize more data than\n\nsmall LMs, whereas Tirumala et al. (2022) further\ndemonstrate that LLMs pay special attention early\nin training to numbers and nouns, which act as\nunique identifiers for individual training sentences.\nWe also show that memories used in language inference are tied to specific named entities. And while\nWeller et al. (2023) and Kandpal et al. (2022) find\nthat entity frequency in training data is correlated\nwith performance in factual recall about them, we\nfind that entity frequency is _anti_ - correlated with\nhypothetical generalization performance (\u00a76).\n\nBubeck et al. (2023", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_1800", "chunk_text": "that entity frequency in training data is correlated\nwith performance in factual recall about them, we\nfind that entity frequency is _anti_ - correlated with\nhypothetical generalization performance (\u00a76).\n\nBubeck et al. (2023) argue that GPT-4 understands language \u201cbeyond memorization\u201d. We do\nnot disprove generalization, but we show that GPT4 shows the same hallucinations in Appendix F.\n\n\n**3** **Experimental Design**\n\n\nWe design behavioral experiments on LLMs by\nmodifying NLI datasets with rigorous controls. We\n\n- bserve large behavior changes across three major\nLLM families due to propositional-memory effects\nin \u00a75 and \u00a76, and corpus frequency in \u00a77. Finally,\nwe show the impact on real performance in \u00a78.\n\n\n**3.1** **Two Biases in Inference Predictions**\n\n\nWe claim that the pretraining objective to fit the\ndistribution of natural text leads to biases in LLM\n\ngenerations. We explore two such biases.\n\n\n**The Attestation Bias (** \u039b **)** is the over-reliance of\nan LLM on its propositional memory about a query\nstatement. We claim that when a statement is likely\nto be attested in some way by an LLM\u2019s training\ndata, it is more likely to affirm it as a conclusion in\nNLI tasks, regardless of any premise it is presented\nwith. We measure the attestedness of a sample\nby prompting the LLM to ask if the hypothesis in\nquestion is true, false, or unknown. [3] Attestation\npredictions are denoted by \u039b.\nA biased model will appear to perform well on\ndataset samples with entailment labels that happen\nto align with the bias. Table 1 shows examples\nfrom the Levy/Holt dev set.\n\n\n3Alternatively, LLM perplexity for a statement could be\nused; however, this is not always available, e.g. with GPT-3.\n\n\n**Dev Sample Query: [premise]** _\u21d2_ **[hypothesis]** **Dataset Label** **Bias Prediction**\n\n\nGeysers are common to New Zealand _\u21d2_ Geysers are found in New Zealand Entail \u039b = hypothesis Attested\nGeysers are found in New Zealand _\u21d2_ Geysers are common to New Zealand No-Entail \u039b", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_2250", "chunk_text": " Zealand _\u21d2_ Geysers are found in New Zealand Entail \u039b = hypothesis Attested\nGeysers are found in New Zealand _\u21d2_ Geysers are common to New Zealand No-Entail \u039b = hypothesis Not-Attested\n\n\nWhiskey consists chiefly of alcohol _\u21d2_ Whiskey contains alcohol Entail \u03a6 = _f_ ( _consists chiefly of_ ) **<** _f_ ( _contains_ )\nWhiskey contains alcohol _\u21d2_ Whiskey consists chiefly of alcohol No-Entail \u03a6 = _f_ ( _contains_ ) **>** _f_ ( _consists chiefly of_ )\n\n\nTable 1: Two pairs of samples are consistent with a respective bias. Model predictions made on the basis of the bias\n\n_\u00b7_\nwill appear to predict the direction of entailment for each sample. _f_ ( ) maps a term to its corpus frequency.\n\n\n\nAs discussed in \u00a72, we draw inspiration from the\n_hypothesis-only baseline_ (Poliak et al., 2018), but\n\n- ur test only queries model memory about the hypothesis without any training. We describe prompt\ngeneration in detail in \u00a74.2, with an example in\nappendix Table 13.\n\nDasgupta et al. (2022) show a similar effect in\nLLMs on abstract reasoning tests, related to sentential content, and equate it to human tendencies.\nIn contrast, we examine the risks of propositional\nmemory on more realistic inference tasks.\n\n\n**The Relative Frequency Bias (** \u03a6 **)** is the use by\nLLMs of a simple rule for deciding entailment,\ncalculable from corpus statistics. Entailment is\naffirmed if, ignoring named entities, the eventuality\nin premise _P_ is less frequent in training than that\nin hypothesis _H_ .\nThis bias is reflected in natural text: it is known\nthat nouns follow a trend of becoming more specific\nas corpus-frequency decreases (Rosch et al., 1976;\nCaraballo and Charniak, 1999) and the same is\n\n- bserved for predicates (McKenna et al., 2023).\nSince entailments may carry from a specific term\nto a more general one, e.g. SPRINT _entails_ RUN,\nrelative frequency can often indicate direction of\nentailment. However", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_2700", "chunk_text": "McKenna et al., 2023).\nSince entailments may carry from a specific term\nto a more general one, e.g. SPRINT _entails_ RUN,\nrelative frequency can often indicate direction of\nentailment. However, this is an artifact of natural\ntext and has no direct relationship with meaning.\nTest samples are labeled for agreement with this\nbias separately from models, with examples shown\nin Table 1. Since LLM pre-train corpora are impractically large or proprietary, we instead use Google\nN-grams [4] as a proxy of the natural distribution of\ntext, and thus the distributions of these corpora.\nWe average frequencies between the years 19502019, and compare between _P_ and _H_ . To acquire\nthe generic eventualities in each sample, we exclude any extracted entities and lemmatize predicate phrases; further, we reduce the effect of noise\nand sparsity by requiring a wide margin of difference between _P_ and _H_ frequency estimates. Frequency decisions are denoted by \u03a6.\n\n\n[4https://books.google.com/ngrams](https://books.google.com/ngrams)\n\n\n\n**3.2** **Datasets**\n\n\n**Levy/Holt** consists of premise-hypothesis pairs,\nwith a task formatted: \u201cGiven [premise _P_ ], is it\ntrue that [hypothesis _H_ ]?\u201d (Levy and Dagan, 2016;\nHolt, 2019). Each _P_ - and _H_ - statement has the\nproperty of containing one predicate with two entity arguments, (where the same entities appear in\nboth _P_ and _H_ ) as shown in Table 2. This targeted\ndataset is ideal for precisely measuring model understanding of predicates, because entailment between statements is decidable purely on the basis\n\n- f the predicates and their attributes. We study the\nchallenging directional subset, where entailments\nhold in one direction but _not_ both.\n\n\n**RTE-1** is one of the original and most difficult\ntests of NLI (Dagan et al., 2006). It is not purely\ndirectional on the basis of predicates or consistently\nstructured like Levy/Holt, so we leave it out of the\nbehavioral experiments. However, it is a widely\nunderstood dataset, and we use it to demonstrate\nthe impact of", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_3150", "chunk_text": "\ndirectional on the basis of predicates or consistently\nstructured like Levy/Holt, so we leave it out of the\nbehavioral experiments. However, it is a widely\nunderstood dataset, and we use it to demonstrate\nthe impact of the two biases in general NLI in \u00a78.\n\n\n**Exclusions** are made of NLI datasets relating\nto knowledge of the world, since we aim to test\nLLMs on their capability to reason purely about\nthe semantics of natural language predicates with\n- ut relying on memorized facts. We explicitly\navoid datasets such as MMLU (Hendrycks et al.,\n2021), Natural Questions (Kwiatkowski et al.,\n2019), OpenBookQA (Mihaylov et al., 2018) etc.\n\n\n**3.3** **Dataset Transformations**\n\n\n**The Standard Inference Task (** _I_ **)** is on original\nNLI datasets, in which entailment is determinable\nby using general language inference on sentences.\nIn Levy/Holt, it is determinable just by predicates.\nWe define three dataset transformations to study\nthe change in model responses as targeted information is removed. These are the randomized premise\npredicate setting _IRandPrem_, and two argument\ntransformations: generic arguments _I_ _[GenArg]_, and\ntype-constrained randomized arguments _I_ _[RandArg]_ .\n\n\n**Task** **Label** **Dev Sample Query: [premise]** _\u21d2_ **[hypothesis]**\n\n\n_I_ Entail George Bush **waswas** **[was]** **waswasthethe** **[the]** **thetheGovernorGovernor** **[Governor]** **GovernorGovernorofof** **[of]** **ofof** Texas _\u21d2_ George Bush **isis** **[is]** **isisaa** **[a]** **aapoliticianpolitician** **[p]** **politicianpolitician** **[olitician]** **fromfrom** **[from]** **fromfrom** Texas\n_IRandP rem_ No-Entail George Bush **residedresided** **[resided]** **residedresidedinin** **[in]** **inin** Texas _\u21d2_ George Bush **isis** **[is]** **isisaa** **[a]** **aapoliticianpolitician**", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_3600", "chunk_text": " **[resided]** **residedresidedinin** **[in]** **inin** Texas _\u21d2_ George Bush **isis** **[is]** **isisaa** **[a]** **aapoliticianpolitician** **[p]** **politicianpolitician** **[olitician]** **fromfrom** **[from]** **fromfrom** Texas\n\n\nTable 2: From the original dataset task ( _I_ ) we derive the Random Premise task ( _IRandP rem_ ), respecting typeconstraints. A random premise is highly unlikely to entail the hypothesis, so samples are relabeled No-Entail.\n\n\n**Task** **Label** **Dev Sample Query: [premise]** _\u21d2_ **[hypothesis]**\n\n\n_I_ Entail **IndiaIndiaIndiaIndiaIndia** exports tons of **ricerice** **[rice]** **ricerice** _\u21d2_ **IndiaIndia** **[India]** **IndiaIndia** exports **ricerice** **[rice]** **ricerice**\n_I_ _[GenArg]_ Entail **locationlocationlocationlocationlocationXXXXX** exports tons of **foodfood** **[food]** **foodfoodYY** **[Y]** **YY** _\u21d2_ **locationlocation** **[location]** **locationlocationXX** **[X]** **XX** exports **foodfood** **[food]** **foodfoodYY** **[Y]** **YY**\n_I_ _[RandArg][\u2193]_ Entail **SloterdijkSloterdijkSloterdiSloterdijkSloterdijkjk** exports tons of **oatmealoatmeal** **[oatmeal]** **oatmealoatmealcookiescookies** **[cookies]** **cookiescookies** _\u21d2_ **SloterdijkSloterdijk** **[Sloterdi]** **SloterdijkSloterdijk** **[j][k]** exports **oatmealoatmeal** **[oatmeal]** **oatmealoatmealcookiescookies** **[cookies]** **cookiescookies**\n_I_ _[RandArg][\u2191]_ Entail **HelsinkiHelsinkiHelsinkiHelsinkiHelsinki** exports tons of **GrannyGranny** **[Grann]** **GrannyGranny** **[y]** **SmithSmith** **[Smith]** **SmithSmith**", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_4050", "chunk_text": " Entail **HelsinkiHelsinkiHelsinkiHelsinkiHelsinki** exports tons of **GrannyGranny** **[Grann]** **GrannyGranny** **[y]** **SmithSmith** **[Smith]** **SmithSmith** _\u21d2_ **HelsinkiHelsinki** **[Helsinki]** **HelsinkiHelsinki** exports **GrannyGranny** **[Grann]** **GrannyGranny** **[y]** **SmithSmith** **[Smith]** **SmithSmith**\n\n\nTable 3: An original dev sample ( _I_ ) is transformed by insertion of entity types ( _I_ _[GenArg]_ ); by real entities sampled\nfrom the 5% least frequent in NewsCrawl ( _I_ _[RandArg][\u2193]_ ); and also from the 5% most frequent ( _I_ _[RandArg][\u2191]_ ).\n\n\n\nTransformations involve first identifying the\ntypes of entities in statements, in order to constrain entity or predicate replacements. We type\neach entity with one of the 48 FIGER types (Ling\nand Weld, 2012), such as \u201cperson,\u201d \u201clocation,\u201d etc.\nFirst, an entity linker (Nguyen et al., 2014) identifies the Freebase ID (Bollacker et al., 2008) for an\nentity, from which we then obtain its FIGER type;\nwe assign a default type \u201cthing\u201d in failure cases.\n\n\n**The Random Premise Task (** _IRandPrem_ **)** replaces the original premise predicate with a random predicate, while maintaining the same entity\narguments. This manipulation produces a dataset in\nwhich all samples are labeled No-Entail, since\ntwo randomly paired predicates are very unlikely to\nbe related by entailment. Thus, positive decisions\nby the model are false positive hallucinations. [5]\n\nTo maintain naturalness and grammaticality, we\nconstrain a new predicate to have argument slots\n\n- f the same types as the original premise. For example, \u201c[medicine] is indicated for patients with\n\n[disease]\u201d is swapped for \u201c[medicine] does not\ncure [disease]\u201d. We source candidates from dev\nset premises satisfying the target type-constraints,\nand sample uniform randomly. We map the original\nentities to their respective slots in the new premise.\nExamples are shown in Table 2. _IRandPrem_ is", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_4500", "chunk_text": "\u201d. We source candidates from dev\nset premises satisfying the target type-constraints,\nand sample uniform randomly. We map the original\nentities to their respective slots in the new premise.\nExamples are shown in Table 2. _IRandPrem_ is a\ngood test of model reliance on propositional mem\n- ry, since we prevent entailments while maintaining the attestedness of conclusions (hypotheses).\n\n\n**The Generic Argument Task (** _I_ _[GenArg]_ **)** replaces original entities with unique FIGER-typed\n\n\n5We manually inspected the generated random premise\nentries for the Levy/Holt dataset to verify this: we found\n86.6% of entries are successfully non-entailing, 3.8% undecided cases, and only 9.6% are unintended true entailments.\n\n\n\nidentifiers, e.g. \u201clocation X\u201d and \u201cfood Y.\u201d By\nmasking the identities of entities, this test is designed to remove entity information while maintaining the same entailment label, as a baseline\ncontrol setting. We append unique identifiers (e.g.\n\u201cX,\u201d \u201cY\u201d) to allow tracking of entity slots across\nthe premise and the hypothesis.\n\n\n**The Random Argument Task (** _I_ _[RandArg]_ **)** replaces original entities with other real, random entities of the same FIGER-type. Like _I_ _[GenArg]_, this\ntest is designed to create novel strings by modifying\nstatements without changing entailment labels. But\nnow we test model sensitivity to added extraneous\ninformation. Examples are shown in Table 3.\nWe use entity type constraints here to ensure\npolysemous predicates maintain the same sense.\nFor example, a different sense of _run_ is used\nin \u201c[person] runs [organization]\u201d vs. \u201c[person]\nruns [software]\u201d, but between different entities of\nthe same type, the same senses are used, so the\nexact entity IDs do not affect entailment labels\n(Yarowsky, 1993). We source new entities from\nNewsCrawl (Barrault et al., 2019), a decade-long\nspan of multi-source news text, in which entities\nare typed as above. We sample new entities uniform randomly from the 5% least common entities\nin NewsCrawl ( _I_ _[RandArg][\u2193]_ ), and the 5% most common ( _I_ _[Rand", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_4950", "chunk_text": " typed as above. We sample new entities uniform randomly from the 5% least common entities\nin NewsCrawl ( _I_ _[RandArg][\u2193]_ ), and the 5% most common ( _I_ _[RandArg][\u2191]_ ). We insert the sampled entities\nwhile preserving the rest of each statement.\n\n\n**4** **Querying Models with Prompts**\n\n\n**4.1** **Models**\n\n\n**LLaMA** is a recent LLM model family which\nrivals or surpasses GPT-3 performance while being\n\n- pen to scientific study. A range of model sizes\n\n\nare provided, and we test the largest **LLaMA-65B**\nmodel. LLaMA is not fine-tuned. In preliminary\nexperiments on the Levy/Holt dataset, we found\ntwo popular fine-tuned LLaMA variants, Alpaca\n(Taori et al., 2023) and Vicuna (Chiang et al., 2023),\nperform similarly to LLaMA base models and underperform LLaMA-65B, so we leave them out of\nfurther experiments.\n\n\n**GPT-3 Series** models are closed to deep scientific review (Brown et al., 2020), though they are a\nwidely-used comparison for their performance, and\nhave been reasonably well-studied. We evaluate on\n**text-davinci-003 (GPT-3.5)**, as it is the largest, and\nhas undergone instruction- and RLHF-finetuning,\nenabling interesting comparisons.\n\n\n**PaLM** is larger than GPT-3, which often claims\nstate-of-the-art on evaluation datasets. We use the\n\nlargest **PaLM-540B** base model, which is also only\npretrained, so it serves as a further comparison\npoint to LLaMA.\n\n\nLater GPT models (like text-davinci-003 in our\nexperiments) have been pre-trained and fine-tuned,\nwhile base LLaMA and PaLM have only undergone pre-training, so their contrast indicates what\nstage of training is responsible for the phenomena\nwe study. Our aim is not to judge which LLM\nis superior, but to show the common sources of\nhallucination they share.\nWe also omit models superseded in performance\nby LLaMA (e.g. OPT, GPT-J, etc", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_5400", "chunk_text": " aim is not to judge which LLM\nis superior, but to show the common sources of\nhallucination they share.\nWe also omit models superseded in performance\nby LLaMA (e.g. OPT, GPT-J, etc.), as well as\nproducts that are closed to scientific review (e.g.\nGPT-4, Bard, etc.) [6] .\n\n\n**4.2** **Prompt Design and Evaluation**\n\n\n**Formatting** - f test samples is done by inserting\nthe premise and hypothesis into a prompt template,\nwhich is used to query the model in natural language. Following this, we append a three-way answer choice: A) Entailment, B) Neutral, C) Contradiction, following the typical format in NLI (Bowman et al., 2015).\n\n\n**Selection** - f the prompt template used in test is\ndecided by the highest AUC obtained on the respective dev set. We try 8 promising templates including 5 from Schmitt and Sch\u00fctze (2021), also used\nin other NLI work [7] (Webson and Pavlick, 2022).\n\n\n6We include an analysis of GPT-4 in Appendix F.\n7See Appendix A for details on prompt template selection.\n\n\n\nIdeally, an LLM with advanced language understanding ability could perform inference in zeroshot without annotated examples, which would\nraise confidence that this faculty is ready for downstream tasks. To this end, we examine each LLM in\nzero-shot (detailed in Appendix A), but they exhibit\nseverely degraded, even near-random performance.\nWe turn to few-shot, and hand-annotate a minimal 4 examples in the style of the template, with\nadded explanations about why the given answer\nis correct for each example. These examples are\nprepended before the query (see Appendix A for\nan example). Our goal is to study model behavior\nas conditions change, not to maximize the score on\nany particular dataset. Therefore, we use a minimal 4-example setup, which we find is capable of\nevoking positive responses from all three LLMs on\neach dev set, across most templates.\n\n\n**Scoring** is done by converting choice A into\nEntail and collapsing both B and C choices into\nNo-Entail to align with Levy/Holt and RTE-1\nannotation. For behavioral experiments in \u00a75, \u00a76,\nand \u00a7", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_5850", "chunk_text": "oring** is done by converting choice A into\nEntail and collapsing both B and C choices into\nNo-Entail to align with Levy/Holt and RTE-1\nannotation. For behavioral experiments in \u00a75, \u00a76,\nand \u00a77, we score the model solely based on its textual response. All models successfully choose one\n\n- f A/B/C on all dev questions, showing compatibility with the QA format.\nFor the analysis in \u00a78 which measures model performance across confidence thresholds, we convert\nthe letter choice to a probability with the mapping:\n\n\n_S_ ent = 0 _._ 5 + 0 _._ 5 _\u2217_ I[tok = **A** ] _\u2217_ _S_ tok\n\n_\u2212_ 0 _._ 5 _\u2217_ I[tok _\u2208{_ **B** _,_ **C** _}_ ] _\u2217_ _S_ tok\n\n\nWhere I is the indicator function, and _Sent_ estimates the probability of Entail from a textual\n\n- utput (0 _\u2264_ _S_ ent _\u2264_ 1) with token probability _Stok_ .\nThe linear transformation preserves the ordering of\nmodel confidences, which is sufficient for calculating a precision-recall curve.\n\n\n**5** **Experiment 1: Attestation Bias**\n\n\nWe begin our experiments by assessing LLMs\u2019 reliance on their propositional memory of training\ntext by conditioning each model\u2019s entailment task\npredictions _I_ - n its own predictions of attestation\n\u039b. We do this by comparing estimated probabilities\n\n- f predicting Entail conditioned on whether the\nhypothesis is predicted Attested or not.\nFurther, we test a setting which controls for the\npossibility that original Levy/Holt entailments may\ncoincidentally refer to attested facts, which could\n\n\nFigure 1: Exp-1. Estimated probability of predicting\nEntail for **original** entries in Levy/Holt, conditioned\n\n- n LLMs\u2019 attestation of hypotheses (\u039b). This setting is\nintuitive but may be subject to spurious correlations.\n\n\nFigure 2: Exp-1. Estimated probability of predicting\nEntail for **Random-Premise** entries in Levy/Holt,\nconditioned on LLMs\u2019 attestation of hypotheses (\u039b).\nNow, predicting Entail is false positive hallucination\n(lower is better). Models are", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_6300", "chunk_text": " of predicting\nEntail for **Random-Premise** entries in Levy/Holt,\nconditioned on LLMs\u2019 attestation of hypotheses (\u039b).\nNow, predicting Entail is false positive hallucination\n(lower is better). Models are sensitive to attestation, and\nhallucinate more when the hypothesis is attested.\n\n\nlead to spurious correlation between inference and\nattestation scores without clearly demonstrating use\n\n- f memory versus true entailment. This controlled\nsetting is the random premise task _IRandPrem_,\nwhich converts entailments into non-entailments\n\nwithout altering the hypothesis. An ideal model\ncapable of drawing inferences from information in\ncontext should detect that in the _IRandPrem_ task it\nis no longer possible to infer the hypothesis based\n\n- n the premise (even if the hypothesis is itself\nattested in training), and never predict Entail.\nThus, in _IRandPrem_, all Entail predictions are\nassumed to be false positive hallucinations.\n\n\n**5.1** **Results**\n\n\nWith _I_, _IRandPrem_ and \u039b predictions acquired as\ndescribed in \u00a73.1, we present the conditional probabilities in Figures 1 and 2. It is clear that a model\u2019s\n\n\n\nmemory about the hypothesis plays a part in its predictions of the hypothesis given a premise, either\nrelated or random.\n\nFor _I_, we observe significantly higher probability of predicting Entail when the hypothesis is Attested. In the random premise task\n_IRandPrem_, this trend continues. LLaMA, GPT3.5, and PaLM, respectively, show a 1.9x, 2.2x,\nand 2.0x higher chance of falsely predicting that a\nrandom premise Entails the hypothesis if it already predicts the hypothesis is Attested. This\nfalse positive hallucination and its impact on NLI\nperformance is investigated further in \u00a78.\n\nThis behavior is observed across model families\n\n(LLaMA, GPT, and PaLM), establishing that it is\ndue to pretraining rather than Instruction-tuning\n\n- r RLHF, since LLaMA and PaLM have only undergone pretraining. This behavior is undesirable,\nbecause model predictions on NLI tasks should be\nbased solely on general language understanding,\nnot prior knowledge. We may conclude that mem\n- ry of training data is a significant contributor in\nLLM inference, and may", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_6750", "chunk_text": ". This behavior is undesirable,\nbecause model predictions on NLI tasks should be\nbased solely on general language understanding,\nnot prior knowledge. We may conclude that mem\n- ry of training data is a significant contributor in\nLLM inference, and may be an important source of\nhallucination.\n\n\n**5.2** **Implications for Real Applications**\n\n\nUsing prior knowledge as part of language inference has bad implications for the use of LLMs in\nreal applications. We offer an example scenario\n\n- f a question-answering task where user questions\nare answered from a Knowledge Base (KB). In\ntypical formulations of this task, if a statement in\nthe KB (premise) entails a user query (hypothesis), the premise may be formulated into an answer.\nConsider a KB such as a legal document or HR rulebook. Assume that the text is prepended to the user\nquery and presented to the LLM, as in other works\n(Srinivasan et al., 2022). Given our findings, we\nmight observe the LLM hallucinating answers to\nquestions using information which is not presented\nin the KB, but may have been read by the LLM in\ntext from other sources during pretraining. These\nanswers could be illogical, contradictory, and could\nmisrepresent the views of the KB, or other harms.\nSuch poor use of in-context learning has already\nbeen observed in specific domains like medicine\n(Jimenez Gutierrez et al., 2022).\n\nIn general, this is a risk for LLMs which (a) are\ndeployed for tasks like QA by feeding novel text\n(e.g. a legal document) in-context as part of the\nuser query, and (b) are trained on datasets which are\n\n\n**Levy/Holt (Directional)**\n\n\n\n\n\n\n\nTable 4: Exp-2. Scoring model outputs in different\nargument-replacement tasks. We indicate the **highest**\nand lowest recall score across replacement settings. Recall decreases sharply across settings in all models.\n\n\nprivate or otherwise infeasibly large to read manually, containing many facts and human opinions\nunknowable to both the user and modeler.\n\n\n**6** **Experiment 2:**\n**Entities are Indices to Memory**\n\n\nIn \u00a75, we have established that propositional mem\n- ry explains a significant portion of false positives\nin LLM inference predictions. In this section, we", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_7200", "chunk_text": "**6** **Experiment 2:**\n**Entities are Indices to Memory**\n\n\nIn \u00a75, we have established that propositional mem\n- ry explains a significant portion of false positives\nin LLM inference predictions. In this section, we\ncontinue by showing the importance of named entities in the process of LLMs\u2019 memory recall.\nAs described in \u00a73.3, we manipulate the entities with the _I_ _[GenArg]_ generic argument replacement, and two random entity replacements, one\nwith infrequent-entities _I_ _[RandArg][\u2193]_ and one with\nfrequent-entities _I_ _[RandArg][\u2191]_ (examples in Table 3).\nBy replacing arguments constrained by type, entailment labels are maintained; however, new samples should contain novel strings not attested in pretrain corpora. We expect that an ideal, generalizing\nmodel would maintain its predictions across all\nconditions; a flawed model utilizing the _attestation_\n_bias_ would predict fewer Entail, since entities\nno longer identify these statements in training.\n\n\n**6.1** **Results**\n\n\nWe report results across conditions in Table 4. We\n\n- bserve two phenomena across all three models,\naligning with the above conjecture of \u201cflaws.\u201d\nFirst, we observe that all models\u2019 behavior significantly changes in the same way when original entities are replaced by either entity types or random\nreal entities. Despite similar (or marginally increas\n\n\ning) precision across conditions, recall degrades\nsharply from original entities ( _I_ ) (GPT-3.5 @92.3)\nto random frequent entities ( _I_ _[RandArg][\u2191]_ ) (GPT-3.5\n@55.3). Generic-argument _I_ _[GenArg]_ performance\nalso degrades in this way, showing that this is not a\nmatter of poorly selected real entities, but rather a\nloss of information from the original dataset which\nmodels were using to answer questions.\nSecond, across the 3 models, we observe a significant difference in recall between the two real\nentity conditions _I_ _[RandArg][\u2193]_ and _I_ _[RandArg][\u2191]_, which\nare both composed of unattested statements, but involve entities that differ in typical corpus frequency.\nInfrequent entities ( _I_ _[Rand", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_7650", "chunk_text": "[RandArg][\u2193]_ and _I_ _[RandArg][\u2191]_, which\nare both composed of unattested statements, but involve entities that differ in typical corpus frequency.\nInfrequent entities ( _I_ _[RandArg][\u2193]_ ) yield better generalization and a higher recall (GPT-3.5 @66.5) than\nfrequent entities ( _I_ _[RandArg][\u2191]_ ) (GPT-3.5 @55.3).\nThese findings corroborate those from \u00a75, that\nLLMs use memory as part of language inference,\nand additionally show that these memories are recalled using named entities acting as indices. These\nexperiments demonstrate that too much prior exposure to an entity may impede model generalization\nwhen that entity is discussed in novel inferences:\nthe more a model has read about an entity during\npretraining, the less capable it is of drawing novel\nnatural language inferences involving it. This is\nthe case even though the inferences do not require\ndetailed knowledge of the entity.\nLike \u00a75, the effect is consistent across models,\nindicating LLM pretraining is responsible.\nWe show similar results on RTE-1 in Appendix\nB. Further, instructing LLMs to ignore propositional memory in Appendix C shows little change.\n\n\n**7** **Experiment 3: Relative Frequency Bias**\n\n\nWe continue the conditioning experiments from \u00a75,\nnow exploring the relative frequency bias. Sample labels for this bias are denoted by the modelagnostic \u03a6 as described in \u00a73.1. \u03a6 labels the conformance of sample predicates to the bias: \u03a6 _<_\nmeans _P_ is less corpus-frequent than _H_ by a margin (positive class), \u03a6 _>_ means _P_ more frequent\nthan _H_ by the margin (negative class). To control\nfor differences between datasets, the margin is set\nso that 1/3 of samples are classed as \u201croughly equal\u201d\n(\u03a6 _\u2248_ ), which we discard.\nFollowing the observations in \u00a76, we further apply a generic-argument transformation to control\nfor attestation, yielding _IRandPrem_ _[GenArg]_ [. With the en-]\ntities masked, models cannot recall propositional\nmemory for this task: by re-calculating the \u039b mea\n\nFigure 3: Exp-", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_8100", "chunk_text": " yielding _IRandPrem_ _[GenArg]_ [. With the en-]\ntities masked, models cannot recall propositional\nmemory for this task: by re-calculating the \u039b mea\n\nFigure 3: Exp-3. Estimated probability of predicting\nEntail for **random-premise** Levy/Holt conditioned\n\n- n relative frequencies (\u03a6), with original ( _IRandP rem_ )\n\n- r generic ( _IRandP rem_ _[GenArg]_ [) entities. Predicting][ Entail]\nis false positive hallucination (lower is better). Models\nhallucinate more often when test samples conform to\nthe relative frequency bias (\u03a6 _<_ ) than when not (\u03a6 _>_ ).\n\n\nsure with generic arguments, only 2 hypotheses are\nstill predicted as Attested by GPT-3.5, whereas\nfor LLaMA and PaLM, the numbers are also only\n6.2% and 3.9%. Additionally, as with _IRandPrem_,\nhere the entailment label of each sample remains\nNo-Entail, so any Entail prediction is false\npositive hallucination.\n\n\n**7.1** **Results**\n\n\nWe estimate the probabilities of models predicting Entail conditioned on the frequency label \u03a6,\nbetween _IRandPrem_ and _IRandPrem_ _[GenArg]_ [settings, and]\npresent the results in Figure 3. We observe a clear\nand consistent rise of hallucination when samples\nconform to the bias. Namely, in case of \u03a6 _<_, models\nare more likely to predict Entail, even though\nno semantic relation exists between _P_ and _H_ .\n\nBetween the two settings, with _IRandPrem_, when\nentities are available, this effect is moderate. On\nthe other hand, with _IRandPrem_ _[GenArg]_ [when entity-based]\nmemory is blocked, we observe a decrease in the\n\n- verall level of hallucination, but the separation between \u03a6 _<_ and \u03a6 _>_ becomes more drastic, to 1.6x,\n1.8x and 2.0x for LLaMA, GPT-3.5 and PaLM\nrespectively. This indicates a tension between \u039b\nand \u03a6: propositional memory may be used when\navailable, and if not, the", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_8550", "chunk_text": " 2.0x for LLaMA, GPT-3.5 and PaLM\nrespectively. This indicates a tension between \u039b\nand \u03a6: propositional memory may be used when\navailable, and if not, the predicate pairing may be\nattended to more closely. Again, the \u03a6 effect is\n\n- bserved across the three model families, revealing its root in the large-scale pre-training process,\nrather than model peculiarities or fine-tuning.\n\n\n\n**8** **Impact of Bias on Performance**\n\n\nWe have demonstrated two sources of hallucination\n\nby LLMs on inference tasks. We now assess their\nimpact on model performance to quantify their risk.\nWe compare LLMs\u2019 performance between NLI\nsubsets that are _consistent_ - r _adversarial_ to each\n\nbias. A sample _P_ \u22a8 _H_ ? is _consistent_ with a bias\nwhen the prediction by the bias **agrees with** the\ngold entailment label; conversely, it is _adversarial_\nto a bias when the prediction by the bias **disagrees**\n**with** the label.\n\nFor example, \u201cGoogle bought YouTube \u22a8\nGoogle owns YouTube\u201d is _consistent_ with the attestation bias of every model, because the conclusion\n_Google owns YouTube_ is attested in every LLM\u2019s\ntraining data, and the sample label is Entail;\n\u201cApple owns Samsung \u22ad Apple bought Samsung\u201d\nis also _consistent_, because its conclusion is not attested and the sample label is No-Entail. The\nreverses of these two samples are _adversarial_, since\ntheir respective attestedness (unchanged) does not\nagree with the entailment labels (now flipped). For\neach subset, there is substantial representation in\nboth Levy/Holt and RTE-1 (see appendix Table 9).\nWhile earlier experiments inspected model textual responses to characterize behavior change,\nwe now use area under the precision-recall curve\n(AUC) to summarize model performance over a\ntunable confidence threshold (scoring described in\n\u00a74.2), which is better for measuring practical discriminative power. Following Li et al. (2022), we\nre-scale AUC values to normalize over the label\n\ndistribution, yielding _AUCnorm_ values that assign\nrandom classifiers 0% and perfect classifiers 100%.\nWe report results in Table ", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_9000", "chunk_text": " et al. (2022), we\nre-scale AUC values to normalize over the label\n\ndistribution, yielding _AUCnorm_ values that assign\nrandom classifiers 0% and perfect classifiers 100%.\nWe report results in Table 5. Under the standard inference task _I_, the performance drop from\n\u039bCONSISTENT to \u039bADVERSARIAL is severe for all 3\nLLMs: they deteriorate from very good classifiers to poor or even near-random ones. [8] This\nfragility from the _attestation bias_ can be alleviated\nby masking entities with type-identifiers (condition\n_I_ _[GenArg]_ ), which reduces the performance drop.\nOn the other hand, with the generic arguments\nin _I_ _[GenArg]_, LLMs are forced to focus on the predicates in each proposition. As a result, the impact\n\n- f the _relative frequency bias_ is intensified. From\nthe standard inference task _I_ to _I_ _[GenArg]_, the average performance drop from the _cons._ to _adv._\n\n\n8We note \u039b predictions could possibly be influenced by\nmodel-specific idiosyncrasies in prompt format. We provide\nan analysis in Appendix E, where we find no significant effect.\n\n\n**Levy/Holt** **RTE-1**\n\n\nAttestation (\u039b) Rel. Frequency (\u03a6) Attestation (\u039b) Rel. Frequency (\u03a6)\n\n\n**Model** **Task** _**cons.**_ _**adv.**_ _**diff.**_ _**cons.**_ _**adv.**_ _**diff.**_ _**cons.**_ _**adv.**_ _**diff.**_ _**cons.**_ _**adv.**_ _**diff.**_\n\n\nTable 5: LLM performance on subsets where \u039b/\u03a6 is _consistent_ / _adversarial_ to entailment labels, measured with\n_AUCnorm_ (0% = random chance performance). Decrease from _cons_ to _adv_ subsets are shown in the _diff._ columns.\n\n\n\nsubsets w.r.t. \u03a6 is widened from 10.1% to 16.1%\n\nfor Levy/Holt and from 14.8% to 16.5% for RTE1. The", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_9450", "chunk_text": " the _diff._ columns.\n\n\n\nsubsets w.r.t. \u03a6 is widened from 10.1% to 16.1%\n\nfor Levy/Holt and from 14.8% to 16.5% for RTE1. The differences for \u03a6-consistency subsets are\ngenerally narrower than \u039b-consistency subsets, possibly because the relative frequencies require generalizing from instances, and may be more difficult\nto capture, and potentially because frequency measures with Google N-gram are a crude estimate of\nthe actual frequencies in LLM pre-train corpora.\n\n\n**9** **Conclusion**\n\n\nAcross several major LLM families and experimental settings, we demonstrate two important biases\nin the performance of LLMs on natural language\ninference tasks, which may also manifest in applied\ntasks as hallucination. Contrary to claims of LLM\ngeneral reasoning capabilities, we show that much\n\n- f this performance is achieved by (1) recall of relevant memorizations and (2) corpus-based biases\nlike term frequency. Since these factors are reproduced in all models, we establish that they originate\nin LLM pre-training, and are not corrected during\nGPT-3.5 fine-tuning.\nWe conclude that LLMs, though powerful, use\nunsatisfactory tools for the basic tasks of language\nunderstanding and inference. We propose several\napproaches to control for these biases in evaluation,\nand ultimately conclude that further attention on\nalleviating these biases are needed, before LLMs\nmay be trusted to reason robustly about language.\n\n\n**Limitations**\n\n\nIn this paper, we have discussed two prominent\nsources of hallucination for LLMs in natural lan\nguage inference tasks. We acknowledge that this is\nnot an exhaustive search of all the sources, where\nfurther exploration should be done in future work.\nWe also note that after controlling for the factors\n\n\n\ndiscussed in this paper, there remains residual, unexplained performance on NLI tasks. This residual\nmight be due to other undiscovered biases or possibly generalising inference capability. We leave\nfurther exploration of this residual to future work.\nAs discussed in Appendix A, we compared a\nrange of popular LLM prompting techniques and\nselected the most promising approach. We acknowledge that there could potentially be other\nnovel prompting techniques that could help the\nLLMs resist the influence of the biases discussed\nin this paper. We identify this as", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_9900", "chunk_text": " of popular LLM prompting techniques and\nselected the most promising approach. We acknowledge that there could potentially be other\nnovel prompting techniques that could help the\nLLMs resist the influence of the biases discussed\nin this paper. We identify this as an open question\nand advocate for future research.\n\n\n**Ethical Considerations**\n\n\nThis paper discusses two major sources of hallucination in LLM output when asked to perform\nnatural language inference, which we note is a capability required of many downstream tasks such\nas summarization, question answering, etc. We\nshow that users of LLMs may be subjected to faulty\njudgements if the content of their request overlaps\nwith data in pretraining. However, it is difficult to\nascertain for both a user or modeler exactly what\nis contained in pretraining data, or how this will\ninteract with a user\u2019s query. Our proposed attestation query shows promise in detecting potential\n\n- verlaps, but model responses in applications of\nthese cases are not explored. Further, the relative\nfrequency bias demonstrates a much more subtle\nproblem of corpus distribution that is naturally inherent to model pretraining on human generated\n\ntext.\n\nIn light of these, the potential harms of LLM use\nfor drawing natural language inferences may include: offering inaccurate or irrelevant information\nto a user\u2019s query or contradiction of information\nprovided in-context with a user\u2019s query.\n\n\n**Acknowledgements**\n\n\nThis research was supported by ERC Advanced\nFellowship GA 742137 SEMANTAX and the University of Edinburgh Huawei Laboratory.\n\n\n**References**\n\n\nLo\u00efc Barrault, Ond\u02c7rej Bojar, Marta R. Costa-juss\u00e0,\nChristian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias M\u00fcller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\n[Findings of the 2019 Conference on Machine Trans-](https://doi.org/10.18653/v1/W19-5301)\n[lation (WMT19). In](https://doi.org/10.18653/v1/W19-5301) _Proceedings of the Fourth Con-_\n_ference on Machine Translation (Volume 2: Shared_\n_Task Papers, Day 1)_, pages 1\u201361, Florence,", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_10350", "chunk_text": "10.18653/v1/W19-5301) _Proceedings of the Fourth Con-_\n_ference on Machine Translation (Volume 2: Shared_\n_Task Papers, Day 1)_, pages 1\u201361, Florence, Italy. Association for Computational Linguistics.\n\n\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\n[Sturge, and Jamie Taylor. 2008. Freebase: A col-](https://doi.org/10.1145/1376616.1376746)\n[laboratively created graph database for structuring](https://doi.org/10.1145/1376616.1376746)\n[human knowledge. In](https://doi.org/10.1145/1376616.1376746) _Proceedings of the 2008 ACM_\n_SIGMOD International Conference on Management_\n\n_of Data_, SIGMOD \u201908, page 1247\u20131250, New York,\nNY, USA. Association for Computing Machinery.\n\n\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\n[and Christopher D. Manning. 2015. A large anno-](https://doi.org/10.18653/v1/D15-1075)\n[tated corpus for learning natural language inference.](https://doi.org/10.18653/v1/D15-1075)\nIn _Proceedings of the 2015 Conference on Empiri-_\n_cal Methods in Natural Language Processing_, pages\n632\u2013642, Lisbon, Portugal. Association for Computational Linguistics.\n\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n[2020. Language models are few-shot learners.](http://arxiv.org/abs/", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_10800", "chunk_text": " Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n[2020. Language models are few-shot learners.](http://arxiv.org/abs/2005.14165)\n\n\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio\n[Ribeiro, and Yi Zhang. 2023. Sparks of Artificial](http://arxiv.org/abs/2303.12712)\n[General Intelligence: Early experiments with GPT-4.](http://arxiv.org/abs/2303.12712)\nArXiv:2303.12712 [cs].\n\n\n[Sharon A. Caraballo and Eugene Charniak. 1999. De-](https://aclanthology.org/W99-0609)\n[termining the specificity of nouns from text. In](https://aclanthology.org/W99-0609) _1999_\n_Joint SIGDAT Conference on Empirical Methods in_\n_Natural Language Processing and Very Large Cor-_\n\n_pora_ .\n\n\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n\n\n\n[2023. Quantifying Memorization Across Neural Lan-](http://arxiv.org/abs/2202.07646)\n[guage Models. ArXiv:2202.07646 [cs].](http://arxiv.org/abs/2202.07646)\n\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\n[Stoica, and Eric P. Xing. 2023. Vicuna: An open-](https://vicuna.lmsys.org)\n[source chatbot impressing gpt-4 with 90%* chatgpt](https://vicuna.lmsys.org)\n[quality.](https://vicuna.lmsys.org)\n\n\nAakanksha", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_11250", "chunk_text": "una.lmsys.org)\n[source chatbot impressing gpt-4 with 90%* chatgpt](https://vicuna.lmsys.org)\n[quality.](https://vicuna.lmsys.org)\n\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin\n - dkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\n[and Noah Fiedel. 2022. Palm: Scaling language mod-](http://arxiv.org/abs/2204.02311)\n[eling with pathways.](http://arxiv.org/abs/2204.02311)\n\n\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment challenge. In _Machine Learning Challenges. Evaluating_\n_Predictive Uncertainty, Visual Object Classification,_\n_and Recognising Tectual Entailment_, pages 177\u2013190,\nBerlin, He", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_11700", "chunk_text": ". The pascal recognising textual entailment challenge. In _Machine Learning Challenges. Evaluating_\n_Predictive Uncertainty, Visual Object Classification,_\n_and Recognising Tectual Entailment_, pages 177\u2013190,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\n\n\nIshita Dasgupta, Andrew K. Lampinen, Stephanie\nC. Y. Chan, Antonia Creswell, Dharshan Kumaran,\n[James L. McClelland, and Felix Hill. 2022. Lan-](http://arxiv.org/abs/2207.07051)\n[guage models show human-like content effects on](http://arxiv.org/abs/2207.07051)\n[reasoning. ArXiv:2207.07051 [cs].](http://arxiv.org/abs/2207.07051)\n\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language\nunderstanding. _Proceedings of the International Con-_\n_ference on Learning Representations (ICLR)_ .\n\n\n[Xavier Holt. 2019. Probabilistic Models of Relational](http://arxiv.org/abs/1907.12048)\n[Implication.](http://arxiv.org/abs/1907.12048) _arXiv:1907.12048 [cs, stat]_ . ArXiv:\n1907.12048.\n\n\nBernal Jimenez Gutierrez, Nikolas McNeal, Clayton\nWashington, You Chen, Lang Li, Huan Sun, and\n[Yu Su. 2022. Thinking about GPT-3 in-context learn-](https://aclanthology.org/2022.findings-emnlp.329)\n[ing for biomedical IE? think again. In](https://aclanthology.org/2022.findings-emnlp.329) _Findings of the_\n_Association for Computational Linguistics: EMNLP_\n_2022_, pages 4497\u20134512, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\n\n\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\n[Wallace, and Colin Raffel. 2022. Large language](http://arxiv.org/abs", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_13050", "chunk_text": "ogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\n\n\n\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\n[Training language models to follow instructions with](http://arxiv.org/abs/2203.02155)\n[human feedback. ArXiv:2203.02155 [cs].](http://arxiv.org/abs/2203.02155)\n\n\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\n[Hypothesis only baselines in natural language infer-](https://doi.org/10.18653/v1/S18-2023)\n[ence. In](https://doi.org/10.18653/v1/S18-2023) _Proceedings of the Seventh Joint Confer-_\n_ence on Lexical and Computational Semantics_, pages\n180\u2013191, New Orleans, Louisiana. Association for\nComputational Linguistics.\n\n\nEleanor Rosch, Carolyn B Mervis, Wayne D Gray,\nDavid M Johnson, and Penny Boyes-Braem. 1976.\n[Basic objects in natural categories.](https://doi.org/https://doi.org/10.1016/0010-0285(76)90013-X) _Cognitive Psy-_\n_chology_, 8(3):382\u2013439.\n\n\n[Martin Schmitt and Hinrich Sch\u00fctze. 2021. Language](https://www.aclweb.org/anthology/2021.eacl-main.108)\n[Models for Lexical Inference in Context. In](https://www.aclweb.org/anthology/2021.eacl-main.108) _Proceed-_\n_ings of the 16th Conference of the European Chap-_\n_ter of the Association for Computational Linguistics:_\n_Main Volume_, pages 1267\u20131280, Online. Association\nfor Computational Linguistics.\n\n\nKrishna Srinivasan, Karthik Raman, Anupam Samanta,\nLingrui Liao, Luca Bertelli, and Michael Bendersky.\n[2022. QU", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_13500", "chunk_text": ". Association\nfor Computational Linguistics.\n\n\nKrishna Srinivasan, Karthik Raman, Anupam Samanta,\nLingrui Liao, Luca Bertelli, and Michael Bendersky.\n[2022. QUILL: Query intent with large language mod-](https://aclanthology.org/2022.emnlp-industry.50)\n[els using retrieval augmentation and multi-stage dis-](https://aclanthology.org/2022.emnlp-industry.50)\n[tillation. In](https://aclanthology.org/2022.emnlp-industry.50) _Proceedings of the 2022 Conference on_\n_Empirical Methods in Natural Language Processing:_\n_Industry Track_, pages 492\u2013501, Abu Dhabi, UAE.\nAssociation for Computational Linguistics.\n\n\nAarne Talman and Stergios Chatzikyriakidis. 2019.\n\n[Testing the Generalization Power of Neural Network](https://doi.org/10.18653/v1/W19-4810)\n[Models across NLI Benchmarks. In](https://doi.org/10.18653/v1/W19-4810) _Proceedings of_\n_the 2019 ACL Workshop BlackboxNLP: Analyzing_\n_and Interpreting Neural Networks for NLP_, pages 85\u2013\n94, Florence, Italy. Association for Computational\nLinguistics.\n\n\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama\n[model. https://github.com/tatsu-lab/](https://github.com/tatsu-lab/stanford_alpaca)\n[stanford_alpaca.](https://github.com/tatsu-lab/stanford_alpaca)\n\n\nKushal Tirumala, Aram Markosyan, Luke Zettlemoyer,\n[and Armen Aghajanyan. 2022. Memorization with-](https://proceedings.neurips.cc/paper_files/paper/2022/file/fa0509f4dab6807e2cb465715bf2d249-Paper-Conference.pdf)\n\n[out overfitting: Analyzing the training dynamics of](https://proceedings.neur", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_13950", "chunk_text": "_files/paper/2022/file/fa0509f4dab6807e2cb465715bf2d249-Paper-Conference.pdf)\n\n[out overfitting: Analyzing the training dynamics of](https://proceedings.neurips.cc/paper_files/paper/2022/file/fa0509f4dab6807e2cb465715bf2d249-Paper-Conference.pdf)\n[large language models. In](https://proceedings.neurips.cc/paper_files/paper/2022/file/fa0509f4dab6807e2cb465715bf2d249-Paper-Conference.pdf) _Advances in Neural Infor-_\n_mation Processing Systems_, volume 35, pages 38274\u2013\n38290. Curran Associates, Inc.\n\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\n[Grave, and Guillaume Lample. 2023. Llama: Open](http://arxiv.org/abs/2302.13971)\n[and efficient foundation language models.](http://arxiv.org/abs/2302.13971)\n\n\n[Albert Webson and Ellie Pavlick. 2022. Do prompt-](https://doi.org/10.18653/v1/2022.naacl-main.167)\n[based models really understand the meaning of their](https://doi.org/10.18653/v1/2022.naacl-main.167)\n\n\n[prompts? In](https://doi.org/10.18653/v1/2022.naacl-main.167) _Proceedings of the 2022 Conference of_\n_the North American Chapter of the Association for_\n_Computational Linguistics: Human Language Tech-_\n_nologies_, pages 2300\u20132344, Seattle, United States.\nAssociation for Computational Linguistics.\n\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\n[and Denny Zhou. 2022. Chain-of-thought prompt-](https://", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_14400", "chunk_text": ", Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\n[and Denny Zhou. 2022. Chain-of-thought prompt-](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)\n[ing elicits reasoning in large language models. In](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)\n_Advances in Neural Information Processing Systems_,\nvolume 35, pages 24824\u201324837. Curran Associates,\nInc.\n\n\nOrion Weller, Marc Marone, Nathaniel Weir, Dawn\nLawrie, Daniel Khashabi, and Benjamin Van Durme.\n[2023. \"according to ...\" prompting language models](http://arxiv.org/abs/2305.13252)\n[improves quoting from pre-training data.](http://arxiv.org/abs/2305.13252)\n\n\nDavid Yarowsky. 1993. [One sense per collocation.](https://aclanthology.org/H93-1052)\nIn _Human Language Technology: Proceedings of_\n_a Workshop Held at Plainsboro, New Jersey, March_\n_21-24, 1993_ .\n\n\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B. Hashimoto.\n[2023. Benchmarking large language models for news](http://arxiv.org/abs/2301.13848)\n[summarization.](http://arxiv.org/abs/2301.13848)\n\n\n**A** **Prompt Format Selection**\n\n\nIn prompt-based interactions with the LLMs, several types of context information could be added\nto help models produce accurate and robust predictions. We attend to two design choices in prompt\nengineering: prompt templates and in-context examples.\n\n\n**Prompt templates** are known to have a direct\nand sometimes decisive impact on LLM behavior.\nAs such, we carefully select a range of clear and\nconcise templates as promising candidates. As\ndisc", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_14850", "chunk_text": ": prompt templates and in-context examples.\n\n\n**Prompt templates** are known to have a direct\nand sometimes decisive impact on LLM behavior.\nAs such, we carefully select a range of clear and\nconcise templates as promising candidates. As\ndiscussed in \u00a74.2, we run each template through\nthe dev sets of each dataset, and select the template\nwith the best discriminative power according to\nAUC scores (similarly to \u00a78). The candidate set of\ntemplates includes 3 concise templates we wrote:\n\n\n1. If [PREMISE], then [HYPOTHESIS].\n\n\n2. [PREMISE], so [HYPOTHESIS].\n\n\n3. [PREMISE] entails [HYPOTHESIS].\n\n\nWe also considered the 5 prompt templates\nused in bias work on LMs for textual entailments\n\n(Schmitt and Sch\u00fctze, 2021):\n\n\n4. [PREMISE], which means that [HYPOTHESIS].\n\n\n5. [HYPOTHESIS], because [PREMISE].\n\n\n\n6. It is not the case that [HYPOTHESIS], let alone\nthat [PREMISE].\n\n\n7. [HYPOTHESIS] _NEG_, which means that\n\n[PREMISE] _NEG_ .\n\n\n8. [PREMISE] _NEG_, because [HYPOTHESIS] _NEG_ .\n\n\nIn preliminary experiments with GPT-3.5, we observed that LLMs are not responsive to the 3 contrapositive prompts from Schmitt and Sch\u00fctze (2021)\n(colored gray), performing at random. We also\n\n- bserved that prompt number 5 from Schmitt and\nSch\u00fctze (2021) also consistently underperforms the\n\n- ther 4 templates, so we use the remaining 4 templates (namely, template no. 1, 2, 3, 4) as our final\ncandidate set.\n\n\n**In-Context Examples** have been widely used for\ninteractions with LLMs since Brown et al. (2020).\nFurther, Wei et al. (2022) has demonstrated that\nincluding chain-of-thought explanation, namely\nstep-by-step explanations, in the in-context examples, helps LLMs perform reasoning tasks. On the\n\n- ther hand, Ouyang et al. (2022) has suggested\nthat instruction-t", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_15300", "chunk_text": " chain-of-thought explanation, namely\nstep-by-step explanations, in the in-context examples, helps LLMs perform reasoning tasks. On the\n\n- ther hand, Ouyang et al. (2022) has suggested\nthat instruction-tuned LLMs are also capable of\nperforming tasks in zero-shot, without exposure to\nany in-context examples.\nWe compared zero-shot and few-shot in our preliminary experiments with LLaMA and GPT-3.5 on\nLevy/Holt directional **dev** set. Following Touvron\net al. (2023), for zero-shot, we prepend a textual\ndescription of the task to each test sample; for fewshot, we prepend a minimal 4 examples with explanations. Instantiated prompts in the two settings\nare demonstrated in Table 13. Here we report the\ndev set results with the best-performing templates.\nWe found that for the two pre-trained LLMs,\nnamely, LLaMA and PaLM, zero-shot performance\n\n- n the Levy/Holt directional dev set is near-random,\nat 56.6% and 61.5% _AUC_ respectively (random is\n50%); with 4 in-context examples, the models begin to exhibit non-trivial behavior, with 65.0% and\n80.2% _AUC_, respectively. This is not surprising,\nsince pre-trained LLMs without instruction finetuning should not be expected to perform complex\ntasks zero-shot. For GPT-3.5, the performance is\nstill much lower in zero-shot, at 64.5%, compared\nto 74.6% in few-shot.\n\nAs discussed in \u00a74.2, ideally we would like\nLLMs to have zero-shot natural language abilities readily available for downstream tasks. However, in light of this observation, our primary experiments are conducted in the few-shot setting\n\n\nTable 6: Scoring model outputs in different conditions\n\n- f RTE-1. We indicate the **highest** and lowest recall\nscore across replacement settings.\n\n\nthroughout, in order to better explore the abilities\n\n- f these LLMs.\n\n\n**B** **RTE-1 Results For Experiment 2:**\n**Entities are Indices to Memory**\n\n\nThe RTE-1 dataset contains complex natural language statements with varied linguistic features,\nso predictions about entailment are not decidable\n\n- nly on the basis of contained predicates", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_15750", "chunk_text": "1 Results For Experiment 2:**\n**Entities are Indices to Memory**\n\n\nThe RTE-1 dataset contains complex natural language statements with varied linguistic features,\nso predictions about entailment are not decidable\n\n- nly on the basis of contained predicates. However,\nRTE-1 is a difficult challenge set for models, and\ninteresting to compare to in the broader domain of\nNLI. Though the sentences are much more complex, we are able to conduct an analogous experiment as in \u00a76 by first identifying spans of named\nentities and their respective entity types, then replacing the entities with new ones. As before, we\ncompare model scores on the original dataset to\nthree test conditions: generic arguments (\u201clocation\nX\u201d, \u201cperson Y\u201d, etc.), sampled low-frequency entities constrained to the same type, and the same\nfor high-frequency sampled entities. Since only\nthe entities in each statement have been altered,\nthe entailment labels between premise/hypothesis\npairs remain unchanged, and an ideal model capable of generalizing inference would make the same\npredictions across dataset conditions. Results are\nshown in Table 6.\n\nWe observe similar trends to those reported on\nLevy/Holt. GPT-3.5 performs very consistently between Levy/Holt and RTE-1 in terms of degrading\nrecall when information is changed in each sample.\nWe observe that model performance is worse than\nthe original dataset when using generic arguments,\nand worse still using type-constrained random ar\n\n\nguments. We further observe that across all three\nLLMs across both datasets, models consistently\nachieve worse recall using high-frequency entities\nthan low-frequency entities, supporting the claim\nthat increasing the frequency of entity occurrence\nin training data impedes generalization.\nDifferent from in Levy/Holt, we observe some\nnoise in LLaMA\u2019s predictions; the recall on the\n\n- riginal task is actually lower than the generic argument condition and the low-frequency entity condition. We note that overall, LLaMA is the weakest\nLLM tested in this experiment on both Levy/Holt\nand RTE-1, and that its performance on RTE-1 is\nparticularly low. We suggest that the increased difficulty of RTE-1 over Levy/Holt (due to having\nmuch more linguistic variation) is simply too complex for LLaMA, which is neither the largest LLM\ntested, nor instruction-finetuned.\nWe also observe a smaller gap between PaLM\u2019s\nrecall", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_16200", "chunk_text": " Levy/Holt (due to having\nmuch more linguistic variation) is simply too complex for LLaMA, which is neither the largest LLM\ntested, nor instruction-finetuned.\nWe also observe a smaller gap between PaLM\u2019s\nrecall rates across dataset conditions, though the\ngaps are consistent with our claims. While the\nmodel appears able to generalize to conditions in\nwhich random real arguments are inserted, recall\n\n- n the generic argument condition is significantly\ndegraded. Failure on this control condition indicates that the model may not be generalizing as\nwell as the other conditions would imply.\n\n\n**C** **The Ineffectiveness of Instructing**\n**LLMs to Stop Conditioning on**\n**Attested Information**\n\n\nIn \u00a75 and \u00a76, we showed that entailment predictions from LLMs are strongly biased by their predictions on the attestation of hypotheses. We wondered whether there are intuitive prompt engineering techniques to steer its behavior away from attending to attestation.\nTowards this goal, we experimented with\nprepending a brief task description to the few-shot\nprompts in part B of Table 13, explicitly instructing\nthe models to ignore the attestedness of individual\nstatements: _Please check the entailments between_\n\n_the following hypothetical statements. Ignore the_\n_veracity of these statements._\nWe replicated the experiments in \u00a75 and \u00a76 with\nGPT-3.5, since GPT-3.5 is an instruction-finetuned\nmodel trained to be responsive to prompts, where\nthe other two LLM families are only pre-trained.\nDespite having been instruction-finetuned, the results with GPT-3.5 show only marginal improvements in model behavior.\n\n\n**task** **GPT-3.5** **Instructed to Ignore Attestedness** **Not Instructed**\n\n\n_I_ _P_ (Entail _|_ Attested) 74.3 77.6\n_I_ _P_ (Entail _| \u00ac_ Attested) 57.8 63.6\n\n\n_IRandP rem_ _P_ (Entail _|_ Attested) 39.0 41.3\n_IRandP rem_ _P_ (Entail _| \u00ac_ Attested) 17.6 18.8\n\n\nTable 7: We estimate the probability of positive predictions of Entail in _I_ and _IRandP rem_ tasks respectively\ngiven", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_16650", "chunk_text": " _P_ (Entail _| \u00ac_ Attested) 17.6 18.8\n\n\nTable 7: We estimate the probability of positive predictions of Entail in _I_ and _IRandP rem_ tasks respectively\ngiven that the hypothesis is attested, namely \u039b = Attested. **Not instructed** results are copied from Figure 2 and\nlisted here for ease of comparison; also note that all _IRandP rem_ = Entail predictions are false positives.\n\n\n**Levy/Holt (Directional)**\n\n\n**GPT-3.5 Condition** **Task** Precision Recall \u2206 **-Recall**\n\n\n\n\n\n\n\nTable 8: GPT-3.5 predictions when models are explicitly instructed to avoid taking the attestedness of individual\nstatements into account. In the upper half are the instructed behavior, and in the lower half are the regular few-shot\nbehavior as in Table 4. Differences in recalls remain at a similar scale, with precision again stable, where the benefit\nfrom the explicit instruction is marginal.\n\n\n\nIn Table 7, we show that instructing GPT-3.5\nto ignore attestation does not help narrow the gap\nbetween \u039b = Attested and \u039b = _\u00ac_ Attested;\ninstead, probabilities of predicting Entail went\ndown by similar amounts, indicating that the model\nis becoming slightly more conservative in predicting positives when instructed to ignore attestation,\nbut not in a principled manner.\nFurther, as shown in Table 8, despite the explicit instruction, recall still drops at similar scales\nwhen arguments are randomly replaced with the\nsame sets of frequent/infrequent replacement entities as before. Since GPT-3.5 has been instruction\nfinetuned to respond to prompts, its failure means\neradicating such biases from model outputs is a\ndifficult task, one that needs further research attention.\n\n\n**D** **Statistics of Consistency Subsets**\n\n\nThe statistics of consistency subsets are presented\nin Table 9.\n\n\n**E** **The Reliability of** \u039b **Measure and Its**\n**Relation to Consensus of Attestation**\n\n\nThe \u039b-consistency subsets most directly capture\nthe impacts of the _attestation bias_ . However, these\n\n\n\nsubset separations are based on \u039b predictions from\nindividual models, which can be noisy, subject to\nmodel-specific idiosyncracies such as trigger words\n\n- r", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_17100", "chunk_text": "the impacts of the _attestation bias_ . However, these\n\n\n\nsubset separations are based on \u039b predictions from\nindividual models, which can be noisy, subject to\nmodel-specific idiosyncracies such as trigger words\n\n- r certain syntactic structures in the prompt, etc.\nTo verify that the performance gaps in \u039bconsistency subsets that we observe in \u00a78 comes\nfrom predicted attestedness and not some idiosyncrasy, we experiment with another pair of subsets\nbased on _consensus attestation_ instead of individu\nally _predicted attestation_ .\n\nWe use a majority vote among the three\nindependently-trained LLMs to approximate _con-_\n_sensus attestation_ . The approximation is denoted\nas \u039b [\u02dc] . This is because any model-specific idiosyncrasies should not be shared between LLMs in\ndependently trained from different source corpora\nin general. Therefore, with the majority vote, we\nreduce this noise and acquire predictions on the\n_consensus attestation_ - f statements.\nPerformances of LLMs between \u039b [\u02dc]  - consistency\nsubsets are listed in Table 10. Gaps between\nthe \u039b [\u02dc] - consistency subsets that are larger than \u039bconsistency gaps are colored red; those narrower\nthan \u039b-consistency gaps are colored green. It is\nclear that the gaps are consistent between \u039b/\u039b [\u02dc] consistency experiments, where the gaps are even\nlarger on many occasions. This confirms that the\n\n\n**# of Entries** **Levy/Holt** **RTE-1**\n\n\nLLaMA GPT-3.5 PaLM LLaMA GPT-3.5 PaLM\n\n\nVCONSISTENT 955 947 999 479 447 480\nVADVERSARIAL 829 837 785 321 353 320\n\n\nFCONSISTENT 972 286\nFADVERSARIAL 220 247\n\n\nTable 9: Subsets defined by the consistency between entailment label _L_ and either \u039b (hypothesis attestation\nprediction from each LLM) or \u03a6 (model-agnostic relative frequency bias). CONSISTENT subsets are where _L_ agrees\nwith \u039b/\u03a6. ADVERSARIAL subsets are where _L_ disagrees with \ufffd", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_17550", "chunk_text": "estation\nprediction from each LLM) or \u03a6 (model-agnostic relative frequency bias). CONSISTENT subsets are where _L_ agrees\nwith \u039b/\u03a6. ADVERSARIAL subsets are where _L_ disagrees with \u039b/\u03a6.\n\n\n\n**Levy/Holt**\n\n\nTable 10: LLM performance on Levy/Holt subsets\nwhere Attestation \u039b [\u02dc] is Consistent/Adversarial to the\n\nlabels, measured with _AUCnorm_ (0% = random chance\nperformance). Performance drops from \u039b [\u02dc] _cons_ to \u039b [\u02dc] _adv_\nare presented in the _diff._ columns, sharper decreases\nthan \u039b-comparisons in Table 5 are colored **red**, milder\n\n- nes are colored **green** .\n\n\nperformance gaps in \u039b-consistency experiments\ncan be credited to the _attestation bias_, rather than\nmodel-specific idiosyncrasies.\nIt is also to be noted that, since the \u03a6-consistency\nsubsets are separated based on the model-agnostic\ncriterion \u03a6, model-specific idiosyncrasies are not a\nproblem for \u03a6-consistency comparisons.\n\n\n**F** **Impacts of Bias on GPT-4 Performance**\n\n\nGPT-4 (OpenAI, 2023) is a recent, strong LLM\nclaiming SOTA performance on various NLP tasks.\nDue to its closed-source nature and the impossibility of fully tracking the sources of its behaviors, we\nrefrain from reporting results with it in the main\ncontent of this paper.\nHowever, in order to provide a richer context for the _attestation bias_ and the _relative fre-_\n_quency bias_, in this section we report the performance differences of GPT-4 between subsets con\nsistent/adversarial to the two biases.\n\nAs a light-weight experiment, we elicit GPT-4\npredictions in the original _I_ task in the zero-shot\nsetting, and re-use subsets from experiments in\n\u00a78. Specifically, for the _attestation bias_, we use\n\n\n\nthe majority vote \u039b [\u02dc] among LLaMA, GPT-3.5 and\nPaLM, to approximate \u039b predictions from GPT-4\nitself; for the _relative frequency bias_, we keep the\n\u03a6 measure for approx", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_18000", "chunk_text": " [\u02dc] among LLaMA, GPT-3.5 and\nPaLM, to approximate \u039b predictions from GPT-4\nitself; for the _relative frequency bias_, we keep the\n\u03a6 measure for approximating corpus-frequency of\n\nterms.\n\nBecause GPT-4 is a commercial service and does\n\nnot provide logit confidence with their discrete predictions, _AUCnorm_ values could not be calculated.\nTherefore, we are forced to report **the** _**F-1 scores**_\n**at the binary prediction point of confidence** . As\nresults in Table 12 show, we observe the same trend\n\nas in \u00a78: for the subset adversarial to each factor,\nGPT-4 performance also drops substantially.\nThis experiment is designed to provide more context for the two biases discussed in the paper and\n**NOT** to compare GPT-4 with other models; however, we can conclude that GPT-4 is subject to the\nsame fragilities as the other LLMs w.r.t. the two biases, where our conclusions and recommendations\nalso apply.\n\n\n**G** **Dataset Statistics and Dev Set**\n\n**Performance**\n\n\nIn the paper, we have examined the behavior and\nperformance of three major LLM families on two\nNLI datasets: Levy/Holt and RTE-1.\nThe directional portion of Levy/Holt dataset [9]\n\ncontains 630 entries in its dev set, and 1784 entries\nin its test set; the RTE-1 dataset [10] contains 567\n\nentries in its dev set, and 800 entries in its test\n\nset. Each dataset has a 50%/50% class distribution\n\nbetween Entail and No-Entail (for RTE-1\ndev set, the numbers of entries in the two label\nclasses differ by 1).\nIn Table 11, we report dev set performance and\nthe best prompt template used for each model on\neach dataset. Note that no training is involved in\n\n\n[9https://github.com/mjhosseini/](https://github.com/mjhosseini/entgraph_eval/tree/master/LevyHoltDS)\n[entgraph_eval/tree/master/LevyHoltDS](https://github.com/mjhosseini/entgraph_eval/tree/master/LevyHoltDS)\n\n[10https://www.kaggle.com/datasets/](https://www.kaggle", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_18450", "chunk_text": "/tree/master/LevyHoltDS](https://github.com/mjhosseini/entgraph_eval/tree/master/LevyHoltDS)\n\n[10https://www.kaggle.com/datasets/](https://www.kaggle.com/datasets/nltkdata/rte-corpus?resource=download)\n[nltkdata/rte-corpus?resource=download](https://www.kaggle.com/datasets/nltkdata/rte-corpus?resource=download)\n\n\n**Levy/Holt** **RTE-1**\n\n\n**Model** **Task** Best tplt. ID DEV set _AUCnorm_ Best tplt. ID DEV set _AUCnorm_\n\n\n\nLLaMA\n\n\nGPT-3.5\n\n\nPaLM\n\n\n\n_I_ #4 30.0 #3 62.5\n_I_ _[GenArg]_ #1 34.6 #3 52.3\n_I_ _[RandArg][\u2193]_ #1 31.8 #1 51.3\n_I_ _[RandArg][\u2191]_ #1 26.3 #3 43.8\n\n\n_I_ #1 49.2 #3 74.8\n_I_ _[GenArg]_ #1 39.8 #3 64.8\n_I_ _[RandArg][\u2193]_ #1 43.4 #3 63.6\n_I_ _[RandArg][\u2191]_ #1 34.2 #3 66.0\n\n\n_I_ #1 60.9 #4 84.5\n_I_ _[GenArg]_ #1 48.1 #4 79.4\n_I_ _[RandArg][\u2193]_ #1 43.6 #3 79.8\n_I_ _[RandArg][\u2191]_ #1 35.3 #3 78.3\n\n\n\nTable 11: LLM **dev set** performance on the two datasets, measured with _AUCnorm_ (0% = random chance\nperformance). AUC is calculated using estimated model scores as in \u00a74.2 and then normalized into AUC _norm_ . We\nselect the highest scoring template on each dev task (shown in this table) and use this in the corresponding test set\nevaluation (shown in the main text).\n\n\n**F-1 score** **Task", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_18900", "chunk_text": " normalized into AUC _norm_ . We\nselect the highest scoring template on each dev task (shown in this table) and use this in the corresponding test set\nevaluation (shown in the main text).\n\n\n**F-1 score** **Task** **Levy/Holt**\n\n\n\u02dc\u039b _Cons_ \u02dc\u039b _Adv_\n\n\n_random baseline_ _I_ 70.3 62.0\nGPT-4 _I_ 85.1 ( **+14.8** ) 67.6 (+5.6)\n\n\n\u03a6 _Cons_ \u03a6 _Adv_\n\n\n_random baseline_ _I_ 66.7 66.7\nGPT-4 _I_ 74.6 ( **+7.9** ) 69.7 (+3.0)\n\n\nTable 12: LLM performance on Levy/Holt subsets\nwhere Attestation \u039b [\u02dc] is Consistent/Adversarial to the\n\nlabels, measured with **F-1 score** . _random baseline_ is the\nhighest F-1 score from a random classifier, by reaching\nrandom precision and 100% recall. For each GPT-4\nscore, we also show the improvement over random (in\nparentheses).\n\n\nthis paper, and prompt template selection is the\n\n- nly hyper-parameter tuned on the dev sets. These\nselected best prompt templates are then used on the\nrespective test sets, where the results are used for\nthe analysis throughout the paper.\nFor random-premise experiments, AUC values\ncannot be meaningfully calculated because gold\nlabels are always No-Entail. For these experiments, we use the most frequently-selected\nprompt template on each dataset, namely template\n#1 for Levy/Holt dataset, and template #3 for RTE1 dataset.\n\n\n**A. Zero-shot Example Instantiated Prompt**\n\n\nPlease check the entailments between the following statements.\n\n\nIf kanamycin kills infections, then kanamycin is useful in infections.\nA) Entailment\nB) Neutral\nC) Contradiction\n\n\n**B. Few-shot Example Instantiated Prompt**\n\n\nIf Google bought Youtube, then Google owns Youtube.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer: A) Entailment. Owning is a consequence of buying.\nIf Google owns Youtube, then Google bought Youtube.\nA) Entailment", "token_count": 500, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2305.14552_hallucination_manakul:chunk_19350", "chunk_text": " owns Youtube.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer: A) Entailment. Owning is a consequence of buying.\nIf Google owns Youtube, then Google bought Youtube.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer: B) Neutral. Owning does not imply buying, the ownership may come from other means.\nIf John went to the mall, then John drove to the mall.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer: B) Neutral. John may have gone to the mall by other means.\nIf John drove to the mall, then John went to the mall.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer: A) Entailment. Driving is a means of going to the mall.\nIf ephedrine is widely used in medicine, then ephedrine is used in medicine.\nA) Entailment\nB) Neutral\nC) Contradiction\nAnswer:\n\n\n**C. Hypothesis-only Example Instantiated Prompt**\n\n\nGoogle bought Youtube.\nA) True\nB) Unknown\nC) False\nAnswer: A) True.\nYoshua Bengio likes oak trees.\nA) True\nB) Unknown\nC) False\nAnswer: B) Unknown.\nThe sun rises from the west.\nA) True\nB) Unknown\nC) False\nAnswer: C) False.\nephedrine is used in medicine.\nA) True\nB) Unknown\nC) False\nAnswer:\n\n\nTable 13: Example instantiated prompts in Zero-shot / Few-shot settings, for the sample \u201cPREMISE: [ephedrine\nis widely used in medicine], HYPOTHESIS: [ephedrine is used in medicine]\u201d. The few-shot prompts in part B are\nused throughout the main experiments in this paper. We also present an example of the prompts we use for the\nhypothesis-only \u039b measure as described in \u00a73.1.\n\n\n", "token_count": 420, "metadata": {"arxiv_id": "2305.14552", "title": "Sources of Hallucination by Large Language Models on Inference Tasks", "authors": ["Nick McKenna", "Tianyi Li", "Liang Cheng", "Mohammad Javad Hosseini", "Mark Johnson", "Mark Steedman"], "year": 2023, "url": "https://arxiv.org/pdf/2305.14552v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_0", "chunk_text": "Published as a conference paper at ICLR 2025\n\n## - - VISRAG: VISION BASED RETRIEVAL AUGMENTED - GENERATION ON MULTI MODALITY DOCUMENTS\n\n\n**Shi Yu** [1] _[\u2217]_ **, Chaoyue Tang** [2] _[\u2217]_ **, Bokai Xu** [2] _[\u2217]_ **, Junbo Cui** [2] _[\u2217]_ **, Junhao Ran** [3] **, Yukun Yan** [1] _[\u2020]_ **,**\n**Zhenghao Liu** [4] **, Shuo Wang** [1] **, Xu Han** [1] **, Zhiyuan Liu** [1] _[\u2020]_ **, Maosong Sun** [1]\n\n1Department of Computer Science and Technology, Tsinghua University\n2ModelBest Inc. 3Rice University 4Northeastern University\nyus21@mails.tsinghua.edu.cn\n\n\nABSTRACT\n\n\nRetrieval-augmented generation (RAG) is an effective technique that enables large\nlanguage models (LLMs) to utilize external knowledge sources for generation.\nHowever, current RAG systems are solely based on text, rendering it impossible\nto utilize vision information like layout and images that play crucial roles in realworld multi-modality documents. In this paper, we introduce VisRAG, which\ntackles this issue by establishing a vision-language model (VLM)-based RAG\npipeline. In this pipeline, instead of first parsing the document to obtain text,\nthe document is directly embedded using a VLM as an image and then retrieved\nto enhance the generation of a VLM. Compared to traditional text-based RAG,\nVisRAG maximizes the retention and utilization of the data information in the\n\n     - riginal documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in\nVisRAG and explore a variety of generation methods. Experiments demonstrate\nthat VisRAG outperforms traditional RAG in both the retrieval and generation\nstages, achieving a 20\u201340% end-to-end performance gain over traditional textbased RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing\ntraining data and demonstrates strong generalization capability, positioning", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_450", "chunk_text": " the retrieval and generation\nstages, achieving a 20\u201340% end-to-end performance gain over traditional textbased RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing\ntraining data and demonstrates strong generalization capability, positioning it as a\npromising solution for RAG on multi-modality documents. Our code and data are\n[available at https://github.com/openbmb/visrag.](https://github.com/openbmb/visrag)\n\n\n1 INTRODUCTION\n\n\nTrained on massive data, large language models (LLMs) have shown strong abilities in common\nNLP tasks using their parametric knowledge (Wei et al., 2022; Zhao et al., 2023; Achiam et al.,\n2023). However, the issue of hallucination (Ji et al., 2023; Bang et al., 2023) and the challenge of\nupdating the parametric knowledge limit their real-world application in specific domains. Retrievalaugmented generation (RAG) alleviates this problem by supplying the LLM with information retrieved from a custom outer knowledge base (Guu et al., 2020; Lewis et al., 2020; Yu et al., 2023).\nOpen-source RAG frameworks like llamaindex (Liu, 2022) have been developed to facilitate the\nresearch and deployment of RAG.\n\n\nTypical retrieval-augmented generation (RAG) pipelines are _text-based_, operating on segmented\ntexts as retrieval units (Yu et al., 2023; Asai et al., 2024; Yan et al., 2024), which we refer to as\nTextRAG. In real-world scenarios, knowledge is often presented in multi-modality documents such\nas textbooks and manuals, which may have texts and figures intersected together. To acquire texts\nfrom such data sources, a _parsing_ stage is required, which typically involves a cascade of processes,\nincluding layout recognition, optical character recognition (OCR), and post-processing steps like\ntext joining (Zhang et al., 2024; Liu, 2022). While effective in most scenarios, the parsing process\ninevitably introduces errors, which can negatively impact the retrieval and generation phases. More\n- ver, TextRAG utilizes only textual information, overlooking potential information present in other\nmodalities like images. Although research has been conducted on image retrieval and multi-modal\n\n\n_\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_900", "chunk_text": ", which can negatively impact the retrieval and generation phases. More\n- ver, TextRAG utilizes only textual information, overlooking potential information present in other\nmodalities like images. Although research has been conducted on image retrieval and multi-modal\n\n\n_\u2217_ Equal contribution.\n\n_\u2020_ Corresponding authors.\n\n\n1\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\nRAG, these approaches primarily focus on predefined scenarios wherein images and descriptive\ntexts are properly extracted and paired (Wei et al., 2023; Sharifymoghaddam et al., 2024; Zhou\net al., 2024), differing from real-world scenarios where texts and images (including figures) are\n\n- ften interleaved within a single document page.\n\n\nThe recent development of vision-language models (VLMs) has introduced a promising approach\nto understanding complex visual cues in images and documents (OpenBMB, 2024b; Wang et al.,\n2024). By integrating a language model with a vision encoder, VLMs demonstrate superior abilities in applications such as describing pictures (Alayrac et al., 2022), explaining figures (Bavishi\net al., 2023), and transcribing (printed and handwritten) text from document images (Laurenc\u00b8on\net al., 2024). Given the robust capabilities of VLMs in capturing multi-modal information present in\nimages, an intriguing question arises: can the basic language model in the retrieval and generation\ncomponents of TextRAG be substituted with a VLM, thus the parsing stage is bypassed and all the\ninformation of the document is preserved?\n\n\nIn this paper, we present **Vis** ion-based **R** etrieval- **a** ugmented **G** eneration (VisRAG), to study the feasibility of building a pure-vision RAG pipeline using VLMs. VisRAG is built with a VLM-based\nretriever VisRAG-Ret and generator VisRAG-Gen. Inherited the bi-encoder of text-based dense retriever (Karpukhin et al., 2020), VisRAG-Ret maps the query and the document into an embedding\nspace, but utilizing the document\u2019s image directly instead of relying on extracted textual content.\nThe embedding is obtained by applying weighted mean pooling on the final hidden states of the input text or vision tokens. After retrieving top- _k_", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_1350", "chunk_text": " into an embedding\nspace, but utilizing the document\u2019s image directly instead of relying on extracted textual content.\nThe embedding is obtained by applying weighted mean pooling on the final hidden states of the input text or vision tokens. After retrieving top- _k_ document images, VisRAG processes these images\nto generate the answer. While it is straightforward to use a VLM that supports multi-image input for\ngeneration, for VLMs that can only accept one single image, we propose page concatenation and\nweighted selection techniques to enable the handling of multiple documents. Throughout the process, VisRAG preserves all information in its original visual format, thereby preventing the potential\ninformation loss or distortion that might occur in traditional RAG pipelines.\n\n\nTo evaluate VisRAG on real-world multi-modal documents, we construct datasets from open-source visual question answering (VQA) datasets and synthetic query-document pairs derived from webcrawled PDFs. In terms of retrieval, VisRAGRet outperforms state-of-the-art text- and visioncentric retrievers and achieves better results than\nsolely relying on its constituent vision encoder\n\n- r language model under identical training conditions. For generation, VisRAG-Gen surpasses\ntraditional text-based generators with open-source\nVLMs. With VLMs capable of handling multiple images, VisRAG shows increasing performance gains with more retrieved documents, indicating the potential for multi-page reasoning. As Figure 1: TextRAG vs. VisRAG on final gendepicted in Figure 1, in a direct comparison of eration accuracy. In TextRAG, parsed text\npipeline performances, VisRAG achieves a 40% rel- serves as the basis for both retrieval and genative improvement over TextRAG using MiniCPM- eration processes. In contrast, VisRAG leverV 2.6 (OpenBMB, 2024b) as the generator and a ages the original document image directly by\n20% relative improvement with GPT-4o (OpenAI, using a VLM-based retriever and generator.\n2024) as the generator, attributed to the cascade ef- Details can be found in Sec. 5.1.\nfect. Further analysis reveals that VisRAG possesses\nbetter training data efficiency and generalization ability than baseline models, and demonstrates robustness across both text-centric and vision-centric documents. VisRAG shows great promise in\nreplacing TextRAG as", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_1800", "chunk_text": "fect. Further analysis reveals that VisRAG possesses\nbetter training data efficiency and generalization ability than baseline models, and demonstrates robustness across both text-centric and vision-centric documents. VisRAG shows great promise in\nreplacing TextRAG as the next-generation standard for RAG pipelines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: TextRAG vs. VisRAG on final generation accuracy. In TextRAG, parsed text\nserves as the basis for both retrieval and generation processes. In contrast, VisRAG leverages the original document image directly by\nusing a VLM-based retriever and generator.\nDetails can be found in Sec. 5.1.\n\n\n\n2 RELATED WORK\n\n\n**Retrieval-augmented Generation (RAG).** RAG enhances large language models (LLMs) by\nincorporating retrieved information from external knowledge bases, which assists in addressing\nknowledge-intensive tasks (Guu et al., 2020), reducing hallucinations (Semnani et al., 2023), and\n\n\n2\n\n\nPublished as a conference paper at ICLR 2025\n\n\nacquiring new knowledge (Vu et al., 2023). An RAG pipeline typically comprises a text-based\nretriever that fetches relevant information from the knowledge base given the user query, and an\nLLM-based generator that reads the query along with the retrieved information to generate an answer (Shi et al., 2024b; Yu et al., 2023). Prior research on RAG primarily focuses on: a) improving\nthe retriever, which is typically a text encoder producing text embeddings, through generator feedback (Yu et al., 2023; Shi et al., 2024b); b) enhancing the generator via supervised fine-tuning (Lin\net al., 2024; Xu et al., 2024a), in-context pre-training (Shi et al., 2024a), or advanced prompting (Xu\net al., 2024c); and c) developing advanced RAG pipelines to handle long-form or multi-hop question answering (Jiang et al., 2023; Asai et al., 2024). However, research on RAG has predominantly\ntargeted cleaned text corpora like Wikipedia from an academic standpoint. Building effective RAG\npipelines for real-world, multi-modal documents remains a challenge.\n\n\n**Vision-language Models.** Recent advancements in vision-language models (VLMs", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_2250", "chunk_text": " has predominantly\ntargeted cleaned text corpora like Wikipedia from an academic standpoint. Building effective RAG\npipelines for real-world, multi-modal documents remains a challenge.\n\n\n**Vision-language Models.** Recent advancements in vision-language models (VLMs) have greatly\nimproved fine-grained multi-modal understanding. Since CLIP (Radford et al., 2021) pioneered\ncontrastive visual-text alignment, models like Flamingo (Alayrac et al., 2022), LLaVA (Liu et al.,\n2023b), and BLIP (Li et al., 2022) have expanded LLMs to process visual inputs by connecting\nlanguages models with a CLIP-style vision encoder. Research has then shifted towards more advanced multi-task and multi-stage pre-training paradigms, enabling models to generalize across a\nwide range of vision-language tasks (Liu et al., 2024a; Bai et al., 2023; Wang et al., 2023; Dai et al.,\n2023). This is followed by notable advancements in high-resolution visual understanding (Xu et al.,\n2024b; Bavishi et al., 2023; Lin et al., 2023) and OCR capabilities (Kim et al., 2022; Lee et al., 2023;\nHong et al., 2024; Chen et al., 2024b). Specifically, VLMs like the DocOwl series (Ye et al., 2023a;\nHu et al., 2024b;a), UReader (Ye et al., 2023b), and TextMonkey (Liu et al., 2024b) are purposebuilt to tackle OCR-free document understanding. More recently, breakthroughs have been made\nin multi-image understanding (Li et al., 2024a; Wang et al., 2024). Recent open-source VLMs like\nthe MiniCPM-V (Yao et al., 2024) and Qwen2-VL (Wang et al., 2024) series combine the merits of\nrecent techniques, achieving state-of-the-art performance. Those features of VLMs provide a foundation for our vision-based RAG pipeline, which requires multi-modal document understanding.\n\n\n**Multi-modality Retrieval and RAG.** Multi-modal retrieval encompasses a wide range of tasks,\nsuch as retrieving a matching image given the text (Han et al., ", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_2700", "chunk_text": " our vision-based RAG pipeline, which requires multi-modal document understanding.\n\n\n**Multi-modality Retrieval and RAG.** Multi-modal retrieval encompasses a wide range of tasks,\nsuch as retrieving a matching image given the text (Han et al., 2017), retrieving a text-image pair\nto answer a question (Chang et al., 2022), and retrieving texts that answer the given query about a\nprovided image (Hu et al., 2023a; Luo et al., 2023), etc. Wei et al. (2023) propose UniIR, a universal\nmulti-modal retrieval model capable of addressing the aforementioned multiple tasks. The retrieved\ninformation is then employed for incorporating knowledge (Hu et al., 2023b; Luo et al., 2021) or\nin-context learning (Tan et al., 2024; Liu et al., 2023a), with the aim of generating answers or images (Sharifymoghaddam et al., 2024). Prior research mentioned above is conducted on academic\ndatasets, where texts and images are meticulously extracted from raw data and paired (e.g., images\nwith their captions), to make it feasible to do separate encoding of data in different modalities. This\nhinders their applicability in real-world RAG scenarios, as real-world multi-modal documents are often presented in mixed modalities, and information may be distributed across various combinations\n\n- f modalities. Concurrent works DSE (Ma et al., 2024) and ColPali (Faysse et al., 2024) address this\nissue by directly encoding the image of a document for retrieval. However, as these studies focus\n\n- n retrieval, they lack a comprehensive comparison of their approaches with text-based retrieval in\nboth in-domain and out-of-domain settings, and do not conduct an end-to-end RAG evaluation.\n\n\n3 METHODOLOGY\n\n\nIn this section, we first recap the typical RAG pipeline (Sec. 3.1), then present our VisRAG framework (Sec. 3.2) and the construction of our training and evaluation data (Sec. 3.3).\n\n\n3.1 PRELIMINARY: RETRIEVAL-AUGMENTED GENERATION\n\n\nA typical retrieval-augmented generation (RAG) pipeline consists of a retriever and a generator,\nboth built on large language models (LLMs) [1] . This pipeline operates on", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_3150", "chunk_text": " RETRIEVAL-AUGMENTED GENERATION\n\n\nA typical retrieval-augmented generation (RAG) pipeline consists of a retriever and a generator,\nboth built on large language models (LLMs) [1] . This pipeline operates on a knowledge corpus _D_,\n\n\n1In many cases, the retriever uses language models smaller than 1B parameters, which may not be considered \u201clarge\u201d, but we use the term LLM for simplicity.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\nFigure 2: TextRAG (left) vs. VisRAG (right). Traditional text-based RAG (TextRAG) relies on\nparsed texts for retrieval and generation, losing visual information in multi-modal documents. Our\nvision-based RAG (VisRAG) employs a VLM-based retriever and generator to directly process the\ndocument page\u2019s image, thereby preserving all information in the original page.\n\n\nwhich is processed into units for retrieval and generation, denoted as _D_ = _{d_ 1 _, . . ., dn}_, where\n_n_ is the number of retrieval units. Given a text query _q_ and the retrieval corpus _D_, the retriever\nfunctions as _R_ : ( _q, D_ ) _\u2192DR_, taking _q_ and _D_ as inputs and producing a candidate set _DR \u2282D_ .\nTo enable efficient search, the units in the knowledge corpus _D_ are pre-encoded into embeddings.\nDuring RAG pipeline inference, approximate nearest neighbor (ANN) search is applied to retrieve\n_DR_, which serves as the knowledge source for generation. The generation process can be defined as\na function _G_ : ( _q, DR_ ) _\u2192_ _a_, where _a_ represents the answer and _G_ denotes the LLM generator. This\nis achieved by prompting the LLM with the query and the retrieved units _DR_ to generate an answer.\n\n\nAs shown in Figure 2 (left), traditional RAG frameworks (TextRAG) typically utilize text-based\nunits for retrieval and generation. However, in real-world scenarios, data often appear in complex,\nmulti-modal documents, requiring an additional parsing step to obtain text. In this paper, we propose\nto use the _page_ as the fundamental unit for retrieval and generation, which is directly processed\nby vision language models (VLMs) as", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_3600", "chunk_text": "multi-modal documents, requiring an additional parsing step to obtain text. In this paper, we propose\nto use the _page_ as the fundamental unit for retrieval and generation, which is directly processed\nby vision language models (VLMs) as an image without further processing during retrieval and\ngeneration. In subsequent sections, we use the terms \u201cpage\u201d and \u201cdocument\u201d interchangeably.\n\n\n3.2 VISRAG: VISION-BASED RETRIEVAL-AUGMENTED GENERATION\n\n\nIn this section, we present **Vis** ion-based **R** etrieval- **a** ugmented **G** eneration (VisRAG), as shown in\nFigure 2 (right). In contrast to traditional RAG frameworks which use text segments for both retrieval and generation, VisRAG leverages the image of the document to preserve all information.\n\n\n3.2.1 RETRIEVAL\n\n\nThe first stage of VisRAG, VisRAG-Ret, aims to retrieve a set of pages from the corpus _D_ given\n_q_ . We follow the dual-encoder paradigm in text-based dense retrieval models (Karpukhin et al.,\n2020) but employ a VLM rather than an LLM to encode the query and page. Specifically, the query\nand page are encoded separately as text and image in the VLM, producing in a sequence of hidden\nstates. To derive the final embedding, and given that we use generative VLMs with causual attention,\nwe adopt the position-weighted mean pooling over the last-layer VLM hidden states (Muennighoff,\n2022), giving higher weights to later tokens:\n\n\n\n**v** =\n\n\n\n_S_\n\n- _wi_ **h** _i,_ (1)\n\n\n_i_ =1\n\n\n\nwhere **h** _i_ is the _i_ - th hidden state, _S_ is the sequence length, _wi_ = ~~\ufffd~~ _Sji_ =1 _[j]_ [is the] _[ i]_ [-th weight, and] **[ v]** [ is]\n\nthe query or page embedding. The similarity score is calculated by the cosine similarity of the query\n\n\n4\n\n\nPublished as a conference paper at ICLR 2025\n\n\nand page embedding. VisRAG-Ret is optimized using the InfoNCE loss:\n\n\nexp( _s_ ( _q, d_ [+] ) _/\u03c4", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_4050", "chunk_text": "\n\n\n4\n\n\nPublished as a conference paper at ICLR 2025\n\n\nand page embedding. VisRAG-Ret is optimized using the InfoNCE loss:\n\n\nexp( _s_ ( _q, d_ [+] ) _/\u03c4_ )\n_l_ ( _q, d_ [+] _, D_ _[\u2212]_ ) = _\u2212_ log (2)\nexp( _s_ ( _q, d_ [+] ) _/\u03c4_ ) + ~~[\ufffd]~~ _d_ _[\u2212]_ _\u2208D_ _[\u2212]_ [exp(] _[s]_ [(] _[q, d][\u2212]_ [)] _[/\u03c4]_ [)] _[,]_\n\n\nwhere _d_ [+], _D_ _[\u2212]_ are positive document and the negative document set of _q_, respectively, _s_ ( _q, d_ ) is the\nsimilarity score between _q_ and _d_, and _\u03c4_ is the temperature.\n\n\n3.2.2 GENERATION\n\n\nThe second stage of VisRAG, VisRAG-Gen, focuses on generating the answer according to the user\nquery and retrieved pages using a VLM. We propose the following mechanisms to enable VisRAGGen to handle multiple retrieved pages in _DR_ for generation. The prompts used for generation is\npresented in Appendix E.\n\n\n**Page Concatenation.** A straightforward approach is to concatenate all pages in _DR_ into a single\nimage to accommodate most VLMs that are trained to accept a single image. Formally,\n\n\n_a \u2190\u2212_ VLM-Single( _q,_ Concat( _{d|d \u2208DR}_ )) _,_ (3)\n\n\nwhere VLM-Single is a VLM that accepts a single image with text prompt and Concat is the image\nconcatenation operation. In this paper, we experiment with horizontal concatenation.\n\n\n**Weighted Selection.** Another approach is to ask the VLM to generate an answer for every page\nfrom top- _k_, and select a final one with the highest confidence (Lewis et al., 2020; Shi et al., 2024b).\nThe final confidence is defined as the weighted generation probability of the answer:\n\n\n_P_ ( _a|q, DR_ ) = _P_ ( _a|q, d_ ) _\u00b7 \u03bb_ ( _", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_4500", "chunk_text": "., 2024b).\nThe final confidence is defined as the weighted generation probability of the answer:\n\n\n_P_ ( _a|q, DR_ ) = _P_ ( _a|q, d_ ) _\u00b7 \u03bb_ ( _q, d_ ) _,_ (4)\n\n\nwhere _P_ ( _a|d, q_ ) is calculated as the reciprocal of the perplexity of generating the answer _a_ conditioned on the single document _d_, and _\u03bb_ ( _d, q_ ) is the normalized retrieval score:\n\n\n_e_ _[s]_ [(] _[q,d]_ [)]\n_\u03bb_ ( _q, d_ ) = ~~\ufffd~~ _d_ _[\u2032]_ _\u2208DR_ _[e][s]_ [(] _[q,d][\u2032]_ [)] _[.]_ (5)\n\n\n**VLMs Accepting Multiple Images.** Some recent VLMs like MiniCPM-V 2.6 (OpenBMB,\n2024b) and Qwen-VL 2 (Wang et al., 2024) are designed and trained to accept multiple images\nas input to perform cross-image reasoning. This capability may be useful for the generation as the\nrequired information could be located on a single page from the retrieved document set _DR_ for\nsingle-hop questions or spread across multiple pages for multi-hop questions. Formally, we have\n\n\n_a \u2190\u2212_ VLM-Multi( _q, {d|d \u2208DR}_ ) _,_ (6)\n\n\nwhere VLM-Multi is the VLM that accepts multiple images with text prompt.\n\n\n3.3 DATA CONSTRUCTION\n\n\nTo effectively build and evaluate RAG pipelines on multi-modal documents, we construct our\ndatasets using a combination of visual question answering (VQA) datasets and synthetic data. The\nstatistics of our constructed dataset are provided in Table 1.\n\n\n**Data Sources.** We collect question-document pairs from a series of VQA datasets, targeting different document types: MP-DocVQA (Tito et al., 2023) for industrial documents, ArXivQA (Li\net al., 2024b), ChartQA (Masry et al., 2022), InfographicsVQA (Mathew et al., 2022), and\nPlotQA (Methani et al., 2020) for various figure types,", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_4950", "chunk_text": "2024b), ChartQA (Masry et al., 2022), InfographicsVQA (Mathew et al., 2022), and\nPlotQA (Methani et al., 2020) for various figure types, and SlideVQA (Tanaka et al., 2023) for\npresentation slides. All datasets feature questions that can be answered using a single document\n(page), except for SlideVQA, which includes multi-hop questions requiring information from multiple pages. We follow the original datasets\u2019 train-test splits, except for MP-DocVQA and InfographicsVQA, where the validation split serves as our evaluation set. Additionally, we enhance our\ntraining set by collecting openly available PDFs from online sources and generating queries using\nGPT-4o (OpenAI, 2024), with details presented in Appendix A.1. We assemble the retrieval corpus\nby gathering the document associated with each query from the training and evaluation sets.\n\n\n5\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 1: Dataset statistics. We collect data from visual question answering (VQA) datasets for training and evaluation and synthetic additional query-document pairs for training. We apply filtering on\nVQA datasets to remove context-dependent queries that are not suitable for retrieval.\n\n\n**Train** **Evaluation**\n**Source** **Document Type** **# Q-D Pairs** **# Q (% Preserved)** **# D** **# Pos. D per Q**\n\n\nArXivQA (2024b) Arxiv Figures 25,856 816 (8%) 8,066 1.00\nChartQA (2022) Charts 4,224 63 (5%) 500 1.00\nMP-DocVQA (2023) Industrial Documents 10,624 591 (11%) 741 1.00\nInfoVQA (2022) Infographics 17,664 718 (26%) 459 1.00\nPlotQA (2020) Scientific Plots 56,192 863 (4%) 9,593 1.00\nSlideVQA (2023) Slide Decks 8,192 556 (25%) 1,284 1.26\n\n\nSynthetic Various 239,358  -  -  \n\n**Query Filtering.** Some queries extracted from VQA datasets are _context-dependent_, which lack\nspecificity to", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_5400", "chunk_text": "192 556 (25%) 1,284 1.26\n\n\nSynthetic Various 239,358  -  -  \n\n**Query Filtering.** Some queries extracted from VQA datasets are _context-dependent_, which lack\nspecificity to a certain entity. For instance, the response to \u201cWhere was the conference held?\u201d varies\nbased on the contextual document. Using such context-dependent queries in open retrieval tasks is\nineffective because they lack strong document specificity. To address this, we implement an additional filtering stage to remove these context-dependent questions, where we prompt GPT-4o (OpenAI, 2024) with human-annotated in-context samples to generate the classification label. Table 1\nshows a substantial reduction in context-dependent questions across evaluation sets. The details of\nfiltering are presented in Appendix A.2.\n\n\n**Evaluation Metrics.** We report the retrieval and generation performance on the evaluation sets\n\n- f the datasets sourced from VQA datasets. For retrieval, we use MRR@10 and Recall@10 as the\nmetrics. For generation, consistent with methods applied to the source datasets, we report the answer\naccuracy, employing a relaxed exact match metric which allows a 5% error margin for numeric\nresponses (Masry et al., 2022; Methani et al., 2020).\n\n\n4 EXPERIMENTAL METHODOLOGY\n\n\nIn this section, we introduce our setup for experiments. Descriptions of the LLMs/VLMs used in\n\n- ur experiments can be found in Appendix C.\n\n\n**Document Parsing.** To evaluate the performance of VisRAG against TextRAG, we introduce two\ntext extraction methods. The first, \u201c(OCR)\u201d, employs a pipeline that uses PPOCR (Du et al.,\n2020) to detect text regions and then merges nearby boxes to reduce fragmentation. The second,\n\u201c(Captioner)\u201d, is a model-based approach that directly extracts text from document images using\nMiniCPM-V 2.0 (OpenBMB, 2024a; Yao et al., 2024) fine-tuned on paired (document image, extracted text) data. More details are provided in Appendix B.\n\n\n**Retrieval Experiments.** VisRAG-Ret is a document embedding model built on MiniCPM-V\n2.0, a vision-language model that integrates SigLIP (Zhai et al., 2023) as the vision encoder", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_5850", "chunk_text": "rieval Experiments.** VisRAG-Ret is a document embedding model built on MiniCPM-V\n2.0, a vision-language model that integrates SigLIP (Zhai et al., 2023) as the vision encoder and\nMiniCPM (Hu et al., 2024c) as the language model. To ensure fair comparisons, we organize experiments into three settings: off-the-shelf, out-of-domain, and in-domain, as depicted below. We\nreport VisRAG-Ret\u2019s performance in both out-of-domain and in-domain settings.\n\n\n    - Off-the-shelf: We directly evaluate popular text and image retrieval models on extracted\ntexts, including BM25 (OCR), a lexical model; bge-large-en-v1.5 (Xiao et al., 2023) (OCR)\nand NV-Embed-v2 (Lee et al., 2024) (OCR), state-of-the-art text embedding models with\nsizes 335M and 7.85B, respectively; and SigLIP, a CLIP-style (Radford et al., 2021) vision\nmodel serving as the encoder for MiniCPM-V series.\n\n\n   - Out-of-domain: Out-of-domain models are trained solely on synthetic data and evaluated on the VQA datasets without in-domain supervision. These models include\nMiniCPM (OCR), MiniCPM (Captioner), and SigLIP. MiniCPM (OCR) and (Captioner)\nare MiniCPM-based text embedding models trained and evaluated on extracted text.\n\n\n6\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 2: Overall retrieval performance in MRR@10. The best retrieval performance in each group\nis marked in **bold**, and the second best performance is underlined. We train ColPali (Faysse et al.,\n2024) on our dataset. Corresponding Recall@10 performance can be found in Table 6.\n\n\n    - In-domain: Models in this category are trained on the blend of the VQA training data and\nsynthetic data. We evaluate the same set of models as in the out-of-domain setting to show\nmodel performance when supervised labels are available. We also report the performance\n\n     - f ColPali (Faysse et al., 2024) on our evaluation data. ColPali is a page embedding model\nthat encodes a screenshot of a page into", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_6300", "chunk_text": " labels are available. We also report the performance\n\n     - f ColPali (Faysse et al., 2024) on our evaluation data. ColPali is a page embedding model\nthat encodes a screenshot of a page into multiple vectors. We train ColPali on our dataset\nusing the official code and hyper-parameters provided in its paper.\n\n\n**Generation Experiments.** To evaluate generation performance, we fix the retrieval model to\nVisRAG-Ret and report the performance of various generation models and methods. For VisRAGGen, we compare the performance of the single-image VLM MiniCPM-V 2.0, which only accepts a\nsingle image, against the multi-image VLM MiniCPM-V 2.6 (OpenBMB, 2024b; Yao et al., 2024)\nand GPT-4o (OpenAI, 2024). MiniCPM-V 2.6 is an upgrade of MiniCPM-V 2.0, incorporating\nQwen2-7B (Yang et al., 2024) as the language model and supporting multi-image input. We evaluate\nthe performance of page concatenation and weighted selection on the single-image VLM. Additionally, we report the performance of text-based generation baselines, including MiniCPM (OCR) and\nGPT-4o (OCR), where only extracted texts are used for generation. For all experiments, we report\nresults using the top-1, top-2, and top-3 retrieved documents, as well as an \u201cOracle\u201d condition where\nthe model is provided with only the positive document(s) to show the performance upper bound.\n\n\n**Implementation Details.** VisRAG-Ret is fine-tuned using in-batch negatives (Karpukhin et al.,\n2020) for one epoch with a batch size of 128 on 8 NVIDIA A100 80GB GPUs. The temperature\nparameter in Equation 2 is set to 0.02. Baseline retrievers are fine-tuned with the same hyperparameters, and textual baselines utilize extracted text data as document-side input. The generation\npart does not use any fine-tuning; we directly use off-the-shelf LLMs/VLMs for generation.\n\n\n5 EVALUATION RESULTS\n\n\nIn this section, we first present the overall performance of VisRAG (Sec. 5.1), followed by analyses", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_6750", "chunk_text": "-tuning; we directly use off-the-shelf LLMs/VLMs for generation.\n\n\n5 EVALUATION RESULTS\n\n\nIn this section, we first present the overall performance of VisRAG (Sec. 5.1), followed by analyses\n\n- f training data efficiency (Sec. 5.2) and performance on different subsets (Sec. 5.3).\n\n\n5.1 OVERALL PERFORMANCE\n\n\n**Retrieval Performance.** In this experiment, we compare VisRAG-Ret with (a) off-the-shelf models, and trained baselines in (b) out-of-domain setting where we only leverage synthetic data, and in\n(c) in-domain setting where we leverage both in-domain and synthetic training data.\n\n\nAs shown in Table 2(a)(b), VisRAG-Ret, trained on out-of-domain data, significantly outperforms\nboth off-the-shelf models BM25 and bge-large, and achieves 95% of the performance of NV-Embedv2, a state-of-the-art text retrieval model with 7.85B parameters. Note that bge-large and NV\n\n7\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 3: Overall generation performance in accuracy (%). All models and methods utilize the same\nretriever, VisRAG. Performance relative to Oracle is colored in blue.\n\n\n**Model / Method** **Input** **ArxivQA** **ChartQA** **DocVQA** **InfoVQA** **PlotQA** **SlideVQA** **Average**\n\n(a) TextRAG-Gen: _Text-based Generation_\n\n\n\n\n\n|top-1<br>MiniCPM (OCR) t to op p- -2<br>3<br>Oracle|4338(962%)<br>42. (93.<br>16 5%)<br>. .<br>4412(978%)<br>45. (10.<br>10 0%)<br>.|2540(727%)<br>. .<br>2381(682%)<br>20. (59.<br>63 1%)<br>. .<br>3492(100%)<br>.|3147(759%)<br>33. (81.<br>67 2%)<br>. .<br>3181(767%)<br>41. (10.<br>46 0%)", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_7200", "chunk_text": "%)<br>.|3147(759%)<br>33. (81.<br>67 2%)<br>. .<br>3181(767%)<br>41. (10.<br>46 0%)<br>.|2019(929%)<br>. .<br>2019(929%)<br>18. (84.<br>25 0%)<br>. .<br>2173(100%)<br>.|1634(940%)<br>. .<br>1414(813%)<br>16. (94.<br>34 0%)<br>. .<br>1738(100%)<br>.|2932(948%)<br>. .<br>3040(983%)<br>. .<br>2914(942%)<br>. .<br>3094(100%)<br>.|2768(878%)<br>. .<br>2739(859%)<br>26. (84.<br>71 3%)<br>. .<br>3192(100%)<br>.|\n|---|---|---|---|---|---|---|---|\n|GPT-4o (OCR)<br>top-1<br>top-2<br>top-3<br>Oracle|58.33 (95.0%)<br>59.44 (96.8%)<br>61.76 (100.6%)<br>61.40 (100%)|42.86 (64.3%)<br>47.62 (71.4%)<br>44.44 (66.7%)<br>66.67 (100%)|49.92 (78.2%)<br>56.51 (88.6%)<br>55.67 (87.3%)<br>63.79 (100%)|45.82 (90.6%)<br>47.08 (93.1%)<br>49.58 (98.1%)<br>50.56 (100%)|13.90 (68.2%)<br>15.87 (77.8%)<br>14.72 (72.2%)<br>20.39 (100%)|47.12 (85.6%)<br>51.08 (", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_7650", "chunk_text": ".2%)<br>15.87 (77.8%)<br>14.72 (72.2%)<br>20.39 (100%)|47.12 (85.6%)<br>51.08 (92.8%)<br>49.28 (89.5%)<br>55.04 (100%)|42.99 (80.3%)<br>46.27 (86.8%)<br>45.91 (85.7%)<br>52.97 (100%)|\n\n\n(b) VisRAG-Gen: _Single-image VLM (MiniCPM-V 2.0)_\n\n|top-1<br>Page Concatenation t to op p- -2<br>3<br>Oracle|5907(980%)<br>57. (95.<br>35 1%)<br>59. (98.<br>19 2%)<br>60. (10.<br>29 0%)<br>.|3492(880%)<br>19. (48.<br>05 0%)<br>. .<br>2222(560%)<br>39. (10.<br>68 0%)<br>.|3942(744%)<br>. .<br>3232(610%)<br>. .<br>2487(470%)<br>52. (10.<br>96 0%)<br>.|2953(865%)<br>. .<br>2214(649%)<br>. .<br>2033(596%)<br>. .<br>3412(100%)<br>.|1784(774%)<br>15. (66.<br>41 8%)<br>16. (73.<br>92 4%)<br>23. (10.<br>06 0%)<br>.|3615(918%)<br>33. (84.<br>45 9%)<br>. .<br>3022(767%)<br>. .<br>3939(100%)<br>.|3616(860%)<br>29. (70.<br>95 1%)<br>28. (68.<br>96 5%)<br>41. (10.<br>58 0", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_8100", "chunk_text": "<br>.|3616(860%)<br>29. (70.<br>95 1%)<br>28. (68.<br>96 5%)<br>41. (10.<br>58 0%)<br>.|\n|---|---|---|---|---|---|---|---|\n|Weighted Selection<br>top-1<br>top-2<br>top-3<br>Oracle|59.07 (98.0%)<br>60.29 (100.0%)<br>60.78 (100.8%)<br>60.29 (100%)|34.92 (88.0%)<br>33.33 (84.0%)<br>31.75 (80.0%)<br>39.68 (100%)|39.42 (74.4%)<br>39.26 (74.1%)<br>38.41 (72.5%)<br>52.96 (100%)|29.53 (86.5%)<br>28.97 (84.9%)<br>28.69 (84.1%)<br>34.12 (100%)|17.84 (77.4%)<br>18.08 (78.4%)<br>17.03 (73.9%)<br>23.06 (100%)|36.15 (87.4%)<br>36.69 (88.7%)<br>36.33 (87.8%)<br>41.37 (100%)|36.16 (85.3%)<br>36.10 (85.0%)<br>35.50 (83.2%)<br>41.91 (100%)|\n\n\n\n(c) VisRAG-Gen: _Multi-image VLM_\n\n\n|top-1<br>MiniCPM-V 2 .6 t to op p- -2<br>3<br>Oracle|6630(933%)<br>66. (94.<br>79 0%)<br>67. (95.<br>77 3%)<br>. .<br>7108(100%)<br>.|4762(698%)<br>52. (76.<br>38 7%)<br>53. (79.<br>97 1", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_8550", "chunk_text": "3%)<br>. .<br>7108(100%)<br>.|4762(698%)<br>52. (76.<br>38 7%)<br>53. (79.<br>97 1%)<br>68. (10.<br>25 0%)<br>.|6024(724%)<br>67. (80.<br>17 7%)<br>. .<br>7090(852%)<br>83. (10.<br>25 0%)<br>.|5641(886%)<br>53. (84.<br>90 7%)<br>54. (85.<br>46 6%)<br>63. (10.<br>65 0%)<br>.|4079(651%)<br>38. (61.<br>35 2%)<br>. .<br>3893(621%)<br>62. (10.<br>69 0%)<br>.|4856(841%)<br>50. (88.<br>90 2%)<br>50. (87.<br>72 9%)<br>57. (10.<br>73 0%)<br>.|5332(789%)<br>54. (80.<br>92 9%)<br>56. (82.<br>12 5%)<br>67. (10.<br>78 0%)<br>.|\n|---|---|---|---|---|---|---|---|\n|GPT-4o<br>top-1<br>top-2<br>top-3<br>Oracle|64.71 (98.0%)<br>63.36 (95.9%)<br>62.01 (93.9%)<br>66.05 (100%)|52.38 (76.7%)<br>49.21 (72.1%)<br>53.97 (79.1%)<br>68.25 (100%)|58.88 (74.2%)<br>64.13 (80.8%)<br>67.17 (84.6%)<br>79.36 (100%)|63.09 (88.3%)<br>66.85 (93.6", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_9000", "chunk_text": "<br>64.13 (80.8%)<br>67.17 (84.6%)<br>79.36 (100%)|63.09 (88.3%)<br>66.85 (93.6%)<br>66.43 (93.0%)<br>71.45 (100%)|20.74 (66.3%)<br>20.16 (64.4%)<br>19.35 (61.9%)<br>31.29 (100%)|54.86 (85.0%)<br>58.45 (90.5%)<br>60.97 (94.4%)<br>64.57 (100%)|52.44 (81.4%)<br>53.69 (82.9%)<br>54.98 (84.5%)<br>63.49 (100%)|\n\n\n\nEmbed-v2 are trained on millions of query-doc pairs (Xiao et al., 2023; Lee et al., 2024), which\nare 10\u00d7 more than our training data. Although bge-large outperforms BM25 on benchmarks like\nMTEB (Muennighoff et al., 2023), it fails on our datasets, indicating text-based embedding models\ntrained on clean text struggle with texts parsed from real-world documents.\n\n\nWhen trained with the same data setup, as demonstrated in Table 2(b)(c), VisRAG-Ret outperforms\ntext models MiniCPM (OCR) & (Captioner) and the vision model SigLIP by a significant margin.\nThe advantage is more pronounced in the out-of-domain setting, where VisRAG-Ret achieves 13%\nand 20% gains over MiniCPM (OCR) and SigLIP, respectively, compared to 4% and 9% in the\nin-domain setting. This indicates that VisRAG-Ret has better generalization capability compared\nto text- and vision-centric models. Notably, despite utilizing the same VLM MiniCPM-V 2.0 for\nparsing, MiniCPM (Captioner) performs worse than VisRAG-Ret, indicating that directly encoding\nwith VLMs works better than using VLMs for parsing. This can be attributed to the inevitable\ninformation loss when multi-modality information is transcribed into text.\n\n\nFurther analysis reveals", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_9450", "chunk_text": " than VisRAG-Ret, indicating that directly encoding\nwith VLMs works better than using VLMs for parsing. This can be attributed to the inevitable\ninformation loss when multi-modality information is transcribed into text.\n\n\nFurther analysis reveals that MiniCPM (OCR) and SigLIP perform differently across datasets:\nSigLIP excels in ArxivQA and ChartQA, while MiniCPM (OCR) significantly outperforms SigLIP\nin DocVQA and InfographicsVQA. This may be due to the different focuses of the two models:\nMiniCPM focuses on text, while SigLIP focuses on visual signals. VisRAG-Ret, built on top of\nMiniCPM-V 2.0, with a SigLIP encoder and a MiniCPM language model, combines the merits of\nboth and performs well across all datasets, capturing more holistic information from a document.\n\n\nCompared to ColPali, a multi-vector document page embedding model, VisRAG-Ret not only maintains superior performance but also achieves much better memory efficiency. ColPali represents a\npage with 256KB of data distributed across 1030 128-dim vectors (Faysse et al., 2024), whereas\nVisRAG-Ret uses just 4.5KB in a single 2304-dimensional vector. This makes VisRAG-Ret more\nsuitable for scaling to millions or billions of documents in real-world applications.\n\n\n**Generation Performance.** In this experiment, we apply a series of text- and vision-based generators and methods on top of the same retriever VisRAG-Ret to study their effectiveness in generating\nthe answer given the query and retrieved documents. Table 3 shows the performance of (a) text-based\ngeneration (TextRAG-Gen), (b) generation using the VLM MiniCPM-V 2.0 which only accepts a\nsingle image as input, and (c) generation using VLMs which accept multiple images as input.\n\n\nWhen models are provided with only the ground-truth documents (\u201cOracle\u201d), VisRAG-Gen models,\nwhich process the document image directly, significantly outperform TextRAG-Gen models, which\n\n\n8\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nFigure 3: Pipeline performance of (a) TextRAG and (b) VisRAG on InfographicsVQA. We visualize\nthe portion", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_9900", "chunk_text": "Gen models, which\n\n\n8\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nFigure 3: Pipeline performance of (a) TextRAG and (b) VisRAG on InfographicsVQA. We visualize\nthe portion of queries that have the positive document retrieved at the top-1 position (\u201cCorrect Retrieval\u201d), and that are answered correctly given the top-1 retrieved document (\u201cCorrect Generation\u201d).\n\n\nrely solely on extracted text. For instance, MiniCPM-V 2.0 achieves 30% higher performance than\nMiniCPM (OCR) when using ground-truth documents. This underscores the importance of visual\nclues in extracting answers from documents.\n\n\nIn practical scenarios where models receive the top-1 to 3 retrieved documents, which may include noise, VisRAG-Gen consistently outperforms TextRAG-Gen within the same model series.\nSpecifically, for MiniCPM-V 2.0, capable of processing only a single image, the weighted selection\napproach demonstrates better performance than page concatenation when handling 2 or 3 retrieved\ndocuments. However, neither method shows a performance improvement as the number of retrieved\ndocuments increases, a trend commonly observed in TextRAG pipelines (Zhu et al., 2024). In contrast, MiniCPM-V 2.6 and GPT-4o, both capable of processing multiple images as input, exhibit\na notable performance gain as the number of retrieved documents increases, suggesting that only\nVLMs pre-trained on multi-image data can effectively reason over multiple retrieved pages.\n\n\n**End-to-end Performance.** In this experiment, we study the effectiveness of the VisRAG _pipeline_,\nby comparing it with the TextRAG pipeline. We construct TextRAG using MiniCPM (OCR) and\nMiniCPM-V 2.6 (OCR) for retrieval and generation, respectively, and VisRAG using VisRAG-Ret\nfor retrieval and MiniCPM-V 2.6 for generation. The performance on InfographicsVQA is visually\nrepresented in Figure 3. Notebly, VisRAG achieves a higher rate of accurately retrieving documents than TextRAG, and demonstrates a significantly improved rate of correct answer generation\nfrom accurately retrieved documents. The cumulative improvements in both retrieval and generation phases result in an overall accuracy increment from 25% to 51%. Across the six evaluation\ndatasets, VisRAG shows", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_10350", "chunk_text": " and demonstrates a significantly improved rate of correct answer generation\nfrom accurately retrieved documents. The cumulative improvements in both retrieval and generation phases result in an overall accuracy increment from 25% to 51%. Across the six evaluation\ndatasets, VisRAG shows a 40% relative accuracy increment on average, as illustrated in Figure 1.\n\n\n\nThe case study of VisRAG and TextRAG is presented in Appendix F.\n\n\n5.2 TRAINING DATA EFFICIENCY\n\n\nIn this experiment, we study the training data\nefficiency of VisRAG-Ret by evaluating the performance of VisRAG-Ret trained under different\namounts of synthetic training data, i.e. in the out\n- f-domain setting. As shown in Figure 4, to achieve\nthe same performance as bge-large (OCR), VisRAG\nrequires training on only 20K examples, whereas\nMiniCPM (OCR) needs about 75K examples. In\nlater training stages, VisRAG still maintains a 13%\nperformance advantage over MiniCPM (OCR). Although NV-Embed-v2 (OCR) slightly outperforms\nVisRAG trained on our 240K synthetic dataset, it is\n\n\n9\n\n\n\n\n\n\n|80|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n|0<br>5.0e+04<br>1.0e+05<br>1.5e+05<br>2.0e+05<br># Train Q~~-~~D Pairs<br>0<br>20<br>40<br>60<br>80<br> <br>VisRAG~~-~~Ret<br>~~MiniCPM (OCR)~~<br>bge~~-~~large (OCR)<br>N~~V-~~Embed~~-~~v2 (OCR)|||||||||||\n|0<br>5.0e+04<br>1.0e+05<br>1.5e+05<br>2.0e+05<br># Train Q~~-~~D Pairs<br>0<br>20<br>40<br>60<br>80<br> <br>VisRAG~~-~~Ret<br>~~MiniCPM (OCR)~~<br>bge~~", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_10800", "chunk_text": " Train Q~~-~~D Pairs<br>0<br>20<br>40<br>60<br>80<br> <br>VisRAG~~-~~Ret<br>~~MiniCPM (OCR)~~<br>bge~~-~~large (OCR)<br>N~~V-~~Embed~~-~~v2 (OCR)|||||||||||\n|0<br>5.0e+04<br>1.0e+05<br>1.5e+05<br>2.0e+05<br># Train Q~~-~~D Pairs<br>0<br>20<br>40<br>60<br>80<br> <br>VisRAG~~-~~Ret<br>~~MiniCPM (OCR)~~<br>bge~~-~~large (OCR)<br>N~~V-~~Embed~~-~~v2 (OCR)|||||||||||\n|0<br>5.0e+04<br>1.0e+05<br>1.5e+05<br>2.0e+05<br># Train Q~~-~~D Pairs<br>0<br>20<br>40<br>60<br>80<br> <br>VisRAG~~-~~Ret<br>~~MiniCPM (OCR)~~<br>bge~~-~~large (OCR)<br>N~~V-~~Embed~~-~~v2 (OCR)|||||||||||\n|0<br>5.0e+04<br>1.0e+05<br>1.5e+05<br>2.0e+05<br># Train Q~~-~~D Pairs<br>0<br>20<br>40<br>60<br>80<br> <br>VisRAG~~-~~Ret<br>~~MiniCPM (OCR)~~<br>bge~~-~~large (OCR)<br>N~~V-~~Embed~~-~~v2 (OCR)||||||Vis<br>~~Mini~~|AG~~-~~Re<br>~~CPM (~~|t<br>~~ OCR~~|~~ )~~|~~ )~~|\n|0<br>5.0e+04<br>1.0e+05<br>1.5e+05<br>2.0e+05<br># Train Q~~-~~D Pairs<br>0<br>20<br>40<br>60<br>80<br> <br", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_11250", "chunk_text": ".0e+05<br>1.5e+05<br>2.0e+05<br># Train Q~~-~~D Pairs<br>0<br>20<br>40<br>60<br>80<br> <br>VisRAG~~-~~Ret<br>~~MiniCPM (OCR)~~<br>bge~~-~~large (OCR)<br>N~~V-~~Embed~~-~~v2 (OCR)||||||<br>bge~~-~~ <br>N~~V-~~|<br>large (O<br>Embed~~-~~|<br> CR<br>v2 (|<br> )<br> OCR)||\n\n\n\nFigure 4: Average retrieval performance of\nVisRAG-Ret vs. MiniCPM (OCR) trained\nwith different numbers of training examples.\n\n\nPublished as a conference paper at ICLR 2025\n\n\nRetrieval Retrieval & Generation\n\n\nArxivQA\n\n\nInfographicsVQA\n\n\nFigure 5: Relative retrieval and generation performance of VisRAG, VisRAG (SigLIP), and TextRAG on different subsets of queries. The X-axes represent the query subsets where the lengths of\nthe positive documents fall within specific percentile ranges. For comparative analysis, we set TextRAG\u2019s performance to zero and show the performance differences of other models from TextRAG.\n\n\ntrained on millions of curated query-document pairs and has an 8B parameter scale. The results\nsuggest that capturing holistic document information is more effective and efficient than merely\nincreasing training data and model parameters when relying solely on the text modality.\n\n\n5.3 PERFORMANCE ON DIFFERENT DATA SUBSETS\n\n\nIn this experiment, we assess the retrieval and generation performance of VisRAG and TextRAG defined in Figure 3, as well as VisRAG (SigLIP), which replaces the retriever in VisRAG with SigLIP.\nIn Figure 5, we report their performance across different data subsets of ArxivQA and InfographicsVQA by categorizing queries based on the lengths of their positive documents, measured by the\nnumber of tokens of the extracted text. Documents with a higher volume of extracted text may pri\n- ritize textual information over visual content. For each group, we calculate and plot the average\nperformance differences between VisRAG and TextRAG, as well as between VisRAG (SigLIP) and\nTextRAG", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_11700", "chunk_text": " pri\n- ritize textual information over visual content. For each group, we calculate and plot the average\nperformance differences between VisRAG and TextRAG, as well as between VisRAG (SigLIP) and\nTextRAG, to compare how each model performs relative to TextRAG. We observe that, in general,\nthe relative performance of VisRAG and VisRAG (SigLIP) improves as the length of the relevant\ndocument decreases. This suggests that models with vision encoders can better understand documents that emphasize visual information. However, VisRAG (SigLIP) consistently underperforms\nVisRAG across all data subsets and, in some cases, even performs worse than TextRAG. In contrast,\nVisRAG outperforms TextRAG on most subsets, indicating that the underlying language model in\nVisRAG is crucial for better understanding the semantics conveyed through visual cues.\n\n\n6 CONCLUSION\n\n\nIn this paper, we propose VisRAG, a novel retrieval-augmented generation (RAG) paradigm that utilizes vision-language models (VLMs) to facilitate retrieval and generation within an RAG pipeline,\nthereby eliminating the parsing stage required in traditional text-based RAG. Our empirical results demonstrate that VisRAG consistently outperforms text-based RAG on retrieval and generation\nwhile maintaining a simpler pipeline. We hope that VisRAG will inspire future RAG development\nto incorporate VLMs for handling multi-modal documents.\n\n\n10\n\n\nPublished as a conference paper at ICLR 2025\n\n\nACKNOWLEDGMENTS\n\n\nThis work is supported by the Institute Guo Qiang at Tsinghua University. It is also partially supported by the National Natural Science Foundation of China under Grant No. 62206042.\n\n\nREFERENCES\n\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\nreport. _arXiv preprint arXiv:2303.08774_, 2023.\n\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Fl", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_12150", "chunk_text": "iste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. In _Proceedings of NeurIPS_, volume 35, pp. 23716\u201323736, 2022.\n\n\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection. In _Proceedings of ICLR_, 2024.\n\n\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.\n\n\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia,\nZiwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of\nchatgpt on reasoning, hallucination, and interactivity. In _Proceedings of AACL/IJCNLP 2023_, pp.\n675\u2013718, 2023.\n\n\nRohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani,\n[and Sa\u02d8gnak Tas\u00b8\u0131rlar. Introducing our multimodal models, 2023. URL https://www.adept.](https://www.adept.ai/blog/fuyu-8b)\n[ai/blog/fuyu-8b.](https://www.adept.ai/blog/fuyu-8b)\n\n\nYingshan Chang, Mridu Narang, Hisami Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan Bisk.\nWebqa: Multihop and multimodal qa. In _Proceedings of CVPR_, pp. 16495\u201316504, 2022.\n\n\nGuiming Hardy Chen, Shunian Chen, Ruifei Zhang, Jun", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_12600", "chunk_text": "isk.\nWebqa: Multihop and multimodal qa. In _Proceedings of CVPR_, pp. 16495\u201316504, 2022.\n\n\nGuiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized\ndata for a lite vision-language model. _arXiv preprint arXiv:2402.11684_, 2024a.\n\n\nWentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu,\nGuirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile\ngui agents. _arXiv preprint arXiv:2406.11317_, 2024b.\n\n\nGordon V Cormack, Charles LA Clarke, and Stefan Buettcher. Reciprocal rank fusion outperforms\ncondorcet and individual rank learning methods. In _Proceedings of SIGIR_, 2009.\n\n\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. In _Proceedings of NeurIPS_, 2023.\n\n\nYuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu,\nYehua Yang, Qingqing Dang, and Haoshuang Wang. Pp-OCR: A Practical Ultra Lightweight\nOCR System. _arXiv_, abs/2009.09941, 2020.\n\n\nManuel Faysse, Hugues Sibille, Tony Wu, Gautier Viaud, C\u00b4eline Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. _arXiv preprint arXiv:2407.01449_,\n2024.\n\n\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_13500", "chunk_text": ", Kenton Lee, Kristina\nToutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In _Proceedings of the IEEE/CVF International Conference on_\n_Computer Vision_, pp. 12065\u201312075, 2023a.\n\n\nShengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng,\nYewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao,\nChenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the Potential of Small Language\nModels with Scalable Training Strategies. In _First Conference on Language Modeling_, volume\nabs/2404.06395, 2024c.\n\n\nZiniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid,\nDavid A Ross, and Alireza Fathi. Reveal: Retrieval-augmented visual-language pre-training with\nmulti-source multimodal knowledge memory. In _Proceedings of CVPR_, pp. 23369\u201323379, 2023b.\n\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Comput._\n_Surv._, (12):248:1\u2013248:38, 2023.\n\n\nZhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\nJamie Callan, and Graham Neubig. Active retrieval augmented generation. In _Proceedings of_\n_EMNLP_, pp. 7969\u20137992, 2023.\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_13950", "chunk_text": "9\u20137992, 2023.\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In _Pro-_\n_ceedings of EMNLP_, pp. 6769\u20136781, 2020.\n\n\nGeewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim,\nWonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In _Proceedings of ECCV_, pp. 498\u2013517. Springer, 2022.\n\n\nHugo Laurenc\u00b8on, L\u00b4eo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building\nvision-language models? _arXiv preprint arXiv:2405.02246_, 2024.\n\n\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models. _arXiv_, abs/2405.17428, 2024.\n\n\nKenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct:\nScreenshot parsing as pretraining for visual language understanding. In _Proceedings of ICML_,\npp. 18893\u201318912, 2023.\n\n\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, Sebastian Riedel, and\nDouwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In _Proceed-_\n_ings of NeurIPS_, 2020.\n\n\n12\n\n\nPublished as a conference paper at ICLR 2025\n\n\nBo Li,", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_14400", "chunk_text": "we Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In _Proceed-_\n_ings of NeurIPS_, 2020.\n\n\n12\n\n\nPublished as a conference paper at ICLR 2025\n\n\nBo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei\nLi, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. _arXiv preprint_\n_arXiv:2408.03326_, 2024a.\n\n\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In _Proceedings of ICML_, pp.\n12888\u201312900, 2022.\n\n\nLei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language\nModels. In _Proceedings of ACL_, pp. 14369\u201314387, 2024b.\n\n\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual\ninstruction tuning. In _Proceedings of ICLR_, 2024.\n\n\nZiyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi\nShao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for\nmulti-modal large language models. _arXiv preprint arXiv:2311.07575_, 2023.\n\n\nBingshuai Liu, Chenyang Lyu, Zijun Min, Zhanyu Wang, Jinsong Su, and Longyue Wang. Retrievalaugmented multi-modal chain-of-thoughts reasoning for large language models. _arXiv preprint_\n_arXiv:2312.01714_, 2023a.\n\n\nHaotian Liu, Chunyuan Li, Qingyang Wu,", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_14850", "chunk_text": "mented multi-modal chain-of-thoughts reasoning for large language models. _arXiv preprint_\n_arXiv:2312.01714_, 2023a.\n\n\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _Proceed-_\n_ings of NeurIPS_, volume 36, pp. 34892\u201334916, 2023b.\n\n\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction\ntuning. In _Proceedings of CVPR_, pp. 26296\u201326306, 2024a.\n\n\n[Jerry Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index.](https://github.com/jerryjliu/llama_index)\n\n\nYuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai.\nTextmonkey: An ocr-free large multimodal model for understanding document. _arXiv preprint_\n_arXiv:2403.04473_, 2024b.\n\n\nMan Luo, Yankai Zeng, Pratyay Banerjee, and Chitta Baral. Weakly-supervised visual-retrieverreader for knowledge-based question answering. In _Proceedings of EMNLP_, pp. 6417\u20136431,\n2021.\n\n\nMan Luo, Zhiyuan Fang, Tejas Gokhale, Yezhou Yang, and Chitta Baral. End-to-end knowledge\nretrieval with multi-modal queries. In _Proceedings of ACL_, pp. 8573\u20138589, 2023.\n\n\nXueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal\nretrieval via document screenshot embedding. _arXiv preprint arXiv:2406.11251_, 2024.\n\n\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In _Proceedings_\n\n_of ACL_, pp. 2263\u20132279, 2022.\n\n\nM", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_16650", "chunk_text": " Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n\n\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan\nSung, Denny Zhou, Quoc Le, et al. Freshllms: Refreshing large language models with search\nengine augmentation. _arXiv preprint arXiv:2310.03214_, 2023.\n\n\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu,\nJialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng\nLiu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language model\u2019s\nperception of the world at any resolution, 2024.\n\n\n14\n\n\nPublished as a conference paper at ICLR 2025\n\n\nWeihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,\nLei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. _arXiv_\n_preprint arXiv:2311.03079_, 2023.\n\n\nCong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen.\nUniir: Training and benchmarking universal multimodal information retrievers. _arXiv preprint_\n_arXiv:2311.17136_, 2023.\n\n\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\nmodels. _TMLR_, 2022.\n\n\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_17100", "chunk_text": ". Emergent abilities of large language\nmodels. _TMLR_, 2022.\n\n\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack:\nPackaged resources to advance general chinese embedding. _arXiv preprint arXiv:2309.07597_,\n2023.\n\n\nPeng Xu, Wei Ping, Xianchao Wu, Zihan Liu, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa\n2: Bridging the gap to proprietary llms in long context and rag capabilities. _arXiv preprint_\n_arXiv:2407.14482_, 2024a.\n\n\nRuyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan\nLiu, Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and highresolution images. _arXiv preprint arXiv:2403.11703_, 2024b.\n\n\nZhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan\nLiu, and Ge Yu. Activerag: Revealing the treasures of knowledge via active learning. _arXiv_\n_preprint arXiv:2402.13547_, 2024c.\n\n\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation.\n_arXiv preprint arXiv:2401.15884_, 2024.\n\n\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. _arXiv preprint_\n_arXiv:2407.10671_, 2024.\n\n\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li,\nWeilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zh", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_18000", "chunk_text": ", Chou Leuang Yu, Danny Pan,\nEsther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual\nlarge language model series. _arXiv preprint arXiv:2405.19327_, 2024.\n\n\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _ArXiv_\n_preprint_, 2023.\n\n\n15\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTianshuo Zhou, Sen Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, and Ge Yu.\nMARVEL: unlocking the multi-modal capability of dense retrieval via visual module plugin. In\n_Proceedings of ACL_, pp. 14608\u201314624, 2024.\n\n\nKunlun Zhu, Yifan Luo, Dingling Xu, Ruobing Wang, Shi Yu, Shuo Wang, Yukun Yan, Zhenghao\nLiu, Xu Han, Zhiyuan Liu, et al. Rageval: Scenario specific rag evaluation dataset generation\nframework. _arXiv preprint arXiv:2408.01262_, 2024.\n\n\n16\n\n\nPublished as a conference paper at ICLR 2025\n\n\nA DATA CONSTRUCTION DETAILS\n\n\nA.1 SYNTHETIC DATA\n\n\nTable 4: Statistics of crawled documents. We prompt GPT-4o to generate queries on these docu\nments.\n\n\n**Name** **Source** **Description** **# Pages**\n\n\nTextbooks https://openstax.org/ College-level textbooks including various subjects 10,000\nICML Papers ICML 2023 ICML papers on various topics 5,000\nNeurIPS Papers NeurIPS 2023 NeurIPS papers on various topics 5,000\nManuallib https://www.manualslib.com/ Manuals of various kinds of products 20,000\n\n\nTo augment the training dataset of VisRAG, we gather additional documents from the web and utilize\nGPT-4o to generate queries based on these documents. The sources of the collected documents are\nlisted in Table 4. The prompt employed is", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_18450", "chunk_text": " augment the training dataset of VisRAG, we gather additional documents from the web and utilize\nGPT-4o to generate queries based on these documents. The sources of the collected documents are\nlisted in Table 4. The prompt employed is shown in Figure 6.\n\n\nFigure 6: Prompt for GPT-4o to generate queries, where _{{_ document _}}_ is the document page.\n\n\nA.2 QUERY FILTERING\n\n\nAs mentioned in Sec. 3.3, a significant portion of queries in VQA datasets are context-dependent and\nthus unsuitable for retrieval. To filter out such queries, we prompt GPT-4o (OpenAI, 2024) using the\ninstruction shown in Figure 7, which includes human-annotated samples from DocVQA. Although\nthis filtering step reduces context-dependent queries, a small number may still remain. However,\ntheir presence is minimal and does not significantly impact the overall quality of our dataset.\n\n\n17\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nFigure 7: Prompt for GPT-4o to classify queries, where _{{_ query _}}_ is the query to be classified.\nLabel B denotes context-dependent queries.\n\n\n18\n\n\nPublished as a conference paper at ICLR 2025\n\n\nB DOCUMENT PARSING\n\n\nIn this paper, we experiment with two categories of document parsing strategies: pipeline-based\nparsing and model-based parsing.\n\n\nB.1 PIPELINE-BASED PARSING\n\n\nWe consider the following document parsing pipelines:\n\n\n**Pytesseract.** Pytesseract is a Python wrapper for Google\u2019s Tesseract OCR engine, offering a\nstraightforward interface for text extraction from images. Unlike more complex methods, Pytesseract requires minimal pre-processing. By invoking the image ~~t~~ - ~~s~~ tring function, OCR is performed in a single step, directly returning the extracted text. Tesseract internally handles bounding\nboxes, confidence scores, and orientation correction.\n\n\n**PPOCR-based Methods.** PaddlePaddle OCR (PPOCR) (Du et al., 2020) is widely used for document text extraction, covering text detection, classification, and recognition. First, a text detection\nmodel identifies text regions and generates bounding boxes. These regions are then processed by a\nclassification model to correct orientation issues like rotation or flipping. Next, a recognition model\nextracts the textual content from the corrected bounding boxes, returning", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_18900", "chunk_text": " a text detection\nmodel identifies text regions and generates bounding boxes. These regions are then processed by a\nclassification model to correct orientation issues like rotation or flipping. Next, a recognition model\nextracts the textual content from the corrected bounding boxes, returning recognized text with confidence scores. Only results with confidence scores above 0.6 are retained, and the bounding box\ncoordinates, along with the recognized text, are stored for further processing. We apply the following\nstrategies to obtain the final parsing result:\n\n\n   - Adjacent Merging: To enhance text coherence, this policy combines adjacent text boxes\nbased on vertical proximity (within 15 pixels) and horizontal alignment (within 100 pixels),\nreducing text fragmentation. This iterative merging process consolidates eligible text boxes\ninto unified bounding boxes with concatenated text. Finally, the text from the remaining\nbounding boxes is combined with line breaks to produce the final result.\n\n    - Layout Preserving: This policy maintains the original document structure by ordering text\nboxes based on their spatial positions. Spaces and line breaks are dynamically inserted to\nreflect horizontal and vertical gaps between text regions. This approach ensures that the\nextracted text mirrors the original document layout, preserving its formatting in the final\nresult.\n\n\nWe run the aforementioned pipelines on our dataset to obtain text-based training and evaluation\ndata, and fine-tune a MiniCPM retriever to assess performance. The results are presented in Table 5.\nMethods based on PPOCR demonstrate significantly better performance compared to pytesseract,\nwith adjacent merging and layout preserving yielding similar results. Consequently, we opt to use\nthe adjacent merging policy for our \u201c(OCR)\u201d runs.\n\n\nTable 5: Overall retrieval performance of different document parsing pipelines.\n\n\nB.2 MODEL-BASED PARSING\n\n\nIn addition to pipeline-based methods, we also employ a model-based parsing approach using\nMiniCPM-V 2.0 to directly transcribe document images into text. This method is referred to as\n\u201c(Captioner)\u201d.\n\n\nTo train this model, we collect data from two sources: a) ALLaVA (Chen et al., 2024a) (image, caption) pairs, and b) VQA documents with descriptions generated by GPT-4V. We use the prompt\nin Figure 8 to instruct GPT-4V to generate detailed descriptions of documents from DocVQA,\nChartQA, SlideVQA, InfographicsVQA, TextVQA (", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_19350", "chunk_text": " by GPT-4V. We use the prompt\nin Figure 8 to instruct GPT-4V to generate detailed descriptions of documents from DocVQA,\nChartQA, SlideVQA, InfographicsVQA, TextVQA (Singh et al., 2019), and ArxivQA.\n\n\n19\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\nFigure 8: Prompt for GPT-4V to generate page description, where _{{_ document _}}_ is the document page.\n\n\nWe train MiniCPM-V 2.0 with a batch size of 2048 and a learning rate of 5e-6 for 1 epoch.\n\n\nC MODELS USED IN THIS PAPER\n\n\n**MiniCPM** (Hu et al., 2024c) is a large language model (LLM) with 2.4 billion non-embedding parameters, demonstrating capabilities comparable to much larger models, such as Llama2-7B (Touvron et al., 2023) and Gemma-7B (Team et al., 2024). In this paper, we employ MiniCPM to\nconstruct the baseline text-based retriever (Table 2) and generator (Table 3).\n\n\n**SigLIP** (Zhai et al., 2023) is a CLIP-style multi-modal model designed to align text and vision\nrepresentations. We utilize SigLIP-400m, released by Hugging Face [2], which incorporates Flash\nAttention 2, increases maximum resolution to 980x980, and adopts the NaViT strategy to allow (a)\nvariable resolution images and (b) aspect ratio preserved images. In this paper, SigLIP is used to\ndevelop the baseline vision-based retriever (Table 2).\n\n\n**MiniCPM-V 2.0** (OpenBMB, 2024a; Yao et al., 2024) is a vision-language model (VLM) with\n2.8 billion non-embedding parameters, built upon SigLIP-400m and MiniCPM. It can process single\nimages up to 1.8 million pixels (e.g., 1344x1344) at any aspect ratio. We use MiniCPM-V 2.0 to\nbuild VisRAG-Ret (Table 2) and VisRAG-Gen (Table 3(b)), as well as the", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_19800", "chunk_text": "1344x1344) at any aspect ratio. We use MiniCPM-V 2.0 to\nbuild VisRAG-Ret (Table 2) and VisRAG-Gen (Table 3(b)), as well as the document parsing model.\n\n\n**MiniCPM-V 2.6** (OpenBMB, 2024b; Yao et al., 2024) is an upgrade of MiniCPM-V 2.0 and\nMiniCPM-Llama3-V 2.5 (Yao et al., 2024). It is built upon SigLIP-400M and Qwen2-7B (Yang\net al., 2024) with a total of 8.5B parameters, exihibiting a significant performance improvement\n\n- ver MiniCPM-Llama3-V 2.5 (Yao et al., 2024). Different from previous models, MiniCPM-V\n2.6 can accept multiple images as the input and perform multi-modal in-context learning. It also\ndemonstrates stronger OCR capabilities. We use MiniCPM-V 2.6 to build VisRAG-Gen (Table 3)\nand a text-based generation baseline MiniCPM-V 2.6 (OCR) (Figure 3, Figure 5).\n\n\nNote that, MiniCPM-Llama3-V 2.5 (Yao et al., 2024) is not used in this paper.\n\n\n**GPT-4o** (OpenAI, 2024) is OpenAI\u2019s latest multi-modal model, capable of processing any combination of text, audio, image, and video inputs and generating outputs in text, audio, and image\nformats. We use GPT-4o to construct VisRAG-Gen (Table 3) and to synthesize training data.\n\n\nD RETRIEVAL PERFORMANCE IN RECALL@10\n\n\nTable 6 presents the retrieval performance in Recall@10.\n\n\nE PROMPTS FOR GENERATION\n\n\nWe present the prompts of VisRAG-Gen and TextRAG-Gen in Table 7.\n\n\n2https://huggingface.co/HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit\n\n\n20\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 6: Overall retrieval performance in Recall@10.\n\n\nTable 7: Prompt templates for generation. \u201c", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_20250", "chunk_text": "-so400m-14-980-flash-attn2-navit\n\n\n20\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 6: Overall retrieval performance in Recall@10.\n\n\nTable 7: Prompt templates for generation. \u201cOthers\u201d refers to all VQA datasets except ArxivQA.\n\n\n\n\n\n\n\n\n|Col1|TextRAG|VisRAG|\n|---|---|---|\n|**ArxivQA**|Hint:_ {{_ parsed document(s)_ }}_<br>Question:_ {{_ query_ }}_<br>Options:<br>A._ {{_ Option 1_ }}_<br>B._ {{_ Option 2_ }}_<br>C._ {{_ Option 3_ }}_<br>D._ {{_ Option 4_ }}_<br>Answer directly with the letter of the correct option as<br>the frst character.|_{{_ document(s)_ }}_<br>Question:_ {_query_ }}_<br>Options:<br>A._ {{_ Option 1_ }}_<br>B._ {{_ Option 2_ }}_<br>C._ {{_ Option 3_ }}_<br>D._ {{_ Option 4_ }}_<br>Answer directly with the letter of the correct option as<br>the frst character.|\n|**Others**|Image:_{{_ parsed document(s)_ }}_<br>Answer the question using a single word or phrase.<br>Question:_{{_ query_ }}_<br>Answer:|_{{_ document(s)_ }}_<br>Answer the question using a single word or phrase.<br>Question:_{{_ query_ }}_<br>Answer:|\n\n\n\nF CASE STUDY\n\n\nWe show two cases in Table 8 and Table 9. In both instances, we compare VisRAG with TextRAG,\nmaintaining the same setup as described in the \u201cEnd-to-end Performance\u201d paragraph in Sec. 5.1.\n\n\nIn the first case from DocVQA, the user queries about \u201cClub Jetty,\u201d however, the term \u201cClub Jetty\u201d in\nthe relevant document is not successfully extracted due to its decorative font. This leads to TextRAG\nfailing to retrieve the document, while VisRAG successfully retrieves it.\n\n\nIn the second case from InfographicsVQA, although both TextRAG and VisRAG successfully retrieve the document, TextRAG generates an incorrect response due to the loss of layout information,\nmaking", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_20700", "chunk_text": " document, while VisRAG successfully retrieves it.\n\n\nIn the second case from InfographicsVQA, although both TextRAG and VisRAG successfully retrieve the document, TextRAG generates an incorrect response due to the loss of layout information,\nmaking it unclear which number (53% or 49%) pertains to Europe. VisRAG effectively utilizes the\nlayout information and generates the correct answer.\n\n\n21\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 8: Case study from DocVQA. In this case, VisRAG successfully retrieves the ground-truth\ndocument, while TextRAG fails, leading to VisRAG\u2019s correct generation and TextRAG\u2019s incorrect\ngeneration.\n\n\n\n\n\n\n\n\n\n|Col1|TextRAG|VisRAG|\n|---|---|---|\n|**Query**|On which day is Club Jetty closed?|On which day is Club Jetty closed?|\n|**Retrieved**<br>**Top-1 Document**|\u2717**Incorrect**|\u2713**Correct**|\n|**Document**<br>**Parsing Result**|SMOKERS_\u2190-_ EXPRESS_\u2190-_ Express_\u2190-_ Airlines_\u2190-_ Yes that\u2019s<br>right.<br>An Airline for_\u2190-_ smokers is coming!<br>But you_\u2190-_ say,<br>they can\u2019t do that, what about_\u2190-_ the FAA regulations?_\u2190-_ No<br>problem. Smokers Express is_\u2190-_ a club, providing service_\u2190-_ to<br>members only: With a little bit_\u2190-_ of luck and your strong_\u2190-_<br>support we may see Smokers_\u2190-_ Express Airlines making_\u2190-_<br>news and carrying smokers_\u2190-_ in style by this summer._\u2190-_ K<br>No screaming babies_\u2190-_ (members must be 18)_\u2190-_ M Compli-<br>mentary newspaper_\u2190-_ N Free destination area maps_\u2190-_ O Dis-<br>counts on area attractions_\u2190-_ p Infight phone service_\u2190-_ Q Dis-<br>count cruise packages_\u2190-_ from Smokers Travel_\u2190-_ R A subscrip-<br>tion to \u201dLet\u2019s Party\u201d_\u2190-_ the offcial Smokers_\u2190-_ Smokers Express<br>is the brainchild_\u2190-_ of William Walts and_\u2190-_ George \u201dMickey", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_21150", "chunk_text": " R A subscrip-<br>tion to \u201dLet\u2019s Party\u201d_\u2190-_ the offcial Smokers_\u2190-_ Smokers Express<br>is the brainchild_\u2190-_ of William Walts and_\u2190-_ George \u201dMickey\u201d<br>Richardson, a_\u2190-_ couple of Cocoa Beach,_\u2190-_ Florida business-<br>men who like to_\u2190-_ smoke.<br>They organized_\u2190-_ the club, in<br>December of last year._\u2190-_ The club is headquartered_\u2190-_ at the<br>Space Coast airport_\u2190-_ near Cocoa Beach and_\u2190-_ has made ar-<br>rangements to lease_\u2190-_ up to 29 specially equipped_\u2190-_ and re-<br>cently reconditioned DC-9s._\u2190-_ Some of the destinations they_\u2190-_<br>plan to serve with non-stop service_\u2190-_ from Space Coast exec-<br>utive airport_\u2190-_ include Orlando, Atlanta, Chicago,_\u2190-_ Dallas,<br>Las Vegas, and Atlantic City_\u2190-_ (Express Travel Magazine)_\u2190-_<br>S Rental car discounts_\u2190-_ T Smokers Express discount home_\u2190-_<br>shopping guide_\u2190-_ U Great contests and sweepstakes_\u2190-_ for mem-<br>bers only_\u2190-_ V Free Lotto ticket for each passenger_\u2190-_ W Discount<br>air freight rates_\u2190-_ X Discount coupons for destination_\u2190-_ area<br>restaurants_\u2190-_ Y Special party fights to Las Vegas_\u2190-_ and Atlantic<br>City with every 7th and_\u2190-_ 11th fight free_\u2190-_ Z The best trained,<br>most attentive_\u2190-_ staff of employee/owners_\u2190-_ in the industry._\u2190-_<br>With the help of consultant,_\u2190-_ Bryant Chestnut (formerly of<br>the_\u2190-_ FAA), Smokers Express is_\u2190-_ beginning the FAA_\u2190-_ Cer-<br>tifcation process._\u2190-_ Those are the ABC\u2019s of traveling_\u2190-_ on a<br>great fun new_\u2190-_ smokers airline where membership_\u2190-_ does have<br>real privileges._\u2190-_ The frst 50,000 memberships are_\u2190-_ charter<br>life-time._\u2190-_ Membership in the club costs_\u2190-_ $25 annually and<br>includes_\u2190-_ a number of special perks_\u2190", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_21600", "chunk_text": "real privileges._\u2190-_ The frst 50,000 memberships are_\u2190-_ charter<br>life-time._\u2190-_ Membership in the club costs_\u2190-_ $25 annually and<br>includes_\u2190-_ a number of special perks_\u2190-_ which you will fnd<br>interesting._\u2190-_ Membership is restricted_\u2190-_ to persons 18 years of<br>age_\u2190-_ or older. Take a look at_\u2190-_ what members will receive:_\u2190-_<br>If you would like more_\u2190-_ information about Smokers_\u2190-_ Express<br>Airlines you can call or_\u2190-_ write:_\u2190-_ Smokers Express_\u2190-_ Suite<br>102_\u2190-_ 25 South Atlantic Avenue_\u2190-_ Cocoa Beach, FL 32931_\u2190-_<br>(407) 783-6124_\u2190-_ A Smokers Express Numbered_\u2190-_ Members<br>Certifcate_\u2190-_ B Smokers Express Gold Travel_\u2190-_ Card_\u2190-_ C<br>V.I.P. Lounges at fight initiating_\u2190-_ airports_\u2190-_ D Free smokes<br>in fight_\u2190-_ E Free headphones_\u2190-_ F Free infight movies_\u2190-_ G<br>Full beverage service_\u2190-_ H Real ashtrays_\u2190-_ Smoker Express is<br>taking_\u2190-_ applications for personnel_\u2190-_ for practically every as-<br>pect of_\u2190-_ operations. These positions_\u2190-_ are available to mem-<br>bers only._\u2190-_ t Real food for real people\u2014Steaks_\u2190-_ & Burgers_\u2190-_<br>Great tasting munchies for happy_\u2190-_ hour._\u2190-_ American Smoker\u2019s<br>Journal_\u2190-_ 38 WINTER ISSUE|FXPLOREKAUAI_\u2190-_ (We mail gift paks)_\u2190-_ Windsurfng_\u2190-_<br>KAUAIWINDSURFING_\u2190-_<br>EXPERIENCEIS_\u2190-_<br>NOW<br>OPEN_\u2190-_<br>Learn<br>to<br>Windsurf_\u2190-_<br>(certifed<br>instruction)_\u2190-_<br>Special introductory_\u2190-_ Lesson Rate_\u2190-_ on your way_\u2190-_ fresh_\u2190-_<br>from the roaster_\u2190-_ fern grotto_\u2190-_ WAILUA_\u2190-_ MARINA_\u2190", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_22050", "chunk_text": "_<br>Special introductory_\u2190-_ Lesson Rate_\u2190-_ on your way_\u2190-_ fresh_\u2190-_<br>from the roaster_\u2190-_ fern grotto_\u2190-_ WAILUA_\u2190-_ MARINA_\u2190-_<br>RESTAURANT_\u2190-_ On the banks of the Wailua River_\u2190-_ to<br>you_\u2190-_ COFFEE_\u2190-_ & NUT_\u2190-_ ROASTING_\u2190-_ CENTER_\u2190-_<br>\u201dHOME STYLE COOKING\u201d_\u2190-_ famous baked stuffed pork<br>chops_\u2190-_<br>and<br>28<br>other<br>entrees_\u2190-_<br>EASY<br>LEARNING_\u2190-_<br>EXCURSIONS_\u2190-_ RENTALS_\u2190-_ Phone: 245-9290_\u2190-_ or Kauai<br>Surf ext. 7830_\u2190-_ The Market Place-shop 39_\u2190-_ at the Coconut<br>Plantation_\u2190-_ Waipouli, Kauai_\u2190-_ coffee tea nuts spices herbs_\u2190-_<br>Complimentary transportation_\u2190-_ (from Wailua area Hotels-<br>dinner only)_\u2190-_ Phone: 822-4311_\u2190-_ NOW! lunch daily from<br>11 a.m._\u2190-_ PAPERBACK_\u2190-_ HUT_\u2190-_ Hi, my name is Sunny<br>..._\u2190-_ and I own one of the most_\u2190-_ unique restaurants in the<br>world_\u2190-_ in Lihue, Kauai._\u2190-_ It\u2019s called the Casa Blanca,_\u2190-_<br>and we offer Kauai\u2019s only late_\u2190-_ gourmet dining service in a<br>very_\u2190-_ friendly and casual atmosphere._\u2190-_ We\u2019re open every<br>night from_\u2190-_ 5:30-10:30 for dinner with_\u2190-_ Brunch on Sundays<br>and live_\u2190-_ entertainment in our OASIS_\u2190-_ lounge until the wee<br>small_\u2190-_ hours. Oh Yes, we specialize_\u2190-_ in Italian and French_\u2190-_<br>cuisine with lots of fresh_\u2190-_ local seafood and Kauai\u2019s_\u2190-_ only<br>Fresh Fruit Daquiris._\u2190-_ Call us for reservations at 245-9181_\u2190-_<br>", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_22500", "chunk_text": "\u2190-_<br>cuisine with lots of fresh_\u2190-_ local seafood and Kauai\u2019s_\u2190-_ only<br>Fresh Fruit Daquiris._\u2190-_ Call us for reservations at 245-9181_\u2190-_<br>and free hotel pickup_\u2190-_ from most resorts._\u2190-_ I know you\u2019ll<br>love_\u2190-_ Kauai and have the_\u2190-_ time of your life_\u2190-_ at the Casa<br>Blanca._\u2190-_ the_\u2190-_ Bestsellers_\u2190-_ Games_\u2190-_ Hawaiiana_\u2190-_ We<br>have the most complete selection_\u2190-_ of paperback books on<br>the island._\u2190-_ Over 5,000 books in stock._\u2190-_ OPEN EARLY-<br>CLOSE LATE_\u2190-_ The Market Place at Coconut Plantation_\u2190-_<br>Waipouli, Kauai_\u2190-_ 822-3216_\u2190-_ CLUBIETTY_\u2190-_ Restaurant<br>and Cabaret_\u2190-_ Nawiliwili Bay_\u2190-_ CANTONESE FOOD_\u2190-_<br>a specialty of the house_\u2190-_ COMPLETE MENU-including_\u2190-_<br>STEAK-LOBSTER-MAHIMAHI_\u2190-_<br>DINNER:<br>5:30-9:45<br>p.m._\u2190-_ Closed TUESDAYS_\u2190-_ MUSIC to Dine & Dance by-<br>7:30 p.m._\u2190-_ After dinner Dance Band & DISCO_\u2190-_ Courtesy<br>pick-up-Lihue area_\u2190-_ 245.4970....after hours 245.3856_\u2190-_ 2989<br>HALEKO ROAD_\u2190-_ 245-9181_\u2190-_ SUGAR MILL SNACKS_\u2190-_<br>ASIAJOE_\u2190-_ .MUUMUUS. SOUVENIRS_\u2190-_ HANDICRAFTS<br>IMPORTS_\u2190-_<br>COCONUT_\u2190-_<br>PLANTATION-_\u2190-_<br>MARKET<br>PLACE_\u2190-_ 3_\u2190-_ o Fresh Fruit_\u2190-_ Drinks_\u2190-_ e Cold_\u2190-_ Drinks_\u2190-_<br>e Sandwiches_\u2190-_ Macadamia_\u2190-_ Nut Waffe_\u2190-_ Fresh Fruit_\u2190-_ o<br>Ice Cream_\u2190-_ c Berry_\u2190", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_22950", "chunk_text": " Drinks_\u2190-_ e Cold_\u2190-_ Drinks_\u2190-_<br>e Sandwiches_\u2190-_ Macadamia_\u2190-_ Nut Waffe_\u2190-_ Fresh Fruit_\u2190-_ o<br>Ice Cream_\u2190-_ c Berry_\u2190-_ VELVET PAINTINGS. T-SHIRTS_\u2190-_<br>The Market Place At Coconut Plantation_\u2190-_ 484 Kuhio Hwy. at<br>Waipouli, Kapaa, Kauai_\u2190-_ OPEN 7 AM M-S; Sun. 8 AM_\u2190-_<br>822-9981_\u2190-_ 36_\u2190-_ Latitude 20/November 1978|\n|**Answer**|Mondays \u2717**Incorrect**|Tuesdays \u2713**Correct**|\n\n\n22\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 9: Case study from InfographicsVQA. In this case, both VisRAG and TextRAG successfully\nretrieve the correct document; however, only VisRAG effectively leverages the layout information,\nenabling accurate generation. In contrast, TextRAG suffers from information loss of the layout,\nresulting in incorrect responses.\n\n|Col1|TextRAG|VisRAG|\n|---|---|---|\n|**Query**|Whatpercent of account holders in Europe are using LinkedIn for fnding job?|Whatpercent of account holders in Europe are using LinkedIn for fnding job?|\n|**Retrieved**<br>**Top-1 Document**|\u2713**Both Correct**|\u2713**Both Correct**|\n|**Document**<br>**Parsing Result**|Social media_\u2190-_ job seeking trends_\u2190-_ Michael Page\u2019s annual global survey of fnancial services<br>and banking_\u2190-_ employees was conducted in April 2014,more than 3,300 people participated_\u2190-_<br>Linkedln_\u2190-_ Linkedin\u2019s popularity continues to grow, though many job seekers don\u2019t think of it<br>as part of_\u2190-_ their strategy.So hirers need to look to other sourcing channels too_\u2190-_ What pro-<br>portion of account holders_\u2190-_ use Linkedin for job seeking?_\u2190-_ 93_\u2190-_ %_\u2190-_ 30%_\u2190-_ of respon-<br>dents have_\u2190-_ anaccount-up_\u2190-_ 10", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_23400", "chunk_text": " of account holders_\u2190-_ use Linkedin for job seeking?_\u2190-_ 93_\u2190-_ %_\u2190-_ 30%_\u2190-_ of respon-<br>dents have_\u2190-_ anaccount-up_\u2190-_ 10% from last year_\u2190-_ more women_\u2190-_ than men say_\u2190-_ they don\u2019t<br>have_\u2190-_ an account_\u2190-_ 53%_\u2190-_ In Europe_\u2190-_ 49%_\u2190-_ In North America_\u2190-_ 40%_\u2190-_ In the UK_\u2190-_<br>Facebook_\u2190-_ Despite last year\u2019s hype around Graph Search,Facebook hasn\u2019t made any progress with<br>monetising_\u2190-_ its recruitment potential -jobseekers remain very negative about Facebook playing any<br>part_\u2190-_ 13%_\u2190-_ said they\u2019d be happy_\u2190-_ to see adverts_\u2190-_ 92%_\u2190-_ said they would not be_\u2190-_ happy<br>to be contacted by_\u2190-_ a recruiter on Facebook_\u2190-_ 1%_\u2190-_ Don\u2019t bank on social media \u2013 Michael<br>Page brings you a broader range of talent, and jobs_\u2190-_ www.michaelpage.com.au/salarycentre_\u2190-_ of<br>respondents_\u2190-_ (who are job seekers) said they_\u2190-_ would use it to look for jobs_\u2190-_ MichaelPage_\u2190-_<br>Financial Services_\u2190-_ Specialists in fnancial services recruitment_\u2190-_ www.michaelpage.com.au_\u2190-_|Social media_\u2190-_ job seeking trends_\u2190-_ Michael Page\u2019s annual global survey of fnancial services<br>and banking_\u2190-_ employees was conducted in April 2014,more than 3,300 people participated_\u2190-_<br>Linkedln_\u2190-_ Linkedin\u2019s popularity continues to grow, though many job seekers don\u2019t think of it<br>as part of_\u2190-_ their strategy.So hirers need to look to other sourcing channels too_\u2190-_ What pro-<br>portion of account holders_\u2190-_ use Linkedin for job seeking?_\u2190-_ 93_\u2190-_ %_\u2190-_ 30%_\u2190-_ of respon-<br>dents have_\u2190-_ anaccount-up_\u2190-_ 10% from last year_\u2190-_ more women_\u2190-_ than men say_\u2190-_ they don\u2019t<br", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_23850", "chunk_text": "\u2190-_ 30%_\u2190-_ of respon-<br>dents have_\u2190-_ anaccount-up_\u2190-_ 10% from last year_\u2190-_ more women_\u2190-_ than men say_\u2190-_ they don\u2019t<br>have_\u2190-_ an account_\u2190-_ 53%_\u2190-_ In Europe_\u2190-_ 49%_\u2190-_ In North America_\u2190-_ 40%_\u2190-_ In the UK_\u2190-_<br>Facebook_\u2190-_ Despite last year\u2019s hype around Graph Search,Facebook hasn\u2019t made any progress with<br>monetising_\u2190-_ its recruitment potential -jobseekers remain very negative about Facebook playing any<br>part_\u2190-_ 13%_\u2190-_ said they\u2019d be happy_\u2190-_ to see adverts_\u2190-_ 92%_\u2190-_ said they would not be_\u2190-_ happy<br>to be contacted by_\u2190-_ a recruiter on Facebook_\u2190-_ 1%_\u2190-_ Don\u2019t bank on social media \u2013 Michael<br>Page brings you a broader range of talent, and jobs_\u2190-_ www.michaelpage.com.au/salarycentre_\u2190-_ of<br>respondents_\u2190-_ (who are job seekers) said they_\u2190-_ would use it to look for jobs_\u2190-_ MichaelPage_\u2190-_<br>Financial Services_\u2190-_ Specialists in fnancial services recruitment_\u2190-_ www.michaelpage.com.au_\u2190-_|\n|**Answer**|49% \u2717**Incorrect**|53% \u2713**Correct**|\n\n\n\n23\n\n\nPublished as a conference paper at ICLR 2025\n\n\nG ADDITIONAL RETRIEVAL AND GENERATION RESULTS\n\n\nTable 10: Additional retrieval performance in MRR@10.\n\n\nTable 11: Additional generation performance in accuracy (%). All models and methods utilize the\nsame retriever, VisRAG. Performance relative to Oracle is colored in blue.\n\n\n\n\n\n\n|top-6<br>MiniCPM-V 2 .6 t Oo rp a- c1 l0<br>e|6789(955%)<br>64. (91.<br>95 4%)<br>. .<br>7108(100%)<br>.|5714(837%)<br>57. (83.<br>14 7%)<br>68. (10.<br>25 0%)<br>.|7005(", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_24300", "chunk_text": ">7108(100%)<br>.|5714(837%)<br>57. (83.<br>14 7%)<br>68. (10.<br>25 0%)<br>.|7005(841%)<br>54. (65.<br>48 4%)<br>83. (10.<br>25 0%)<br>.|5125(805%)<br>36. (57.<br>49 3%)<br>63. (10.<br>65 0%)<br>.|3581(571%)<br>. .<br>3094(494%)<br>62. (10.<br>69 0%)<br>.|5180(897%)<br>51. (89.<br>80 7%)<br>57. (10.<br>73 0%)<br>.|5566(818%)<br>. .<br>4930(728%)<br>67. (10.<br>78 0%)<br>.|\n|---|---|---|---|---|---|---|---|\n|Qwen2-VL<br>top-1<br>top-2<br>top-3<br>Oracle|66.30 (94.7%)<br>65.44 (93.5%)<br>67.03 (95.8%)<br>69.98 (100%)|53.97 (73.9%)<br>52.38 (71.7%)<br>57.14 (78.3%)<br>73.02 (100%)|65.82 (75.5%)<br>70.90 (81.4%)<br>73.60 (84.5%)<br>87.14 (100%)|55.71 (86.4%)<br>55.15 (85.5%)<br>52.79 (81.9%)<br>64.48 (100%)|51.33 (64.0%)<br>47.05 (58.7%)<br>44.96 (56.1%)<br>80.19 (100%)|55.58 (85.1%)<br>58.99 (90.4%)<br>58.63 (89.8%)", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_24750", "chunk_text": "br>44.96 (56.1%)<br>80.19 (100%)|55.58 (85.1%)<br>58.99 (90.4%)<br>58.63 (89.8%)<br>65.29 (100%)|58.12 (80.0%)<br>58.32 (80.2%)<br>59.03 (81.0%)<br>73.35 (100%)|\n\n\n\nIn this section, we present supplementary evaluation results for both retrieval and generation on our\ndataset.\n\n\nTable 10 shows additional retrieval results obtained by applying reciprocal rank fusion (RRF) (Cormack et al., 2009) to combine the outputs of MiniCPM (OCR) and SigLIP. It is a straightforward\nmethod to integrate textual information extracted from the page with its visual clues. The results\nindicate that fusing text and image modalities provides a meaningful performance boost over individual modality baselines. However, this approach still falls short of the performance achieved\nby our VisRAG-Ret model (71.49 for out-of-domain, 77.91 for in-domain). This underscores the\nsuperior capability of VisRAG-Ret in understanding both modalities within a unified architecture.\n\n\nTable 11 provides additional generation results using top-6 and top-10 retrieved documents from\nVisRAG-Ret. For these experiments, we evaluate the performance of MiniCPM-V 2.0 using the\npage concatenation method and MiniCPM-V 2.6 with direct feeding. We also report the performance of another SOTA VLM, Qwen2-VL-7B-Instruct (Wang et al., 2024). The results indicate\nsignificant performance degradation when handling a larger number of retrieved pages, for both\npage concatenation (MiniCPM-V 2.0) and multi-page input (MiniCPM-V 2.6). MiniCPM-V 2.6\nexhibits greater robustness to increasing context compared to MiniCPM-V 2.0. Open-source VLMs\nstill face challenges in reasoning over multiple pages and extracting relevant information from noisy\nretrieved data. Results for Qwen2-VL demonstrate stronger document understanding capabilities,\n\n- utperforming MiniCPM-V 2.6 in these tasks.\n\n\nH RETRIEVAL EFFICIENCY", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_25200", "chunk_text": " pages and extracting relevant information from noisy\nretrieved data. Results for Qwen2-VL demonstrate stronger document understanding capabilities,\n\n- utperforming MiniCPM-V 2.6 in these tasks.\n\n\nH RETRIEVAL EFFICIENCY\n\n\nIn this experiment, we evaluate the retrieval efficiency of VisRAG-Ret and MiniCPM (OCR) by\nmeasuring two key components: offline document parsing and encoding latency, and online query\nencoding and search latency. Query and document encoding are conducted on an NVIDIA A100\n40G GPU with a batch size of 1, while document parsing is performed on a single core of an Intel\nXeon Platinum 8350C CPU. The reported latencies are averaged over the queries and documents\nfrom the PlotQA dataset. The results are summarized in Table 12.\n\n\nAs shown in the table, although VisRAG-Ret, a VLM-based model, requires more time for document\nencoding compared to MiniCPM (OCR), it bypasses the time-consuming parsing stage required by\n\n\n24\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 12: Retrieval efficiency (ms). We report offline latencies per document, including document\nparsing and encoding latencies, as well as online latencies per query, including query encoding and\nsearch latencies.\n\n|Col1|OfflineLatencyperDocument<br>Parsing Encoding Total|OnlineLatencyperQuery<br>Encoding Search Total|\n|---|---|---|\n|MiniCPM (OCR)<br>VisRAG-Ret|284<br>28<br>312<br>\u2013<br>121<br>121|28<br>26<br>54<br>28<br>26<br>54|\n\n\n\nMiniCPM (OCR). This leads to a 58% reduction in total document processing time for VisRAG-Ret.\nFor online query processing, the latencies of VisRAG-Ret and MiniCPM (OCR) are nearly identical,\nas the queries consist solely of textual inputs.\n\n\nI RETRIEVAL PERFORMANCE ON TEXT RETRIEVAL BENCHMARKS\n\n\nTable 13: Retrieval performance on subsets of the text retrieval benchmark BEIR (Thakur et al.,\n2021) in NDCG@10. VisRAG-Ret performs retrieval on rendered document screenshots.\n\n\n**Model** **SciFact** **NFCorpus** **Scidocs**\n\n\nMiniCP", "token_count": 500, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2410.10594_vision_rag_yu:chunk_25650", "chunk_text": " (Thakur et al.,\n2021) in NDCG@10. VisRAG-Ret performs retrieval on rendered document screenshots.\n\n\n**Model** **SciFact** **NFCorpus** **Scidocs**\n\n\nMiniCPM (OCR) 61.04 14.12 13.01\nVisRAG-Ret 62.47 27.02 16.25\n\n\nTo evaluate how VisRAG-Ret performs in retrieval scenarios involving only textual data, we conduct\nan experiment using the BEIR (Thakur et al., 2021) text retrieval benchmark. To evaluate VisRAGRet, we convert the document texts into rendered screenshots and apply VisRAG-Ret to this modified\ndataset. We use the Pillow [3] library to convert text documents into screenshots, setting a width of\n800px, a font size of 24px, and the DejaVuSans font. The height of each screenshot varies depending\n\n- n the document length, with a margin of 20px and a line spacing of 4px. For comparison, we\ninclude MiniCPM (OCR) in the evaluation, utilizing raw textual data directly available in BEIR.\nNote that the term \u201cOCR\u201d in MiniCPM (OCR) is used solely for naming consistency.\n\n\nAs shown in Table 13, VisRAG-Ret, relying only on the rendered screenshots, significantly outperforms MiniCPM (OCR) which uses textual information. This result highlights that VisRAG-Ret\u2019s\npooling-based representation effectively captures textual details and is well-suited for text-heavy\ndocument retrieval.\n\n\n3https://python-pillow.org/\n\n\n25\n\n\n", "token_count": 346, "metadata": {"arxiv_id": "2410.10594", "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents", "authors": ["Shi Yu", "Chaoyue Tang", "Bokai Xu", "Junbo Cui", "Junhao Ran", "Yukun Yan", "Zhenghao Liu", "Shuo Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "year": 2024, "url": "https://arxiv.org/pdf/2410.10594v2"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_0", "chunk_text": "# **On the Theoretical Limitations of** **Embedding-Based Retrieval**\n\n**Orion Weller** [*,1,2] **, Michael Boratko** [1] **, Iftekhar Naim** [1] **and Jinhyuk Lee** [1]\n\n1Google DeepMind, 2Johns Hopkins University\n\n\n**Vector embeddings have been tasked with an ever-increasing set of retrieval tasks over the years, with a**\n**nascent rise in using them for reasoning, instruction-following, coding, and more. These new benchmarks**\n**push embeddings to work for** _**any query**_ **and** _**any notion of relevance**_ **that could be given. While prior**\n**works have pointed out theoretical limitations of vector embeddings, there is a common assumption**\n**that these difficulties are exclusively due to unrealistic queries, and those that are not can be overcome**\n\n**with better training data and larger models. In this work, we demonstrate that we may encounter these**\n\n**theoretical limitations in realistic settings with extremely simple queries. We connect known results**\n**in learning theory, showing that the number of top-** _\ud835\udc58_ **subsets of documents capable of being returned**\n**as the result of some query is limited by the dimension of the embedding. We empirically show that**\n**this holds true even if we restrict to** _\ud835\udc58_ = 2 **, and directly optimize on the test set with free parameterized**\n\n**embeddings. We then create a realistic dataset called LIMIT that stress tests models based on these**\n**theoretical results, and observe that even state-of-the-art models fail on this dataset despite the simple**\n**nature of the task. Our work shows the limits of embedding models under the existing single vector**\n**paradigm and calls for future research to develop methods that can resolve this fundamental limitation.**\n\n##### **1. Introduction**\n\n\nOver the last two decades, information retrieval (IR) has moved from models dominated by sparse\ntechniques (such as BM25 [Robertson et al., 1995]) to those that use neural language models (LM)\nas their backbones [Lee et al., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\nThese neural models are predominantly used in a single vector capacity, where they output a single", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_450", "chunk_text": "., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\nThese neural models are predominantly used in a single vector capacity, where they output a single\n_embedding_ representing the entire input (also known as _dense retrieval_ ). These embedding models\nare capable of generalizing to new retrieval datasets and have been tasked with solving increasingly\ncomplicated retrieval problems [Thakur et al., 2021, Enevoldsen et al., 2025, Lee et al., 2025].\n\n\nIn recent years this has been pushed even further with the rise of instruction-following retrieval\nbenchmarks, where models are asked to represent **any relevance definition** for **any query** [Weller\net al., 2025a,b, Song et al., 2025, Xiao et al., 2024, Su et al., 2024]. For example, the QUEST dataset\n\n[Malaviya et al., 2023] uses logical operators to combine different concepts, studying the difficulty\n\n- f retrieval for complex queries (e.g., \u201cMoths or Insects or Arthropods of Guadeloupe\u201d). On the\n\n- ther hand, datasets like BRIGHT [Su et al., 2024] explore the challenges stemming from different\ndefinitions of relevance by defining relevance in ways that require reasoning. One subtask includes\nreasoning over a given Leetcode problem (the query) to find other Leetcode problems that share a\nsub-task (e.g. others problems using dynamic programming). Although models cannot solve these\nbenchmarks yet, the community has proposed these problems in order to push the boundaries of\nwhat dense retrievers are capable of\u2014which is now implicitly _every task_ that could be defined.\n\n\nRather than proposing empirical benchmarks to gauge what embedding models can achieve, we\nseek to understand at a more fundamental level what the limitations are. Since embedding models use\n\n\n\u2217Work done during internship at GDM.\nData and code are available at `[https://github.com/google-deepmind/limit](https://github.com/google-deepmind/limit)`\n\n\n\u00a9 2025 Google DeepMind. All rights reserved\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n##### Query-Document Relevance LIMIT : A real-world instantiation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeslie Laham likes Apples and Candy.\n\n\n\n\n\n\n\n\n|Col1", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_900", "chunk_text": "2025 Google DeepMind. All rights reserved\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n##### Query-Document Relevance LIMIT : A real-world instantiation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeslie Laham likes Apples and Candy.\n\n\n\n\n\n\n\n\n|Col1|Col2|\n|---|---|\n|_Quokkas_|_Quokkas_|\n|_Apples_|_Apples_|\n|||\n\n\n\n\n### \u2026\n\nFigure 1 | A depiction of the LIMIT dataset creation process, based on theoretical limitations. We test\n**all combinations** - f relevance for _\ud835\udc41_ documents (i.e. in the figure, all combinations of relevance for\nthree documents with two relevant documents per query) and instantiate it using a simple mapping.\n**Despite this simplicity, SoTA MTEB models perform poorly, scoring less than 20 recall@100.**\n\n\nvector representations in geometric space, there exists well-studied fields of mathematical research\n\n[Papadimitriou and Sipser, 1982] that could be used to analyze these representations.\n\n\nOur work aims to bridge this gap, connecting known theoretical results in geometric algebra\nwith modern advances in neural information retrieval. We draw upon research in communication\ncomplexity theory to provide a lower bound on the embedding dimension needed to represent a given\ncombination of relevant documents and queries. Specifically, we show that for a given embedding\ndimension _\ud835\udc51_ **there exists top-** _\ud835\udc58_ **combinations of documents that cannot be returned** - no matter\nthe query\u2014highlighting a theoretical and fundamental limit to embedding models.\n\n\nTo show that this theoretical limit is true for any retrieval model or training dataset, we test a\nsetting where the vectors themselves are directly optimized with the test data. This allows us to\nempirically show how the embedding dimension enables the solving of retrieval tasks. We find there\nexists a crucial point for each embedding dimension ( _\ud835\udc51_ ) where the number of documents is too large\nfor the embedding dimension to encode all combinations. We then gather these crucial points for a\nvariety of _\ud835\udc51_ and show that this relationship can be modeled empirically with a polynomial function.\n\n\nWe also go one step further and construct a realistic but simple dataset based on these theoretical limitations (called LIMIT). Despite the simplicity of the task (e.g., `who likes Apples?` and\n`Jon likes Apples, ...`", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_1350", "chunk_text": ".\n\n\nWe also go one step further and construct a realistic but simple dataset based on these theoretical limitations (called LIMIT). Despite the simplicity of the task (e.g., `who likes Apples?` and\n`Jon likes Apples, ...` ), we find it is very difficult for even state-of-the-art embedding models [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\nunderpinnings, and impossible [1] for models with small embedding dimensions.\n\n\nOverall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\nmodels, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\n(by free embedding optimization), and (3) a simple real-world natural language instantiation called\nLIMIT that even state-of-the-art embedding models cannot solve.\n\n\nThese results imply interesting findings for the community: on one hand we see neural embedding\nmodels becoming immensely successful. However, academic benchmarks test only a small amount of\nthe queries that could be issued (and these queries are often overfitted to), hiding these limitations.\nOur work shows that as the tasks given to embedding models require returning ever-increasing\ncombinations of top- _\ud835\udc58_ relevant documents (e.g., through instructions connecting previously unrelated\n\n\n1At least with current optimization techniques for retrieval.\n\n\n2\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\ndocuments with logical operators), we will reach a limit of combinations they cannot represent.\n\n\nThus, the community should be aware of these limitations, both when designing evaluations (as\nLIMIT shows) and by choosing alternative retrieval approaches \u2013 such as cross-encoders or multi-vector\nmodels \u2013 when attempting to create models that can handle the full range of instruction-based queries,\ni.e. _any query and relevance definition_ .\n\n##### **2. Related Work**\n\n\n**2.1. Neural Embedding Models**\n\n\nThere has been immense progress on embedding models in recent years [Lee et al., 2019, Craswell\net al., 2020, BehnamGhader et al., 2024], moving from simple web search (text-only) to advanced\ninstruction-following and multi-modal representations. These models generally followed advances in\nlanguage models, such as pre-trained LMs [Hoffmann et al., 2022], multi", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_1800", "chunk_text": "4], moving from simple web search (text-only) to advanced\ninstruction-following and multi-modal representations. These models generally followed advances in\nlanguage models, such as pre-trained LMs [Hoffmann et al., 2022], multi-modal LMs [Li et al., 2024,\nTeam, 2024], and advances in instruction-following [Zhou et al., 2023, Ouyang et al., 2022]. Some\n\n- f the prominent examples in retrieval include CoPali [Faysse et al., 2024] and DSE [Ma et al., 2024]\nwhich focus on multimodal embeddings, Instructor [Su et al., 2022] and FollowIR [Weller et al.,\n2024a] for instruction following, and GritLM [Muennighoff et al., 2024] and Gemini Embeddings\n\n[Lee et al., 2025] for pre-trained LMs turned embedders.\n\n\nOur work, though focused solely on textual representations for simplicity, **applies to all modalities**\n\n**of single vector embeddings for any domain of dataset** . As the space of things to represent grows\n(through instructions or multi-modality) they will increasingly run into these theoretical limitations.\n\n\n**2.2. Empirical tasks pushing the limits of dense retrieval**\n\n\nRetrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\nNotable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\n2024], a diverse set of instructions [Weller et al., 2024a, Zhou et al., 2024, Oh et al., 2024], and to\nhandle reasoning over the queries [Xiao et al., 2024, Su et al., 2024]. This has pushed the focus of\nembedding models from basic keyword matching to embeddings that can represent the full semantic\nmeaning of language. As such, it is more common than ever to connect what were previously unrelated\ndocuments into the top- _\ud835\udc58_ relevant set, [2] increasing the number of combinations that models must be\nable to represent. This has motivated our interest in understanding the limits of what embeddings\ncan represent, as current work expects it to handle _every_ task.\n\n\nPrevious work has explored empirically the limits of models: Reimers and G", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_2250", "chunk_text": " must be\nable to represent. This has motivated our interest in understanding the limits of what embeddings\ncan represent, as current work expects it to handle _every_ task.\n\n\nPrevious work has explored empirically the limits of models: Reimers and Gurevych [2020] showed\nthat smaller dimension embedding models have more false positives, especially with larger-scale\ncorpora. Ormazabal et al. [2019] showed the empirical limitations of models in the cross-lingual\nsetting and Yin and Shen [2018] showed how embedding dimensions relate to the bias-variance\ntradeoff. In contrast, our work provides a theoretical connection between the embedding dimension\nand the sign-rank of the query relevance ( _qrel_ ) matrix, while also showing empirical limitations.\n\n\n**2.3. Theoretical Limits of Vectors in Geometric Space**\n\n\nUnderstanding and finding nearest neighbors in semantic space has a long history in mathematics\nresearch, with early work such as the Voronoi diagram being studied as far back as 1644 and formalized\nin 1908 [Voronoi, 1908]. The order-k version of the Voronoi diagram (i.e. the Voronoi diagram\n\n\n2You can imagine an easy way to connect any two documents merely by using logical operators, i.e. X and Y.\n\n\n3\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\ndepicting the set of closest k points) is an obvious analog to information retrieval and has been studied\nfor many years [Clarkson, 1988]. However, proofs placing a bound on the count of the number of\nregions in the order-k Voronoi problem are notoriously different to bound tightly and do not provide\nmuch practical insight for IR [Bohler et al., 2015, Lee, 1982, Chen et al., 2023].\n\n\nWe approach this problem from another angle by proving that the set of the constraints implied\nby the top- _\ud835\udc58_ retrieval problem can be formalized to show that it places a lower bound on the\ndimensionality of the embedding needed to represent it. We then show that this dimensionality can be\nmuch larger than the dimensionality of embedding models for practical IR problems. This approach\nrelies on previous work in the communication complexity theory community to place bounds using\nthe sign-rank of a matrix. Due to the difficulty of of computing", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_2700", "chunk_text": " be\nmuch larger than the dimensionality of embedding models for practical IR problems. This approach\nrelies on previous work in the communication complexity theory community to place bounds using\nthe sign-rank of a matrix. Due to the difficulty of of computing the sign-rank, we rely on known\nprevious work that has already proven the sign-rank of known matrices [Hatami et al., 2022, Alon\net al., 2014, Chierichetti et al., 2017, Chattopadhyay and Mande, 2018, Hatami and Hatami, 2024].\nOur results also provide a proof of a method that can place a lower bound on the sign rank through\nwhat we call _free embeddings_ in \u00a74 (i.e. if it can be solved, then the dimension _\ud835\udc51_ is \u2264 to the sign rank).\n\n##### **3. Representational Capacity of Vector Embeddings**\n\n\nIn this section we prove the implication of known results from communication complexity theory to\nthe setting of vector embeddings.\n\n\n**3.1. Formalization**\n\n\nWe consider a set of _\ud835\udc5a_ queries and _\ud835\udc5b_ documents with a ground-truth relevance matrix _\ud835\udc34_ \u2208{0 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_,\nwhere _\ud835\udc34\ud835\udc56\ud835\udc57_ = 1 if and only if document _\ud835\udc57_ is relevant to query _\ud835\udc56_ . [3] Vector embedding models map each\nquery to a vector _\ud835\udc62\ud835\udc56_ \u2208 \u211d _[\ud835\udc51]_ and each document to a vector _\ud835\udc63\ud835\udc57_ \u2208 \u211d _[\ud835\udc51]_ . Relevance is modeled by the dot\nproduct _\ud835\udc62_ _[\ud835\udc47]_ _\ud835\udc56_ _[\ud835\udc63][\ud835\udc57]_ [, with the goal that relevant documents should score higher than irrelevant ones.]\n\n\nConcatenating the vectors for queries in a matrix _\ud835\udc48_ \u2208 \u211d _[\ud835\udc51]_ [\u00d7] _[\ud835\udc5a]_ and those for documents in a matrix\n_\ud835\udc49_ \u2208 \u211d _[\ud835\udc51]_ [\u00d7] _[\ud835\udc5b]_, these dot products are the entries of the score matrix _", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_3150", "chunk_text": " _[\ud835\udc5a]_ and those for documents in a matrix\n_\ud835\udc49_ \u2208 \u211d _[\ud835\udc51]_ [\u00d7] _[\ud835\udc5b]_, these dot products are the entries of the score matrix _\ud835\udc35_ = _\ud835\udc48_ _[\ud835\udc47]_ _\ud835\udc49_ . The smallest embedding\ndimension _\ud835\udc51_ that can realize a given score matrix is, by definition, the rank of _\ud835\udc35_ . Therefore, our\ngoal is equivalent to finding the minimum rank of a score matrix _\ud835\udc35_ that correctly orders documents\naccording to the relevance specified in _\ud835\udc34_, which we formalize in the following definition.\n\n\n**Definition 1.** Given a matrix _\ud835\udc34_ \u2208 \u211d _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_, the **row-wise order-preserving rank of** _\ud835\udc34_ is the smallest\ninteger _\ud835\udc51_ such that there exists a rank- _\ud835\udc51_ matrix _\ud835\udc35_ that preserves the relative order of entries in each\nrow of _\ud835\udc34_ . We denote this as\n\n\nrankrop _\ud835\udc34_ = min{rank _\ud835\udc35_ | _\ud835\udc35_ \u2208 \u211d _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ _,_ such that for all _\ud835\udc56, \ud835\udc57, \ud835\udc58,_ if _\ud835\udc34\ud835\udc56\ud835\udc57_ _> \ud835\udc34\ud835\udc56\ud835\udc58_ then _\ud835\udc35\ud835\udc56\ud835\udc57_ _> \ud835\udc35\ud835\udc56\ud835\udc58_ } _._\n\n\nIn other words, if _\ud835\udc34_ is a binary ground-truth relevance matrix, rankrop _\ud835\udc34_ is the minimum dimension\nnecessary for any vector embedding model to return relevant documents before irrelevant ones for\nall queries. Alternatively, we might require that the scores of relevant documents can be cleanly\nseparated from those of irrelevant ones by a threshold.\n\n\n**Definition 2.** Given a binary matrix _\ud835\udc34_ \u2208{0 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ :\n\n\n  - The **row-wise thresholdable rank of** _\ud835\udc34_ (rankrt _\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_3600", "chunk_text": "_ \u2208{0 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ :\n\n\n  - The **row-wise thresholdable rank of** _\ud835\udc34_ (rankrt _\ud835\udc34_ ) is the minimum rank of a matrix _\ud835\udc35_ for which\nthere exist row-specific thresholds { _\ud835\udf0f\ud835\udc56_ } _[\ud835\udc5a]_ _\ud835\udc56_ =1 [such that for all] _[ \ud835\udc56, \ud835\udc57]_ [,] _[ \ud835\udc35][\ud835\udc56\ud835\udc57]_ _[> \ud835\udf0f][\ud835\udc56]_ [if] _[ \ud835\udc34][\ud835\udc56\ud835\udc57]_ [=][ 1 and] _[ \ud835\udc35][\ud835\udc56\ud835\udc57]_ _[< \ud835\udf0f][\ud835\udc56]_ [if]\n_\ud835\udc34_ = 0.\n_\ud835\udc56\ud835\udc57_\n\n\n3The matrix _\ud835\udc34_ is often called the \u201cqrels\u201d (query relevance judgments) matrix in information retrieval.\n\n\n4\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n  - The **globally thresholdable rank of** _\ud835\udc34_ (rankgt _\ud835\udc34_ ) is the minimum rank of a matrix _\ud835\udc35_ for which\nthere exists a single threshold _\ud835\udf0f_ such that for all _\ud835\udc56, \ud835\udc57_, _\ud835\udc35\ud835\udc56\ud835\udc57_ _> \ud835\udf0f_ if _\ud835\udc34\ud835\udc56\ud835\udc57_ = 1 and _\ud835\udc35\ud835\udc56\ud835\udc57_ _< \ud835\udf0f_ if _\ud835\udc34\ud835\udc56\ud835\udc57_ = 0.\n\n\n**Remark 1.** This two-sided separation condition may be seen as slightly stronger than requiring,\n_\ud835\udc35\ud835\udc56\ud835\udc57_ _> \ud835\udf0f\ud835\udc56_ if and only if _\ud835\udc34\ud835\udc56\ud835\udc57_ = 1, however since there are only finitely many elements of _\ud835\udc35\ud835\udc56\ud835\udc57_ we could\nalways perturb the latter threshold by a sufficient number such that the two-sided condition holds. [4]\n\n\n**3.2. Theoretical Bounds**\n\n\nFor binary matrices", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_4050", "chunk_text": " elements of _\ud835\udc35\ud835\udc56\ud835\udc57_ we could\nalways perturb the latter threshold by a sufficient number such that the two-sided condition holds. [4]\n\n\n**3.2. Theoretical Bounds**\n\n\nFor binary matrices, row-wise ordering and row-wise thresholding are equivalent notions of representational capacity.\n\n\n**Proposition 1.** _For a binary matrix \ud835\udc34_ \u2208{0 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ _, we have that_ rank _rop \ud835\udc34_ = rank _rt \ud835\udc34._\n\n\n_Proof._ (\u2264) Suppose _\ud835\udc35_ and _\ud835\udf0f_ satisfy the row-wise thresholdable rank condition. Since _\ud835\udc34_ is a binary\nmatrix _\ud835\udc34\ud835\udc56\ud835\udc57_ _> \ud835\udc34\ud835\udc56\ud835\udc58_ implies _\ud835\udc34\ud835\udc56\ud835\udc57_ = 1 and _\ud835\udc34\ud835\udc56\ud835\udc58_ = 0, thus _\ud835\udc35\ud835\udc56\ud835\udc57_ _> \ud835\udf0f\ud835\udc56_ _> \ud835\udc35\ud835\udc56\ud835\udc58_, and hence _\ud835\udc35_ also satisfies the row-wise\n\n- rder-preserving condition.\n\n\n(\u2265) Let _\ud835\udc35_ satisfy the row-wise order-preserving condition, so _\ud835\udc34\ud835\udc56\ud835\udc57_ _> \ud835\udc34\ud835\udc56\ud835\udc58_ implies _\ud835\udc35\ud835\udc56\ud835\udc57_ _> \ud835\udc35\ud835\udc56\ud835\udc58_ . For each\nrow _\ud835\udc56_, let _\ud835\udc48\ud835\udc56_ = { _\ud835\udc35\ud835\udc56\ud835\udc57_ | _\ud835\udc34\ud835\udc56\ud835\udc57_ = 1} and _\ud835\udc3f\ud835\udc56_ = { _\ud835\udc35\ud835\udc56\ud835\udc57_ | _\ud835\udc34\ud835\udc56\ud835\udc57_ = 0}. The row-wise order-preserving condition implies\nthat every element of _\ud835\udc48\ud835\udc56_ is greater than every element of _\ud835\udc3f\ud835\udc56_ . We can therefore always find a threshold\n_\ud835\udf0f\ud835\udc56_ separating them ( _e.g_ . _\ud835\udf0f\ud835\udc56_ = (max _", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_4500", "chunk_text": " greater than every element of _\ud835\udc3f\ud835\udc56_ . We can therefore always find a threshold\n_\ud835\udf0f\ud835\udc56_ separating them ( _e.g_ . _\ud835\udf0f\ud835\udc56_ = (max _\ud835\udc3f\ud835\udc56_ + min _\ud835\udc48\ud835\udc56_ )/2 if both are non-empty, trivial otherwise). Thus _\ud835\udc35_ is\nalso row-wise thresholdable to _\ud835\udc34_ .\n\n\nThe notions we have described so far are closely related to the sign rank of a matrix, which we\nuse in the rest of the paper to establish our main bounds.\n\n\n**Definition 3** (Sign Rank) **.** The sign rank of a matrix _\ud835\udc40_ \u2208{\u22121 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ is the smallest integer _\ud835\udc51_ such\nthat there exists a rank _\ud835\udc51_ matrix _\ud835\udc35_ \u2208 \u211d _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ whose entries have the same sign as those of _\ud835\udc40_, i.e.\n\n\nrank\u00b1 _\ud835\udc40_ = min{rank _\ud835\udc35_ | _\ud835\udc35_ \u2208 \u211d _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ such that for all _\ud835\udc56, \ud835\udc57_ we have sign _\ud835\udc35\ud835\udc56\ud835\udc57_ = _\ud835\udc40\ud835\udc56\ud835\udc57_ } _._\n\n\nIn what follows, we use **1** _\ud835\udc5b_ to denote the _\ud835\udc5b_  - dimensional vector of ones, and **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ to denote an\n_\ud835\udc5a_ \u00d7 _\ud835\udc5b_ matrix of ones.\n\n\n**Proposition 2.** _Let \ud835\udc34_ \u2208{0 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ _be a binary matrix. Then_ 2 _\ud835\udc34_ - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ \u2208{\u22121 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ _, and we have_\n\n\nrank\u00b1(2 _\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_4950", "chunk_text": " _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ \u2208{\u22121 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ _, and we have_\n\n\nrank\u00b1(2 _\ud835\udc34_      - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ ) \u2212 1 \u2264 rank _rop \ud835\udc34_ = rank _rt \ud835\udc34_ \u2264 rank _gt \ud835\udc34_ \u2264 rank\u00b1(2 _\ud835\udc34_      - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ )\n\n\n_Proof._ N.b. the equality was already established in Proposition 1. We prove each inequality separately.\n\n\n**1.** rank **rt** _\ud835\udc34_ \u2264 rank **gt** _\ud835\udc34_ **:** True by definition, since any matrix satisfying the globally thresholdable\ncondition trivially satisfies a row-wise thresholdable condition with the same threshold for each row.\n\n\n**2.** rank **gt** _\ud835\udc34_ \u2264 rank\u00b1(2 _\ud835\udc34_   - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ ) **:** Let _\ud835\udc35_ be any matrix whose entries have the same sign as 2 _\ud835\udc34_   - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_\n\n\n_\ud835\udc35_ _>_ 0 \u21d0\u21d2 2 _\ud835\udc34_             - 1 _>_ 0 \u21d0\u21d2 _\ud835\udc34_ = 1 _._\n_\ud835\udc56\ud835\udc57_ _\ud835\udc56\ud835\udc57_ _\ud835\udc56\ud835\udc57_\n\n\nThus _\ud835\udc35_ satisfies the globally thresholdable condition with a threshold of 0.\n\n\n4i.e. without loss of generality, we may assume the thresholds in the above definitions are not equal to any elements of\n_\ud835\udc35_ since we could increase the threshold of _\ud835\udf0f_ by a sufficiently _\ud835\udf16_ to preserve the inequality.\n\n\n5\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n**3.** rank\u00b1(2 _\ud835\udc34_   - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ ) \u2212 1 \u2264 rank **rt** _\ud835\udc34_ **:** Suppose _\ud835\udc35_ satisfies the row-wise thresholding", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_5400", "chunk_text": "2 _\ud835\udc34_   - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ ) \u2212 1 \u2264 rank **rt** _\ud835\udc34_ **:** Suppose _\ud835\udc35_ satisfies the row-wise thresholding condition with\nminimal rank, so rankrt _\ud835\udc34_ = rank _\ud835\udc35_ and there exists _\ud835\udf0f_ \u2208 \u211d _[\ud835\udc5a]_ such that _\ud835\udc35\ud835\udc56\ud835\udc57_ _> \ud835\udf0f\ud835\udc56_ if _\ud835\udc34\ud835\udc56\ud835\udc57_ = 1 and _\ud835\udc35\ud835\udc56\ud835\udc57_ _< \ud835\udf0f\ud835\udc56_ if\n_\ud835\udc34\ud835\udc56\ud835\udc57_ = 0. Then the entries of _\ud835\udc35_ - _\ud835\udf0f_ **1** _[\ud835\udc47]_ _\ud835\udc5b_ [have the same sign as 2] _[\ud835\udc34]_ [\u2212] **[1]** _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ [, since][ (] _[\ud835\udc35]_ [\u2212] _[\ud835\udf0f]_ **[1]** _[\ud835\udc47]_ _\ud835\udc5b_ [)] _[\ud835\udc56\ud835\udc57]_ [=] _[ \ud835\udc35][\ud835\udc56\ud835\udc57]_ [\u2212] _[\ud835\udf0f][\ud835\udc56]_ [and]\n\n\n_\ud835\udc35\ud835\udc56\ud835\udc57_                 - _\ud835\udf0f\ud835\udc56_ _>_ 0 \u21d0\u21d2 _\ud835\udc34\ud835\udc56\ud835\udc57_ = 1 \u21d0\u21d2 2 _\ud835\udc34\ud835\udc56\ud835\udc57_                 - 1 _>_ 0 _,_ and (1)\n\n_\ud835\udc35\ud835\udc56\ud835\udc57_                 - _\ud835\udf0f\ud835\udc56_ _<_ 0 \u21d0\u21d2 _\ud835\udc34\ud835\udc56\ud835\udc57_ = 0 \u21d0\u21d2 2 _\ud835\udc34\ud835\udc56\ud835\udc57_                 - 1 _<_ 0 _._ (2)\n\n\nThus rank\u00b1(2 _\ud835\udc34_ - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ ) \u2264 rank( _\ud835\udc35", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_5850", "chunk_text": "\ud835\udc57_                 - 1 _<_ 0 _._ (2)\n\n\nThus rank\u00b1(2 _\ud835\udc34_ - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ ) \u2264 rank( _\ud835\udc35_ - _\ud835\udf0f_ **1** _[\ud835\udc47]_ _\ud835\udc5b_ [) \u2264] [rank][(] _[\ud835\udc35]_ [) +][ rank][(] _[\ud835\udf0f]_ **[1]** _[\ud835\udc47]_ _\ud835\udc5b_ [)][ =][ rank][rt] _[ \ud835\udc34]_ [+][ 1.]\n\n\nCombining these gives the desired chain of inequalities.\n\n\n**3.3. Consequences**\n\n\nIn the context of a vector embedding model, this provides a lower and upper bound on the dimension\n\n- f vectors required to exactly capture a given set of retrieval objectives, in the sense of row-wise\n\n- rdering, row-wise thresholding, or global thresholding. In particular, given some binary relevance\nmatrix _\ud835\udc34_ \u2208{0 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_, we need at least rank\u00b1(2 _\ud835\udc34_ - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ ) \u2212 1 dimensions to capture the relationships\nin _\ud835\udc34_ exactly, and can always accomplish this in at most rank\u00b1(2 _\ud835\udc34_ - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ ) dimensions.\n\n\nPractically, this means:\n\n\n1. For any fixed dimension _\ud835\udc51_, there exists a binary relevance matrix which cannot be captured\nvia _\ud835\udc51_    - dimensional embeddings (as there are matrices with arbitrarily high sign-rank). In other\nwords, retrieval tasks whose **qrel matrices have higher sign-rank are more difficult** to capture\nexactly for embedding models, requiring higher embedding dimensions.\n2. If we are able to embed a given matrix _\ud835\udc34_ \u2208{0 _,_ 1} _[\ud835\udc5a]_ [\u00d7] _[\ud835\udc5b]_ in a row-wise order-preserving manner in\n_\ud835\udc51_ dimensions, this implies a bound on the sign rank of 2 _\ud835\udc34_   - **1** _\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_6300", "chunk_text": " [\u00d7] _[\ud835\udc5b]_ in a row-wise order-preserving manner in\n_\ud835\udc51_ dimensions, this implies a bound on the sign rank of 2 _\ud835\udc34_   - **1** _\ud835\udc5a_ \u00d7 _\ud835\udc5b_ . In particular, this suggests\na _practical mechanism_ for determining an upper-bound on sign-rank for matrices via gradient\ndescent optimization of free embedding representations.\n\n##### **4. Empirical Connection: Best Case Optimization**\n\n\nWe have now established a theoretical limitation of embedding models based on the sign-rank of the\nqrel matrix and their embedding dimension _\ud835\udc51_ . Now we seek to show that this empirically as well.\n\n\nTo show the strongest optimization case possible, we design experiments where the vectors\nthemselves are directly optimizable with gradient descent. [5] We call this \u201cfree embedding\u201d optimization,\nas the embeddings are free to be optimized and not constrained by natural language, which imposes\nconstraints on any realistic embedding model. Thus, this shows whether it is feasible for **any**\n**embedding model** to solve this problem: if the free embedding optimization cannot solve the\nproblem, real retrieval models will not be able to either. It is also worth noting that we do this by\ndirectly optimizing the embeddings over the target qrel matrix (test set). This will not generalize to a\nnew dataset, but is done to show the highest performance that could possibly occur.\n\n\n**Experimental Settings** We create a random document matrix (size _\ud835\udc5b_ ) and a random query matrix\nwith top- _\ud835\udc58_ sets (of all combinations, i.e. size _\ud835\udc5a_ = [\ufffd] _[\ud835\udc5b]_ _\ud835\udc58_ - ), both with unit vectors. We then directly optimize\nfor solving the constraints with the Adam optimizer [Kingma and Ba, 2014]. [6] Each gradient update\nis a full pass through all correct triples (i.e. full dataset batch-size) with the InfoNCE loss function\n\n\n5This could also be viewed as an embedding model where each query/doc are a separate vector via a lookup table.\n6We found similar results with SGD, but we use Adam for speed and similarity with existing training methods.\n\n\n6\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n[Oord et al., 2018], [", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_6750", "chunk_text": " lookup table.\n6We found similar results with SGD, but we use Adam for speed and similarity with existing training methods.\n\n\n6\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n[Oord et al., 2018], [7] with all other documents as in-batch negatives (i.e. full dataset in batch). As\nnearly all embedding models use normalized vectors, we do also (normalizing after updates). We\nperform early stopping when there is no improvement in the loss for 1000 iterations. We gradually\nincrease the number of documents (and thus the binomial amount of queries) until the optimization\nis no longer able to solve the problem (i.e. achieve 100% accuracy). We call this the _critical-n_ point.\n\n\nWe focus on relatively small sizes for _\ud835\udc5b_, _\ud835\udc58_, and _\ud835\udc51_ due to the combinatorial explosion of combinations\nwith larger document values (i.e. 50k docs with top- _\ud835\udc58_ - f 100 gives 7.7e+311 combinations, which\nwould be equivalent to the number of query vectors of dimension _\ud835\udc51_ in that free embedding experiment).\nWe use _\ud835\udc58_ = 2 and increase _\ud835\udc5b_ by one for each _\ud835\udc51_ value until it breaks. We fit a polynomial regression\nline to the data so we can model and extrapolate results outwards.\n\n\n\n**Results** Figure 2 shows that the curve fits\na 3rd degree polynomial curve, with formula\n_\ud835\udc66_ = \u221210 _._ 5322 + 4 _._ 0309 _\ud835\udc51_ + 0 _._ 0520 _\ud835\udc51_ [2] + 0 _._ 0037 _\ud835\udc51_ [3]\n\n( _\ud835\udc5f_ [2] =0.999). Extrapolating this curve outward\ngives the critical-n values (for embedding size):\n500k (512), 1.7m (768), 4m (1024), 107m\n(3072), 250m (4096). We note that this is the\nbest case: a real embedding model cannot directly optimize the query and document vectors\nto match the test qrel matrix (and is constrained\nby factors such as \u201cmodeling natural language\u201d).\nHowever, these numbers", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_7200", "chunk_text": " We note that this is the\nbest case: a real embedding model cannot directly optimize the query and document vectors\nto match the test qrel matrix (and is constrained\nby factors such as \u201cmodeling natural language\u201d).\nHowever, these numbers already show that for\nweb-scale search, even the largest embedding\ndimensions with ideal test-set optimization are\nnot enough to model all combinations.\n\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n|<br>|<br>|~~ritical Po~~<br>Regression|~~ nts~~<br> (Degree 3)||\n|<br>|<br>||||\n||||||\n||||||\n||||||\n\n\nFigure 2 | The critical-n value where the\ndimensionality is too small to successfully\nrepresent all the top-2 combinations. We plot the\ntrend line as a polynomial function.\n\n\n##### **5. Empirical Connection: Real-World Datasets**\n\nThe free embedding experiments provide empirical evidence that our theoretical results hold true.\nHowever, they still are abstract - what does this mean for real embedding models? In this section\nwe (1) draw connections from this theory to existing datasets and (2) create an trivially simple yet\nextremely difficult retrieval task for existing SOTA models.\n\n\n**5.1. Connection to Existing Datasets**\n\n\nExisting retrieval datasets typically use a static evaluation set with limited numbers of queries, as\nrelevance annotation is expensive to do for each query. This means practically that the space of\nqueries used for evaluation is a very small sample of the number of potential queries. For example, the\nQUEST dataset [Malaviya et al., 2023] has 325k documents and queries with 20 relevant documents\nper query, with a total of 3357 queries. The number of unique top-20 document sets that could\nbe returned with the QUEST corpus would be [\ufffd][325] 20 _[\ud835\udc58]_ - which is equal to 7.1e+91 (larger than the\nestimate of atoms in the observable universe, 10 [82] ). Thus, the 3k queries in QUEST can only cover an\ninfinitesimally small part of the qrel combination space.\n\n\n7In preliminary experiments, we found that InfoNCE performed best, beating MSE and Margin. As we are directly\n\n\n\n_", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_7650", "chunk_text": " 3k queries in QUEST can only cover an\ninfinitesimally small part of the qrel combination space.\n\n\n7In preliminary experiments, we found that InfoNCE performed best, beating MSE and Margin. As we are directly\n\n\n\n_\ud835\udc40_\n\n- ptimizing the vectors with full-dataset batches, this is Ltotal = \u2212 _\ud835\udc40_ [1] - _\ud835\udc56_ =1 [log]\n\n\ndocuments for query _\ud835\udc5e\ud835\udc56_ and _\ud835\udc51\ud835\udc58_ are the non-relevant documents.\n\n\n\n\n- _\ud835\udc51\ud835\udc5f_ \u2208 _\ud835\udc45\ud835\udc56_ [exp][(][sim][(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51][\ud835\udc5f]_ [)/] _[\ud835\udf0f]_ [)]\n\n~~\ufffd~~ _\ud835\udc51\ud835\udc58_ \u2208 _\ud835\udc37_ [exp][(][sim][(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc58_ [)/] _[\ud835\udf0f]_ [)] [where] _[ \ud835\udc51][\ud835\udc5f]_ [is the relevant]\n\n\n7\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\nAlthough it not possible to instantiate all combinations when using large-scale corpora, search\nevaluation datasets are a proxy for what any user would ask for and ideally would be designed to test\nmany combinations, as users will do. In many cases, developers of new evaluations simply choose\nto use fewer queries due to cost or computational expense of evaluation. For example, QUEST\u2019s\nquery \u201cNovels from 1849 or George Sand novels\u201d combines two categories of novels with the \u201cOR\u201d\n\n- perator \u2013 one could instantiate new queries to relate concepts through OR\u2019ing other categories\ntogether. Similarly, with the rise of search agents, we see greater usage of hyper-specific queries:\nBrowseComp [Wei et al., 2025] has 5+ conditions per query, including range operators. With these\ntools, it is possible to sub-select any top- _\ud835\udc58_ relevant set with the right operators if the documents are\nsufficiently expressive (i.e. non-trivial). Thus, that existing datasets choose to only instantiate some\n\n- f these combinations is mainly for practical reasons and not because of a lack of existence.\n\n\nIn contrast", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_8100", "chunk_text": " right operators if the documents are\nsufficiently expressive (i.e. non-trivial). Thus, that existing datasets choose to only instantiate some\n\n- f these combinations is mainly for practical reasons and not because of a lack of existence.\n\n\nIn contrast to these previous works, we seek to build a dataset that evaluates all combinations of\ntop- _\ud835\udc58_ sets for a small number of documents. Rather than using difficult query operators like QUEST,\nBrowseComp, etc. (which are already difficult for reasons outside of the qrel matrix) we choose very\nsimple query and documents to highlight the difficulty of representing all top- _\ud835\udc58_ sets themselves.\n\n\n**5.2. The LIMIT Dataset**\n\n\n**Dataset Construction** In order to have a natural language version of this dataset, we need some\nway to map combinations of documents into something that could be retrieved with a query. One\nsimple way to do this is to create a synthetic version with latent variables for queries and documents\nand then instantiate it with natural language. For this mapping, we choose to use attributes that\nsomeone could like (i.e. Jon likes Hawaiian pizza, sports cars, etc. ) as they are plentiful and don\u2019t\npresent issues w.r.t. other items: one can like Hawaiian pizza but dislike pepperoni, all preferences\nare valid. We then enforce two constraints for realism: (1) users shouldn\u2019t have too many attributes,\nthus keeping the documents short (less than 50 per user) and (2) each query should only ask for one\nitem to keep the task simple (i.e. \u201cwho likes X\u201d). We gather a list of attributes a person could like\nthrough prompting Gemini 2.5 Pro. We then clean it to a final 1850 items by iteratively asking it to\nremove duplicates/hypernyms, while also checking the top failures with BM25 to ensure no overlap.\n\n\nWe choose to use 50k documents in order to have a hard but relatively small corpus and 1000\nqueries to maintain statistical significance while still being fast to evaluate. For each query, we choose\nto use two relevant documents (i.e. _\ud835\udc58_ =2), both for simplicity in instantiating and to mirror previous\nwork (i.e. NQ, HotpotQA, etc. [Kwiatkowski et al., 2019, Yang et al., 2018]).\n\n\nOur last step is to", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_8550", "chunk_text": " for simplicity in instantiating and to mirror previous\nwork (i.e. NQ, HotpotQA, etc. [Kwiatkowski et al., 2019, Yang et al., 2018]).\n\n\nOur last step is to choose a qrel matrix to instantiate these attributes. Although we could not prove\nthe hardest qrel matrix definitively with theory (as the sign rank is notoriously hard to prove), we\nspeculate based on intuition that our theoretical results imply that the more interconnected the qrel\nmatrix is (e.g. dense with all combinations) the harder it would be for models to represent. [8] Following\nthis, we use the qrel matrix with the highest number of documents for which all combinations would\nbe just above 1000 queries for a top- _\ud835\udc58_ - f 2 (46 docs, since [\ufffd][46] 2 - is 1035, the smallest above 1k).\n\n\nWe then assign random natural language attributes to the queries, adding these attributes to their\nrespective relevant documents (c.f. Figure 1). We give each document a random first and last name\nfrom open-source lists of names. Finally, we randomly sample new attributes for each document until\nall documents have the same number of attributes. As this setup has many more documents than\nthose that are relevant to any query (46 relevant documents, 49.95k non-relevant to any query) we\nalso create a \u201csmall\u201d version with only the 46 documents that are relevant to one of the 1000 queries.\n\n\n8See Appendix 10 for specific metrics that show the difference between LIMIT and other IR datasets.\n\n\n8\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n\n\n\n\n\n\n\n\n|Reca|ll@2|\n|---|---|\n|||\n|||\n|||\n\n\n|Recal|l@10|\n|---|---|\n|||\n|||\n|||\n\n\n|Recal|l@100|\n|---|---|\n|||\n|||\n\n\n\n\n\n\n\n\n\n\n\nFigure 3 | Scores on the LIMIT task. Despite the simplicity of the task we see that SOTA models\nstruggle. We also see that the dimensionality of the model is a limiting factor and that as the\ndimension increases, so does performance. Even multi-vector models struggle. Lexical models like\nBM25 do very well due to their higher dimensionality. Stars indicate models trained with MRL.\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_9000", "chunk_text": " of the model is a limiting factor and that as the\ndimension increases, so does performance. Even multi-vector models struggle. Lexical models like\nBM25 do very well due to their higher dimensionality. Stars indicate models trained with MRL.\n\n\n**Models** We evaluate the state-of-the-art embedding models including GritLM [Muennighoff et al.,\n2024], Qwen 3 Embeddings [Zhang et al., 2025], Promptriever [Weller et al., 2024b], Gemini\nEmbeddings [Lee et al., 2025], Snowflake\u2019s Arctic Embed Large v2.0 [Yu et al., 2024], and E5-Mistral\nInstruct [Wang et al., 2022, 2023]. These models range in embedding dimension (1024 to 4096)\nas well as in training style (instruction-based, hard negative optimized, etc.). We also evaluate\nthree non-single vector models to show the distinction: BM25 [Robertson et al., 1995, L\u00f9, 2024],\ngte-ModernColBERT [Chaffin, 2025, Chaffin and Sourty, 2024], and a token-wise TF-IDF. [9]\n\n\nWe show results at the full embedding dimension and also with truncated embedding dimension\n(typically used with matryoshka learning, aka MRL [Kusupati et al., 2022]). For models not trained\nwith MRL this will result in sub-par scores, thus, models trained with MRL are indicating with stars in\nthe plots. However, as there are no LLMs with an embedding dimension smaller than 384, we include\nMRL for all models to small dimensions (32) to show the impact of embedding dimensionality.\n\n\n**Results** Figure 3 shows the results on the full LIMIT while Figure 4 shows the results on the small\n(46 document) version. **The results are surprising - models severely struggle even though the task**\n**is trivially simple.** For example, in the full setting models struggle to reach even 20% recall@100\nand in the 46 document version models cannot solve the task even with recall@20.\n\n\nWe see that model performance depends crucially on the embedding dimensionality (better\nperformance with bigger dimensions). Interestingly, models trained with more diverse instruction,\nsuch as Promptriever, perform better", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_9450", "chunk_text": " version models cannot solve the task even with recall@20.\n\n\nWe see that model performance depends crucially on the embedding dimensionality (better\nperformance with bigger dimensions). Interestingly, models trained with more diverse instruction,\nsuch as Promptriever, perform better, perhaps because their training allows them to use more of their\nembedding dimensions (compared to models which are trained with MRL and on a smaller range of\ntasks that can perhaps be consolidated into a smaller embedding manifold).\n\n\nFor alternative architectures, GTE-ModernColBERT does significantly better than single-vector\n\n\n9This model turns each unique item into a token and then does TF-IDF. We build it to show that it gets 100% on all tasks\n(as it reverse engineers our dataset construction) and thus we do not include it in future charts.\n\n\n9\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n\n\n\n\n\n\n\n\n|Reca|ll@2|\n|---|---|\n|||\n|||\n|||\n\n\n|Recal|l@10|\n|---|---|\n|||\n|||\n\n\n|Recal|l@20|\n|---|---|\n|||\n|||\n\n\n\n\n\n\n\n\n\n\n\nFigure 4 | Scores on the LIMIT small task (N=46) over embedding dimensions. Despite having just\n46 documents, model struggle even with recall@10 and cannot solve the task even with recall@20.\n\n\nmodels (although still far from solving the task) while BM25 comes close to perfect scores. Both of\nthese alterative architectures (sparse and multi-vector) offer various trade-offs, see \u00a75.6 for analysis.\n\n\n**5.3. Is this Domain Shift?**\n\n\n\nAlthough our queries look similar to standard web search\nqueries, we wondered whether there could be some domain shift causing the low performance. If so, we would\nexpect that training on a training set of similar examples\nwould significantly improve performance. On the other\nhand, if the task was intrinsically hard, training on the\ntraining set would provide little help whereas training\n\n- n the test set would allow the model to overfit to those\ntokens (similar to the free parameterized experiments).\n\n\n\n\n\n\n\n\n\nTo test this we take an off the shelf embedding model\nand train it on either the training set (created synthetically\n\nFigure 5 | Training on LIMIT train does\n\nusing non-test set attributes) or the official test set of LIMIT.\n\nnot significantly help, indicating the\n\nWe use `lightonai/mod", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_9900", "chunk_text": " train it on either the training set (created synthetically\n\nFigure 5 | Training on LIMIT train does\n\nusing non-test set attributes) or the official test set of LIMIT.\n\nnot significantly help, indicating the\n\nWe use `lightonai/modernbert-embed-large` and\n\nissue is not domain shift. But models\n\nfine-tune it on these splits, using the full dataset for in\n\ncan solve it if they overfit to the test set.\n\nbatch negatives (excluding positives) using SentenceTransformers [Reimers and Gurevych, 2019]. We show a range of dimensions by projecting the hidden\nlayer down to the specified size during training (rather than using MRL).\n\n\n\nFigure 5 | Training on LIMIT train does\nnot significantly help, indicating the\nissue is not domain shift. But models\ncan solve it if they overfit to the test set.\n\n\n\n**Results** Figure 5 shows the model trained on the training set cannot solve the problem, although\nit does see very minor improvement from near zero recall@10 to up to 2.8 recall@10. The lack of\nperformance gains when training in-domain indicate that poor performance is not due to domain\nshift. By training the model on the test set we see it can learn the task, overfitting on the tokens\nin the test queries. This aligns with our free embedding results, that it is possible to overfit to the\n_\ud835\udc41_ = 46 version with only 12 dimensions. However, it is notable that the real embedding model with\n64 dimensions still cannot completely solve the task, indicating that real world models are multiple\n\n\n10\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n\n\n\n\n|Cy|ycle|\n|---|---|\n|Cy|cle|\n|||\n|||\n|32 512 1024<br>2048<br>3072<br>409|32 512 1024<br>2048<br>3072<br>409|\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6 | Model results from LIMIT datasets created with different qrel patterns. The dense qrel\npattern that uses the maximum number of combinations is significantly harder than the other\npatterns. Note that the \u201cdense\u201d version is the main LIMIT shown in Figure 3.\n\n\ntimes more limited than free-embeddings, exacerbating the limitations shown in Figure 2.\n\n\n**5.4. Effects of Qrel Patterns**\n\n\nAs mentioned in previous", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_10350", "chunk_text": " the \u201cdense\u201d version is the main LIMIT shown in Figure 3.\n\n\ntimes more limited than free-embeddings, exacerbating the limitations shown in Figure 2.\n\n\n**5.4. Effects of Qrel Patterns**\n\n\nAs mentioned in previous sections, the crucial difference that makes LIMIT hard is that it tests models\n\n- n more combinations of documents than typically used. Although this makes intuitive sense, here\nwe ablate this decision and show that methods that do not test as many combinations (i.e. when the\nqrels are represented as a graph, have lower graph density) are easier empirically.\n\n\n**Experiment Setup** We instantiate LIMIT from four different qrel patterns: (1) _random_ sampling\nfrom all combinations (2) a _cycle_ - based setup where the next query is relevant to one document\nfrom the previous query and the following next document, (3) a _disjoint_ pattern where each query is\nrelevant to two new documents and (4) the pattern that maximizes the number of connections (n\nchoose k) for the largest number of documents that fit in the query set ( _dense_, our standard setup).\nFor all configurations, we use the same setup as the main LIMIT (50k docs, 1k queries, _\ud835\udc58_ =2, etc).\n\n\n**Results** We see in Figure 6 that all patterns except dense have relatively similar performance.\nHowever, moving to _dense_ shows strikingly lower scores across the board for all models: GritLM drops\n50 absolute recall@100, whereas E5-Mistral has an almost 10x reduction (40.4 vs 4.8 recall@100).\n\n\n\n**5.5. Correlation with MTEB**\n\n\nBEIR (used in MTEB v1) [Thakur et al., 2021, Muennighoff\net al., 2022] has frequently been cited as something that embedding models have overfit to [Weller et al., 2025b, Thakur\net al., 2025]. We compare performance on LIMIT to BEIR\nin Figure 7. We see that performance is generally not correlated and that smaller models (like Arctic Embed) do worse\n\n- n both, likely due to embedding dimension and pre-trained\nmodel knowledge.\n\n\n\n|Qwen3 E. Gemini Emb.|Col2|\n|---|---|\n|<br>emn", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_10800", "chunk_text": " that smaller models (like Arctic Embed) do worse\n\n- n both, likely due to embedding dimension and pre-trained\nmodel knowledge.\n\n\n\n|Qwen3 E. Gemini Emb.|Col2|\n|---|---|\n|<br>emn m.<br>GritLM<br>E5~~-~~Mistral<br>Promptriever<br>Snowflake Arctic Emb.|<br>emn m.<br>GritLM<br>E5~~-~~Mistral<br>Promptriever<br>Snowflake Arctic Emb.|\n|.0<br>|0.1<br>0.|\n\n\nFigure 7 | No obvious correlation\nbetween BEIR vs LIMIT.\n\n\n11\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n**5.6. Alternatives to Embedding Models**\n\n\nOur previous results show both theoretically and empirically that embedding models cannot represent\nall combinations of documents in their top- _\ud835\udc58_ sets, making them unable to represent and solve some\nretrieval tasks. As current embedding models have grown larger (e.g. up to 4096), this has helped\nreduce negative effects for smaller dataset sizes. However, with enough combinations of top- _\ud835\udc58_ sets\nthe dimensionality would have to increase to an infeasible size for non-toy datasets.\n\n\nThus, our results show an interesting tradeoff: embeddings can represent a large amount of\ncombinations but not _all_ combinations. Although they are useful for first stage results to a degree,\nmore expressive retriever architectures will be needed. We briefly discuss some of these below.\n\n\n**Cross-Encoders** Although not suitable for first stage retrieval at scale, they are already typically\nused to improve first stage results. However, is LIMIT challenging for rerankers also?\n\n\nWe evaluate a long context reranker, Gemini-2.5-Pro [Comanici et al., 2025] on the small setting\nas a comparison. We give Gemini all 46 documents and all 1000 queries at once, asking it to output\nthe relevant documents for each query with one generation. We find that it can successfully solve\n(100%) all 1000 queries in one forward pass. This is in contrast to even the best embedding models\nwith a recall@2 of less than 60% (Figure 4). Thus we can see that LIMIT is simple for state-of-the-art\nreranker models as they do not", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_11250", "chunk_text": " pass. This is in contrast to even the best embedding models\nwith a recall@2 of less than 60% (Figure 4). Thus we can see that LIMIT is simple for state-of-the-art\nreranker models as they do not have the same limitations based on embedding dimension. However,\nthey still have the limitation of being more computationally expensive than embedding models and\nthus cannot be used for first stage retrieval when there are large numbers of documents.\n\n\n**Multi-vector models** Multi-vector models are more expressive through the use of multiple vectors\nper sequence combined with the MaxSim operator [Khattab and Zaharia, 2020]. These models show\npromise on the LIMIT dataset, with scores greatly above the single-vector models despite using a\nsmaller backbone (ModernBERT, Warner et al. [2024]). However, these models are not generally\nused for instruction-following or reasoning-based tasks, leaving it an open question to how well\nmulti-vector techniques will transfer to these more advanced tasks.\n\n\n**Sparse models** Sparse models (both lexical and neural versions) can be thought of as single vector\nmodels but with very high dimensionality. This dimensionality helps BM25 avoid the problems of the\nneural embedding models as seen in Figure 3. Since the _\ud835\udc51_ - f their vectors is high, they can scale to\nmany more combinations than their dense vector counterparts. However, it is less clear how to apply\nsparse models to instruction-following and reasoning-based tasks where there is no lexical or even\nparaphrase-like overlap. We leave this direction to future work.\n\n##### **6. Conclusion**\n\n\nWe introduce the LIMIT dataset, which highlights the fundamental limitations of embedding models.\nWe provide a theoretical connection that shows that embedding models cannot represent all combinations of top- _\ud835\udc58_ documents until they have a large enough embedding dimension _\ud835\udc51_ . We show these\ntheoretical results hold empirically as well, through best case optimization of the vectors themselves.\nWe then make a practical connection to existing state-of-the-art models by creating a simple natural\nlanguage instantiation of the theory, called LIMIT, that these models cannot solve. Our results imply\nthat the community should consider how instruction-based retrieval will impact retrievers, as there\nwill be combinations of top- _\ud835\udc58_ documents cannot represent.\n\n\n12\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n##### **Limitations**\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_11700", "chunk_text": " should consider how instruction-based retrieval will impact retrievers, as there\nwill be combinations of top- _\ud835\udc58_ documents cannot represent.\n\n\n12\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n##### **Limitations**\n\n\nAlthough our experiments provide theoretical insight for the most common type of embedding model\n(single vector) they do not hold necessarily for other architectures, such as multi-vector models.\nAlthough we showed initial empirical results with non-single vector models, we leave it to future work\nto extend our theoretical connections to these settings.\n\n\nWe also did not show theoretical results for the setting where the user allows some mistakes, e.g.\ncapturing only the majority of the combinations. We leave putting a bound on this scenario to future\nwork and would invite the reader to examine works like Ben-David et al. [2002].\n\n\nWe have showed the theoretical connection that proves that some combinations cannot be represented by embedding models, however, we cannot prove apriori which _types_ - f combinations they\nwill fail on. Thus, it is possible that there are some instruction-following or reasoning tasks they can\nsolve perfectly, however, _we do know_ that there exists some tasks that they will never be able to solve.\n\n##### **Acknowledgments**\n\n\nWe thank Tanmaya Dabral, Zhongli Ding, Anthony Chen, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova for their helpful feedback.\n\n##### **References**\n\n\nN. Alon, S. Moran, and A. Yehudayoff. Sign rank, vc dimension and spectral gaps. In _Electronic_\n_Colloquium on Computational Complexity (ECCC)_, volume 21, page 10, 2014.\n\n\nP. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy. Llm2vec: Large\nlanguage models are secretly powerful text encoders. _arXiv preprint arXiv:2404.05961_, 2024.\n\n\nS. Ben-David, N. Eiron, and H. U. Simon. Limitations of learning via embeddings in euclidean half\nspaces. _Journal of Machine Learning Research_, 3(Nov):441\u2013461, 2002.\n\n\nC. Bohler, P. Cheilaris, R. Klein, C.-H.", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_12150", "chunk_text": " of learning via embeddings in euclidean half\nspaces. _Journal of Machine Learning Research_, 3(Nov):441\u2013461, 2002.\n\n\nC. Bohler, P. Cheilaris, R. Klein, C.-H. Liu, E. Papadopoulou, and M. Zavershynskyi. On the\ncomplexity of higher order abstract voronoi diagrams. _Computational Geometry_, 48(8):539\u2013\n551, 2015. ISSN 0925-7721. doi: https://doi.org/10.1016/j.comgeo.2015.04.008. URL\n`[https://www.sciencedirect.com/science/article/pii/S0925772115000346](https://www.sciencedirect.com/science/article/pii/S0925772115000346)` .\n\n\nA. Chaffin. Gte-moderncolbert, 2025. URL `[https://huggingface.co/lightonai/](https://huggingface.co/lightonai/GTE-ModernColBERT-v1)`\n`[GTE-ModernColBERT-v1](https://huggingface.co/lightonai/GTE-ModernColBERT-v1)` .\n\n\nA. Chaffin and R. Sourty. Pylate: Flexible training and retrieval for late interaction models, 2024.\nURL `[https://github.com/lightonai/pylate](https://github.com/lightonai/pylate)` .\n\n\nA. Chattopadhyay and N. Mande. A short list of equalities induces large sign rank. In _2018 IEEE 59th_\n_Annual Symposium on Foundations of Computer Science (FOCS)_, pages 47\u201358. IEEE, 2018.\n\n\nB. Y. Chen, H. Huang, H.-P. Chen, W. Liu, X.-Y. Chen, and T. Jia. Efficient algorithm for constructing\n\n - rder k voronoi diagrams in road networks. _ISPRS International Journal of Geo-Information_, 12(4):\n172, 2023.\n\n\nF. Chierichetti, S. Gollapudi, R. Kumar, S. Lattanzi, R. Panigrahy, and D. P. Woodruff. Algorithms\nfor \\ _\u2113\ud835\udc5d_ low-rank approximation. In _International", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_13050", "chunk_text": "7.01449_, 2024.\n\n\nH. Hatami and P. Hatami. Structure in communication complexity and constant-cost complexity\nclasses. _arXiv preprint arXiv:2401.14623_, 2024.\n\n\nH. Hatami, P. Hatami, W. Pires, R. Tao, and R. Zhao. Lower bound methods for sign-rank and\ntheir limitations. In _Approximation, Randomization, and Combinatorial Optimization. Algorithms_\n_and Techniques (APPROX/RANDOM 2022)_, pages 22\u20131. Schloss Dagstuhl\u2013Leibniz-Zentrum f\u00fcr\nInformatik, 2022.\n\n\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.\nHendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. _arXiv preprint_\n_arXiv:2203.15556_, 2022.\n\n\nG. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave. Unsupervised\ndense information retrieval with contrastive learning. _arXiv preprint arXiv:2112.09118_, 2021.\n\n\nO. Khattab and M. Zaharia. Colbert: Efficient and effective passage search via contextualized late\ninteraction over bert. In _Proceedings of the 43rd International ACM SIGIR conference on research and_\n_development in Information Retrieval_, pages 39\u201348, 2020.\n\n\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_,\n2014.\n\n\nA. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen,\nS. Kakade, P. Jain, et al. Matryoshka representation learning. _Advances in Neural Information_\n_Processing Systems_, 35:30233\u201330249, 2022.\n\n\nT. Kwiatkowski,", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_14400", "chunk_text": " information retrieval models. _arXiv preprint arXiv:2402.14334_, 2024.\n\n\nA. v. d. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. _arXiv_\n_preprint arXiv:1807.03748_, 2018.\n\n\nA. Ormazabal, M. Artetxe, G. Labaka, A. Soroa, and E. Agirre. Analyzing the limitations of cross-lingual\nword embedding mappings. _arXiv preprint arXiv:1906.05407_, 2019.\n\n\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, et al. Training language models to follow instructions with human feedback. _Advances in_\n_neural information processing systems_, 35:27730\u201327744, 2022.\n\n\nC. H. Papadimitriou and M. Sipser. Communication complexity. In _Proceedings of the fourteenth annual_\n_ACM symposium on Theory of computing_, pages 196\u2013200, 1982.\n\n\nN. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In\n_Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing_ . Association\nfor Computational Linguistics, 11 2019. URL `[https://arxiv.org/abs/1908.10084](https://arxiv.org/abs/1908.10084)` .\n\n\nN. Reimers and I. Gurevych. The curse of dense low-dimensional information retrieval for large index\nsizes. _arXiv preprint arXiv:2012.14210_, 2020.\n\n\nS. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, M. Gatford, et al. Okapi at trec-3. _Nist_\n_Special Publication Sp_, 109:109, 1995.\n\n\n15\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\nT. Song, G. Gan, M. Shang, and Y.", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_14850", "chunk_text": ". _Nist_\n_Special Publication Sp_, 109:109, 1995.\n\n\n15\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\nT. Song, G. Gan, M. Shang, and Y. Zhao. Ifir: A comprehensive benchmark for evaluating instructionfollowing in expert-domain information retrieval. _arXiv preprint arXiv:2503.04644_, 2025.\n\n\nH. Su, W. Shi, J. Kasai, Y. Wang, Y. Hu, M. Ostendorf, W.-t. Yih, N. A. Smith, L. Zettlemoyer, and T. Yu.\nOne embedder, any task: Instruction-finetuned text embeddings. _arXiv preprint arXiv:2212.09741_,\n2022.\n\n\nH. Su, H. Yen, M. Xia, W. Shi, N. Muennighoff, H.-y. Wang, H. Liu, Q. Shi, Z. S. Siegel, M. Tang, et al.\nBright: A realistic and challenging benchmark for reasoning-intensive retrieval. _arXiv preprint_\n_arXiv:2407.12883_, 2024.\n\n\nC. Team. Chameleon: Mixed-modal early-fusion foundation models. _arXiv preprint arXiv:2405.09818_,\n2024.\n\n\nN. Thakur, N. Reimers, A. R\u00fcckl\u00e9, A. Srivastava, and I. Gurevych. Beir: A heterogenous benchmark\nfor zero-shot evaluation of information retrieval models. _arXiv preprint arXiv:2104.08663_, 2021.\n\n\nN. Thakur, J. Lin, S. Havens, M. Carbin, O. Khattab, and A. Drozdov. Freshstack: Building realistic\nbenchmarks for evaluating retrieval on technical documents. _arXiv preprint arXiv:2504.13128_,\n2025.\n\n\nG. Voronoi. Nouvelles applications des param\u00e8tres continus \u00e0 la th\u00e9orie des formes quadratiques.\ndeuxi\u00e8me m\u00e9moire. recherches sur les parall\u00e9llo\u00e8dres primitifs. _Journal f\u00fcr die reine und angewandte_\n_Mathematik (Crelles Journal)_, 1908", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_15300", "chunk_text": "atiques.\ndeuxi\u00e8me m\u00e9moire. recherches sur les parall\u00e9llo\u00e8dres primitifs. _Journal f\u00fcr die reine und angewandte_\n_Mathematik (Crelles Journal)_, 1908(134):198\u2013287, 1908.\n\n\nD. Wadden, S. Lin, K. Lo, L. L. Wang, M. van Zuylen, A. Cohan, and H. Hajishirzi. Fact or fiction:\nVerifying scientific claims. _arXiv preprint arXiv:2004.14974_, 2020.\n\n\nL. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by\nweakly-supervised contrastive pre-training. _arXiv preprint arXiv:2212.03533_, 2022.\n\n\nL. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei. Improving text embeddings with\nlarge language models. _arXiv preprint arXiv:2401.00368_, 2023.\n\n\nB. Warner, A. Chaffin, B. Clavi\u00e9, O. Weller, O. Hallstr\u00f6m, S. Taghadouini, A. Gallagher, R. Biswas,\nF. Ladhak, T. Aarsen, et al. Smarter, better, faster, longer: A modern bidirectional encoder for fast,\nmemory efficient, and long context finetuning and inference. _arXiv preprint arXiv:2412.13663_,\n2024.\n\n\nJ. Wei, Z. Sun, S. Papay, S. McKinney, J. Han, I. Fulford, H. W. Chung, A. T. Passos, W. Fedus, and\nA. Glaese. Browsecomp: A simple yet challenging benchmark for browsing agents. _arXiv preprint_\n_arXiv:2504.12516_, 2025.\n\n\nO. Weller, B. Chang, S. MacAvaney, K. Lo, A. Cohan, B. Van Durme, D. Lawrie, and L. Soldaini.\nFollowir: Evaluating and teaching information retrieval models to", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_15750", "chunk_text": "eller, B. Chang, S. MacAvaney, K. Lo, A. Cohan, B. Van Durme, D. Lawrie, and L. Soldaini.\nFollowir: Evaluating and teaching information retrieval models to follow instructions. _arXiv preprint_\n_arXiv:2403.15246_, 2024a.\n\n\nO. Weller, B. Van Durme, D. Lawrie, A. Paranjape, Y. Zhang, and J. Hessel. Promptriever: Instructiontrained retrievers can be prompted like language models. _arXiv preprint arXiv:2409.11136_, 2024b.\n\n\nO. Weller, B. Chang, E. Yang, M. Yarmohammadi, S. Barham, S. MacAvaney, A. Cohan, L. Soldaini,\nB. Van Durme, and D. Lawrie. mfollowir: a multilingual benchmark for instruction following in\nretrieval. _arXiv preprint arXiv:2501.19264_, 2025a.\n\n\n16\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\nO. Weller, K. Ricci, E. Yang, A. Yates, D. Lawrie, and B. Van Durme. Rank1: Test-time compute for\nreranking in information retrieval. _arXiv preprint arXiv:2502.18418_, 2025b.\n\n\nC. Xiao, G. T. Hudson, and N. A. Moubayed. Rar-b: Reasoning as retrieval benchmark. _arXiv preprint_\n_arXiv:2404.06347_, 2024.\n\n\nZ. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning. Hotpotqa: A\ndataset for diverse, explainable multi-hop question answering. _arXiv preprint arXiv:1809.09600_,\n2018.\n\n\nZ. Yin and Y. Shen. On the dimensionality of word embedding. _Advances in neural information_\n_processing systems_, 31, 2018.\n\n\nP. Yu, L. Merrick, G. Nuti, and D. Campos. Arctic-embed 2.0:", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_16200", "chunk_text": " dimensionality of word embedding. _Advances in neural information_\n_processing systems_, 31, 2018.\n\n\nP. Yu, L. Merrick, G. Nuti, and D. Campos. Arctic-embed 2.0: Multilingual retrieval without compromise. _arXiv preprint arXiv:2412.04506_, 2024.\n\n\nY. Zhang, M. Li, D. Long, X. Zhang, H. Lin, B. Yang, P. Xie, A. Yang, D. Liu, J. Lin, F. Huang, and\nJ. Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models.\n_arXiv preprint arXiv:2506.05176_, 2025.\n\n\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following\nevaluation for large language models. _arXiv preprint arXiv:2311.07911_, 2023.\n\n\nJ. Zhou, Y. Zheng, W. Chen, Q. Zheng, Z. Shang, W. Zhang, R. Meng, and X. Shen. Beyond content\nrelevance: Evaluating instruction following in retrieval models. _ArXiv_, abs/2410.23841, 2024. URL\n`[https://api.semanticscholar.org/CorpusID:273707185](https://api.semanticscholar.org/CorpusID:273707185)` .\n\n##### **7. Using the Triangle Inequality to Provide Theoretical Limits**\n\n\nIt is tempting to use the triangle inequality to show that embedding models have theoretical limitations.\nThis is true for metric spaces, however, vector search often uses cosine similarity which operates in\nnon-metric space. Thus, for realistic scenarios, we cannot use the triangle inequality to bound what\nembedding models can represent.\n\n##### **8. Relationship to Order-K Voronoi Regions**\n\n\nWe also provide an explanation for how our results compare to Clarkson [1988] which put bounds\n\n- n the number of regions in the order- _\ud835\udc58_ Voronoi graph. The order- _\ud835\udc58_ Voronoi graph is defined as the\nset of points having a particular set of _\ud835\udc5b_ points in _\ufffd", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_16650", "chunk_text": " regions in the order- _\ud835\udc58_ Voronoi graph. The order- _\ud835\udc58_ Voronoi graph is defined as the\nset of points having a particular set of _\ud835\udc5b_ points in _\ud835\udc46_ as its _\ud835\udc5b_ nearest neighbors. This maps nicely to\nretrieval, as each order- _\ud835\udc58_ region is equivalent to one retrieved set of top- _\ud835\udc58_ results. Then the count of\nunique regions in the Voronoi graph is the total number of combinations that could be returned for\nthose points. However, creating an empirical order-k Voronoi graph is computationally infeasible for\n_\ud835\udc51_ - 3, and theoretically it is hard to bound tightly. Thus we use a different approach for showing the\nlimitations of embedding models, through the use of the sign-rank.\n\n##### **9. Hyperparameter and Compute Details**\n\n\n**Inference** We use the default length settings for evaluating models using the MTEB framework\n\n[Enevoldsen et al., 2025]. As our dataset has relatively short documents (around 100 tokens), this\ndoes not cause an issue.\n\n\n17\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n**Training** For training on the LIMIT training and test set we use the SentenceTransformers library\n\n[Reimers and Gurevych, 2019] using the MultipleNegativesRankingLoss. We use a full dataset batch\nsize and employ the no duplicates sampler to ensure that no in-batch negatives are duplicates of the\npositive docs. We use a learning rate of 5e-5. We train for 5 epochs and limit the training set slightly\nto the size of the test set (from 2.5k to 2k examples, matching test).\n\n\n**Compute** Inference and training for LIMIT is done with A100 GPUs on Google Colab Pro. The\nfree embedding experiments are done mainly on H100 GPUs and TPU v5\u2019s for larger size _\ud835\udc41_ to\naccommodate higher VRAM for full-dataset batch vector optimization.\n\n##### **10. Metrics Measuring Qrel Graph Density**\n\n\nWe show two metrics that treat the qrel matrix as a graph and show that LIMIT has unique properties\ncompared to standard IR datasets (Table 1). We call these metrics Graph Density and Average", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_17100", "chunk_text": ". Metrics Measuring Qrel Graph Density**\n\n\nWe show two metrics that treat the qrel matrix as a graph and show that LIMIT has unique properties\ncompared to standard IR datasets (Table 1). We call these metrics Graph Density and Average Query\nStrength and describe them below.\n\n\n**Graph Density** We use the qrel matrix to construct the graph, where nodes are documents and an\nedge exists between two documents if they are both relevant to at least one common query.\n\n\nFor a given graph _\ud835\udc3a_ = ( _\ud835\udc49, \ud835\udc38_ ) with _\ud835\udc49_ being the set of nodes and _\ud835\udc38_ being the set of edges, the graph\ndensity is defined as the ratio of the number of edges in the graph to the maximum possible number\n\n- f edges. For an undirected graph, the maximum possible number of edges is [|] _[\ud835\udc49]_ [|] [(|] _[\ud835\udc49]_ 2 [|] [\u2212][1][)] . Thus, the\n\ndensity _\ud835\udf0c_ is calculated as:\n\n\n\n_\ud835\udc38_ | 2| _\ud835\udc38_ |\n\n=\n\n_\ud835\udc49_ | - 1) | _\ud835\udc49_ |(| _\ud835\udc49_ | \u2212 1)\n\n2\n\n\n\n| _\ud835\udc38_ |\n_\ud835\udf0c_ =\n\n\n\n| _\ud835\udc49_ | (| _\ud835\udc49_ | - 1)\n\n\n\nThis metric indicates how connected the graph is; a density of 1 signifies a complete graph (all\npossible edges exist), while a density close to 0 indicates a sparse graph. For a qrel dataset, the\n\n\n**Average Query Strength** In a query-query graph where nodes are queries and edges represent\nsimilarity between queries (e.g., Jaccard similarity of their relevant documents), the _strength_ - f a\nquery node _\ud835\udc56_, denoted _\ud835\udc60\ud835\udc56_, is defined as the sum of the weights of all edges incident to it. If _\ud835\udc64\ud835\udc56\ud835\udc57_ is the\nweight of the edge between query _\ud835\udc56_ and query _\ud835\udc57_, and _\ud835\udc41_ ( _\ud835\udc56_ ) is the set of neighbors of query _\ud835\udc56_, then the\nstrength is:\n\n\n_\ud835\udc60\ud835\udc56_ = \ufffd", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_17550", "chunk_text": "\ufffd_ and query _\ud835\udc57_, and _\ud835\udc41_ ( _\ud835\udc56_ ) is the set of neighbors of query _\ud835\udc56_, then the\nstrength is:\n\n\n_\ud835\udc60\ud835\udc56_ = \u2211\ufe01 _\ud835\udc64\ud835\udc56\ud835\udc57_\n\n_\ud835\udc57_ \u2208 _\ud835\udc41_ ( _\ud835\udc56_ )\n\n\nThe Average Query Strength \u00af _\ud835\udc60_ is the mean of these strengths across all query nodes in the graph:\n\n\n\n\u00af 1\n_\ud835\udc60_ =\n| _\ud835\udc49\ud835\udc44_ |\n\n\n\n_\ud835\udc60\ud835\udc56_\n\n\u2211\ufe01\n\n_\ud835\udc56_ \u2208 _\ud835\udc49\ud835\udc44_\n\n\n\nwhere _\ud835\udc49\ud835\udc44_ is the set of all query nodes in the graph. This metric provides an overall measure of how\nstrongly connected queries are to each other on average within the dataset, based on their shared\nrelevant documents.\n\n\n18\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n**Comparisons to other datasets** We compare with standard IR Datasets such as NQ [Kwiatkowski\net al., 2019], HotpotQA [Yang et al., 2018], and SciFact [Wadden et al., 2020]. We also show an\ninstruction-following dataset, FollowIR Core17 [Weller et al., 2024a]. For all datasets, we use the\ntest set only. The results in Table 1 show that LIMIT has significantly higher values for both of these\nmetrics (i.e. 28 for query similarity compared to 0.6 or lower for the others).\n\n\nTable 1 | Metrics measuring the density of the qrel matrix. We see that LIMIT is significantly higher\nthan other datasets, but that the closest are instruction-following datasets such as Core17 from\nFollowIR. Our empirical ablations suggest (although cannot definitively prove) that datasets with\nhigher values here will be harder for retrieval models to represent.\n\n\n**Dataset Name** **Graph Density** **Average Query Strength**\n\n\nNQ 0 0\nHotPotQA 0.000037 0.1104\nSciFact 0.001449 0.4222\n\nFollowIR Core17 0.025641 0.5912\n\nLIMIT 0.085481 28.465", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_18000", "chunk_text": "QA 0.000037 0.1104\nSciFact 0.001449 0.4222\n\nFollowIR Core17 0.025641 0.5912\n\nLIMIT 0.085481 28.4653\n\n##### **11. Table Forms of Figures**\n\n\nIn this section we show the table form of various figures. For Figure 3 it is Table 5, Figure 4 in Table 4,\nFigure 2 in Table 6, Figure 5 in Table 2, and Figure 6 in Table 3.\n\n\n19\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\nSplit Dim Recall@2 Recall@10 Recall@100\n\n\nTest 32 85.5 98.4 100.0\n\nTest 64 90.4 98.7 100.0\n\nTest 128 93.1 99.5 99.9\n\nTest 256 94.2 99.7 100.0\n\nTest 384 95.6 99.6 100.0\n\nTest 512 94.0 99.5 99.9\n\nTest 768 96.1 99.8 100.0\n\nTest 1024 96.5 99.8 100.0\n\n\nTrain 32 0.0 0.0 0.0\n\nTrain 64 0.1 0.3 2.2\n\nTrain 128 0.2 0.7 3.1\n\nTrain 256 0.0 0.0 0.4\n\nTrain 384 1.1 2.7 8.3\n\nTrain 512 0.7 2.3 9.8\n\nTrain 768 0.7 2.4 9.9\n\nTrain 1024 1.0 2.8 11.2\n\n\nTable 2 | Fine-tuning results in table form. See Figure 5 for the comparable plot.\n\n\n\n20\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\nModel Dim Random Dense Cycle Disjoint\n\n\nBM25 default 96.1 93.0 96.0 96.6\n\nE5-Mistral 7B 32 1.7 0.6 1.7 2.2\n\nE5-Mistral 7", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_18450", "chunk_text": "96.1 93.0 96.0 96.6\n\nE5-Mistral 7B 32 1.7 0.6 1.7 2.2\n\nE5-Mistral 7B 64 4.3 0.5 3.3 4.8\n\nE5-Mistral 7B 128 10.3 0.9 9.1 10.5\n\nE5-Mistral 7B 256 16.9 1.2 14.0 15.5\n\nE5-Mistral 7B 512 26.4 2.5 24.0 26.6\n\nE5-Mistral 7B 768 31.5 3.1 27.7 30.0\n\nE5-Mistral 7B 1024 34.0 3.8 29.5 32.8\n\nE5-Mistral 7B 2048 36.8 4.3 33.6 36.7\n\nE5-Mistral 7B 3072 38.9 4.7 35.8 37.6\n\nE5-Mistral 7B 4096 40.4 4.8 36.6 38.8\n\nGTE-ModernColBERT default 71.1 61.8 65.3 70.1\n\nGritLM 7B 32 1.5 0.6 1.9 1.5\n\nGritLM 7B 64 3.6 0.6 2.9 3.9\n\nGritLM 7B 128 8.0 1.6 6.3 8.4\n\nGritLM 7B 256 15.8 2.0 14.4 16.0\n\nGritLM 7B 512 33.7 4.5 29.5 33.8\n\nGritLM 7B 768 39.0 5.6 34.4 40.1\n\nGritLM 7B 1024 43.3 6.6 37.4 44.1\n\nGritLM 7B 2048 ", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_18900", "chunk_text": "0 5.6 34.4 40.1\n\nGritLM 7B 1024 43.3 6.6 37.4 44.1\n\nGritLM 7B 2048 55.3 9.0 49.0 55.8\n\nGritLM 7B 3072 61.5 10.9 54.3 61.6\n\nGritLM 7B 4096 61.8 10.4 56.6 63.2\nPromptriever Llama3 8B 32 0.7 0.6 1.2 1.1\nPromptriever Llama3 8B 64 2.6 1.1 2.8 2.3\nPromptriever Llama3 8B 128 5.7 1.3 5.7 7.1\nPromptriever Llama3 8B 256 16.2 1.7 12.6 16.3\nPromptriever Llama3 8B 512 31.9 4.7 26.0 29.0\nPromptriever Llama3 8B 768 37.5 8.5 33.2 37.5\nPromptriever Llama3 8B 1024 42.3 11.8 37.5 40.5\nPromptriever Llama3 8B 2048 52.7 14.1 49.1 53.7\nPromptriever Llama3 8B 3072 56.6 15.8 52.9 57.4\nPromptriever Llama3 8B 4096 62.0 19.4 58.6 63.6\nQwen3 Embed 32 3.2 0.7 2.7 2.6\nQwen3 Embed 64 5.4 1.1 5.0 5.7\nQwen3 Embed 128 9.9 1.9 7.9 9.4\nQwen3 Embed 256 14.2 2.4 11.6 12.5\nQwen3 Embed 512", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_19350", "chunk_text": "3 Embed 128 9.9 1.9 7.9 9.4\nQwen3 Embed 256 14.2 2.4 11.6 12.5\nQwen3 Embed 512 18.0 3.3 14.7 15.9\nQwen3 Embed 768 19.5 3.5 15.5 18.0\nQwen3 Embed 1024 20.4 3.6 16.1 18.7\nQwen3 Embed 2048 22.3 4.1 17.2 21.4\nQwen3 Embed 3072 21.9 4.3 17.9 21.1\nQwen3 Embed 4096 22.7 4.5 17.8 20.9\nGemini Embed 2 0.0 0.1 0.1 0.0\n\nGemini Embed 4 0.0 0.0 0.0 0.1\n\nGemini Embed 8 0.2 0.0 0.0 0.2\n\nGemini Embed 16 0.2 0.0 0.2 0.1\n\nGemini Embed 32 0.4 0.0 0.2 0.1\n\nGemini Embed 64 0.6 0.2 0.3 0.5\n\nGemini Embed 128 1.4 0.3 0.8 1.4\n\nGemini Embed 256 7.1 1.2 5.8 7.4\n\nGemini Embed 512 18.9 3.6 17.6 19.7\n\nGemini Embed 768 33.5 7.6 31.0 34.5\n\nGemini Embed 1024 36.5 8.1 33.8 37.6\n\nGemini Embed 2048 41.1 8.5 36.2 40.6\n\nGemini Embed 3072 42.9 10.0 38.3 43.1\nSnowflake Arctic L 32 1.5 0.7 1.4 1.6\nSnowflake", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_19800", "chunk_text": "6\n\nGemini Embed 3072 42.9 10.0 38.3 43.1\nSnowflake Arctic L 32 1.5 0.7 1.4 1.6\nSnowflake Arctic L 64 3.5 0.5 3.3 3.1\nSnowflake Arctic L 128 8.1 1.1 6.8 7.6\nSnowflake Arctic L 256 15.8 1.6 12.3 14.0\nSnowflake Arctic L 512 17.9 2.3 14.0 16.3\nSnowflake Arctic L 768 19.3 2.5 15.9 18.6\nSnowflake Arctic L 1024 21.0 2.4 17.6 20.0\nSnowflake Arctic L 2048 21.0 2.4 17.6 20.0\nSnowflake Arctic L 3072 21.0 2.4 17.6 20.0\nSnowflake Arctic L 4096 21.0 2.4 17.6 20.0\n\n\nTable 3 | Results for various qrel patterns. See Figure 6 for the comparable plot.\n\n\n\n21\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\nModel Dim Recall@2 Recall@10 Recall@20\n\n\nBM25 default 97.8 100.0 100.0\n\nE5-Mistral 7B 32 7.9 32.6 56.2\n\nE5-Mistral 7B 64 10.2 37.0 60.3\n\nE5-Mistral 7B 128 14.5 41.9 65.9\n\nE5-Mistral 7B 256 15.3 45.9 69.7\n\nE5-Mistral 7B 512 22.2 54.7 74.8\n\nE5-Mistral 7B 768 21.6 57.5 79.2\n\nE5-Mistral 7B 1024 24.5 60.5 80.0\n\nE5-Mistral 7B 2048 28.9 ", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_20250", "chunk_text": "6 57.5 79.2\n\nE5-Mistral 7B 1024 24.5 60.5 80.0\n\nE5-Mistral 7B 2048 28.9 66.3 83.2\n\nE5-Mistral 7B 3072 29.9 67.8 85.3\n\nE5-Mistral 7B 4096 29.5 68.1 85.2\nGTE-ModernColBERT default 83.5 97.6 99.1\n\nGritLM 7B 32 7.8 33.5 56.3\n\nGritLM 7B 64 9.4 35.9 59.6\n\nGritLM 7B 128 14.2 42.7 64.9\n\nGritLM 7B 256 17.3 46.2 68.3\n\nGritLM 7B 512 21.8 55.6 76.7\n\nGritLM 7B 768 23.8 58.1 80.1\n\nGritLM 7B 1024 26.2 61.4 80.1\n\nGritLM 7B 2048 33.0 69.1 86.2\n\nGritLM 7B 3072 36.3 72.9 89.9\n\nGritLM 7B 4096 38.4 75.4 90.5\nPromptriever Llama3 8B 32 6.1 31.4 56.0\nPromptriever Llama3 8B 64 8.9 35.8 62.3\nPromptriever Llama3 8B 128 13.7 44.5 67.6\nPromptriever Llama3 8B 256 18.5 52.1 74.1\nPromptriever Llama3 8B 512 27.0 61.8 81.7\nPromptriever Llama3 8B 768 35.5 69.0 84.7\nPromptriever Llama3 8B 1024 38.0", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_20700", "chunk_text": "0 61.8 81.7\nPromptriever Llama3 8B 768 35.5 69.0 84.7\nPromptriever Llama3 8B 1024 38.0 73.5 89.1\nPromptriever Llama3 8B 2048 46.2 83.6 94.2\nPromptriever Llama3 8B 3072 49.2 87.3 96.6\nPromptriever Llama3 8B 4096 54.3 90.0 97.7\nQwen3 Embed 32 8.3 30.6 53.9\nQwen3 Embed 64 9.4 35.5 57.6\nQwen3 Embed 128 11.6 38.3 60.8\nQwen3 Embed 256 14.3 41.6 63.8\nQwen3 Embed 512 16.1 43.7 66.0\nQwen3 Embed 768 17.2 45.3 69.3\nQwen3 Embed 1024 17.8 48.7 70.3\nQwen3 Embed 2048 19.5 51.5 72.4\nQwen3 Embed 3072 19.3 52.8 73.3\nQwen3 Embed 4096 19.0 52.3 73.8\nGemini Embed 2 4.2 23.0 45.5\n\nGemini Embed 4 4.2 21.9 46.0\n\nGemini Embed 8 4.9 23.2 47.0\n\nGemini Embed 16 5.2 24.7 47.5\n\nGemini Embed 32 6.3 25.2 50.6\n\nGemini Embed 64 6.9 30.6 55.0\n\nGemini Embed 128 7.7 37.0 62.9\n\nGemini Embed 256 14.6 46.9 69.7\n\nGemini Embed 512 23.3 58.4 77.9\n\nGemini Embed 768 28.8 ", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_21150", "chunk_text": "62.9\n\nGemini Embed 256 14.6 46.9 69.7\n\nGemini Embed 512 23.3 58.4 77.9\n\nGemini Embed 768 28.8 67.5 84.5\n\nGemini Embed 1024 31.8 69.9 86.1\n\nGemini Embed 2048 31.9 70.3 87.1\n\nGemini Embed 3072 33.7 72.4 87.9\nSnowflake Arctic L 32 8.3 30.3 53.8\nSnowflake Arctic L 64 9.0 35.4 58.5\nSnowflake Arctic L 128 12.7 41.3 65.1\nSnowflake Arctic L 256 16.0 48.2 72.6\nSnowflake Arctic L 512 16.7 51.3 74.1\nSnowflake Arctic L 768 17.9 53.5 74.6\nSnowflake Arctic L 1024 19.4 54.9 76.0\nSnowflake Arctic L 2048 19.4 54.9 76.0\nSnowflake Arctic L 3072 19.4 54.9 76.0\nSnowflake Arctic L 4096 19.4 54.9 76.0\n\n\nTable 4 | Results for the LIMIT small version. See comparable Figure 4.\n\n\n\n22\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\nModel Dim Recall@2 Recall@10 Recall@100\n\n\nE5-Mistral 7B 32 0.0 0.0 0.5\n\nE5-Mistral 7B 64 0.0 0.1 0.4\n\nE5-Mistral 7B 128 0.1 0.3 1.0\n\nE5-Mistral 7B 256 0.4 0.9 1.9\n\nE5-Mistral 7B 512 0.7 1.3 3.8\n\nE5-Mistral 7B 768 0.9 1.7 4.3\n\nE5-Mistral 7B ", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_21600", "chunk_text": " 7B 512 0.7 1.3 3.8\n\nE5-Mistral 7B 768 0.9 1.7 4.3\n\nE5-Mistral 7B 1024 0.9 1.8 5.9\n\nE5-Mistral 7B 2048 1.0 1.9 6.8\n\nE5-Mistral 7B 3072 1.3 2.0 7.7\n\nE5-Mistral 7B 4096 1.3 2.2 8.3\nSnowflake Arctic L 32 0.0 0.1 0.6\nSnowflake Arctic L 64 0.2 0.4 1.7\nSnowflake Arctic L 128 0.1 0.3 1.8\nSnowflake Arctic L 256 0.2 0.8 2.5\nSnowflake Arctic L 512 0.3 1.0 2.5\nSnowflake Arctic L 768 0.4 1.1 3.1\nSnowflake Arctic L 1024 0.4 0.8 3.3\nSnowflake Arctic L 2048 0.4 0.8 3.3\nSnowflake Arctic L 3072 0.4 0.8 3.3\nSnowflake Arctic L 4096 0.4 0.8 3.3\nGritLM 7B 32 0.0 0.0 0.8\n\nGritLM 7B 64 0.0 0.1 0.3\n\nGritLM 7B 128 0.1 0.3 1.3\n\nGritLM 7B 256 0.1 0.4 2.8\n\nGritLM 7B 512 0.6 1.8 6.5\n\nGritLM 7B 768 1.5 3.1 8.7\n\nGritLM 7B 1024 1.8 3.5 10.6\n\nGritLM 7B 2048 2.3 4.3 11.8", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_22050", "chunk_text": ".1 8.7\n\nGritLM 7B 1024 1.8 3.5 10.6\n\nGritLM 7B 2048 2.3 4.3 11.8\n\nGritLM 7B 3072 2.0 4.3 12.9\n\nGritLM 7B 4096 2.4 4.1 12.9\nPromptriever Llama3 8B 32 0.0 0.0 0.1\nPromptriever Llama3 8B 64 0.0 0.0 0.3\nPromptriever Llama3 8B 128 0.0 0.1 0.6\nPromptriever Llama3 8B 256 0.2 0.4 1.8\nPromptriever Llama3 8B 512 0.6 1.4 5.4\nPromptriever Llama3 8B 768 1.3 3.1 8.7\nPromptriever Llama3 8B 1024 2.1 4.4 12.8\nPromptriever Llama3 8B 2048 3.2 6.5 18.1\nPromptriever Llama3 8B 3072 2.9 6.3 17.8\nPromptriever Llama3 8B 4096 3.0 6.8 18.9\nQwen3 Embed 32 0.0 0.1 1.1\nQwen3 Embed 64 0.0 0.2 1.0\nQwen3 Embed 128 0.3 0.4 1.8\nQwen3 Embed 256 0.4 0.8 3.2\nQwen3 Embed 512 0.6 1.3 3.3\nQwen3 Embed 768 0.7 1.5 3.8\nQwen3 Embed 1024 0.7 1.6 4.6\nQwen3 Embed 2048 0.9 1.7 4.7\nQwen3 Embed 3072", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_22500", "chunk_text": ".8\nQwen3 Embed 1024 0.7 1.6 4.6\nQwen3 Embed 2048 0.9 1.7 4.7\nQwen3 Embed 3072 0.8 1.6 4.8\nQwen3 Embed 4096 0.8 1.8 4.8\nGemini Embed 2 0.0 0.0 0.1\n\nGemini Embed 4 0.0 0.0 0.0\n\nGemini Embed 8 0.0 0.0 0.0\n\nGemini Embed 16 0.0 0.0 0.0\n\nGemini Embed 32 0.0 0.0 0.0\n\nGemini Embed 64 0.0 0.0 0.3\n\nGemini Embed 128 0.0 0.1 0.3\n\nGemini Embed 256 0.0 0.1 1.2\n\nGemini Embed 512 0.2 1.1 3.6\n\nGemini Embed 768 0.9 2.5 7.6\n\nGemini Embed 1024 1.3 2.7 8.1\n\nGemini Embed 2048 1.5 3.1 8.5\n\nGemini Embed 3072 1.6 3.5 10.0\n\nGTE-ModernColBERT default 23.1 34.6 54.8\n\nBM25 default 85.7 90.4 93.6\n\n\nTable 5 | Results on LIMIT. See comparable Figure 3.\n\n\n\n23\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\n_\ud835\udc51_ Critical- _\ud835\udc5b_\n\n\n4 10\n\n5 14\n\n6 19\n\n7 24\n\n8 28\n\n9 32\n\n10 36\n\n11 42\n\n12 47\n\n13 54\n\n14 62\n\n15 70\n\n16 79\n\n17 89\n\n18 99\n\n19 109\n\n20 120\n\n21 132\n\n22 144\n\n23 157\n\n24 170\n\n25 184\n\n26 198\n\n27 213\n\n28 229\n\n29 ", "token_count": 500, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2508.21038_embedding_limit_weller:chunk_22950", "chunk_text": "17 89\n\n18 99\n\n19 109\n\n20 120\n\n21 132\n\n22 144\n\n23 157\n\n24 170\n\n25 184\n\n26 198\n\n27 213\n\n28 229\n\n29 245\n\n30 261\n\n31 278\n\n32 296\n\n33 314\n\n34 333\n\n35 352\n\n36 372\n\n37 392\n\n38 413\n\n39 434\n\n40 460\n\n41 484\n\n42 505\n\n43 545\n\n44 605\n\n45 626\n\n\nTable 6 | Critical Values of n for different d values in the Free Embedding optimization experiments.\nSee Figure 2 for the corresponding figure.\n\n\nModel BEIR LIMIT R@100\n\n\nSnowflake Arctic 55.22 3.3\nPromptriever 56.40 18.9\nE5-Mistral 57.07 8.3\n\nGritLM 57.40 12.9\n\nGemini Embed 62.65 10.0\n\nQwen3 Embed 62.76 4.8\n\n\nTable 7 | BEIR vs LIMIT results. See Figure 7 for the comparable plot.\n\n\n24\n\n\n", "token_count": 249, "metadata": {"arxiv_id": "2508.21038", "title": "On the Theoretical Limitations of Embedding-Based Retrieval", "authors": ["Orion Weller", "Michael Boratko", "Iftekhar Naim", "Jinhyuk Lee"], "year": 2025, "url": "https://arxiv.org/pdf/2508.21038v1"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_0", "chunk_text": "## **Lost in the Middle: How Language Models Use Long Contexts**\n\n**Nelson F. Liu** [1] _[\u2217]_ **Kevin Lin** [2] **John Hewitt** [1] **Ashwin Paranjape** [3]\n\n**Michele Bevilacqua** [3] **Fabio Petroni** [3] **Percy Liang** [1]\n\n1Stanford University 2University of California, Berkeley 3Samaya AI\n[nfliu@cs.stanford.edu](mailto:nfliu@cs.stanford.edu)\n\n\n\n**Abstract**\n\n\nWhile recent language models have the ability to take long contexts as input, relatively\nlittle is known about how well they _use_\nlonger context. We analyze the performance\n\n   - f language models on two tasks that require\nidentifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that\nperformance can degrade significantly when\nchanging the position of relevant information, indicating that current language models\ndo not robustly make use of information in\nlong input contexts. In particular, we observe\nthat performance is often highest when relevant information occurs at the beginning or\nend of the input context, and significantly\ndegrades when models must access relevant\ninformation in the middle of long contexts,\neven for explicitly long-context models. Our\nanalysis provides a better understanding of\nhow language models use their input context\nand provides new evaluation protocols for\nfuture long-context language models.\n\n\n**1** **Introduction**\n\n\nLanguage models have become an important and\nflexible building block in a variety of user-facing\nlanguage technologies, including conversational\ninterfaces, search and summarization, and collaborative writing (Shuster et al., 2022; Thoppilan et al.,\n2022; Lee et al., 2022, _inter alia_ ). These models\nperform downstream tasks primarily via prompting:\nall relevant task specification and data to process is\nformatted as a textual input context, and the model\nreturns a generated text completion. These input\ncontexts can contain thousands of tokens, especially when language models are used to process\nlong documents (e.g., legal or scientific documents,\nconversation histories, etc.) or when language models are augmented with external information (e.g.,\n\n\n*Work partially completed as an intern at Samaya AI.\n\n\n\n\n|Total|Col2|Ret|rievedDocuments( ~4", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_450", "chunk_text": "., legal or scientific documents,\nconversation histories, etc.) or when language models are augmented with external information (e.g.,\n\n\n*Work partially completed as an intern at Samaya AI.\n\n\n\n\n|Total|Col2|Ret|rievedDocuments( ~4Ktoken|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\n\n\nFigure 1: Changing the location of relevant information\n(in this case, the position of the passage that answers an\ninput question) within the language model\u2019s input context results in a U-shaped performance curve\u2014models\nare better at using relevant information that occurs at the\nvery beginning (primacy bias) or end of its input context\n(recency bias), and performance degrades significantly\nwhen models must access and use information located\n\nin the middle of its input context.\n\n\nrelevant documents from a search engine, database\nquery results, etc; Petroni et al., 2020; Ram et al.,\n2023; Shi et al., 2023; Mallen et al., 2023; Schick\net al., 2023, _inter alia_ ).\n\n\nHandling these use-cases requires language models to successfully operate over long sequences. Existing language models are generally implemented\nwith Transformers (Vaswani et al., 2017), which require memory and compute that increases quadratically in sequence length. As a result, Transformer language models were often trained with\nrelatively small context windows (between 5122048 tokens). Recent improvements in hardware\n(e.g., faster GPUs with more memory) and algorithms (Dai et al., 2019; Dao et al., 2022; Poli et al.,\n\n\n2023; Rubin and Berant, 2023, _inter alia_ ) have\nresulted in language models with larger context\nwindows (e.g., 4096, 32K, and even 100K tokens),\nbut it remains unclear how these extended-context\n\nlanguage models make use of their input contexts\nwhen performing downstream tasks.\n\n\nWe empirically investigate this question via\ncontrolled experiments with a variety of state-ofthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\n", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_900", "chunk_text": " question via\ncontrolled experiments with a variety of state-ofthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\nAnthropic\u2019s Claude-1.3) language models in settings that require accessing and using information\nwithin an input context. In particular, our experiments make controlled changes to the input context\nsize and the position of the relevant information\nwithin the input context and study their effects on\nlanguage model performance. If language models\ncan robustly use information within long input contexts, then their performance should be _minimally_\n_affected_ by the position of the relevant information\nin the input context.\n\n\nWe first experiment with multi-document question answering, which requires models to reason\n\n- ver provided documents to find relevant information and use it to answer a given question; this task\nmimics the retrieval-augmented generation setup\nunderlying many commercial generative search and\nquestion answering applications (e.g., Bing Chat).\nIn this setting, we control (i) the input context\nlength by changing the number of documents in\nthe input context (akin to retrieving more or less\ndocuments in retrieval-augmented generation), and\n(ii) control the position of the relevant information\nwithin the input context by changing the order of\nthe documents to place the relevant document at\nthe beginning, middle or end of the context.\n\n\nWe find that changing the position of relevant\ninformation in the input context can substantially\naffect model performance, indicating that current\nlanguage models do not robustly access and use\ninformation in long input contexts. Furthermore,\nwe observe a distinctive U-shaped performance\ncurve (Figure 1); language model performance is\nhighest when relevant information occurs at the\nvery beginning (primacy bias) or end of its input context (recency bias), and performance significantly degrades when models must access and\nuse information in the middle of their input context (\u00a72.3). For example, when relevant information is placed in the middle of its input context, GPT-3.5-Turbo\u2019s performance on the multi\n\n\ndocument question task is lower than its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_1350", "chunk_text": " its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily\nbetter at using their input context (\u00a72.3).\nGiven that language models struggle to retrieve\nand use relevant information in the multi-document\n\nquestion answering task, to what extent can language models even _retrieve_ from their input contexts? We study this question with a synthetic keyvalue retrieval task, which is designed to be a minimal testbed for the basic ability to retrieve matching\ntokens from the input context. In this task, models\nare given a collection of JSON-formatted key-value\npairs and must return the value associated with a\nspecific key. Similar to the multi-document QA\ntask, the key-value retrieval task admits controlled\nchanges to the input context length (adding more\nkey-value pairs) and the position of relevant information. Although some models perform the\nsynthetic key-value retrieval task perfectly, other\nmodels struggle to simply retrieve matching tokens\nthat occur in the middle of their input context and\ncontinue to exhibit a U-shaped performance curve.\nTo better understand why language models struggle to robustly access and use information in their\ninput contexts, we study the role of model architecture (decoder-only vs. encoder-decoder), queryaware contextualization, and instruction fine-tuning\n(\u00a74). We find that:\n\n\n  - Encoder-decoder models are relatively robust\nto changes in the position of relevant information within their input context, but only when\nevaluated on sequences within its trainingtime sequence length. When evaluated on\nsequences longer than those seen during training, we observe a U-shaped performance\ncurve (\u00a74.1).\n\n\n  - Query-aware contextualization (placing the\nquery before _and_ after the documents or keyvalue pairs) enables near-perfect performance\n\n   - n the synthetic key-value task, but minimally\nchanges trends in multi-document QA (\u00a74.2).\n\n\n  - Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of\nrelevant information in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_1800", "chunk_text": " in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_2250", "chunk_text": "** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f the documents contains the answer\n\n\n[1nelsonliu.me/papers/lost-in-the-middle](https://nelsonliu.me/papers/lost-in-the-middle)\n\n\n\nto the question and _k \u2212_ 1 \u201cdistractor\u201d documents\ndo not. This task requires the model to access the\ndocument that contains the answer within its input\ncontext and use it to answer the question. Figure 2\npresents an example.\n\nWe instantiate this task with data from\n\nNaturalQuestions-Open (Lee et al., 2019;\nKwiatkowski et al., 2019), which contains\nhistorical queries issued to the Google search\nengine, coupled with human-annotated answers\nextracted from Wikipedia. In particular, we take\nthe 2655 queries where the annotated long answer\nis a paragraph (as opposed to a list or a table). We\nuse passages (chunks of at most 100 tokens) from\nWikipedia as documents within our input contexts.\nFor each of the queries, we need a document\nthat contains the answer and _k \u2212_ 1 distractor\n\ndocuments that do not contain the answer. To\n\n- btain a document that answers the question, we\nuse the Wikipedia paragraph that contains the\nanswer from the NaturalQuestions annotations.\n\nTo collect _k \u2212_ 1 distractor documents that do not\n\ncontain the answer, we use a retrieval system (Contriever, fine-tuned on MS-MARCO; Izacard et al.,\n2021) to retrieve the _k \u2212_ 1 Wikipedia chunks that\nare most relevant to the query and do not contain\nany of the NaturalQuestions-annotated answers. [2][,][3]\n\nIn the input context, the distractor documents are\npresented in order of decreasing relevance. [4]\n\nTo modulate the position of relevant information\nwithin the input context, we adjust the order of the\ndocuments to change the position of the document\nthat contains the answer (Figure 3). To modulate\nthe input context length in this task, we increase or\ndecrease the number of retrieved documents that\n\ndo not contain the answer (Figure 4", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_2700", "chunk_text": " change the position of the document\nthat contains the answer (Figure 3). To modulate\nthe input context length in this task, we increase or\ndecrease the number of retrieved documents that\n\ndo not contain the answer (Figure 4).\nFollowing Kandpal et al. (2022) and Mallen et al.\n(2023), we use accuracy as our primary evaluation\nmetric, judging whether any of the correct answers\n(as taken from the NaturalQuestions annotations)\nappear in the predicted output.\n\n\n2Ambiguity in NaturalQuestions-Open means that a small\nnumber of distractor passages may contain a reasonable answer. We additionally run experiments on subset of unambiguous questions, finding similar results and conclusions; see\nAppendix A.\n3We also explored using random documents as distractors,\nsee Appendix B for more details.\n4Since there might be a prior over \u201csearch results\u201d appearing in ranked order, we explored randomly ordering the _k \u2212_ 1\ndistractor documents and mentioning that the documents are\nrandomly ordered in the task description, but found the same\ntrends. See Appendix C for more details.\n\n\nInput Context\n\n\nWrite a high-quality answer for the given question using only the provided search\nresults (some of which might be irrelevant).\n\n\nDocument [1](Title: Asian Americans in science and technology) Prize in physics for\ndiscovery of the subatomic particle J/\u03c8. Subrahmanyan Chandrasekhar shared...\n**Document [2](Title: List of Nobel laureates in Physics) The first Nobel Prize in**\n**Physics was awarded in 1901 to Wilhelm Conrad R\u00f6ntgen, of Germany, who received...**\nDocument [3](Title: Scientist) and pursued through a unique method, was essentially\nin place. Ram\u00f3n y Cajal won the Nobel Prize in 1906 for his remarkable...\n\n\nQuestion: who got the first nobel prize in physics\n\nAnswer:\n\n\nDesired Answer\n\n\nWilhelm Conrad R\u00f6ntgen\n\n\nFigure 2: Example of the multi-document question answering task, with an input context and the desired model\nanswer. The document containing the answer is bolded within the input context here for clarity.\n\n\n\nInput Context\n\n\nWrite a high-quality answer for the given question\nusing only the provided search results (some of\nwhich might be irrelevant).\n\n\n**Document [1](Title: List of Nobel laureates in**\n**Physics) ...**\nDocument [2](Title", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_3150", "chunk_text": "\n\n\nWrite a high-quality answer for the given question\nusing only the provided search results (some of\nwhich might be irrelevant).\n\n\n**Document [1](Title: List of Nobel laureates in**\n**Physics) ...**\nDocument [2](Title: Asian Americans in science and\n\ntechnology) ...\nDocument [3](Title: Scientist) ...\n\n\nQuestion: who got the first nobel prize in physics\n\nAnswer:\n\n\nDesired Answer\n\n\nWilhelm Conrad R\u00f6ntgen\n\n\nFigure 3: Modulating the position of relevant information within the input context for the multi-document\nquestion answering example presented in Figure 2. Re\n- rdering the documents in the input context does not\naffect the desired output.\n\n\nOur experimental setup is similar to the needlein-a-haystack experiments of Ivgi et al. (2023), who\ncompare question answering performance when the\nrelevant paragraph is placed (i) at the beginning of\nthe input or (ii) a random position within the input. They find that encoder-decoder models have\nsignificantly higher performance when relevant information is placed at the start of the input context.\nIn contrast, we study finer-grained changes in the\nposition of relevant information.\n\n\n**2.2** **Models**\n\n\nWe analyze several state-of-the-art open and closed\nlanguage models. We use greedy decoding when\ngenerating outputs and leave exploration of other\ndecoding methods to future work. We use a standard set of prompts for each model (Figure 2).\n\n\n\n**Input Context**\n\n\nWrite a high-quality answer for the given question\nusing only the provided search results (some of\nwhich might be irrelevant).\n\n\nDocument [1](Title: Asian Americans in science and\n\ntechnology) ...\n**Document [2](Title: List of Nobel laureates in**\n**Physics) ...**\nDocument [3](Title: Scientist) ...\n\nDocument [4](Title: Norwegian Americans) ...\nDocument [5](Title: Maria Goeppert Mayer) ...\n\n\nQuestion: who got the first nobel prize in physics\n\nAnswer:\n\n\nDesired Answer\n\n\nWilhelm Conrad R\u00f6ntgen\n\n\nFigure 4: Modulating the input context length of the\nmulti-document question answering example presented\nin Figure 2. Adding documents that do not contain the\nanswer increases the length of the input context, but\ndoes not affect the desired output.\n\n\n**Open models.** We experiment with MPT-30BInstruct, which", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_3600", "chunk_text": " presented\nin Figure 2. Adding documents that do not contain the\nanswer increases the length of the input context, but\ndoes not affect the desired output.\n\n\n**Open models.** We experiment with MPT-30BInstruct, which has a maximum context length of\n8192 tokens. The model was initially pre-trained\n\n- n 1 trillion tokens using 2048-token sequences,\nfollowed by an additional sequence length adaptation pre-training phase on 50 billion tokens using\n8192-token sequences. MPT-30B-Instruct uses ALiBi (Press et al., 2022) to represent positional information. We also evaluate LongChat-13B (16K) (Li\net al., 2023), which extends the LLaMA-13B (Touvron et al., 2023a) context window from 2048 to\n16384 tokens by using condensed rotary positional\nembeddings before fine-tuning with 16384-token\n\nsequences.\n\n\n**Closed models.** We use the OpenAI API to experiment with GPT-3.5-Turbo and GPT-3.5-Turbo\n\n\n|oa ereve|ocumens (~ o e|\n|---|---|\n|||\n|||\n|||\n|||\n|||\n|||\n\n\n|oa e|reve oc|umens|(~ o e|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n\n\n|oa|ereve o|Col3|cumen|s (~ o e|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\nFigure 5: The effect of changing the position of relevant information (document containing the answer) on multidocument question answering performance. Lower positions are closer to the start of the input context. Performance\nis highest when relevant information occurs at the very start or end of the context, and rapidly degrades when models\nmust reason over information in the middle of their input context.\n\n\n\n(16K). [5] GPT-3.5-Turbo has a maximum context\nlength of 4K tokens, and GPT-3.5-Turbo (16K) is a\nversion with an extended maximum context length\n\n- f 16K tokens. We evaluate Claude-1.3 and Claude", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_4050", "chunk_text": " context\nlength of 4K tokens, and GPT-3.5-Turbo (16K) is a\nversion with an extended maximum context length\n\n- f 16K tokens. We evaluate Claude-1.3 and Claude\n1.3 (100K) with the Anthropic API; Claude-1.3\nhas a maximum context length of 8K tokens, and\nClaude-1.3 (100K) has an extended context length\n\n- f 100K tokens. [6]\n\n\n**2.3** **Results and Discussion**\n\n\nWe experiment with input contexts containing 10,\n20, and 30 total documents. Figure 5 presents multidocument question answering performance when\nvarying the position of relevant information within\nthe input context. To contextualize model performance, we also evaluate on the closed-book and\n\n- racle settings (Table 1). In the closed-book setting,\nmodels are not given any documents in their input\ncontext, and must rely on their parametric memory\nto generate the correct answer. On the other hand,\nin the oracle setting, language models are given the\nsingle document that contains the answer and must\nuse it to answer the question.\n\n\n**Model performance is highest when relevant in-**\n**formation occurs at the beginning or end of its**\n**input context.** As illustrated in Figure 5, changing the position of relevant information in the input context leads to substantial decreases in model\nperformance. In particular, we see a distinctive U\n\n5We use the 0613 OpenAI model versions.\n6We also evaluate GPT-4 (8K) on a subset of multidocument QA experiments, finding similar results and trends\nas other models (though GPT-4 has higher absolute performance). Evaluating GPT-4 on the full multi-document QA\nand key-value retrieval experiments would cost upwards of\n$6000. See Appendix D for GPT-4 results and discussion.\n\n\n\nModel Closed-Book Oracle\n\n\nLongChat-13B (16K) 35.0% 83.4%\nMPT-30B-Instruct 31.5% 81.9%\n\nGPT-3.5-Turbo 56.1% 88.3%\n\nGPT-3.5-Turbo (16K) 56.0% 88.6%\n\nClaude-1.3 48.3% 76", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_4500", "chunk_text": "5-Turbo 56.1% 88.3%\n\nGPT-3.5-Turbo (16K) 56.0% 88.6%\n\nClaude-1.3 48.3% 76.1%\n\nClaude-1.3 (100K) 48.2% 76.4%\n\n\nTable 1: Closed-book and oracle accuracy of language\nmodels on the multi-document question answering task.\n\n\nshaped performance curve\u2014models are often much\nbetter at using relevant information that occurs at\nthe very beginning (primacy bias) and very end of\ncontexts (recency bias), and suffer degraded performance when forced to use information within the\n\nmiddle of its input context. For example, GPT-3.5Turbo\u2019s multi-document QA performance can drop\nby more than 20%\u2014in the worst case, performance\nin 20- and 30-document settings is lower than performance without _any_ input documents (i.e., closedbook performance; 56.1%). These results indicate\nthat current models cannot effectively reason over\ntheir entire context window when prompted for\ndownstream tasks.\n\n\n**Extended-context models are not necessarily bet-**\n**ter at using input context.** When the input context fits in the context window of both a model\nand its extended-context counterpart, we see that\nperformance between them is nearly identical. For\nexample, the 10- and 20-document settings both\nfit in the context window of GPT-3.5-Turbo and\nGPT-3.5-Turbo (16K), and we observe that their\nperformance as a function of position of relative\ninformation is nearly superimposed (solid purple\nand dashed brown series in Figure 5). These results\n\n\nInput Context\n\n\nExtract the value corresponding to the specified key in the JSON object below.\n\n\nJSON data:\n\n{\"2a8d601d-1d69-4e64-9f90-8ad825a74195\": \"bb3ba2a5-7de8-434b-a86e-a88bb9fa7289\",\n\n\"a54e2eed-e625-4570-9f74-3624e77d6684\": \"d1ff29be-4e2a-4208-a182-0cea716be3d4\",\n\n\" **9f4a92b9", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_4950", "chunk_text": "-4570-9f74-3624e77d6684\": \"d1ff29be-4e2a-4208-a182-0cea716be3d4\",\n\n\" **9f4a92b9-5f69-4725-ba1e-403f08dea695** \": \"703a7ce5-f17f-4e6d-b895-5836ba5ec71c\",\n\n\"52a9c80c-da51-4fc9-bf70-4a4901bc2ac3\": \"b2f8ea3d-4b1b-49e0-a141-b9823991ebeb\",\n\n\"f4eb1c53-af0a-4dc4-a3a5-c2d50851a178\": \"d733b0d2-6af3-44e1-8592-e5637fdb76fb\"}\n\n\nKey: \" **9f4a92b9-5f69-4725-ba1e-403f08dea695** \"\nCorresponding value:\n\n\nDesired Output\n\n\n703a7ce5-f17f-4e6d-b895-5836ba5ec71c\n\n\nFigure 6: Example of the key-value retrieval task, with an input context and the desired model output. Given a key,\nthe goal is to return the associated value. All keys and values are 128-bit UUIDs. The relevant key-value pair for\nanswering the query is bolded here within the input context for clarity.\n\n\n\nindicate that extended-context models are not nec\nessarily better than their non-extended counterparts\nat using their input context.\n\n\n**3** **How Well Can Language Models**\n**Retrieve From Input Contexts?**\n\n\nGiven that language models struggle to retrieve\nand use information from the middle of their input\ncontexts in the multi-document question answering\ntask, to what extent can they simply _retrieve_ from\ninput contexts? We study this question with a synthetic key-value retrieval task, which is designed to\nprovide a minimal testbed for the basic ability to\nretrieve matching tokens from an input context.\n\n\n**3.1** **Experimental Setup**\n\n\nIn our synthetic key-value retrieval task, the inputs\nare (i) a string-serialized JSON object with _k_ keyvalue pairs, where each of the keys and values are", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_5400", "chunk_text": " input context.\n\n\n**3.1** **Experimental Setup**\n\n\nIn our synthetic key-value retrieval task, the inputs\nare (i) a string-serialized JSON object with _k_ keyvalue pairs, where each of the keys and values are\nunique, randomly-generated UUIDs and (ii) a key\nwithin the aforementioned JSON object. The goal\nis to return the value associated with the specified\nkey. Thus, each JSON object contains one relevant\nkey-value pair (where the value is to be returned),\nand _k \u2212_ 1 irrelevant \u201cdistractor\u201d key-value pairs.\nFigure 6 provides an example input context and its\ncorresponding desired output. We again measure\naccuracy by evaluating whether the correct value\nappears in the predicted output.\nOur synthetic key-value retrieval task shares similar goals with the Little Retrieval Test of Papailiopoulos et al. (2023) and the fine-grained line retrieval task of Li et al. (2023), but we explicitly\nseek to distill and simplify the task by removing as\n\n\n\nmuch natural language semantics as possible (using\nrandom UUIDs instead), since language features\nmay present potential confounders. For example,\nTransformer language models may have varying\nsensitivity to different linguistic features in their\ninput (O\u2019Connor and Andreas, 2021).\nTo modulate the position of relevant information\nwithin the input context, we change the position\n\n- f the key to retrieve within the serialized JSON\n\n- bject. To modulate the input context length, we\nchange the number of input JSON key-value pairs\n_k_ by adding or removing random keys, changing\nthe number of distractor key-value pairs.\n\n\n**3.2** **Results and Discussion**\n\n\nWe experiment with input contexts containing\n75, 140, and 300 key-value pairs (500 examples\neach). We use the same set of models as the multidocument question answering experiments, see\n\u00a72.2 for more details.\nFigure 7 presents key-value retrieval performance. Claude-1.3 and Claude-1.3 (100K) do\nnearly perfectly on all evaluated input context\nlengths, but other models struggle, especially\nwhen contexts have 140 or 300 key-value pairs\u2014\nalthough the synthetic key-value retrieval task only\nrequires identifying exact match within the input\ncontext, not all models achieve high performance.\nSimilar to our multi-document QA results", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_5850", "chunk_text": " struggle, especially\nwhen contexts have 140 or 300 key-value pairs\u2014\nalthough the synthetic key-value retrieval task only\nrequires identifying exact match within the input\ncontext, not all models achieve high performance.\nSimilar to our multi-document QA results, GPT3.5-Turbo, GPT-3.5-Turbo (16K), and MPT-30BInstruct have the lowest performance when they\nmust access key-value pairs in the middle of their\ninput context. LongChat-13B (16K) exhibits a different trend in the 140 key-value setting; we qualitatively observe that when relevant information is\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n||||\n||||\n||||\n||||\n||||\n||||\n\n\n|Col1|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n\n\n\nFigure 7: The effect of changing the input context length and the position of relevant information on key-value\nretrieval performance. Lower positions are closer to the start of the input context. Although some models show\nperfect accuracy on this synthetic task (e.g., Claude-1.3 and Claude-1.3 (100K)), we see again that performance is\n\n- ften highest when relevant information is occurs at the very start or end of the context, and rapidly degrades when\nmodels must retrieve from the middle of the input context.\n\n\n\nplaced at the start of the input context, LongChat13B (16K) tends to generate code to retrieve the\nkey, rather than outputting the value directly.\n\n\n**4** **Why Are Language Models Not Robust**\n**to Changes in the Position of Relevant**\n**Information?**\n\n\nOur multi-document question answering and keyvalue retrieval results show that language models\nstruggle to robustly access and use information in\nlong input contexts, since performance degrades\nsignificantly when changing the position of relevant information. To better understand why, we perform some preliminary investigations into the role\n\n- f model architecture (decoder-only vs. encoderdecoder), query-aware contextualization, and instruction fine-tuning.\n\n\n**4.1** **Effect", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_6300", "chunk_text": " the position of relevant information. To better understand why, we perform some preliminary investigations into the role\n\n- f model architecture (decoder-only vs. encoderdecoder), query-aware contextualization, and instruction fine-tuning.\n\n\n**4.1** **Effect of Model Architecture**\n\n\nThe open models we evaluated are all decoder-only\nmodels\u2014at each timestep, they may only attend\nto prior tokens. To better understand the potential effects of model architecture on how language\nmodel use context, we compare decoder-only and\nencoder-decoder language models.\nWe experiment with Flan-T5-XXL (Raffel et al.,\n2020; Chung et al., 2022) and Flan-UL2 (Tay et al.,\n2023). Flan-T5-XXL is trained with a sequences\n\n- f 512 tokens (encoder and decoder). Flan-UL2 is\ninitially trained with sequences of 512 tokens (encoder and decoder), but is then pre-trained for an\nextra 100K steps with 1024 tokens (encoder and decoder) before instruction fine-tuning on sequences\nwith 2048 tokens in the encoder and 512 tokens\n\nin the decoder. However, since these models use\n\n\n\nrelative positional embeddings, they can (in principle) extrapolate beyond these maximum context\nlengths; Shaham et al. (2023) find that both models can perform well with sequences of up to 8K\ntokens.\n\nFigure 8 compares the performance of decoder\n- nly and encoder-decoder models. When Flan-UL2\nis evaluated on sequences within its 2048-token\ntraining-time context window (Figure 8; left subplot), its performance is relatively robust to changes\nin the position of relevant information within the\ninput context (1.9% absolute difference between\nbest- and worst-case performance). When evaluated on settings with sequences longer than 2048\ntokens (Figure 8; center and right), Flan-UL2 performance begins to degrade when relevant information is placed in the middle. Flan-T5-XXL shows\na similar trend, where longer input contexts result\nin a greater performance degradation when placing\nrelevant information in the middle of the input context. We hypothesize that encoder-decoder models\nmay make better use of their context windows because their bidirectional encoder allows processing\neach document in the context of future documents,\npotentially improving relative importance estimation between documents.\n\n\n", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_6750", "chunk_text": " the middle of the input context. We hypothesize that encoder-decoder models\nmay make better use of their context windows because their bidirectional encoder allows processing\neach document in the context of future documents,\npotentially improving relative importance estimation between documents.\n\n\n**4.2** **Effect of Query-Aware Contextualization**\n\n\nOur multi-document QA and key-value retrieval\nexperiments place the query (i.e., question to answer or key to retrieve) after the data to process\n(i.e., the documents or the key-value pairs). As a\nresult, decoder-only models cannot attend to query\ntokens when contextualizing documents or keyvalue pairs, since the query only appears at the end\n\n\n|reve o|cumens (~|\n|---|---|\n|||\n|||\n|||\n|||\n|||\n\n\n|a ereve o|Col2|cumen|s (~ o e|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n\n\n\nFigure 8: When encoder-decoder models (Flan-UL2 and Flan-T5-XXL) evaluated on sequences that are _shorter_\nthan their encoder\u2019s training-time maximum sequence length (2048 and 512 tokens, respectively), they are relatively\nrobust to changes in the position of relevant information within their input context (left subplot). In contrast, when\nthese models are evaluated on sequences _longer_ than those seen during training (center and right subplots), we\n\n- bserve a U-shaped performance curve\u2014performance is higher when relevant information occurs at the beginning\n\n- r end of the input context, as opposed to the middle of the input context.\n\n\n\n\n|20 Total Retrieved Documen|Col2|\n|---|---|\n|20 Total Retrieved Docume<br>|20 Total Retrieved Docume<br>|\n|tokens, que|ry~~-~~aware context|\n|||\n|||\n|||\n|||\n\n\n\n\n\n\n\nFigure 9: Query-aware contextualization (placing the\nquery before _and_ after the documents) does not substantially improve robustness of language models to\nchanging the position of relevant information in multidocument QA; performance slightly increases when\nrelevant information occurs at the very beginning, but\n\n- therwise slightly decreases.\n\n\n- f the prompt and decoder-only models can only\nattend to prior tokens at each timestep. In contrast,\nencoder-decoder models (which seem more robust\nto changes in the", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_7200", "chunk_text": " occurs at the very beginning, but\n\n- therwise slightly decreases.\n\n\n- f the prompt and decoder-only models can only\nattend to prior tokens at each timestep. In contrast,\nencoder-decoder models (which seem more robust\nto changes in the position of relevant information;\n\u00a74.1) use a bidirectional encoder to contextualize\ninput contexts\u2014can we use this observation to improve decoder-only models by placing the query before _and_ after the data, enabling query-aware contextualization of documents (or key-value pairs)?\n\nWe find that query-aware contextualization dramatically improves performance on the key-value\nretrieval task\u2014all models achieve near-perfect per\n\n\nformance on the 75, 140, and 300 key-value pair\nsettings. For example, GPT-3.5-Turbo (16K) with\nquery-aware contextualization achieves perfect performance when evaluated with 300 key-value pairs.\nIn contrast, without query-aware contextualization, the worst-case performance is 45.6% (Figure 7). Despite the significant impact on keyvalue retrieval performance, query-aware contextualization minimally affects performance trends in\nthe multi-document question answering task (Figure 9); it slightly improves performance when the\nrelevant information is located at the very beginning of the input context, but slightly decreases\nperformance in other settings.\n\n\n**4.3** **Effect of Instruction Fine-Tuning**\n\n\nThe models we evaluated are all instruction finetuned\u2014after their initial pre-training, they undergo\nsupervised fine-tuning on a dataset of instructions\nand responses. The task specification and/or instruction is commonly placed at the beginning of\nthe input context in supervised instruction finetuning data, which might lead instruction finetuned language models to place more weight on\nthe start of the input context. To better understand\nthe potential effects of instruction fine-tuning on\nhow language models use long input contexts, we\ncompare the multi-document question answering\nperformance of MPT-30B-Instruct against its base\nmodel (i.e., before instruction fine-tuning) MPT30B. We use the same experimental setup as \u00a72.\nFigure 10 compares the multi-document QA\nperformance of MPT-30B and MPT-30B-Instruct\nas a function of the position of the relevant in\n\n|Tota|Col2|lRet|rievedDocuments(~4Ktoken|Col5|Col6|Col7|\n|---", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_7650", "chunk_text": "30B and MPT-30B-Instruct\nas a function of the position of the relevant in\n\n|Tota|Col2|lRet|rievedDocuments(~4Ktoken|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\n\n\nFigure 10: Multi-document QA performance of MPT30B-Instruct compared against its base model (i.e., before instruction fine-tuning) MPT-30B. Both models\nhave a U-shaped performance curve, where performance\nis much higher when relevant information occurs at the\nstart or end of the input context, indicating that the\ninstruction fine-tuning process itself is not necessarily\nresponsible for these performance trends.\n\n\nformation in the input context. Surprisingly, we\nsee that both MPT-30B and MPT-30B-Instruct ex\nhibit a U-shaped performance curve, where performance is highest when relevant information occurs\nat the very beginning or very end of the context.\nAlthough the absolute performance of MPT-30BInstruct is uniformly higher than that of MPT-30B,\ntheir overall performance trends are similar. We\nalso observe that instruction fine-tuning slightly reduces the worst-case performance disparity from\nnearly 10% between the base model best- and\nworst-case performance to around 4%.\n\nThese observations complement prior work,\nwhich found that non-instruction fine-tuned language models are biased towards recent tokens (i.e.,\nthe end of the input context; Khandelwal et al.,\n2018; Press et al., 2021). This recency bias has\nbeen observed in past work when evaluating models on next-word prediction of contiguous text, a\nsetting where language models minimally benefit\nfrom long-range information (Sun et al., 2021). In\ncontrast, our results show that language models\nare capable of using longer-range information (i.e.,\nthe beginning of the input context) when prompted\nwith instruction-formatted data. We hypothesize\nthat non-instruction fine-tuned language models\nlearn to use these long contexts from similarlyformatted data that may occur in Internet text seen\nduring pre-training, e.g., StackOverflow questions\n\n\n\nand answers.\n\nTo better understand the effect of additional finetuning and model scale, we also experimented\n", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_8100", "chunk_text": "learn to use these long contexts from similarlyformatted data that may occur in Internet text seen\nduring pre-training, e.g., StackOverflow questions\n\n\n\nand answers.\n\nTo better understand the effect of additional finetuning and model scale, we also experimented\nwith Llama-2 models of varying sizes (7B, 13B,\nand 70B) with and without additional supervised\nfine-tuning and reinforcement learning from human feedback (Appendix E). We find that the Ushaped performance curve only appears in sufficiently large language models (with or without additional fine-tuning)\u2014the 7B Llama-2 models are\nsolely recency biased, while the 13B and 70B models exhibit a U-shaped performance curve. In addition, we see that the Llama-2 supervised fine-tuning\nand reinforcement learning from human feedback\nprocedure slightly mitigates the positional bias in\nsmaller models (13B, akin to trends shown when\ncomparing MPT-30B and MPT-30B-Instruct), but\nminimally affects trends on larger models (70B).\n\n\n**5** **Is More Context Is Always Better?**\n**A Case Study With Open-Domain QA**\n\n\nOur results indicate that prompting language models with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing\naccuracy. Even if a language model can take in\n16K tokens, is it actually beneficial to provide 16K\ntokens of context? The answer to this question\nis ultimately downstream task-specific since it depends on the marginal value of the added context\nand the model\u2019s ability to effectively use long input\ncontexts, but we perform a case study with opendomain question answering on NaturalQuestionsOpen to better understand this trade-off in existing\nlanguage models.\nWe use language models in a standard retrieverreader setup. A retrieval system (Contriever, finetuned on MS-MARCO) takes an input query from\nNaturalQuestions-Open and returns the _k_ documents from Wikipedia with the highest relevance\nscore. To condition language models on these retrieved documents, we simply include them in the\nprompt. We evaluate retriever recall and reader\naccuracy (whether any of the annotated answers\nappear in the predicted output) as a function of the\nnumber of", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_8550", "chunk_text": ". To condition language models on these retrieved documents, we simply include them in the\nprompt. We evaluate retriever recall and reader\naccuracy (whether any of the annotated answers\nappear in the predicted output) as a function of the\nnumber of retrieved documents _k_ . We use a subset\n\n- f NaturalQuestions-Open where the long answer\nis a paragraph (as opposed to a table or a list).\nFigure 11 presents retriever recall and open\n\nFigure 11: Retriever recall and model performance as a\nfunction of the number of retrieved documents. Model\n\nperformance saturates long before retriever recall, indicating that the models have difficulty making use of the\nextra retrieved documents.\n\n\ndomain QA results. We see that reader model\nperformance saturates long before retriever performance saturates, indicating that readers are not\neffectively using the extra context. Using more\nthan 20 retrieved documents only marginally improves reader performance ( _\u223c_ 1.5% for GPT-3.5Turbo and _\u223c_ 1% for Claude-1.3), while significantly\nincreasing the input context length (and thus latency and cost). These results, coupled with the\n\n- bservation that models are often better at retriev\ning and using information at the start or end of\nthe input contexts, suggest that effective reranking of retrieved documents (pushing relevant information closer to the start of the input context) or\nranked list truncation (retrieving fewer documents\nwhen appropriate; Arampatzis et al., 2009) may be\npromising directions for improving how languagemodel-based readers use retrieved context.\n\n\n**6** **Related Work**\n\n\n**6.1** **Long-Context Language Models**\n\n\nThere is much prior work in designing performant\nlanguage models with cheaper scaling than Transformers in the context length. Many lines of work\npursue Transformer variants with attention modifications like recurrence (Dai et al., 2019), factorizing attention into computationally less intensive\napproximations (Beltagy et al., 2020; Zaheer et al.,\n2020), or low-rank approximations (Wang et al.,\n2020; Peng et al., 2021). Dao et al. (2022) instead provide a faster exact attention by a carefully\n\n\ncrafted IO-aware CUDA kernel. Separately, there\nare attempts to do away with", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_9000", "chunk_text": "ang et al.,\n2020; Peng et al., 2021). Dao et al. (2022) instead provide a faster exact attention by a carefully\n\n\ncrafted IO-aware CUDA kernel. Separately, there\nare attempts to do away with attention entirely to\nremove quadratic sequence length complexity, often through convolution and/or linear RNNs, e.g.,\nin RWKV (Peng, 2023), S4 (Gu et al., 2022), or\nHyena (Poli et al., 2023). Many prior efforts evaluate perplexity on a diverse web corpus as a proxy\nfor the ability to process long contexts; this work\nshows that precise knowledge access on long contexts may be an added challenge.\n\n\n**6.2** **How Do Language Models Use Context?**\n\n\nThe pioneering work of Khandelwal et al. (2018)\nshowed that small LSTM language models make\nincreasingly coarse use of longer-term context;\nSankar et al. (2019) found similar results in dialogue models. In a similar vein, Daniluk et al.\n(2017) find that attentive LSTM language models tend to mainly use recent history. Petroni\net al. (2020) were among the first to demonstrate\nthe potential of combining context from an information retrieval system with a pretrained language models for unsupervised question answering.\nO\u2019Connor and Andreas (2021) found that many\ninformation-destroying operations had marginal effects on Transformer LMs\u2019 predictions. Krishna\net al. (2022) found that long-context neural generation in modestly-sized Transformer language\nmodels degenerates because models fail to properly condition on long context. Finally, studying\nlong-context models, Sun et al. (2021) found that\nlonger contexts improves prediction of only a few\ntokens, an empirical finding consistent with the\ntheory of Sharan et al. (2018), who showed that\nsequence distributions with bounded mutual information necessarily lead to marginal _average_ prediction benefits from increasingly long context. Qin\net al. (2023) analyze how efficient Transformers\nperform on a variety of long-context downstream\nNLP tasks, finding that long-context transformers\nare recency-biased and do not effectively use long\nrange context.\n\n\n**6.3** **The Serial-Position Effect**\n\n\nThe U-shaped curve we observe in this work has\na connection in psychology known as the", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_9450", "chunk_text": "-context transformers\nare recency-biased and do not effectively use long\nrange context.\n\n\n**6.3** **The Serial-Position Effect**\n\n\nThe U-shaped curve we observe in this work has\na connection in psychology known as the _serial-_\n_position effect_ (Ebbinghaus, 1913; Murdock Jr,\n1962), that states that in free-association recall\n\n- f elements from a list, humans tend to best remember the first and last elements of the list. The\nserial-position effect plays a role in understanding\nhow humans develop short- and long-term mem\n\n- ry. Observing a serial-position-like effect in language models is perhaps surprising, since the selfattention mechanisms underlying Transformer language models is technically equally capable of retrieving any token from their contexts.\n\n\n**7** **Conclusion**\n\n\nWe empirically study how language models use\nlong input contexts via a series of controlled experiments. We show that language model performance degrades significantly when changing the\nposition of relevant information, indicating that\nmodels struggle to robustly access and use information in long input contexts. In particular, performance is often lowest when models must use\n\ninformation in the middle of long input contexts.\nWe conduct a preliminary investigation of the role\n\n- f (i) model architecture, (ii) query-aware contextualization, and (iii) instruction fine-tuning to better\nunderstand how they affect how language models\nuse context. Finally, we conclude with a practical case study of open-domain question answering,\nfinding that the performance of language model\nreaders saturates far before retriever recall. Our\n\nresults and analysis provide a better understanding\n\n- f how language models use their input context\nand provides new evaluation protocols for future\nlong-context models.\n\n\n**Acknowledgments**\n\n\nWe would like to thank Luke Zettlemoyer, who\nserved as our TACL action editor, and the the\nanonymous reviewers for their comments and feedback. We also thank Claudiu Leoveanu-Condrei,\nMegan Leszczynski, Dmytro Okhonko, Maithra\nRaghu, Eric Wallace and Sang Michael Xie for\nfeedback and discussions that helped improve this\nwork. Further, we are grateful to Sewon Min for\nher help with the AmbigQA dataset. This work\nwas supported by the Stanford Center for Research\n\n- n Foundation Models (CRFM), by OpenAI via\nan API credits", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_9900", "chunk_text": "work. Further, we are grateful to Sewon Min for\nher help with the AmbigQA dataset. This work\nwas supported by the Stanford Center for Research\n\n- n Foundation Models (CRFM), by OpenAI via\nan API credits grant to the Stanford CRFM, and\nby Anthropic via the Claude academic access pro\ngram.\n\n\n**References**\n\n\nAvi Arampatzis, Jaap Kamps, and Stephen Robertson. 2009. Where to stop reading a ranked list?\nthreshold optimization using truncated score distributions. In _Proc. of SIGIR_ .\n\n\n\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer. ArXiv:2004.05150.\n\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha\nBrahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen,\nAakanksha Chowdhery, Alex Castro-Ros, Marie\nPellat, Kevin Robinson, Dasha Valter, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent\nZhao, Yanping Huang, Andrew Dai, Hongkun\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob\nDevlin, Adam Roberts, Denny Zhou, Quoc V.\nLe, and Jason Wei. 2022. Scaling instructionfinetuned language models. ArXiv:2210.11416.\n\n\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime\nCarbonell, Quoc Le, and Ruslan Salakhutdinov.\n2019. Transformer-XL: Attentive language models beyond a fixed-length context. In _Proc. of_\n\n_ACL_ .\n\n\nMicha\u0142 Daniluk, Tim Rockt\u00e4schel, Johannes Welbl,\nand Sebastian Riedel. 2017. Frustratingly short\nattention spans in neural language modeling. In\n_Proc. of ICLR_ .\n\n\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. FlashAttention: Fast\nand memory-efficient exact attention with IO", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_10350", "chunk_text": ". In\n_Proc. of ICLR_ .\n\n\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. FlashAttention: Fast\nand memory-efficient exact attention with IOawareness. ArXiv:2205.14135.\n\n\nHermann Ebbinghaus. 1913. Memory: A contribution to experimental psychology. _H. A. Ruger &_\n_C. E. Bussenius, Trans._\n\n\nAlbert Gu, Karan Goel, and Christopher R\u00e9. 2022.\nEfficiently modeling long sequences with structured state spaces. In _Proc. of ICLR_ .\n\n\nMaor Ivgi, Uri Shaham, and Jonathan Berant. 2023.\nEfficient long-text understanding with short-text\nmodels. _Transactions of the Association for_\n_Computational Linguistics_, 11:284\u2013299.\n\n\nGautier Izacard, Mathilde Caron, Lucas Hosseini,\nSebastian Riedel, Piotr Bojanowski, Armand\nJoulin, and Edouard Grave. 2021. Unsupervised\ndense information retrieval with contrastive\n\nlearning. ArXiv:2112.09118.\n\n\nGautier Izacard and Edouard Grave. 2021. Lever\naging passage retrieval with generative models\n\n\nfor open domain question answering. In _Proc._\n\n_of EACL_ .\n\n\nNikhil Kandpal, Haikang Deng, Adam Roberts,\nEric Wallace, and Colin Raffel. 2022. Large language models struggle to learn long-tail knowledge. ArXiv:2211.08411.\n\n\nUrvashi Khandelwal, He He, Peng Qi, and Dan\nJurafsky. 2018. Sharp nearby, fuzzy far away:\nHow neural language models use context. In\n_Proc. of ACL_ .\n\n\nKalpesh Krishna, Yapei Chang, John Wieting, and\nMohit Iyyer. 2022. RankGen: Improving text\ngeneration with large ranking models. In _Proc._\n\n_of EMNLP_ .\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia\nRedfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob\nDevlin, Kenton Lee, Kristina", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_10800", "chunk_text": " Kwiatkowski, Jennimaria Palomaki, Olivia\nRedfield, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob\nDevlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and\nSlav Petrov. 2019. Natural Questions: A benchmark for question answering research. _Trans-_\n_actions of the Association for Computational_\n_Linguistics_, 7:452\u2013466.\n\n\nKenton Lee, Ming-Wei Chang, and Kristina\nToutanova. 2019. Latent retrieval for weakly\nsupervised open domain question answering. In\n_Proc. of ACL_ .\n\n\nMina Lee, Percy Liang, and Qian Yang. 2022.\nCoAuthor: Designing a human-AI collaborative\nwriting dataset for exploring language model capabilities. In _Proc. of CHI_ .\n\n\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng,\nLianmin Zheng, Joseph E. Gonzalez, Ion Stoica,\n[Xuezhe Ma,, and Hao Zhang. 2023. How long](https://lmsys.org/blog/2023-06-29-longchat)\n[can open-source LLMs truly promise on context](https://lmsys.org/blog/2023-06-29-longchat)\n[length?](https://lmsys.org/blog/2023-06-29-longchat)\n\n\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Daniel Khashabi, and Hannaneh Hajishirzi.\n2023. When not to trust language models: Investigating effectiveness of parametric and nonparametric memories. In _Proc. of ACL_ .\n\n\nSewon Min, Julian Michael, Hannaneh Hajishirzi,\nand Luke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In\n_Proc. of EMNLP_ .\n\n\n\nBennet B. Murdock Jr. 1962. The serial position\neffect of free recall. _Journal of experimental_\n_psychology_, 64(5):482.\n\n\nJoe O\u2019Connor and Jacob Andreas. 2021. What con\ntext", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_11250", "chunk_text": ". Murdock Jr. 1962. The serial position\neffect of free recall. _Journal of experimental_\n_psychology_, 64(5):482.\n\n\nJoe O\u2019Connor and Jacob Andreas. 2021. What con\ntext features can Transformer language models\nuse? In _Proc. of ACL_ .\n\n\nDimitris Papailiopoulos, Kangwook Lee, and Jyyong Sohn. 2023. A little retrieval test for\n[large language models. https://github.com/](https://github.com/anadim/the-little-retrieval-test)\n\n[anadim/the-little-retrieval-test.](https://github.com/anadim/the-little-retrieval-test)\n\n\n[Bo Peng. 2023. RWKV-LM. https://github.](https://github.com/BlinkDL/RWKV-LM)\n\n[com/BlinkDL/RWKV-LM.](https://github.com/BlinkDL/RWKV-LM)\n\n\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong.\n2021. Random feature attention. In _Proc. of_\n\n_ICLR_ .\n\n\nFabio Petroni, Patrick Lewis, Aleksandra Piktus,\nTim Rockt\u00e4schel, Yuxiang Wu, Alexander H\nMiller, and Sebastian Riedel. 2020. How context\naffects language models\u2019 factual predictions. In\n_Proc. of AKBC_ .\n\n\nMichael Poli, Stefano Massaroli, Eric Nguyen,\nDaniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua\nBengio, Stefano Ermon, and Christopher R\u00e9.\n2023. Hyena hierarchy: Towards larger convolutional language models. In _Proc. of ICML_ .\n\n\nOfir Press, Noah A. Smith, and Mike Lewis. 2021.\nShortformer: Better language modeling using\nshorter inputs. In _Proc. of ACL_ .\n\n\nOfir Press, Noah A. Smith, and Mike Lewis. 2022.\nTrain short, test long: Attention with linear biases enables input length extrapolation. In _Proc._\n\n_of ICLR_ .\n\n\nGuanghui Qin, Yukun Feng, and Benjamin\nVan Durme. 2023. The NLP task effectiveness\n\n - f long-range transformers. In _", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_12150", "chunk_text": "Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. REPLUG:\nRetrieval-augmented black-box language models. ArXiv:2301.12652.\n\n\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan\nUng, Moya Chen, Kushal Arora, Joshua Lane,\nMorteza Behrooz, William Ngan, Spencer Poff,\nNaman Goyal, Arthur Szlam, Y-Lan Boureau,\nMelanie Kambadur, and Jason Weston. 2022.\nBlenderBot 3: a deployed conversational agent\nthat continually learns to responsibly engage.\nArXiv:2208.03188.\n\n\nSimeng Sun, Kalpesh Krishna, Andrew MattarellaMicke, and Mohit Iyyer. 2021. Do long-range\nlanguage models actually use long-range context? In _Proc. of EMNLP_ .\n\n\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier\nGarcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Siamak Shakeri, Dara Bahri, Tal\n\n\n\nSchuster, Huaixiu Steven Zheng, Denny Zhou,\nNeil Houlsby, and Donald Metzler. 2023.\nUL2: Unifying language learning paradigms.\nArXiv:2205.05131.\n\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, HengTze Cheng, Alicia Jin, Taylor Bos, Leslie\nBaker, Yu Du, YaGuang Li, Hongrae Lee,\nHuaixiu Steven Zheng, Amin Ghafouri, Marcelo\nMenegali, Yanping Huang, Maxim Krikun,\nDmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou,\nChung-Ching Chang, Igor Krivokon, Will Rusch,\nMarc Pickett, Pranesh Srinivasan, Laichee Man,\nKath", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_12600", "chunk_text": " Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou,\nChung-Ching Chang, Igor Krivokon, Will Rusch,\nMarc Pickett, Pranesh Srinivasan, Laichee Man,\nKathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju\nDuke, Johnny Soraker, Ben Zevenbergen, Vin\n - dkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin\nHoffman-John, Josh Lee, Lora Aroyo, Ravi\nRajakumar, Alena Butryna, Matthew Lamm,\nViktoriya Kuzmina, Joe Fenton, Aaron Cohen,\nRachel Bernstein, Ray Kurzweil, Blaise AgueraArcas, Claire Cui, Marian Croak, Ed Chi, and\nQuoc Le. 2022. LaMDA: Language models for\ndialog applications. ArXiv:2201.08239.\n\n\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman\nGoyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. 2023a. LLaMA:\nOpen and efficient foundation language models.\nArXiv:2302.13971.\n\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen,\nGuillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,\nHakan Inan, Marcin Kardas, Viktor Kerkez,\n\nMadian Khabsa, Isabel Kloumann, Artem Korenev, Punit", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_13050", "chunk_text": "oyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,\nHakan Inan, Marcin Kardas, Viktor Kerkez,\n\nMadian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich,\nYinghai Lu, Yuning Mao, Xavier Martinet, Todor\nMihaylov, Pushkar Mishra, Igor Molybog, Yixin\n\n\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi\nRungta, Kalyan Saladi, Alan Schelten, Ruan\nSilva, Eric Michael Smith, Ranjan Subramanian,\nXiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,\nAurelien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. 2023b. Llama\n2: Open foundation and fine-tuned chat models.\nArXiv:2307.09288.\n\n\nAshish Vaswani, Noam Shazeer, Niki Parmar,\n\nJakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n\n\u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _Proc. of NeurIPS_ .\n\n\nSinong Wang, Belinda Z. Li, Madian Khabsa,\nHan Fang, and Hao Ma. 2020. Linformer: Self-attention with linear complexity.\nArXiv:2006.04768.\n\n\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan\nWang, Li Yang, and Amr Ahmed. 2020. Big\nBird: Transformers for longer sequences. In\n_Proc. of NeurIPS_ .\n\n\n**A** **Ambiguity in Multi-Document QA**\n**Distractor Documents**\n\n\nFollowing past work on NaturalQuestions-Open\n(Izacard et", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_13500", "chunk_text": " Transformers for longer sequences. In\n_Proc. of NeurIPS_ .\n\n\n**A** **Ambiguity in Multi-Document QA**\n**Distractor Documents**\n\n\nFollowing past work on NaturalQuestions-Open\n(Izacard et al., 2021; Izacard and Grave, 2021, _inter_\n_alia_ ), we use a Wikipedia dump from late 2018\nas our retrieval corpus. However, this standard\nWikipedia dump has a small amount of temporal\nmismatch with the NaturalQuestions annotations.\n\nFor example, consider the question \u201cwhat nfl\nteam does robert griffin iii play for\u201d. The NaturalQuestions annotated answer is \u201ccurrently a free\nagent\u201d. However, the Wikipedia retrieval corpus\ncontains the information that he plays for the \u201cBaltimore Ravens\u201d, since he was released from the team\nbetween the Wikipedia dump\u2019s timestamp and the\nNaturalQuestions annotation process.\nWe use the ambiguity annotations of Min et al.\n(2020) to create a subset unambiguous questions.\nExperiments on this unambiguous subset of the\ndata show similar results and conclusions as the\n\nexperiments on the full questions collection (Figure 12).\n\n\n|(~4K tokens unambi<br>,|Col2|Col3|Col4|guousques|Col6|tions)|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\nFigure 12: Language model performance on a unambiguous subset of questions.\n\n\n**B** **Random Distractors in**\n\n**Multi-Document QA**\n\n\nWe also run multi-document question answering\nexperiments with random Wikipedia documents as\ndistractors, which allows us to ablate the impact\n\n- f retrieved distractors (hard negatives). Note that\nin this setting, the the document containing the answer can often be identified with simple heuristics\n(e.g., lexical overlap with the query). Figure 13\npresents the results of this experiment. Although\nall models have higher absolute accuracy in this\nsetting, they surprisingly still struggle to reason\n\n- ver their entire input context, indicating that their\nperformance degradation is not solely due to an\ninability to identify relevant documents.\n\n\n**C** **Randomizing Distractor Order in**\n**Multi-Document QA**\n\n\nOur prompt instructs the language model to use\nthe provided search results to answer the question.\nThere may", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_13950", "chunk_text": " to an\ninability to identify relevant documents.\n\n\n**C** **Randomizing Distractor Order in**\n**Multi-Document QA**\n\n\nOur prompt instructs the language model to use\nthe provided search results to answer the question.\nThere may be a prior in the pre-training or instruction fine-tuning data to treat search results as sorted\nby decreasing relevance (i.e., the documents near\nthe beginning of the input context are more likely to\nbe useful than those at the end). To validate that our\nconclusions are not simply a byproduct of this bias,\nwe run experiments with the modified instruction\n\u201cWrite a high-quality answer for the given question using only the provided search results (some\n\n- f which might be irrelevant). The search results\nare ordered randomly.\u201d In addition, we randomly\nshuffle the _k \u2212_ 1 distractor documents.\n\n\n\n\n\n\n\n\n\n\n|20 Total Retr (~4K tokens,|Col2|Col3|rieved Docu random dis|Col5|umen stract|nts tors)|\n|---|---|---|---|---|---|---|\n|(~4K tokens,|(~4K tokens,|(~4K tokens,|random dis|random dis|tract|ors)|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\n\n\n\n|(~4K tokens<br>,|Col2|randomly|Col4|order|ed)|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n\n\n\nFigure 13: Language model performance on multidocument QA when using random distractors, rather\nthan retrieved distractors.\n\n\nFigure 14 presents the results of this experiment.\nWe continue to see a U-shaped performance curve,\nwith performance degrading when language models must use information in the middle of their\n\ninput contexts. Comparing the results in \u00a72.3 with\nthose when randomizing the distractor order and\nmentioning such in the prompt, we see that randomization slightly decreases performance when\nthe relevant information is at the very beginning\n\n- f the context, and slightly increases performance\nwhen using information in the middle and end of\nthe context.\n\n\n**D** **GPT-4 Performance**\n\n\nWe evaluate GPT-4 (8K) on a subset of 500 random multi-document QA examples with 20 total\ndocuments in each input", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_14400", "chunk_text": " the middle and end of\nthe context.\n\n\n**D** **GPT-4 Performance**\n\n\nWe evaluate GPT-4 (8K) on a subset of 500 random multi-document QA examples with 20 total\ndocuments in each input context (Figure 15). GPT4 achieves higher absolute performance than any\n\n- ther language model, but still shows a U-shaped\nperformance curve\u2014its performance is highest\nwhen relevant information occurs at the very start\n\n- r end of the context, and performance degrades\nwhen it must use information in the middle of its\n\ninput context.\n\n\n**E** **Llama-2 Performance**\n\n\nWe evaluate Llama-2 (Touvron et al., 2023b) on\nmulti-document QA with 20 total documents in\neach input context. The Llama tokenizer produces longer sequences than the tokenizers for our\npreviously-studied models, so we discard 20 exam\n\n\n\n\nFigure 14: Language model performance when randomizing the order of the distractors (rather than presenting\nthem in order of decreasing relevance) and mentioning\nas such in the prompt.\n\n\n\n\n\n\n|(~4K tokens 500 questi<br>,|Col2|Col3|onsampl|e)|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n||||||\n\n\n\n\n\n\n\nFigure 15: Although GPT-4 has higher absolute performance than other models, its performance still degrades\nwhen relevant information occurs in the middle of the\n\ninput context.\n\n\nples (out of 2655) that exceed Llama-2\u2019s maximum\ncontext length of 4096 tokens. We experiment with\nmodels of varying sizes (7B, 13B, and 70B parameters), with and without additional supervised\nfine-tuning and reinforcement learning from human feedback (\u201c-chat-\u201d models). The results are\npresented in Figure 16.\nComparing Llama-2 models of varying sizes, we\nfind that only the larger models (13B and 70B)\nexhibit the U-shaped performance curve (i.e., both\nprimacy and recency bias)\u2014the smallest Llama2 models (7B) are solely recency-biased. Given\nthese results, we hypothesize that prior work (e.g.,\nKhandelwal et al., 2018; Sun et al., 2021) did not\npre", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_14850", "chunk_text": " (7B) are solely recency-biased. Given\nthese results, we hypothesize that prior work (e.g.,\nKhandelwal et al., 2018; Sun et al., 2021) did not\npreviously observe any primacy bias in language\nmodels because the models they studied were too\nsmall (less than 1B parameters).\nComparing between Llama-2 models with and\nwithout additional supervised fine-tuning and reinforcement learning from human feedback, we\nsee that additional fine-tuning dramatically improves performance on the multi-document QA\ntask. The 7B models with and without additional\n\nfine-tuning show minimal primacy bias, and are\nlargely recency-biased. The 13B base model has\na dramatic primacy and recency bias\u2014there is a\n20-point accuracy disparity between the best- and\nworst-case performance. Applying additional finetuning to the 13B seems to slightly reduce this\nbias (10-point worst-case degradation), but the bias\nremains significant. However, the 70B models\nwith and without additional fine-tuning have largely\nsimilar trends (showing both primacy and recency\nbias), and additional fine-tuning minimally changes\nthe positional bias severity.\n\n\n|0 Tota|Col2|al Retr|rieved Documents (~4K token|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|Tota|Tota|l Ret|rieved Documents(~4K token|rieved Documents(~4K token|rieved Documents(~4K token|rieved Documents(~4K token|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\nFigure 16: Multi-document QA performance (20 total\ndocuments) of Llama-2 models of varying sizes (7B,\n13B, 70B parameters), with and without additional supervised fine-tuning and reinforcement learning from\nhuman feedback (\u201c-chat-\u201d models).\n\n\n\n\n\n\n\n\n**F** **Token Counts**\n\n\nTable 2, Table 3, and Table 4 present the average and maximum number of tokens in each of the input\ncontexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer,\nGPT-3.5-Turbo and GPT-3.5-Turbo", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_15300", "chunk_text": " each of the input\ncontexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer,\nGPT-3.5-Turbo and GPT-3.5-Turbo (16K) use the same tokenizer, and Claude-1.3 and Claude-1.3 (100K)\nuse the same tokenizer. Furthermore, the Claude-1.3 tokenizer is the same as the GPT-3.5-Turbo tokenizer,\nmodulo some additional special tokens that do not appear in our data. As a result, the token counts for\nthese two model families is the same in our experimental settings.\n\n\nClosed-Book Oracle\n\n\navg _\u00b1_ stdev max avg _\u00b1_ stdev max\n\n\nLongChat-13B (16K) 55.6 _\u00b1_ 2.7 70 219.7 _\u00b1_ 48.5 588\nMPT-30B 43.5 _\u00b1_ 2.2 58 187.9 _\u00b1_ 41.8 482\n\nGPT-3.5-Turbo 15.3 _\u00b1_ 2.2 29 156.0 _\u00b1_ 41.8 449\n\nClaude-1.3 15.3 _\u00b1_ 2.2 29 156.0 _\u00b1_ 41.8 449\n\n\nTable 2: Token count statistics for each of the evaluated models on the closed-book and oracle multi-document\n\nquestion answering settings.\n\n\n10 docs 20 docs 30 docs\n\n\navg _\u00b1_ stdev max avg _\u00b1_ stdev max avg _\u00b1_ stdev max\n\n\nLongChat-13B (16K) 1749.9 _\u00b1_ 112.4 2511 3464.6 _\u00b1_ 202.3 4955 5181.9 _\u00b1_ 294.7 7729\nMPT-30B 1499.7 _\u00b1_ 88.5 1907 2962.4 _\u00b1_ 158.4 3730 4426.9 _\u00b1_ 230.5 5475\n\nGPT-3.5-Turbo 1475.6 _\u00b1_ 86.5 1960 2946.2 _\u00b1_ 155.1 3920 441", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_15750", "chunk_text": "_ 230.5 5475\n\nGPT-3.5-Turbo 1475.6 _\u00b1_ 86.5 1960 2946.2 _\u00b1_ 155.1 3920 4419.2 _\u00b1_ 226.5 6101\n\nClaude-1.3 1475.6 _\u00b1_ 86.5 1960 2946.2 _\u00b1_ 155.1 3920 4419.2 _\u00b1_ 226.5 6101\n\n\nTable 3: Token count statistics for each of the evaluated models on each of the document question answering\nsettings.\n\n\n75 KV pairs 140 KV pairs 300 KV pairs\n\n\navg _\u00b1_ stdev max avg _\u00b1_ stdev max avg _\u00b1_ stdev max\n\n\nLongChat-13B (16K) 5444.5 _\u00b1_ 19.1 5500 10072.4 _\u00b1_ 24.1 10139 21467.3 _\u00b1_ 35.9 21582\nMPT-30B 4110.5 _\u00b1_ 23.8 4187 7600.9 _\u00b1_ 31.1 7687 16192.4 _\u00b1_ 46.6 16319\n\nGPT-3.5-Turbo 3768.7 _\u00b1_ 25.6 3844 6992.8 _\u00b1_ 34.1 7088 14929.4 _\u00b1_ 50.7 15048\n\nClaude-1.3 3768.7 _\u00b1_ 25.6 3844 6992.8 _\u00b1_ 34.1 7088 14929.4 _\u00b1_ 50.7 15048\n\n\nTable 4: Token count statistics for each of the evaluated models on each of the key-value (KV) retrieval settings.\n\n\n**G** **Full Multi-Document Question Answering Results**\n\n\nThis section tabulates model performance when evaluated on the multi-document QA task with varying\nnumbers of documents (Figure 5). \u201cIndex _n_ \u201d indicates performance when the document with the answer\n\n- ccurs at position _n_ + 1, where lower indices are closer to the start of the input context. For example,\nindex 0 refers to performance when", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_16200", "chunk_text": " \u201cIndex _n_ \u201d indicates performance when the document with the answer\n\n- ccurs at position _n_ + 1, where lower indices are closer to the start of the input context. For example,\nindex 0 refers to performance when the document with the answer is placed at the very start of the context\n(i.e., first amongst all documents).\n\n\n**G.1** **10 Total Retrieved Documents**\n\n\nModel Index 0 Index 4 Index 9\n\n\nClaude-1.3 62.9% 58.3% 59.7%\n\nClaude-1.3 (100K) 63.1% 58.3% 59.7%\n\nGPT-3.5-Turbo 76.8% 61.2% 62.4%\n\nGPT-3.5-Turbo (16K) 76.9% 61.0% 62.5%\n\nMPT-30B-Instruct 60.2% 56.2% 59.7%\n\nLongChat-13B (16K) 72.1% 58.9% 58.5%\n\n\nTable 5: Model performance when evaluated on the multi-document QA task with 10 total retrieved documents.\n\n\n**G.2** **20 Total Retrieved Documents**\n\n\nModel Index 0 Index 4 Index 9 Index 14 Index 19\n\n\nClaude-1.3 59.9% 55.9% 56.8% 57.2% 60.1%\n\nClaude-1.3 (100K) 59.8% 55.9% 57.0% 57.4% 60.0%\n\nGPT-3.5-Turbo 75.8% 57.2% 53.8% 55.4% 63.2%\n\nGPT-3.5-Turbo (16K) 75.7% 57.3% 54.1% 55.4% 63.1%\n\nMPT-30B-Instruct 53.7% 51.8% 52.2% 52.7% 56.3%\n\nLongChat-13B (16K) 68.6% 57.4% 55.3% 52.5% 55.0%\n\n\nTable 6:", "token_count": 500, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2307.03172_lost_in_middle_liu:chunk_16650", "chunk_text": "% 52.7% 56.3%\n\nLongChat-13B (16K) 68.6% 57.4% 55.3% 52.5% 55.0%\n\n\nTable 6: Model performance when evaluated on the multi-document QA task with 20 total retrieved documents.\n\n\n**G.3** **30 Total Retrieved Documents**\n\n\nModel Index 0 Index 4 Index 9 Index 14 Index 19 Index 24 Index 29\n\n\nClaude-1.3 59.1% 55.1% 54.8% 55.7% 56.4% 56.2% 59.9%\n\nClaude-1.3 (100K) 59.1% 55.1% 54.9% 55.7% 56.6% 56.1% 60.0%\nGPT-3.5-Turbo (16K) 73.4% 55.1% 50.5% 50.9% 51.8% 54.9% 63.7%\n\nMPT-30B-Instruct 51.6% 51.3% 51.2% 49.0% 49.6% 51.3% 54.1%\n\nLongChat-13B (16K) 66.9% 54.8% 52.5% 52.9% 52.2% 51.3% 55.1%\n\n\nTable 7: Model performance when evaluated on the multi-document QA task with 30 total retrieved documents.\n\n\n", "token_count": 344, "metadata": {"arxiv_id": "2307.03172", "title": "Lost in the Middle: How Language Models Use Long Contexts", "authors": ["Nelson F. Liu", "Kevin Lin", "John Hewitt", "Ashwin Paranjape", "Michele Bevilacqua", "Fabio Petroni", "Percy Liang"], "year": 2023, "url": "https://arxiv.org/pdf/2307.03172v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_0", "chunk_text": "## **Contrastive Learning for Unpaired** **Image-to-Image Translation**\n\nTaesung Park [1] Alexei A. Efros [1] Richard Zhang [2] Jun-Yan Zhu [2]\n\n\nUniversity of California, Berkeley [1] Adobe Research [2]\n\n\n**Abstract.** In image-to-image translation, each patch in the output\nshould reflect the _content_    - f the corresponding patch in the input, independent of domain. We propose a straightforward method for doing\nso \u2013 maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements\n(corresponding patches) to map to a similar point in a learned feature\nspace, relative to other elements (other patches) in the dataset, referred\nto as negatives. We explore several critical design choices for making\ncontrastive learning effective in the image synthesis setting. Notably, we\nuse a multilayer, patch-based approach, rather than operate on entire\nimages. Furthermore, we draw negatives from _within_ the input image\nitself, rather than from the rest of the dataset. We demonstrate that our\nframework enables one-sided translation in the unpaired image-to-image\ntranslation setting, while improving quality and reducing training time.\nIn addition, our method can even be extended to the training setting\nwhere each \u201cdomain\u201d is only a single image.\n\n\n**Keywords:** contrastive learning, noise contrastive estimation, mutual\ninformation, image generation\n\n\n**1** **Introduction**\n\n\nConsider the image-to-image translation problem in Figure 1. We wish for the\n\n- utput to take on the _appearance_ - f the target domain (a zebra), while retaining\nthe structure, or _content_, of the specific input horse. This is, fundamentally, a\ndisentanglement problem: separating the content, which needs to be preserved\nacross domains, from appearance, which must change. Typically, target appearance is enforced using an adversarial loss [21,31], while content is preserved\nusing cycle-consistency [89,81,37]. However, cycle-consistency assumes that the\nrelationship between the two domains is a bijection, which is often too restrictive.\nIn this paper, we propose an alternative, rather straightforward way of maintaining correspondence in content but not appearance \u2013 by maximizing the mutual\ninformation between corresponding input and output patches.\nIn a successful result, given a specific patch on the output,", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_450", "chunk_text": " too restrictive.\nIn this paper, we propose an alternative, rather straightforward way of maintaining correspondence in content but not appearance \u2013 by maximizing the mutual\ninformation between corresponding input and output patches.\nIn a successful result, given a specific patch on the output, for example,\nthe generated zebra forehead highlighted in blue, one should have a good idea\nthat it came from the horse forehead, and not the other parts of the horse or\n\n\n2 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\n\n\n\n\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n||'||\n||||\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||'|()||*|\n||'||||\n\n\n\n\n\n\n\nFig. 1: **Patchwise Contrastive Learning for one-sided translation.** A generated\n\n**output patch** should appear closer to its **corresponding input patch**, in comparison\nto other **random patches** . We use a multilayer, patchwise contrastive loss, which\nmaximizes _mutual information_ between corresponding input and output patches. This\nenables one-sided translation in the unpaired setting.\n\n\nthe background vegetation. We achieve this by using a type of contrastive loss\nfunction, InfoNCE loss [57], which aims to learn an embedding or an encoder\nthat _associates_ corresponding patches to each other, while _disassociating_ them\nfrom others. To do so, the encoder learns to pay attention to the commonalities\nbetween the two domains, such as object parts and shapes, while being invariant\nto the differences, such as the textures of the animals. The two networks, the\ngenerator and encoder, conspire together to generate an image such that patches\ncan be easily traceable to the input.\n\n\nContrastive learning has been an effective tool in unsupervised visual representation learning [9,24,57,80]. In this work, we demonstrate its effectiveness in a\nconditional image synthesis setting and systematically study several key factors\nto make it successful. We find it pertinent to use it on a multilayer, patchwise\nfashion. In addition, we find that drawing negatives _internally_ from within the\ninput image, rather than externally from other images in the dataset, forces the\npatches to better preserve the content of the input.", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_900", "chunk_text": "ayer, patchwise\nfashion. In addition, we find that drawing negatives _internally_ from within the\ninput image, rather than externally from other images in the dataset, forces the\npatches to better preserve the content of the input. Our method requires neither\nmemory bank [80,24] nor specialized architectures [25,3].\n\n\nExtensive experiments show that our faster, lighter model outperforms both\nprior one-sided translation methods [4,18] and state-of-the-art models that rely\n\n- n several auxiliary networks and multiple loss functions. Furthermore, since our\ncontrastive representation is formulated within the same image, our method can\n[even be trained on single images. Our code and models are available at GitHub.](https://github.com/taesungp/contrastive-unpaired-translation)\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 3\n\n\n**2** **Related Work**\n\n\n**Image translation and cycle-consistency.** Paired image-to-image translation [31] maps an image from input to output domain using an adversarial\nloss [21], in conjunction with a reconstruction loss between the result and target.\nIn unpaired translation settings, corresponding examples from domains are not\navailable. In such cases, _cycle-consistency_ has become the de facto method for\nenforcing correspondence [89,81,37], which learns an inverse mapping from the\n\n - utput domain back to the input and checks if the input can be reconstructed.\nAlternatively, UNIT [44] and MUNIT [30] propose to learn a shared intermediate\n\u201ccontent\u201d latent space. Recent works further enable multiple domains and multimodal synthesis [10,90,1,41,45] and improve the quality of results [72,88,20,79,43].\nIn all of the above examples, cycle-consistency is used, often in multiple aspects,\nbetween (a) two image domains [37,81,89] (b) image to latent [10,30,41,44,90], or\n(c) latent to image [30,90]. While effective, the underlying bijective assumption\nbehind cycle-consistency is sometimes too restrictive. Perfect reconstruction is\ndifficult to achieve, especially when images from one domain have additional\ninformation compared to the other domain.\n\n\n**Relationship preservation.** An interesting alternative approach is to encourage relationships present in the input be analogously reflected in the output. For", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_1350", "chunk_text": " reconstruction is\ndifficult to achieve, especially when images from one domain have additional\ninformation compared to the other domain.\n\n\n**Relationship preservation.** An interesting alternative approach is to encourage relationships present in the input be analogously reflected in the output. For\nexample, perceptually similar patches _within_ an input image should be similar\nin the output [88], output and input images share similar content regarding a\npre-defined distance [5,68,71], vector arithmetic between input images is preserved using a margin-based triplet loss [2], distances _between_ input images\nshould be consistent in output images [4], the network should be equivariant to\ngeometric transformations [18]. Among them, TraVeLGAN [2], DistanceGAN [4]\nand GcGAN [18] enable one-way translation and bypass cycle-consistency. However, they rely on relationship between entire images, or often with predefined\ndistance functions. Here we seek to replace cycle-consistency by instead learning\na cross-domain similarity function _between input and output patches_ through\ninformation maximization, without relying on a pre-specified distance.\n\n\n**Emergent perceptual similarity in deep network embeddings.** Defining\na \u201cperceptual\u201d distance function between high-dimensional signals, e.g., images,\nhas been a longstanding problem in computer vision and image processing. The\nmajority of image translation work mentioned uses a per-pixel reconstruction\nmetric, such as _\u2113_ 1. Such metrics do not reflect human perceptual preferences\nand can lead to blurry results. Recently, the deep learning community has found\nthat the VGG classification network [69] trained on ImageNet dataset [14] can\nbe re-purposed as a \u201cperceptual loss\u201d [16,19,34,75,87,52], which can be used in\npaired image translation tasks [8,59,77], and was known to outperform traditional\nmetrics such as SSIM [78] and FSIM [84] on human perceptual tests [87]. In\nparticular, the Contextual Loss [52] boosts the perceptual quality of pretrained\nVGG features, validated by human perceptual judgments [51]. In these cases,\nthe frozen network weights cannot adapt to the data on hand. Furthermore, the\nfrozen similarity function may not be appropriate when comparing data _across_\n\n\n4 Taesung Park, Alexei A. Efros,", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_1800", "chunk_text": "]. In these cases,\nthe frozen network weights cannot adapt to the data on hand. Furthermore, the\nfrozen similarity function may not be appropriate when comparing data _across_\n\n\n4 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\ntwo domains, depending on the pairing. By posing our constraint via mutual\ninformation, our method makes use of negative samples from the data, allowing\nthe cross-domain similarity function to adapt to the particular input and output\ndomains, and bypass using a pre-defined similarity function.\n\n\n**Contrastive representation learning.** Traditional unsupervised learning has\nsought to learn a compressed code which can effectively reconstruct the input [27].\nData imputation \u2013 holding one subset of raw data to predict from another \u2013 has\nemerged as a more effective family of pretext tasks, including denoising [76],\ncontext prediction [15,60], colorization [40,85], cross-channel encoding [86], frame\nprediction [46,55], and multi-sensory prediction [56,58]. However, such methods\nsuffer from the same issue as before \u2014 the need for a pre-specified, hand-designed\nloss function to measure predictive performance.\nRecently, a family of methods based on _maximizing mutual information_ has\nemerged to bypass the above issue [9,24,25,28,47,54,57,73,80]. These methods\nmake use of noise contrastive estimation [23], learning an embedding where\nassociated signals are brought together, in _contrast_ to other samples in the dataset\n(note that similar ideas go back to classic work on metric learning with Siamese\nnets [12]). Associated signals can be an image with itself [49,67,17,24,80], an\nimage with its downstream representation [28,47], neighboring patches within an\nimage [33,25,57], or multiple views of the input image [73], and most successfully,\nan image with a set of transformed versions of itself [9,54]. The design choices\n\n- f the InfoNCE loss, such as the number of negatives and how to sample them,\nhyperparameter settings, and data augmentations all play a critical role and need\nto be carefully studied. We are the first to use InfoNCE loss for the conditional\nimage synthesis tasks. As such, we draw on these important insights, and find\nadditional pertinent", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_2250", "chunk_text": " and data augmentations all play a critical role and need\nto be carefully studied. We are the first to use InfoNCE loss for the conditional\nimage synthesis tasks. As such, we draw on these important insights, and find\nadditional pertinent factors, unique to image synthesis.\n\n\n**3** **Methods**\n\n\nWe wish to translate images from input domain _X \u2282_ R _[H][\u00d7][W][ \u00d7][C]_ to appear like\nan image from the output domain _Y \u2282_ R _[H][\u00d7][W][ \u00d7]_ [3] . We are given a dataset of\nunpaired instances _X_ = _{_ _**x**_ _\u2208X}, Y_ = _{_ _**y**_ _\u2208Y}_ . Our method can operate even\nwhen _X_ and _Y_ - nly contain a single image each.\nOur method only requires learning the mapping in one direction and avoids\nusing inverse auxiliary generators and discriminators. This can largely simplify the\ntraining procedure and reduce training time. We break up our generator function\n_G_ into two components, an encoder _G_ enc followed by a decoder _G_ dec, which are\napplied sequentially to produce output image \u02c6 _**y**_ = _G_ ( _**z**_ ) = _G_ dec( _G_ enc( _**x**_ )).\n\n\n**Adversarial loss.** We use an adversarial loss [21], to encourage the output to\nbe visually similar to images from the target domain, as follows:\n\n\n_L_ GAN( _G, D, X, Y_ ) = E _**y**_ _\u223cY_ log _D_ ( _**y**_ ) + E _**x**_ _\u223cX_ log(1 _\u2212_ _D_ ( _G_ ( _**x**_ ))) _._ (1)\n\n\n**Mutual information maximization.** We use a noise contrastive estimation\nframework [57] to maximize mutual information between input and output. The\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 5\n\n\n\nFeature Sample **positive**\nextraction + **N negatives**\n\n\n\nCompute\nsimilarities to **query**\n\n\n\n(N+1)-way\nclassification\n\n\nSoftmax\n\ncross-entropy\n\n\n\n\n\n\n./\n\n\n\n\n\n\n|Col1|'+<br>()*", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_2700", "chunk_text": "5\n\n\n\nFeature Sample **positive**\nextraction + **N negatives**\n\n\n\nCompute\nsimilarities to **query**\n\n\n\n(N+1)-way\nclassification\n\n\nSoftmax\n\ncross-entropy\n\n\n\n\n\n\n./\n\n\n\n\n\n\n|Col1|'+<br>()*|Col3|Col4|Col5|\n|---|---|---|---|---|\n||'()*<br>+|'()*<br>+|'()*<br>+|*|\n||'|()<br>+|||\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|||+|\n\n\n|Col1|Col2|Col3|\n|---|---|---|\n|||+|\n||||\n\n\n|Col1|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||'|()<br>+||*|\n||'||||\n\n\n\nEncoder MLP\n\n\n**Patchwise Contrastive Loss**\n\n\nFig. 2: **Patchwise Contrastive Loss.** Both images, _**x**_ and \u02c6 _**y**_, are encoded into feature\ntensor. We sample a **query** patch from the output \u02c6 _**y**_ and compare it to the input patch\nat the same location. We set up an (N+1)-way classification problem, where _N_ negative\npatches are sampled from the same input image at different locations. We reuse the\nencoder part _G_ enc of our generator and add a two-layer MLP network. This network\nlearns to project both the input and output patch to a shared embedding space.\n\n\nidea of contrastive learning is to associate two signals, a \u201cquery\u201d and its \u201cpositive\u201d\nexample, in contrast to other points within the dataset, referred to as \u201cnegatives\u201d.\nThe query, positive, and _N_ negatives are mapped to _K_ - dimensional vectors\n_**v**_ _,_ _**v**_ **[+]** _\u2208_ R _[K]_ and _**v**_ _**[\u2212]**_ _\u2208_ R _[N]_ _[\u00d7][K]_, respectively. _**v**_ _n_ _**[\u2212]**_ _[\u2208]_ [R] _[K]_ [ denotes the n-th negative.]\nWe normalize vectors onto a unit sphere to prevent the space from collapsing or", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_3150", "chunk_text": " _**v**_ _n_ _**[\u2212]**_ _[\u2208]_ [R] _[K]_ [ denotes the n-th negative.]\nWe normalize vectors onto a unit sphere to prevent the space from collapsing or\nexpanding. An ( _N_ + 1)\u2013way classification problem is set up, where the distances\nbetween the query and other examples are scaled by a temperature _\u03c4_ = 0 _._ 07 and\npassed as logits [80,24]. The cross-entropy loss is calculated, representing the\nprobability of the positive example being selected over negatives.\n\n\n\n\n\n\n\n_\u2113_ ( _**v**_ _,_ _**v**_ **[+]** _,_ _**v**_ _**[\u2212]**_ ) = _\u2212_ log\n\n\n\nexp( _**v**_ _\u00b7_ _**v**_ **[+]** _/\u03c4_ )\n\n- exp( _**v**_ _\u00b7_ _**v**_ **[+]** _/\u03c4_ ) + ~~[\ufffd]~~ _[N]_ _n_ =1 [exp(] _**[v]**_ _[ \u00b7]_ _**[ v]**_ _n_ _**[\u2212]**_ _[/\u03c4]_ [)]\n\n\n\n_._ (2)\n\n\n\nOur goal is to associate the input and output data. In our context, query refers\nto an output. positive and negatives are corresponding and noncorresponding\ninput. Below, we explore several important design choices, including how to map\nthe images into vectors and how to sample the negatives.\n\n\n**Multilayer, patchwise contrastive learning.** In the unsupervised learning\nsetting, contrastive learning has been used both on an image and patch level [3,25].\nFor our application, we note that not only should the whole images share content,\n\n\n6 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\nbut also corresponding patches between the input and output images. For example,\ngiven a patch showing the legs of an output zebra, one should be able to more\nstrongly associate it to the corresponding legs of the input horse, more so than\nthe other patches of the horse image. Even at the pixel level, the colors of a zebra\nbody (black and white) can be more strongly associated to the color of a horse", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_3600", "chunk_text": " the corresponding legs of the input horse, more so than\nthe other patches of the horse image. Even at the pixel level, the colors of a zebra\nbody (black and white) can be more strongly associated to the color of a horse\nbody than to the background shades of grass. Thus, we employ a _multilayer,_\n_patch-based_ learning objective.\nSince the encoder _G_ enc is computed to produce the image translation, its\nfeature stack is readily available, and we take advantage. Each layer and spatial\nlocation within this feature stack represents a patch of the input image, with\ndeeper layers corresponding to bigger patches. We select _L_ layers of interest and\npass the feature maps through a small two-layer MLP network _Hl_, as used in\nSimCLR [9], producing a stack of features _{_ _**z**_ _l}L_ = _{Hl_ ( _G_ _[l]_ enc [(] _**[x]**_ [))] _[}][L]_ [, where] _[ G][l]_ enc\nrepresents the output of the _l_ - th chosen layer. We index into layers _l \u2208{_ 1 _,_ 2 _, ..., L}_\nand denote _s \u2208{_ 1 _, ..., Sl}_, where _Sl_ is the number of spatial locations in each\nlayer. We refer to the corresponding feature as _**z**_ _l_ _[s]_ _[\u2208]_ [R] _[C][l]_ [ and the other features]\nas _**z**_ _l_ _[S][\\][s]_ _\u2208_ R [(] _[S][l][\u2212]_ [1)] _[\u00d7][C][l]_, where _Cl_ is the number of channels at each layer. Similarly,\nwe encode the output image \u02c6 _**y**_ into _{_ _**z**_ \u02c6 _l}L_ = _{Hl_ ( _G_ _[l]_ enc [(] _[G]_ [(] _**[x]**_ [)))] _[}][L]_ [.]\nWe aim to match corresponding input-output patches at a specific location.\nWe can leverage the other patches _within_ the input as negatives. For example,\na zebra leg should", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_4050", "chunk_text": "x]**_ [)))] _[}][L]_ [.]\nWe aim to match corresponding input-output patches at a specific location.\nWe can leverage the other patches _within_ the input as negatives. For example,\na zebra leg should be more closely associated with an input horse leg than the\n\n- ther patches of the same input, such as other horse parts or the background\nsky and vegetation. We name it as the _PatchNCE_ loss, as illustrated in Figure 2.\nAppendix C.3 provides pseudocode.\n\n\n\n_L_ PatchNCE( _G, H, X_ ) = E _**x**_ _\u223cX_\n\n\n\n_L_ _Sl_\n\n- - _\u2113_ (\u02c6 _**z**_ _l_ _[s][,]_ _**[ z]**_ _l_ _[s][,]_ _**[ z]**_ _l_ _[S][\\][s]_ ) _._ (3)\n\n_l_ =1 _s_ =1\n\n\n\nAlternatively, we can also leverage image patches from the rest of the dataset.\nWe encode a random negative image from the dataset \u02dc _**x**_ into _{_ _**z**_ \u02dc _l}L_, and use\nthe following _external_ NCE loss. In this variant, we maintain a large, consistent\ndictionary of negatives using an auxiliary moving-averaged encoder, following\nMoCo [24]. MoCo enables negatives to be sampled from a longer history, and\nperforms more effective than end-to-end updates [57,25] and memory bank [80].\n\n\n\n_Sl_\n\n- _\u2113_ (\u02c6 _**z**_ _l_ _[s][,]_ _**[ z]**_ _l_ _[s][,]_ [ \u02dc] _**[z]**_ _[l]_ [)] _[,]_ (4)\n\n\n_s_ =1\n\n\n\n_L_ external( _G, H, X_ ) = E _**x**_ _\u223cX,_ _**z**_ \u02dc _\u223cZ\u2212_\n\n\n\n_L_\n\n\n\n_l_ =1\n\n\n\nwhere dataset negatives \u02dc _**z**_ _l_ are sampled from an external dictionary _Z_ _[\u2212]_ from the\nsource domain, whose data are computed using a", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_4500", "chunk_text": "_\n\n\n\n_L_\n\n\n\n_l_ =1\n\n\n\nwhere dataset negatives \u02dc _**z**_ _l_ are sampled from an external dictionary _Z_ _[\u2212]_ from the\nsource domain, whose data are computed using a moving-averaged encoder _H_ [\u02c6] _l_\nand moving-averaged MLP _H_ [\u02c6] . We refer our readers to the original work for more\ndetails [24].\nIn Section 4.1, we show that our encoder _G_ enc learns to capture domaininvariant concepts, such as animal body, grass, and sky for horse _\u2192_ zebra, while\n\n- ur decoder _G_ dec learns to synthesize domain-specific features such as zebra\nstripes. Interestingly, through systematic evaluations, we find that using internal\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 7\n\n\npatches only outperforms using external patches. We hypothesize that by using\ninternal statistics, our encoder does not need to model large intra-class variation\nsuch as white horse vs. brown horse, which is not necessary for generating output\nzebras. Single image internal statistics has been proven effective in many vision\ntasks such as segmentation [32], super-resolution, and denoising [91,66].\n\n\n**Final objective.** Our final objective is as follows. The generated image should\nbe realistic, while patches in the input and output images should share correspondence. Figure 1 illustrates our minimax learning objective. Additionally, we\nmay utilize PatchNCE loss _L_ PatchNCE( _G, H, Y_ ) on images from domain _Y_ to\nprevent the generator from making unnecessary changes. This loss is essentially a\nlearnable, domain-specific version of the identity loss, commonly used by previous\nunpaired translation methods [71,89].\n\n\n_L_ GAN( _G, D, X, Y_ ) + _\u03bbX_ _L_ PatchNCE( _G, H, X_ ) + _\u03bbY L_ PatchNCE( _G, H, Y_ ) _._ (5)\n\n\nWe choose _\u03bbX_ = 1 when we jointly train with the identity loss _\u03bbY_ = 1, and\nchoose a larger value _\u03bbX_ = 10 without the identity loss ( _\u03bbY_ = 0) to compensate\nfor the absence", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_4950", "chunk_text": " 1 when we jointly train with the identity loss _\u03bbY_ = 1, and\nchoose a larger value _\u03bbX_ = 10 without the identity loss ( _\u03bbY_ = 0) to compensate\nfor the absence of the regularizer. We find that the former configuration, named\n_Contrastive Unpaired Translation (CUT)_ hereafter, achieves superior performance\nto existing methods, whereas the latter, named _FastCUT_, can be thought as a\nfaster and lighter version of CycleGAN. Our model is relatively simple compared\nto recent methods that often use 5-10 losses and hyper-parameters.\n\n\n**Discussion.** Li et al. [42] has shown that cycle-consistency loss is the upper\nbound of conditional entropy H( _X|Y_ ) (and H( _Y |X_ )). Therefore, minimizing\ncycle-consistency loss encourages the output \u02c6 _**y**_ to be more dependent on input _**x**_ .\nThis is related to our objective of maximizing the mutual information I( _X, Y_ ), as\nI( _X, Y_ ) = H( _X_ ) _\u2212_ H( _X|Y_ ). As entropy H( _X_ ) is a constant and independent of\nthe generator _G_, maximizing mutual information is equivalent to minimizing the\nconditional entropy. Notably, using contrastive learning, we can achieve a similar\ngoal without introducing inverse mapping networks and additional discriminators.\nIn the unconditional modeling scenario, InfoGAN [7] shows that simple losses\n(e.g., L2 or cross-entropy) can serve as a lower bound for maximizing mutual\ninformation between an image and a low-dimensional code. In our setting, we\nmaximize the mutual information between two high-dimensional image spaces,\nwhere simple losses are no longer effective. Liang et al. [43] proposes an adversarial\nloss based on Siamese networks that encourages the output to be closer to the\ntarget domain than to its source domain. The above method still builds on\ncycle-consistency and two-way translations. Different from the above work, we\nuse contrastive learning to enforce content consistency, rather than to improve\nthe adversarial loss itself. To measure the similarity between two distributions,\nthe Contextual Loss [52] used softmax over cosine disntances of features extracted\nfrom pre-trained networks. In contrast", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_5400", "chunk_text": " learning to enforce content consistency, rather than to improve\nthe adversarial loss itself. To measure the similarity between two distributions,\nthe Contextual Loss [52] used softmax over cosine disntances of features extracted\nfrom pre-trained networks. In contrast, we learn the encoder with the NCE loss\nto associate the input and output patches at the same location.\n\n\n8 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\n\n\n|Input|CUT FastCUT CycleGAN MUNIT DRIT Self-Distance DistanceGAN GcGAN|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n||||||||||\n||||||||||\n||||||||||\n||||||||||\n||||||||||\n||||||||||\n||||||||||\n||||||||||\n\n\n\nFig. 3: **Results** . We compare our methods (CUT and FastCUT) with existing methods\n\n- n the horse _\u2192_ zebra, cat _\u2192_ dog, and Cityscapes datasets. CycleGAN [89], MUNIT [44],\nand DRIT [41], are two-sided methods, while SelfDistance, DistanceGAN [4], and\nGcGAN [18] are one-sided. We show successful cases above the dotted lines. Our full\nversion CUT is able to add the zebra texture to the horse bodies. Our fast variant\n\nFastCUT can also generate competitive results at the least computational cost of\ntraining. The final rows show failure cases. In the first, we are unable to identify the\nunfamiliar pose of the horse and instead add texture to the background. In the second,\nthe method hallucinates a tongue.\n\n\n**4** **Experiments**\n\n\nWe test across several datasets. We first show that our method improves upon\nbaselines in unpaired image translation. We then show that our method can\nextend to _single-image_ [training. Full results are available at our website.](https://taesungp.github.io/ContrastiveUnpairedTranslation)\n\n\n**Training details.** We follow the setting of CycleGAN [89], except that the _\u2113_ 1\ncycle-consistency loss is replaced with our contrastive loss. In detail, we used\nLSGAN [50] and Resnet-based generator [34] with PatchGAN [31]. We", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_5850", "chunk_text": "89], except that the _\u2113_ 1\ncycle-consistency loss is replaced with our contrastive loss. In detail, we used\nLSGAN [50] and Resnet-based generator [34] with PatchGAN [31]. We define our\nencoder as the first half of the generator, and accordingly extract our multilayer\nfeatures from five evenly distributed points of the encoder. For single image\ntranslation, we use a StyelGAN2-based generator [36]. To embed the encoder\u2019s\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 9\n\n\n**Cityscapes** **Cat** _\u2192_ **Dog** **Horse** _\u2192_ **Zebra**\n**Method**\n\n**mAP** _\u2191_ **pixAcc** _\u2191_ **classAcc** _\u2191_ **FID** _\u2193_ **FID** _\u2193_ **FID** _\u2193_ **sec/iter** _\u2193_ **Mem(GB)** _\u2193_\n\n\nCycleGAN [89] 20.4 55.9 25.4 76.3 85.9 77.2 0.40 4.81\nMUNIT [44] 16.9 56.5 22.5 91.4 104.4 133.8 0.39 3.84\nDRIT [41] 17.0 58.7 22.2 155.3 123.4 140.0 0.70 4.85\nDistance [4] 8.4 42.2 12.6 81.8 155.3 72.0 **0.15** 2.72\nSelfDistance [4] 15.3 56.9 20.6 78.8 144.4 80.8 0.16 2.72\nGCGAN [18] 21.2 63.2 26.6 105.2 96.6 86.7 0.26 2.67\nCUT **24.7** **68.8** **30.7** **56.4** **76.2** **45.5** 0.24 3.33\n\nFastCUT 19.1 59.9 24.3 68.8 94.0 ", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_6300", "chunk_text": "7** **56.4** **76.2** **45.5** 0.24 3.33\n\nFastCUT 19.1 59.9 24.3 68.8 94.0 73.4 **0.15** **2.25**\n\n\nTable 1: **Comparison with baselines** We compare our methods across datasets on\ncommon evaluation metrics. CUT denotes our model trained with the identity loss\n( _\u03bbX_ = _\u03bbY_ = 1), and FastCUT without it ( _\u03bbX_ = 10 _, \u03bbY_ = 0). We show FID, a measure of\nimage quality [26] (lower is better). For Cityscapes, we show the semantic segmentation\nscores (mAP, pixAcc, classAcc) to assess the discovered correspondence (higher is better\nfor all metrics). Based on quantitative measures, CUT produces higher quality and\nmore accurate generations with light footprint in terms of training speed (seconds per\nsample) and GPU memory usage. Our variant FastCUT also produces competitive\nresults with even lighter computation cost of training.\n\n\nfeatures, we apply a two-layer MLP with 256 units at each layer. We normalize\nthe vector by its L2 norm. See Appendix C.1 for more training details.\n\n\n**4.1** **Unpaired image translation**\n\n\n**Datasets** We conduct experiments on the following datasets.\n\n_\u2022 Cat\u2192Dog_ contains 5,000 training and 500 val images from AFHQ Dataset [11].\n\n_\u2022 Horse\u2192Zebra_ contains 2,403 training and 260 zebra images from ImageNet [14]\nand was introduced in CycleGAN [89].\n\n_\u2022 Cityscapes_ [13] contains street scenes from German cities, with 2,975 training\nand 500 validation images. We train models at 256 _\u00d7_ 256 resolution. Unlike\nprevious datasets listed, this does have corresponding labels. We can leverage\nthis to measure how well our unpaired algorithm discovers correspondences.\n\n\n**Evaluation protocol.** We adopt the evaluation protocols from [26,89], aimed\nat assessing _visual quality_ and _discovered correspondence_ . For the first, we utilize\nthe widely-used Fr\u00b4echet Inception Distance (FID) metric, which empirically\nestimates the distribution of real and generated images in a deep network space", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_6750", "chunk_text": " quality_ and _discovered correspondence_ . For the first, we utilize\nthe widely-used Fr\u00b4echet Inception Distance (FID) metric, which empirically\nestimates the distribution of real and generated images in a deep network space\nand computes the divergence between them. Intuitively, if the generated images\nare realistic, they should have similar summary statistics as real images, in any\nfeature space. For _Cityscapes_ specifically, we have ground truth of paired label\nmaps. If accurate correspondences are discovered, the algorithm should generate\nimages that are recognizable as the correct class. Using an off-the-shelf network to\ntest \u201csemantic interpretability\u201d of image translation results has been commonly\nused [85,31]. We use the pretrained semantic segmentation network DRN[83].\nWe train the DRN at 256x128 resolution, and compute mean average precision\n\n\n10 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\n\n\n\n**Training settings** **Testing datasets**\n**Method**\n\n\n\n400\n\n\n70\n\n\n40\n\n\n\n**Id Negs Layers Int Ext**\n\n\n\n**Horse** _\u2192_\n**Zebra** **Cityscapes**\n\n\n\n**FID**                  - **FID**                  - **mAP**                  \n\nCUT (default) 255 All 45.5 **56.4** **24.7**\nno id 255 All 39.3 68.5 22.0\n\nno id, 15 neg 15 All 44.1 59.7 23.1\nno id, 15 neg, last 15 Last **38.1** 114.1 16.0\nlast 255 Last 441.7 141.1 14.9\n\nint and ext 255 All 56.4 64.4 20.0\n\next only 255 All 53.0 110.3 16.5\next only, last 255 Last 60.1 389.1 5.6\n\n\n\n40 70 100 400\n\nFID horse ~~--~~ >zebra (lower is better)\n\n\n\n\n\n\n\n\n\n\n\nFig. 4: **Ablations.** The PatchNCE loss is trained with negatives from each layer output\n\n- f the same (internal) image, with the identity preservation regularization. _(Left)_ We try\nremoving the", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_7200", "chunk_text": "\n\n\n\n\n\n\n\n\n\nFig. 4: **Ablations.** The PatchNCE loss is trained with negatives from each layer output\n\n- f the same (internal) image, with the identity preservation regularization. _(Left)_ We try\nremoving the identity loss [ **Id** ], using less negatives [ **Negs** ], using only the last layer of\nthe encoder [ **Layers** ], and varying where patches are sampled, internal [ **Int** ] vs external\n\n[ **Ext** ]. _(Right)_ We plot the FIDs on horse _\u2192_ zebra and Cityscapes dataset. Removing\nthe identity loss ( `no id` ) and reducing negatives ( `no id, 15 neg` ) still perform strongly.\nIn fact, our variant FastCUT does not use the identity loss. However, reducing number\n\n- f layers ( `last` ) or using external patches ( `ext` ) hurts performance.\n\n\n(mAP), pixel-wise accuracy (pixAcc), and average class accuracy (classAcc). See\nAppendix C.2 for more evaluation details.\n\n\n**Comparison to baselines.** In Table 1, we show quantitative measures of our\nand Figure 3, we compare our method to baselines. We present two settings\n\n- f our method in Eqn. 5: CUT with the identity loss ( _\u03bbX_ = _\u03bbY_ = 1), and\nFastCUT without it ( _\u03bbX_ = 10 _, \u03bbY_ = 0). On image quality metrics across\ndatasets, our methods outperform baselines. We show qualitative results in\nFigure 3 and additional results in Appendix A. In addition, our Cityscapes\nsemantic segmentation scores are higher, suggesting that our method is able to\nfind correspondences between output and input.\n\n\n**Speed and memory.** Since our model is one-sided, our method is memoryefficient and fast. For example, our method with the identity loss was 40% faster\nand 31% more memory-efficient than CycleGAN at training time, using the same\narchitectures as CycleGAN (Table 1). Furthermore, our faster variant FastCUT\nis 63% faster and 53% lighter, while achieving superior metrics to CycleGAN.\nTable 1 contains the speed and memory usage of each method measured on\nNVIDIA GTX 1080Ti, and shows that FastCUT achieves competitive FIDs and\nsegment", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_7650", "chunk_text": " and 53% lighter, while achieving superior metrics to CycleGAN.\nTable 1 contains the speed and memory usage of each method measured on\nNVIDIA GTX 1080Ti, and shows that FastCUT achieves competitive FIDs and\nsegmentation scores with a lower time and memory requirement. Therefore, our\nmethod can serves as a practical, lighter alternative in scenarios, when an image\ntranslation model is jointly trained with other components [29,62].\n\n\n**4.2** **Ablation study and analysis**\n\n\nWe find that in the image synthesis setting, similarly to the unsupervised learning\nsetting [25,24,9], implementation choices for contrastive loss are important. Here,\ntry various settings and ablations of our method, summarized in Figure 4. By\ndefault, we use the ResNet-based generator used in CycleGAN [89], with patchNCE using (a) negatives sampled from the input image, (b) multiple layers of the\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 11\n\n\nInput CUT no id last layer only external only\n\n\nFig. 5: **Qualitative ablation results** - f our full method (CUT) are shown: _without_\nthe identity loss _L_ PatchNCE( _G, H, Y_ ) on domain _Y_ ( `no id` ), using only one layer of the\nencoder ( `last layer only` ), and using external instead of internal negatives ( `external`\n\n`only` ). The ablations cause noticeable drop in quality, including repeated building or\nvegetation textures when using only external negatives or the last layer output.\n\n\n\n400\n\n\n200\n\n\n100\n\n\n70\n\n\n40\n\n\n20\n\n\n\nHorse ~~-~~ - Zebra\n\n\n\n\n\n\n\n400\n\n\n200\n\n\n100\n\n\n70\n\n\n40\n\n\n\n\n\n0 50 100 150 200 250 300 350 400\n\n\n\n0 50 100 150 200 250 300 350 400\n\n\n\nFig. 6: **Identity loss** _L_ **PatchNCE** ( _G, H, Y_ ) **on domain** _Y_ **adds stability.** This regularizer encourages an image from the output domain _**y**_ to be unchanged by the generator.\nUsing it (shown in **bold, black** curves), we observe better stability in comparison to\n\n- ther variants. On the left, our variant without the regular", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_8100", "chunk_text": " image from the output domain _**y**_ to be unchanged by the generator.\nUsing it (shown in **bold, black** curves), we observe better stability in comparison to\n\n- ther variants. On the left, our variant without the regularizer, `no id`, achieves better\nFID. However, we see higher variance in the training curve. On the right, training\nwithout the regularizer can lead to collapse.\n\n\nencoder, and (c) a PatchNCE loss _L_ PatchNCE( _G, H, Y_ ) on domain _Y_ . In Figure 4,\nwe show results using several variants and ablations, taken after training for 400\nepochs. We show qualitative examples in Figure 5.\n\n\n**Internal negatives are more effective than external.** By default, we sample\nnegatives from _within_ the same image (internal negatives). We also try adding\nnegatives from other images, using a momentum encoder [24]. However, the\nexternal negatives, either as addition ( `int and ext` ) or replacement of internal\nnegatives ( `ext only` ), hurts performance. In Figure 5, we see a loss of quality,\nsuch as repeated texture in the Cityscapes dataset, indicating that sampling\nnegatives from the same image serves as a stronger signal for preserving content.\n\n\n**Importance of using multiple layers of encoder.** Our method uses multiple\nlayers of the encoder, every four layers from pixels to the 16 [th] layer. This\n\n\n12 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\nis consistent with the standard use of _\u2113_ 1+VGG loss, which uses layers from\nthe pixel level up to a deep convolutional layer. On the other hand, many\ncontrastive learning-based unsupervised learning papers map the whole image\ninto a single representation. To emulate this, we try only using the last layer\n\n- f the encoder ( `last` ), and try a variant using external negatives only ( `ext`\n\n`only, last` ). Performance is drastically reduced in both cases. In unsupervised\nrepresentation learning, the input images are fixed. For our application, the loss is\nbeing used as a signal for synthesizing an image. As such, this indicates that the\ndense supervision provided by using multiple layers of the encoder is important\nwhen performing image synthesis.\n\n\n_L", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_8550", "chunk_text": " images are fixed. For our application, the loss is\nbeing used as a signal for synthesizing an image. As such, this indicates that the\ndense supervision provided by using multiple layers of the encoder is important\nwhen performing image synthesis.\n\n\n_L_ **PatchNCE** ( _G, H, Y_ ) **regularizer stabilizes training.** Given an image from\nthe output domain _**y**_ _\u2208Y_, this regularizer encourages the generator to leave the\nimage unchanged with our patch-based contrastive loss. We also experiment with\na variant without this regularizer, `no id` . As shown in Figure 4, removing the\nregularizer improves results for the horse _\u2192_ zebra task, but decreases performance\n\n- n Cityscapes. We further investigate by showing the training curves in Figure 6,\nacross 400 epochs. In the Cityscapes results, the training can collapse without\nthe regularizer (although it can recover). We observe that although the final FID\nis sometimes better without, the training is more stable with the regularizer.\n\n\n**Visualizing learned similarity by encoder** _G_ **enc** To further understand why\n\n- ur encoder network _G_ enc has learned to perform horse _\u2192_ zebra task, we study\nthe output space of the 1st residual block for both horse and zebra features. As\nshown in Figure 7. Given an input and output image, we compute the distance\nbetween a query patch\u2019s feature vector _**v**_ (highlighted as red or blue dot) to\nfeature vectors _**v**_ _**[\u2212]**_ - f all the patches in the input using exp( _**v**_ _\u00b7_ _**v**_ _**[\u2212]**_ _/\u03c4_ ) (Eqn. 2).\nAdditionally, we perform a PCA dimension reduction on feature vectors from\nboth horse and zebra patches. In (d) and (e), we show the top three principal\ncomponents, which looks similar before and after translation. This indicates that\n\n- ur encoder is able to bring the corresponding patches from two domains into a\nsimilar location in the feature embedding space.\n\n\n**Additional applications** . Figure 8 shows additional results: Parisian street _\u2192_\nBurano\u2019s brightly painted houses and Russian Blue cat _\u2192_ Grumpy cat.\n\n\n**4.3** **High", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_9000", "chunk_text": "similar location in the feature embedding space.\n\n\n**Additional applications** . Figure 8 shows additional results: Parisian street _\u2192_\nBurano\u2019s brightly painted houses and Russian Blue cat _\u2192_ Grumpy cat.\n\n\n**4.3** **High-resolution single image translation**\n\n\nFinally, we conduct experiments in the single image setting, where both the source\nand target domain only have one image each. Here, we transfer a Claude Monet\u2019s\npainting to a natural photograph. Recent methods [64,65] have explored training\nunconditional models on a single image. Bearing the additional challenge of\nrespecting the structure of the input image, conditional image synthesis using only\n\n- ne image has not been explored by previous image-to-image translation methods.\nOur painting _\u2192_ photo task is also different from neural style transfer [19,34]\n(photo _\u2192_ painting) and photo style transfer [48,82] (photo _\u2192_ photo).\nSince the whole image (at HD resolution) cannot fit on a commercial GPU,\nat each iteration we train on 16 random crops of size 128 _\u00d7_ 128. We also randomly\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 13\n\n\n(a) Translated !\" (b) Input (c) Learned similarity from (d) PCA on (e) PCA on\n& query points image # query points to input image # encoding of !\" encoding of #\n\n\nFig. 7: **Visualizing the learned similarity by** _G_ **enc.** Given query points (blue or\nred) on an output image (a) and input (b), we visualize the learned similarity to patches\n\n- n the input image by computing exp( _**v**_ _\u00b7_ _**v**_ _**[\u2212]**_ _/\u03c4_ ) in (c). Here _**v**_ is the query patch in the\n\n- utput and _**v**_ _**[\u2212]**_ denotes patches from the input. This suggests that our encoder may\nlearn cross-domain correspondences implicitly. In (d) and (e), we visualize the top 3\nPCA components of the shared embedding.\n\n\n\nInput Output Input Output\n\n\nParisian Street \u2192 Burano\u2019s painted houses\n\n\n\nInput Output Input Output\n\n\nRussian blue cat \u2192 Grumpy cat\n\n\n\nFig. 8: **Additional applications** - n Parisian street _\u2192_ Burano\u2019s colored houses and", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_9450", "chunk_text": " Output Input Output\n\n\nParisian Street \u2192 Burano\u2019s painted houses\n\n\n\nInput Output Input Output\n\n\nRussian blue cat \u2192 Grumpy cat\n\n\n\nFig. 8: **Additional applications** - n Parisian street _\u2192_ Burano\u2019s colored houses and\nRussian Blue cat _\u2192_ Grumpy cat.\n\n\nscale the image to prevent overfitting. Furthermore, we observe that limiting the\nreceptive field of the discriminator is important for preserving the structure of\nthe input image, as otherwise the GAN loss will force the output image to be\nidentical to the target image. Therefore, the crops are further split into 64 _\u00d7_ 64\npatches before passed to the discriminator. Lastly, we find that using gradient\npenalty [53,35] stabilizes optimization. We call this variant SinCUT.\n\n\nFigure 9 shows a qualitative comparison between our results and baseline\nmethods including two neural style transfer methods (Gatys et al. [19] and\nSTROTSS [39]), one leading photo style transfer method WCT [2] [82], and a\nCycleGAN baseline [89] that uses the _\u2113_ 1 cycle-consistency loss instead of our\ncontrastive loss at the patch level. The input paintings are high-res, ranging from\n1k to 1.5k. Appendix B includes additional examples. We observe that Gatys et\nal. [19] fails to synthesize realistic textures. Existing photo style transfer methods\nsuch as WCT [2] can only modify the color of the input image. Our method SinCUT\n\n- utperforms CycleGAN and is comparable to a leading style transfer method [39],\nwhich is based on optimal transport and self-similarity. Interestingly, our method\n\n\n14 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\nInput Style Input Content SinCUT Gatys et al. WCT [2] STROTSS CycleGAN\n\n\nFig. 9: **High-res painting to photo translation.** We transfer Claude Monet\u2019s paintings to reference natural photographs. The training only requires a single image from\neach domain. We compare our results (SinCUT) to recent style and photo transfer\nmethods including Gatys et al. [19], WCT [2] [82], STROTSS [39], and patch-based\nCycleGAN [89]. Our method generates can reproduce the texture of", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_9900", "chunk_text": "UT) to recent style and photo transfer\nmethods including Gatys et al. [19], WCT [2] [82], STROTSS [39], and patch-based\nCycleGAN [89]. Our method generates can reproduce the texture of the reference photo\nwhile retaining structure of input painting. Our generation is at 1k _\u223c_ 1.5k resolution.\n\n\nis not originally designed for this application. This result suggests the intriguing\nconnection between image-to-image translation and neural style transfer.\n\n\n**5** **Conclusion**\n\n\nWe propose a straightforward method for encouraging content preservation in\nunpaired image translation problems \u2013 by maximizing the mutual information\nbetween input and output with contrastive learning. The objective learns an\nembedding to bringing together corresponding patches in input and output, while\npushing away noncorresponding \u201cnegative\u201d patches. We study several important\ndesign choices. Interestingly, drawing negatives from _within_ the image itself,\nrather than other images, provides a stronger signal. Our method _learns a cross-_\n_domain similarity function_ and is the first image translation algorithm, to our\nknowledge, to not use any pre-defined similarity function (such as _\u2113_ 1 or perceptual\nloss). As our method does not rely on cycle-consistency, it can enable one-sided\nimage translation, with better quality than established baselines. In addition,\n\n- ur method can be used for _single-image_ unpaired translation.\n\n\n**Acknowledgments.** We thank Allan Jabri and Phillip Isola for helpful discussion\nand feedback. Taesung Park is supported by a Samsung Scholarship and an Adobe\nResearch Fellowship, and some of this work was done as an Adobe Research\nintern. This work was partially supported by NSF grant IIS-1633310, grant from\nSAP, and gifts from Berkeley DeepDrive and Adobe.\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 15\n\n\n**References**\n\n\n1. Almahairi, A., Rajeswar, S., Sordoni, A., Bachman, P., Courville, A.: Augmented\ncyclegan: Learning many-to-many mappings from unpaired data. In: International\nConference on Machine Learning (ICML) (2018) 3\n2. Amodio, M., Krishnaswamy, S.: Travelgan: Image-to-image translation by transformation vector learning. In: Proceedings of the IEEE", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_10350", "chunk_text": " International\nConference on Machine Learning (ICML) (2018) 3\n2. Amodio, M., Krishnaswamy, S.: Travelgan: Image-to-image translation by transformation vector learning. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition. pp. 8983\u20138992 (2019) 3\n3. Bachman, P., Hjelm, R.D., Buchwalter, W.: Learning representations by maximizing\nmutual information across views. In: Advances in Neural Information Processing\nSystems (NeurIPS) (2019) 2, 5\n4. Benaim, S., Wolf, L.: One-sided unsupervised domain mapping. In: Advances in\nNeural Information Processing Systems (NeurIPS) (2017) 2, 3, 8, 9, 20\n5. Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D.: Unsupervised\npixel-level domain adaptation with generative adversarial networks. In: IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) (2017) 3\n6. Caesar, H., Uijlings, J., Ferrari, V.: Coco-stuff: Thing and stuff classes in context.\nIn: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)\n27\n\n7. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI) **40** (4), 834\u2013848 (2018) 7, 27\n8. Chen, Q., Koltun, V.: Photographic image synthesis with cascaded refinement\nnetworks. In: IEEE International Conference on Computer Vision (ICCV) (2017) 3\n9. Chen, T., Kornblith, S., Norouzi, M., Hinton, G.: A simple framework for contrastive\nlearning of visual representations. In: International Conference on Machine Learning\n(ICML) (2020) 2, 4, 6, 10\n10. Choi, Y., Choi, M., Kim, M", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_10800", "chunk_text": " framework for contrastive\nlearning of visual representations. In: International Conference on Machine Learning\n(ICML) (2020) 2, 4, 6, 10\n10. Choi, Y., Choi, M., Kim, M., Ha, J.W., Kim, S., Choo, J.: Stargan: Unified\ngenerative adversarial networks for multi-domain image-to-image translation. In:\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 3\n11. Choi, Y., Uh, Y., Yoo, J., Ha, J.W.: Stargan v2: Diverse image synthesis for multiple\ndomains. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n(2020) 9\n12. Chopra, S., Hadsell, R., LeCun, Y.: Learning a similarity metric discriminatively,\nwith application to face verification. In: IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (2005) 4\n13. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,\nFranke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene\nunderstanding. In: IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) (2016) 9, 20\n14. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-Scale\nHierarchical Image Database. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2009) 3, 9\n15. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning by\ncontext prediction. In: IEEE International Conference on Computer Vision (ICCV)\n(2015) 4\n16. Dosovitskiy, A., Brox, T.: Generating images with perceptual similarity metrics\nbased on deep networks. In: Advances in Neural Information Processing Systems\n(2016) 3\n\n\n16 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\n17. Dosovitskiy, A., Fischer, P., Springenberg", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_11700", "chunk_text": "CVPR)\n(2018) 23\n23. Gutmann, M., Hyv\u00a8arinen, A.: Noise-contrastive estimation: A new estimation\nprinciple for unnormalized statistical models. In: International Conference on\nArtificial Intelligence and Statistics (AISTATS) (2010) 4\n24. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum contrast for unsupervised\nvisual representation learning. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2020) 2, 4, 5, 6, 10, 11, 26, 28\n25. H\u00b4enaff, O.J., Razavi, A., Doersch, C., Eslami, S., Oord, A.v.d.: Data-efficient image\nrecognition with contrastive predictive coding. In: IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) (2019) 2, 4, 5, 6, 10\n26. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: GANs\ntrained by a two time-scale update rule converge to a local Nash equilibrium. In:\nAdvances in Neural Information Processing Systems (2017) 9, 26\n27. Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neural\nnetworks. Science **313** (5786), 504\u2013507 (2006) 4\n28. Hjelm, R.D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler,\nA., Bengio, Y.: Learning deep representations by mutual information estimation\nand maximization. arXiv preprint arXiv:1808.06670 (2018) 4\n29. Hoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A.A., Darrell,\nT.: Cycada: Cycle-consistent adversarial domain adaptation. In: International\nConference on Machine Learning (ICML) (2018) 10, 20\n30. Huang", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_12150", "chunk_text": ", K., Efros, A.A., Darrell,\nT.: Cycada: Cycle-consistent adversarial domain adaptation. In: International\nConference on Machine Learning (ICML) (2018) 10, 20\n30. Huang, X., Liu, M.Y., Belongie, S., Kautz, J.: Multimodal unsupervised image-toimage translation. European Conference on Computer Vision (ECCV) (2018) 3,\n20\n31. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversarial networks. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2017) 1, 3, 8, 9, 26\n32. Isola, P., Zoran, D., Krishnan, D., Adelson, E.H.: Crisp boundary detection using pointwise mutual information. In: European Conference on Computer Vision\n(ECCV) (2014) 7\n33. Isola, P., Zoran, D., Krishnan, D., Adelson, E.H.: Learning visual groups from\nco-occurrences in space and time. arXiv preprint arXiv:1511.06811 (2015) 4\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 17\n\n\n34. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and\nsuper-resolution. In: European Conference on Computer Vision (ECCV) (2016) 3,\n8, 12, 26\n35. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative adversarial networks. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2019) 13\n36. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing\nand improving the image quality of stylegan. IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) (2020) 8, 23\n37. Kim, T., Cha, M., Kim, H., Lee, J., Kim, J.: Learning to discover cross-domain", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_12600", "chunk_text": " IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) (2020) 8, 23\n37. Kim, T., Cha, M., Kim, H., Lee, J., Kim, J.: Learning to discover cross-domain\nrelations with generative adversarial networks. In: International Conference on\nMachine Learning (ICML) (2017) 1, 3\n38. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: International\nConference on Learning Representations (ICLR) (2015) 26\n39. Kolkin, N., Salavon, J., Shakhnarovich, G.: Style transfer by relaxed optimal\ntransport and self-similarity. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2019) 13, 14, 23, 24, 25\n40. Larsson, G., Maire, M., Shakhnarovich, G.: Colorization as a proxy task for visual\nunderstanding. In: IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). pp. 6874\u20136883 (2017) 4\n41. Lee, H.Y., Tseng, H.Y., Huang, J.B., Singh, M.K., Yang, M.H.: Diverse imageto-image translation via disentangled representation. In: European Conference on\nComputer Vision (ECCV) (2018) 3, 8, 9, 20\n42. Li, C., Liu, H., Chen, C., Pu, Y., Chen, L., Henao, R., Carin, L.: Alice: Towards\nunderstanding adversarial learning for joint distribution matching. In: Advances in\nNeural Information Processing Systems (2017) 7\n43. Liang, X., Zhang, H., Lin, L., Xing, E.: Generative semantic manipulation with\nmask-contrasting gan. In: European Conference on Computer Vision (ECCV) (2018)\n3, 7\n44. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation networks.\nIn: Advances in Neural Information Processing Systems (2017) 3, 8, 9\n45. Liu, M.Y., Huang, X., Mallya, A., K", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_13500", "chunk_text": " Zhu\n\n\n52. Mechrez, R., Talmi, I., Zelnik-Manor, L.: The contextual loss for image transformation with non-aligned data. In: European Conference on Computer Vision (ECCV)\n(2018) 3, 7, 23\n53. Mescheder, L., Geiger, A., Nowozin, S.: Which training methods for gans do actually\nconverge? In: International Conference on Machine Learning (ICML) (2018) 13, 23\n54. Misra, I., van der Maaten, L.: Self-supervised learning of pretext-invariant representations. arXiv preprint arXiv:1912.01991 (2019) 4\n55. Misra, I., Zitnick, C.L., Hebert, M.: Shuffle and learn: unsupervised learning using\ntemporal order verification. In: European Conference on Computer Vision. pp.\n527\u2013544. Springer (2016) 4\n56. Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep\nlearning. In: International Conference on Machine Learning (ICML) (2011) 4\n57. Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748 (2018) 2, 4, 6\n58. Owens, A., Wu, J., McDermott, J.H., Freeman, W.T., Torralba, A.: Ambient sound\nprovides supervision for visual learning. In: European Conference on Computer\nVision (ECCV) (2016) 4\n59. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with spatiallyadaptive normalization. In: IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2019) 3\n60. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context encoders:\nFeature learning by inpainting. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition. pp. 2536\u20132544 (2016", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_14850", "chunk_text": " P., Larochelle, H., Bengio, Y., Manzagol, P.A.: Extracting and composing robust features with denoising autoencoders. In: International Conference on\nMachine Learning (ICML) (2008) 4\n77. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-resolution\nimage synthesis and semantic manipulation with conditional gans. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018) 3\n78. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:\nfrom error visibility to structural similarity. IEEE transactions on image processing\n**13** (4), 600\u2013612 (2004) 3\n79. Wu, W., Cao, K., Li, C., Qian, C., Loy, C.C.: Transgaga: Geometry-aware unsupervised image-to-image translation. In: IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (2019) 3\n80. Wu, Z., Xiong, Y., Yu, S.X., Lin, D.: Unsupervised feature learning via nonparametric instance discrimination. In: IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (2018) 2, 4, 5, 6\n81. Yi, Z., Zhang, H., Tan, P., Gong, M.: Dualgan: Unsupervised dual learning for\nimage-to-image translation. In: IEEE International Conference on Computer Vision\n(ICCV) (2017) 1, 3\n82. Yoo, J., Uh, Y., Chun, S., Kang, B., Ha, J.W.: Photorealistic style transfer via\nwavelet transforms. In: IEEE International Conference on Computer Vision (ICCV)\n(2019) 12, 13, 14, 23, 24, 25\n83. Yu, F., Koltun, V., Funkhouser, T.: Dilated residual networks. In: IEEE Conference\n\n  - n Computer Vision and Pattern Recognition (CVPR) (2017) 9, 26\n84. Zhang, L., Zhang, L., Mou, X., Zhang, D.: Fsim: A", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_15750", "chunk_text": ".:\nToward multimodal image-to-image translation. In: Advances in Neural Information\nProcessing Systems (2017) 3\n91. Zontak, M., Irani, M.: Internal statistics of a single natural image. In: IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) (2011) 7\n\n\n**Appendix A** **Additional Image-to-Image Results**\n\n\nWe first show additional, randomly selected results on datasets used in our main\npaper. We then show results on additional datasets.\n\n\n**A.1** **Additional comparisons**\n\n\nIn Figure 10, we show additional, randomly selected results for Horse _\u2192_ Zebra\nand Cat _\u2192_ Dog. This is an extension of Figure 3 in the main paper. We compare\nto baseline methods CycleGAN [89], MUNIT [30], DRIT [41], Self-Distance and\nDistanceGAN [4], and GcGAN [18].\n\n\n**A.2** **Additional datasets**\n\n\nIn Figure 11 and Figure 12, we show additional datasets, compared against\nbaseline method CycleGAN [89]. Our method provides better or comparable\nresults, demonstrating its flexibility across a variety of datasets.\n\n_\u2022 Apple\u2192Orange_ contains 996 apple and 1,020 orange images from ImageNet\nand was introduced in CycleGAN [89].\n\n_\u2022 Yosemite Summer\u2192Winter_ contains 1,273 summer and 854 winter images of\nYosemite scraped using the FlickAPI was introduced in CycleGAN [89].\n\n_\u2022 GTA\u2192Cityscapes_ GTA contains 24,966 images [63] and Cityscapes [13] contains\n19,998 images of street scenes from German cities. The task was originally\nused in CyCADA [29].\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 21\n\n\nInput CUT FastCUT CycleGAN MUNIT DRIT DistanceGAN SelfDistGAN GcGAN\n\n\nFig. 10: **Randomly selected Horse** _\u2192_ **Zebra and Cat** _\u2192_ **Dog results** . This is an\nextension of Figure 3 in the main paper.\n\n\n22 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\nInput Ours(idt) CycleGAN Input Ours(idt) CycleGAN\n\n\nFig. 11: **Apple** _\u2192_ **Orange** and **Summer", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_16200", "chunk_text": ", Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\nInput Ours(idt) CycleGAN Input Ours(idt) CycleGAN\n\n\nFig. 11: **Apple** _\u2192_ **Orange** and **Summer** _\u2192_ **Winter Yosemite** . CycleGAN models were\ndownloaded from the authors\u2019 public code repository. Apple _\u2192_ Orange shows that\nCycleGAN may suffer from color flipping issue.\n\n\nInput Ours(idt)\n\n\nFig. 12: **GTA** _\u2192_ **Cityscapes** results at 1024 _\u00d7_ 512 resolution. The model was trained\n\n- n 512 _\u00d7_ 512 crops.\n\n\n.\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 23\n\n\n**Appendix B** **Additional Single Image Translation Results**\n\n\nWe show additional results in Figure 13 and Figure 14, and describe training\ndetails below.\n\n\n**Training details.** At each iteration, the input image is randomly scaled to a\nwidth between 384 to 1024, and we randomly sample 16 crops of size 128 _\u00d7_ 128.\nTo avoid overfitting, we divide crops into 64 _\u00d7_ 64 tiles before passing them to the\ndiscriminator. At test time, since the generator network is fully convolutional, it\ntakes the input image at full size.\nWe found that adopting the architecture of StyleGAN2 [36] instead of CycleGAN slightly improves the output quality, although the difference is marginal. Our\nStyleGAN2-based generator consists of one downsampling block of StyleGAN2\ndiscriminator, 6 StyleGAN2 residual blocks, and one StyleGAN2 upsampling\nblock. Our discriminator has the same architecture as StyleGAN2. Following\nStyleGAN2, we use non-saturating GAN loss [61] with R1 gradient penalty [53].\nSince we do not use style code, the style modulation layer of StyleGAN2 was\nremoved.\n\n\n**Single image results.**\nIn Figure 13 and Figure 14, we show additional comparison results for our\nmethod, Gatys et al. [19], STROTSS [39], WCT [2] [82], and CycleGAN baseline [89].\nNote that the CycleGAN baseline adopts the same augmentation techniques as\nwell as the same generator/discriminator architectures as our method. The image\nresolution is at 1-2", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_16650", "chunk_text": "CT [2] [82], and CycleGAN baseline [89].\nNote that the CycleGAN baseline adopts the same augmentation techniques as\nwell as the same generator/discriminator architectures as our method. The image\nresolution is at 1-2 Megapixels. Please zoom in to see more visual details.\nBoth figures demonstrate that our results look more photorealistic compared\nto CycleGAN baseline, Gatys et al [19], and WCT [2] . The quality of our results is\n\n- n par with results from STROTSS [39]. Note that STROTSS [39] compares to\nand outperforms recent style transfer methods (e.g., [22,52]).\n\n\n24 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n|Col1|Col2|Col3|\n|---|---|---|\n||||\n|Input<br>Gatys et al.|Input<br>Gatys et al.|Input<br>Gatys et al.|\n|Input<br>Gatys et al.|||\n\n\n|Gatys et al.|Col2|Col3|\n|---|---|---|\n||||\n||||\n|Input<br>Gatys et al.|Input<br>Gatys et al.|Input<br>Gatys et al.|\n|Input<br>Gatys et al.|||\n\n\n|Gatys et al.|Col2|Col3|\n|---|---|---|\n||||\n||||\n|Input<br>Gatys et al.|Input<br>Gatys et al.|Input<br>Gatys et al.|\n|Input<br>Gatys et al.|||\n\n\n\nFig. 13: **High-res painting to photo translation (I).** We transfer Monet\u2019s paintings\nto reference natural photos shown as insets at top-left corners. The training only requires\na single image from each domain. We compare our results to recent style and photo\ntransfer methods including Gatys et al. [19], WCT [2] [82], STROTSS [39], and our\nmodified patch-based CycleGAN [89]. Our method can reproduce the texture of the\nreference photos while retaining structure of the input paintings. Our results are at 1k\n_\u223c_ 1.5k resolution.\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 25\n\n|Col1|Col2|Col3", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_17100", "chunk_text": " retaining structure of the input paintings. Our results are at 1k\n_\u223c_ 1.5k resolution.\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 25\n\n|Col1|Col2|Col3|\n|---|---|---|\n||||\n|Input|Input|Input|\n\n\n|Gatys et al.|Col2|Col3|\n|---|---|---|\n||||\n||||\n|Input|Input|Input|\n\n\n|Gatys et al.|Col2|Col3|\n|---|---|---|\n||||\n||||\n|Input<br>Gatys et al.|Input<br>Gatys et al.|Input<br>Gatys et al.|\n|Input<br>Gatys et al.|||\n\n\n\nFig. 14: **High-res painting to photo translation (II).** We transfer Monet\u2019s paintings\nto reference natural photos shown as insets at top-left corners. The training only requires\na single image from each domain. We compare our results to recent style and photo\ntransfer methods including Gatys et al. [19], WCT [2] [82], STROTSS [39], and our\nmodified patch-based CycleGAN [89]. Our method can reproduce the texture of the\nreference photos while retaining structure of the input paintings. Our results are at 1k\n_\u223c_ 1.5k resolution.\n\n\n26 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\n**Appendix C** **Unpaired Translation Details and Analysis**\n\n\n**C.1** **Training details**\n\n\nTo show the effect of the proposed patch-based contrastive loss, we intentionally\nmatch the architecture and hyperparameter settings of CycleGAN, except the loss\nfunction. This includes the ResNet-based generator [34] with 9 residual blocks,\nPatchGAN discriminator [31], Least Square GAN loss [50], batch size of 1, and\nAdam optimizer [38] with learning rate 0.002.\nOur full model CUT is trained up to 400 epochs, while the fast variant\nFastCUT is trained up to 200 epochs, following CycleGAN. Moreover, inspired\nby GcGAN [18], FastCUT is trained with flip-equivariance augmentation, where\nthe input image to the generator is horizontally flipped, and the output features\nare flipped back before computing the Patch", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_17550", "chunk_text": " following CycleGAN. Moreover, inspired\nby GcGAN [18], FastCUT is trained with flip-equivariance augmentation, where\nthe input image to the generator is horizontally flipped, and the output features\nare flipped back before computing the PatchNCE loss. Our encoder _G_ enc is the\nfirst half of the CycleGAN generator [89]. In order to calculate our multi-layer,\npatch-based contrastive loss, we extract features from 5 layers, which are RGB\npixels, the first and second downsampling convolution, and the first and the\nfifth residual block. The layers we use correspond to receptive fields of sizes\n1 _\u00d7_ 1, 9 _\u00d7_ 9, 15 _\u00d7_ 15, 35 _\u00d7_ 35, and 99 _\u00d7_ 99. For each layer\u2019s features, we sample 256\nrandom locations, and apply 2-layer MLP to acquire 256-dim final features. For\n\n- ur baseline model that uses MoCo-style memory bank [24], we follow the setting\n\n- f MoCo, and used momentum value 0.999 with temperature 0.07. The size of\nthe memory bank is 16384 per layer, and we enqueue 256 patches per image per\niteration.\n\n\n**C.2** **Evaluation details**\n\n\nWe list the details of our evaluation protocol.\n\n\n**Fr\u00b4echet Inception Distance (FID [26])** throughout this paper is computed by\nresizing the images to 299-by-299 using bilinear sampling of PyTorch framework,\nand then taking the activations of the last average pooling layer of a pretrained\nInception V3 [70] using the weights provided by the TensorFlow framework.\nWe use the default setting of `[https://github.com/mseitzer/pytorch-fid](https://github.com/mseitzer/pytorch-fid)` . All\ntest set images are used for evaluation, unless noted otherwise.\n\n\n**Semantic segmentation metrics on the Cityscapes dataset** are computed\nas follows. First, we trained a semantic segmentation network using the DRN-D22 [83] architecture. We used the recommended setting from `[https://github.](https://github.com/fyu/drn)`\n`[com/fyu/drn](https://github.com/fyu/drn)`, with batch size 32 and learning rate 0.01, for 250 epochs at\n256x128", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_18000", "chunk_text": "](https://github.com/fyu/drn)`\n`[com/fyu/drn](https://github.com/fyu/drn)`, with batch size 32 and learning rate 0.01, for 250 epochs at\n256x128 resolution. The output images of the 500 validation labels are resized to\n256x128 using bicubic downsampling, passed to the trained DRN network, and\ncompared against the ground truth labels downsampled to the same size using\nnearest-neighbor sampling.\n\n\n**C.3** **Pseudocode**\n\n\nHere we provide the pseudo-code of PatchNCE loss in the PyTorch style. Our\n[code and models are available at our GitHub repo.](https://github.com/taesungp/contrastive-unpaired-translation)\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 27\n\n\n~~**import torch**~~\n\ncross_entropy_loss = torch.nn.CrossEntropyLoss()\n\n\n_# Input: f_q (BxCxS) and sampled features from H(G_enc(x))_\n_# Input: f_k (BxCxS) are sampled features from H(G_enc(G(x))_\n_# Input: tau is the temperature used in NCE loss._\n_# Output: PatchNCE loss_\n\n**def** PatchNCELoss(f_q, f_k, tau=0.07):\n\n_# batch size, channel size, and number of sample locations_\n\nB, C, S = f_q.shape\n\n\n_# calculate v * v+: BxSx1_\n\nl_pos = (f_k * f_q).sum(dim=1)[:, :, **None** ]\n\n\n_# calculate v * v-: BxSxS_\n\nl_neg = torch.bmm(f_q.transpose(1, 2), f_k)\n\n\n_# The diagonal entries are not negatives. Remove them._\n\nidentity_matrix = torch.eye(S)[ **None**, :, :]\n\nl_neg.masked_fill_(identity_matrix, -float('inf'))\n\n\n_# calculate logits: (B)x(S)x(S+1)_\nlogits = torch.cat((l_pos, l_neg), dim=2) / tau\n\n\n_# return NCE loss_\n\npredictions = logits.flatten(0, 1)\n\ntargets = torch.zeros(B * S, dtype=torch.long)\n\n**return** cross_entropy_loss(predictions, targets)\n\n\n**C.4** **Distribution matching**\n\n\nIn Figure 15,", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_18450", "chunk_text": "CE loss_\n\npredictions = logits.flatten(0, 1)\n\ntargets = torch.zeros(B * S, dtype=torch.long)\n\n**return** cross_entropy_loss(predictions, targets)\n\n\n**C.4** **Distribution matching**\n\n\nIn Figure 15, we show an interesting phenomenon of our method, caused by the\ntraining set imbalance of the horse _\u2192_ zebra set. We use an off-the-shelf DeepLab\nmodel [7] trained on COCO-Stuff [6], to measure the percentage of pixels that\nbelong to horses and zebras [1] . The training set exhibits dataset bias [74]. On\naverage, zebras appear in more close-up pictures than horses and take up about\ntwice the number of pixels (37% vs 18%). To perfectly satisfy the discriminator,\na translation model should attempt to match the statistics of the training set.\nOur method allows the flexibility for the horses to change the size, and the\npercentage of output zebra pixels (31%) better matches the training distribution\n(37%) than the CycleGAN baseline (19%). On the other hand, our fast variant\n_FastCUT_ uses a larger weight ( _\u03bbX_ = 10) on the Patch NCE loss and flipequivariance augmentation, and hence behaves more conservatively and more\nsimilar to CycleGAN. The strong distribution matching capacity has pros and\ncons. For certain applications, it can create introduce undesired changes (e.g.,\n\n\n1 Pretrained model from `[https://github.com/kazuto1011/deeplab-pytorch](https://github.com/kazuto1011/deeplab-pytorch)`\n\n\n28 Taesung Park, Alexei A. Efros, Richard Zhang, Jun-Yan Zhu\n\n\nInput CUT FastCUT CycleGAN Source training set Target training set\n\n\n\ndetected pixels:\n\n\n\nzebra 30.8% zebra 25.9% zebra 19.1% horse 17.9% zebra 36.8%\n\n\n\nFig. 15: **Distribution matching.** We measure the percentage of pixels belonging to\nthe horse/zebra bodies, using a pre-trained semantic segmentation model. We find a\ndistribution mismatch between sizes of horses and zebras images \u2013 zebras usually appear\nlarger (36.8% vs. 17.9%). Our full method CUT has the flexibility to enlarge the horses,\nas a", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_18900", "chunk_text": " model. We find a\ndistribution mismatch between sizes of horses and zebras images \u2013 zebras usually appear\nlarger (36.8% vs. 17.9%). Our full method CUT has the flexibility to enlarge the horses,\nas a means of better matching of the training statistics than CycleGAN [89]. Our faster\nvariant FastCUT, trained with a higher PatchNCE loss ( _\u03bbX_ = 10) and flip-equivariance\naugmentation, behaves more conservatively like CycleGAN.\n\n\nzebra patterns on the background for horse _\u2192_ zebra). On the other hand, it can\nenable dramatic geometric changes for applications such as Cat _\u2192_ Dog.\n\n\n**C.5** **Additional Ablation studies**\n\n\nIn the paper, we mainly discussed the impact of loss functions and the number of\npatches on the final performance. Here we present additional ablation studies on\nmore subtle design choices. We run all the variants on horse2zebra datasets [89].\nThe FID of our original model is **46.6** . We compare it to the following two\nvariants of our model:\n\n_\u2022_ Ours without weight sharing for the encoder _G_ enc and MLP projection network\n_H_ : for this variant, when computing features _{_ _**z**_ _l}L_ = _{Hl_ ( _G_ _[l]_ enc [(] _**[x]**_ [))] _[}][L]_ [, we use]\ntwo separate encoders and MLP networks for embedding input images (e.g.,\nhorse) and the generated images (e.g., zebras) to feature space. They do not\nshare any weights. The FID of this variant is **50.5**, worse than our method.\nThis shows that weight sharing helps stabilize training while reducing the\nnumber of parameters in our model.\n\n_\u2022_ Ours without updating the decoder _G_ dec using _PatchNCE_ loss: in this variant,\nwe exclude the gradient propagation of the decoder _G_ dec regarding _PatchNCE_\nloss _L_ PatchNCE. In other words, the decoder _G_ dec only gets updated through\nthe adversarial loss _L_ GAN. The FID of this variant is **444.2**, and the results\ncontain severe artifacts. This shows that our _L_ PatchNCE not only helps learn the\nencoder", "token_count": 500, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
{"chunk_id": "2007.15651_reranking_nogueira:chunk_19350", "chunk_text": " updated through\nthe adversarial loss _L_ GAN. The FID of this variant is **444.2**, and the results\ncontain severe artifacts. This shows that our _L_ PatchNCE not only helps learn the\nencoder _G_ enc, as done in previous unsupervised feature learning methods [24],\nbut also learns a better decoder _G_ dec together with the GAN loss. Intuitively,\nif the generated result has many artifacts and is far from realistic, it would be\ndifficult for the encoder to find correspondences between the input and output,\nproducing a large _PatchNCE_ loss.\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 29\n\n\n**Appendix D** **Changelog**\n\n\n**v1** Initial preprint release (ECCV 2020)\n\n\n**v2 and v3** (1) Fix typos in Eqn. 3 and Eqn. 4. (2) Add additional related work.\n\n\n", "token_count": 207, "metadata": {"arxiv_id": "2007.15651", "title": "Contrastive Learning for Unpaired Image-to-Image Translation", "authors": ["Taesung Park", "Alexei A. Efros", "Richard Zhang", "Jun-Yan Zhu"], "year": 2020, "url": "https://arxiv.org/pdf/2007.15651v3"}}
