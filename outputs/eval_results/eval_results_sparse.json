{"user_input":{"0":"What is Retrieval-Augmented Generation?","1":"Who proposed the FiD (Fusion-in-Decoder) architecture?","2":"What does RAFT stand for?","3":"Why does hybrid retrieval (dense + sparse) outperform either approach alone?","4":"How does ColBERT differ from traditional dense retrieval?","5":"What are the trade-offs between long-context LLMs and RAG systems?","6":"How do GraphRAG and FiD differ in their approach to multi-document reasoning?","7":"What evaluation metrics are recommended for both retrieval and generation in RAG?","8":"How to bake bread?","9":"How do you train a neural network from scratch?","10":"Who are the authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering'?","11":"What is BM25 used for in open-domain QA?","12":"What is the purpose of using Wikipedia in open-domain question answering?","13":"What method does Roberts et al. use to measure the similarity between questions and passages?","14":"What is the purpose of passage embeddings in machine learning?","15":"What is BM25 used for in the context of question answering datasets?","16":"What is TREC used for in the context of passage retrieval experiments?","17":"What is the reason for the lower performance of SQuAD according to Lee et al.?","18":"What CPU model was used to profile the passage retrieval speed for DPR?","19":"What is the purpose of a passage selection score in an end-to-end QA system?","20":"What is REALMWiki?","21":"What significant contributions did Lee et al. make to the field of information retrieval, particularly in the context of dense retrieval models?","22":"How does the sufficient context autorater help in reducing hallucinations in RAG systems?","23":"How does query rewriting improve the performance of retrieval-augmented language models in handling factual information?","24":"How does the combination of parametric memory with non-parametric memory in RAG models improve their performance on knowledge-intensive NLP tasks?","25":"What are some notable approaches for retrieval-augmented text generation tasks and how do they leverage deep learning technology?","26":"How does VisRAG enhance the retrieval and generation stages of RAG by utilizing vision-language models?","27":"How does the SPLADE model address the limitations of previous sparse lexical representation models in terms of efficiency and effectiveness?","28":"How does ARES utilize prediction-powered inference (PPI) to improve model-based evaluation accuracy?","29":"How did the introduction of dense passage retrieval improve open-domain question answering systems?","30":"What is the core idea behind HyDE and how does it improve zero-shot dense retrieval?","31":"What is the purpose of constructing a Retrieval-Augmented Generation Benchmark (RGB) and what specific abilities does it evaluate?","32":"What challenges do language models face when retrieving information from their input contexts in open-domain question answering?","33":"What is the trade-off between providing more context and its impact on language model performance in open-domain QA?","34":"What model did ARES use as its in-context learning baseline?","35":"What are some limitations of using FLAN-T5-XXL in the ARES evaluation framework?","36":"What figure shows the overall performance of FLARE and baseline methods across all tasks\/datasets?","37":"What is GraphRAG and how does it differ from vector RAG in terms of its approach to knowledge graph extraction?","38":"How can the retrieval module be optimized to aid in a specific downstream task such as question answering?","39":"What training methodology did the researchers use for their Dense Passage Retriever model?","40":"What method did Gao et al. introduce to improve LLMs' generation quality without hurting their versatility?","41":"How does the effectiveness of FLARE in multihop QA tasks compare to other baselines, and what factors contribute to its performance?"},"retrieved_contexts":{"0":[" 48.4 _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nReAct _\u2212_ | _\u2212_ 35.1 | _\u2212_ _\u2212_ | _\u2212_ _\u2212_ | _\u2212_\n\nSelfAsk _\u2212_ | _\u2212_ _\u2212_ | _\u2212_ 40.1 | _\u2212_ 15.2 | _\u2212_\n\nDecomP _\u2212_ | 50.0 _\u2212_ | _\u2212_ _\u2212_ | 59.3 _\u2212_ | _\u2212_\n\n\nIRCoT QA **45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8**\n\n\nTable 1: Comparison with other LLM-based ODQA\nsystems on EM and F1 scores. \u2018 _\u2212_ \u2019: score is unavailable. HpQA [Br] : Bridge questions subset of HotpotQA.\nMQ [2H] : MuSiQue 2-hop questions. IRCoT QA with\nGPT3 (ours) outperforms other systems by a large margin. Note: Comparisons aren\u2019t head-to-head as discussed in the text. App. \u00a7C reports updated SOTA numbers, including contemporaneous and newer works.\n\n\nAs shown in Table 1, IRCoT QA significantly\n\n- utperforms all of these recent systems by a large\nmargin, setting a new state of the art in terms of\nwhat\u2019s achievable via retrieval-augmented LLMs\n(without supervised training).\n\n\n**6** **Conclusions**\n\n\nChain-of-thought prompting has significantly improved LLMs\u2019 ability to perform multi-step reason\n\n**Question** **Gold Facts** **NoR QA** **OneR QA** **IRCoT QA**\n\n\n\nThe tap dancing\nsoloist in My\nFeet Keep\nDancing also\nstarred in what\n\n1935 MGM\n\nmusical?\n\n\nWho is the\n\nfather-in-law\n\n- f Queen Hyojeong?\n\n\nWhat is the\n\nname - f the\n\ncastle in the\ncity where the\nperformer - f\nA Collection\n\n1984\u20131989 was\n\nborn?\n\n\n\n_\u21d2_ The tap dancing\nsoloist in My Feet\nKeep Dancing is Fayard\nNicholas.\n_\u21d2_ Fayard Nicholas\nstarred in the 1935\n\nM"," generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n\n\n**2** **Retrieval-Augmented Paradigm**\n\n\nIn this section, we first give a general formulation\n\n- f retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrievalaugmented generation paradigm, including the re\n\nOutput\n\n\n\n\n\nFigure 1: The overview of this survey.\n\n\n\ntrieval source, retrieval metric and integration meth\n- ds.\n\n\n**2.1** **Formulation**\n\n\nMost text generation tasks can be formulated as a\nmapping from input sequence _**x**_ to output sequence\n_**y**_ : _**y**_ = _f_ ( _**x**_ ). For instance, _**x**_ and _**y**_ could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\n\n\n_**y**_ = _f_ ( _**x**_ _,_ _**z**_ ) (1)\n\n\nwhere _**z**_ = _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that _**y**_ _[r]_\n\nmay benefit the response generation, if _**x**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r"," for example,\ndata compression for the retrieval memory.\n\n\n**Local vs. Global Optimization** Theoretically, it\nseems promising to jointly learn retrieval metrics\nand generation models. However, in practice, there\nis an essential gap about the retrieval metric between the training and inference phrases. In the\ntraining phase, the loss is locally back-propagated\nto only a few retrieved examples while in the inference phase the metric is globally conducted among\nall examples in the memory. It would be interesting\nto narrow such a gap when learning a better metric\nfor generation tasks.\n\n\n**Multi-Modalities** With recent advancement in\n\nimage-text retrieval, directly associating images\nwith relevant text becomes possible. This urges\nresearchers to investigate the possibility of retrievalbased text generation in tasks that involve data from\ndifferent modalities. One typical task is image\ncaptioning. Beyond images, other tasks like speechto-text transcription could potentially benefit from\nretrieval-based generation methods as well.\n\n\n**Diverse & Controllable Retrieval** Most of the\n\nexisting approaches adopt a universal metric for\nretrieval, such as lexical similarities of sentences.\nFuture work should explore how to use customized\nmetrics for retrieval. This can be beneficial for\nmore controlled text generation. For example, instances with emotions and styles may be more desirable in the personalized dialogue generation, parallel data that contains specific terminologies is\nmore helpful in machine translation, and so on. On\nthe other hand, using a universal metric for retrieval\nmay lead to the lack of diversity of the retrieval results. Collecting a diverse set of retrieval results\ncan improve the coverage of useful information.\nThus, considering multiple different metrics for retrieval may lead to generation with higher quality\nin the future.\n\n\n**7** **Conclusion**\n\n\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different components of retrieval-augmented text generation including retrieval metrics, retrieval sources, and integration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other generation tasks. We also pointed out some future directions for retrieval-augmented text generation.\n\n\n**References**\n\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly\nlearning to align and translate. _","## **A Survey on Retrieval-Augmented Text Generation**\n\n**Huayang Li** _[\u2665][,][\u2217]_ **Yixuan Su** _[\u2660][,][\u2217]_ **Deng Cai** _[\u2666][,][\u2217]_ **Yan Wang** _[\u2663][,][\u2217]_ **Lemao Liu** _[\u2663][,][\u2217]_\n\n_\u2665_ Nara Institute of Science and Technology _\u2660_ University of Cambridge\n_\u2666_ The Chinese University of Hong Kong _\u2663_ Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\n\n\n\n**Abstract**\n\n\nRecently, retrieval-augmented text generation\nattracted increasing attention of the computational linguistics community. Compared\nwith conventional generation models, retrievalaugmented text generation has remarkable advantages and particularly has achieved state-ofthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrievalaugmented text generation. It firstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks including\ndialogue response generation, machine translation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval technology, has achieved state-of-the-art (SOTA) performance in many NLP tasks and attracted the attention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counterpart, this new paradigm has some remarkable advantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation"," et al. 2023)\nfocuses on human exam and ToolBench (Qin et al. 2023)\nevaluates how well LLMs use external tools. Recently, Adlakha et al. (2023) evaluate the RAG of LLMs in exist QA\ndataset. Different from their work, we focus on 4 required\nabilities of RAG and create Retrieval-Augmented Generation Benchmark to evaluate the LLMs.\n\n\n**Retrieval-Augmented Generation Benchmark**\n\n\nIn this section, we first introduce the specific retrievalaugmented generation abilities we aim to evaluate. Next, we\n\n- utline the process of constructing the RAG benchmark for\nevaluation. Lastly, we present the evaluation metrics.\n\n\n**Required abilities of RAG**\n\n\nExternal knowledge is the key to resolving the problems\n\n- f LLMs such as hallucination and outdated knowledge,\nwhich can make LLMs generate more accurate and reliable\nresponses through retrieval-augmented generation (RAG).\nHowever, LLMs cannot always response as expected with\nRAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating\nthese external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the\nuseful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the\nretrieval-augmented generation of LLMs, and we concern\nabout 4 specific abilities:\n**Noise Robustness** is the robustness of LLMs in noisy\ndocuments. As retrievers are not perfect, the external knowledge they retrieve often contains a significant amount of\nnoise, i.e., documents which are relevant to the question but\ndo not contain any information about the answer. To effectively answer user questions, LLMs must be able to extract\nthe necessary information from documents despite there are\nnoisy documents.\n**Negative Rejection** is a measure of whether LLMs can\ndecline to answer a question when none of the contexts provide useful information. In real-world situations, the search\nengine often fails to retrieve documents containing the answers. In these cases, it is important for the"],"1":["score and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is twice as large as the extractive\nmodels and has 770M parameters. We reuse the hyperparameters as described in Izacard & Grave\n(2020). The original FiD uses the top 100 passages for NaturalQuestions. In our case, we use the top\n50 retrieved passage sequences and concatenate the passages in each sequence before feeding into T5.\nIn order to fit this model into GPU, we make use of PyTorch checkpoint [11] for training.\n\n\nB.2.3 MULTI-HOP RAG\n\n\nThe RAG model aims to generate answer _y_ given question _x_ and the retrieved documents _z_ . Similarly,\nthe goal of multi-hop RAG can be expressed as: generate answer _y_ given question _x_ and retrieved\ndocuments in hop one _z_ 1 and hop two _z_ 2 (Limiting to two hops for HotpotQA). The model has three\ncomponents:\n\n\n   - Hop-one retriever _p\u03b7_ 1( _z_ 1 _|x_ ) with parameter _\u03b7_ 1 to represent the retrieved top-k passage\ndistribution (top-k truncated distribution) given the input question _x_ .\n\n\n    - Hop-two retriever _p\u03b7_ 2( _z_ 2 _|x, z_ 1) with parameter _\u03b7_ 2 to represent the hop-two retrieved top-k\npassage distribution given not only the question _x_ but also the retrieved document _z_ 1 from\nhop-one.\n\n\n   - A generator _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _,, y_ 1: _i\u2212_ 1) to represent the next token distribution given input\nquestion _x_, hop-one retrieved document _z_ 1, hop-two retrieved document _z_ 2 and previous\npredicted token _y_ 1: _i\u2212_ 1 parametrized by _\u03b8_\n\n\n**Multi-Hop RAG Sequence Model** As the RAG Sequence model, this model generates the answer\nsequence given the fixed set of documents from hop-one retriever and hop-two retriever. In order"," generation models,\nnumerical comparison is still a bigger issue compared to extractive models.\n\n\nB MODEL DETAILS\n\n\nB.1 BEST MODEL HYPERPARAMETERS\n\n\nTable 11: Hyperparameters of Retriever\n\nlearning rate 2e-5\nbatch size 150\nmaximum passage length 300\nmaximum query length at initial hop 70\nmaximum query length at 2nd hop 350\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 50\nweight decay 0\n\n\nTable 12: Hyperparameters of Extractive Reader (ELECTRA)\n\n\nlearning rate 5e-5\nbatch size 128\nmaximum sequence length 512\nmaximum answer length 30\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 7\nweight decay 0\n# of negative context per question 5\nweight of SP sentence prediction loss 0.025\n\n\n16\n\n\n\nTable 10: Answer EM using top 50 retrieved passage chains\n\n\n\nComp Bridge\nModel Overall\n(20%) (80%)\n\n\n\nELECTRA 61.7 79.0 57.4\n\nFiD 61.7 75.3 58.3\n\n\nPublished as a conference paper at ICLR 2021\n\n\nB.2 FURTHER DETAILS ABOUT READER MODELS\n\n\nB.2.1 EXTRACTIVE READER\n\n\nThe extractive reader is trained with four loss functions. With the [CLS] token, we predict a\nreranking score based on whether the passage sequence match the groundtruth supporting passages.\nOn top of the representation of each token, we predict a answer start score and answer end score.\nFinally, we prepend each sentence with the [unused0] special token and predict whether the\nsentence is one of the supporting sentences using the representations of the special token. At training\ntime, we pair each question with 1 groundtruth passage sequence and 5 negative passage sequence\nwhich do not contain the answer. At inference time, we feed in the top 250 passage sequences from\nMDR. We rank the predicted answer for each sequence with a linear combination of the reranking\nscore and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is"," on A100 GPUs).\n\n\n**Architecture.** We simplify prior deep sequence model architectures by combining the design of prior SSM architectures\n(Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous\narchitecture design ( **Mamba** ) incorporating selective state spaces.\n\n\nSelective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them\nsuitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong\nperformance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory\nscales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only\nconstant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency\ntogether yield performance improvements on real data up to sequence length 1M.\n\n\nWe empirically validate Mamba\u2019s potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n\n**Synthetics.** On important synthetic tasks such as copying and induction heads that have been proposed as being key to\nlarge language models, Mamba not only solves them easily but can _extrapolate solutions indefinitely long_ ( _>_ 1M tokens).\n\n\n- **Audio and Genomics.** Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers\n\n - n modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing\nFID on a challenging speech generation dataset by more than half). In both settings, its _performance improves with longer_\n_context up to million-length sequences_ .\n\n\n- **Language Modeling.** Mamba is the first _linear-time sequence model that truly achieves Transformer-quality performance_,\nboth in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba\nexceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based\n\n - n LLaMa (Touvron et al. 2023). Our Mamba language model has 5\u00d7 generation throughput compared to Transformers\n\n - f similar size, and Mamba-","7). In that case, systems start by retrieving\n\n\n\nFigure 1: A simple approach to open domain question\nanswering. First, it retrieves support text passages from\nan external source of knowledge such as Wikipedia.\nThen, a generative encoder-decoder model produces\nthe answer, conditioned on the question and the retrieved passages. This approach scales well with the\nnumber of retrieved passages, as the performance keeps\nimproving when retrieving up to one hundred passages.\n\n\nsupport documents, before extracting the answer\nfrom these documents. Different retrieval tech\nniques have been considered, either using sparse\nrepresentations based on TF\/IDF or using dense\nembeddings (Guu et al., 2020; Karpukhin et al.,\n2020). The models which extract the answers are\n\n- ften based on contextualized word representations\nsuch as ELMo or BERT (Peters et al., 2018; Devlin et al., 2019), and predict a span as answer.\nAggregating and combining evidence from multiple passages is not straightforward when using\nextractive models, and multiple techniques have\nbeen proposed to address this limitation (Clark and\nGardner, 2018; Min et al., 2019a).\nIn this paper, we explore a simple approach having the best of both worlds, by building on the\nexciting developments in generative modeling and\nretrieval for open domain question answering. This\nmethod proceeds in two steps, by first retrieving\nsupporting passages using either sparse or dense\n\n\n```\nQuestion + Passage 1 encoder\n\nQuestion + Passage 2 encoder\n\nQuestion + Passage N encoder\n\n```\n\n```\nconcat \u2026 decoder Answer\n\n```\n\n\nFigure 2: Architecture of the Fusion-in-Decoder method.\n\n\n\nrepresentations. Then, a sequence-to-sequence\nmodel generates the answer, taking as input the retrieved passages in addition to the question. While\nconceptually simple, this method sets new state-ofthe-art results on the TriviaQA and NaturalQuestions benchmarks. In particular, we show that the\nperformance of our method significantly improves\nwhen the number of retrieved passages increases.\nWe believe that this is evidence that generative models are good at combining evidence from multiple\npassages, compared to extractive ones.\n\n\n**2** **Related work**\n\n\n**Open domain question answering** is the task\n\n- f answering general domain questions, in which\nthe evidence is not","Ms) have seen improvements in\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\nhave transitioned from traditional term-based techniques like **TF-IDF** (Sp\u00a8arck Jones, 1972) and\n**BM25** (Robertson et al., 1995; Roberts et al., 2020) to deep learning\u2013based strategies (Karpukhin\net al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using\nlarge language models as retrievers due to their ability to memorize extensive knowledge (Yu et al.,\n2022; Sun et al., 2022). Research on the reader component includes **Fusion-in-Decoder (FiD)**\n(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages\nindependently in the encoder and **RETRO** (Borgeaud et al., 2022; Wang et al., 2023), which utilizes\ncross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context.\n\n\nEnd-to-end system training work includes **Atlas** (Izacard et al., 2022), which fine-tunes an encoderdecoder model in conjunction with the retriever; **REALM** (Guu et al., 2020), a bidirectional, masked\nLM fine-tuned for open-domain question answering; and **RAG (Retrieval-Augmented Genera-**\n**tion)** (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural\nretriever. Min et al. (2021) introduced **Joint Passage Retrieval (JPR)** model which uses a treedecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. **Dense Hi-**\n**erarchical Retrieval (DHR)** and **Hybrid Hierarchical Retrieval (HHR)** represent advancements\nin retrieval accuracy by combining document and passage level retrievals and integrating sparse and\ndense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023).\n\n\n[1We will release the code of RAPTOR publicly here.](https:\/\/github.com\/parthsarthi03\/raptor)\n\n\n2\n\n\nPublished as a conference paper at"],"2":[" with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does not know, it can still refer to the context.\n\n\n4.1 Results\n\n\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\n\n- utperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes\/no question, we don\u2019t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.\n\n\nOverall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its\nanswering style not aligning with the ground truth. By applying domain-specific tuning,\nwe significantly enhance its performance. This process enables the model to learn and adopt\nthe appropriate style of answering. However, introducing RAG to a domain-specifically\nfine-tuned (DSF) model doesn\u2019t invariably lead to better outcomes. This might indicate that\nthe model lacks training in context processing and extracting useful information from it. By\nincorporating our method, RAFT, we train the model not only to match its answering style\nwith that required but also to improve its document processing capabilities. Consequently,\n\n- ur approach outperforms all others.\n\n\n4.2 Effect of CoT\n\n\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\nin enhancing the model\u2019s performance. As indicated in Table 2, simply providing the answer\nto a question"," LLM without the correct corresponding context at times can be\nbeneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we maintain this format by providing the golden document with four distractors.\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of\ntraining data without the golden documents in the context proves to be advantageous.\n\n\n**5** **RAFT Generalizes to Top-K RAG**\n\n\nWe now study another important problem: How does the number of distractor documents\nin RAFT affect the model\u2019s performance when augmented with top-k RAG results during\nevaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text\n(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is\nparticularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to\nensure high recall. Such a scenario necessitates the model to have the ability to discern and\ndisregard irrelevant content, focusing solely on pertinent information.\n\n\n5.1 Making Model Robust to top-K RAG\n\n\nTo tackle the challenge of enhancing large language models\u2019 (LLMs) ability to sift through\nirrelevant text within the retrieval pipeline, our analysis revealed that training solely with\ngolden (highly relevant) documents can inadvertently diminish the model\u2019s ability to discern and disregard irrelevant information. To address this, our algorithm, RAFT, adopts\na strategy that integrates golden documents with a mix of irrelevant ones. This method\n- logy prompts us to investigate the ideal fraction of distractor (irrelevant) documents to\nincorporate throughout the training process and to assess how well this training approach\nadapts to different volumes of documents encountered by the Retrieval-Augmented Generation (RAG) during the test phase. Our aim is to refine the balance between relevant\nand irrelevant information to strenghten the model\u2019s efficiency in identifying and utilizing\npertinent content. Notice that Sec 4.4 looked what what P% of training data should include\ndistractors, while in this section, we study test-time scenarios.\n\n\n**Training with Distractor Documents** To enhance the robustness of LLMs against irrelevant\ntext in retrieved documents, we adopted a fin","begin_quote## The Oberoi family is an\n Indian family that is famous for its involvement in hotels, namely through The\n Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\n the Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel\n company with its head office in Delhi. ##end_quote## establishes the head\n office of The Oberoi Group. Therefore, the Oberoi family is part of a hotel\n company whose head office is in Delhi. ##Answer: Delhi\n\n```\n\nFigure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,\ncontrasting them with the correct reasoning and answers. The LLM is prompted to identify\nerrors in its reasoning and extract key insights for improvement. This figure specifically\nrepresents the \u2018GenerateExplanation\u2018 step in the RAFT algorithm (Section 3).\n\n\nTable 1: **RAFT improves RAG performance for all specialized domains** : Across PubMed,\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Finetuning improves significantly of the performance of the base model, RAFT consistently\n\n- utperforms the existing domain-specific finetuning method with or without RAG. This\nsuggests the need to train the model with context. We compare our model with LLaMA\nfinetuning receipes, and provide GPT-3.5 for reference.\n\n\nPubMed HotPot HuggingFace Torch Hub TensorFlow\n\n\nGPT-3.5 + RAG 71.60 **41.5** 29.08 60.21 65.59\n\n\nLLaMA2-7B 56.5 0.54 0.22 0 0\n\nLLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06\n\nDSF 59.7 6.38 61.06 84.94 86.56\n\nDSF + RAG 71.6 4.41 42.59 82.80 60.29\n\n\nRAFT (LLaMA2-7B) **73.30** 35.28 **74.00** **84.95** **86.86**\n\n\n**Datasets** In our experiments, we use the following datasets to evaluate our model and\nall baselines. We","underscores the importance of incorporating both standard instructional tuning and context\ncomprehension into the training dataset to preserve and enhance the model\u2019s ability to\nprocess text effectively.\n\n\nFigure 4: **Comparison of RAFT and DSF** : On the HotPot QA dataset, we can see that DSF\nmodel extracts the wrong information from the context when the question is asking who is\nthe screen writer and it answers a film name. RAFT manages to get the accurate results .\n\n\n4.4 Should we train the LLM always with the golden context for RAG?\n\n\nIn our exploration of whether large language models (LLMs) should always be trained with\nthe golden context for Retrieval-Augmented Generation (RAG), we address a key question:\nwhat proportion (p%) of the training data should include golden documents? Intuitively,\n\n- ne might assume that for effective training in reading and extracting information from\ncontext (e.g., RAG tasks), the golden document should always be included during training\n(P = 100%). However, our findings challenge this assumption: incorporating a portion of\nthe training data without the golden document in the context (P = 80%) appears to enhance\nthe model\u2019s performance on RAG tasks.\n\n\n7\n\n\nPreprint, Under Review\n\n\n\n\n\n\n\n\n\n\n|Test Domain: NQ|Col2|Col3|ain: NQ|Col5|Col6|\n|---|---|---|---|---|---|\n|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ|0<br>20<br>40<br>60<br>80<br>100<br> <br>0.25<br>0.30<br>0.35<br>0.40<br>0.45<br>Final Accuracy<br>Test Domain: NQ|ain: NQ|ain: NQ|ain: NQ|\n|0<br>20<br>40<br>60<br>80<br>100<br>"," distractor documents (] _[d][k][\u2212]_ [1][). For][ (][1] _[ \u2212]_ _[P]_ [)][ fraction of the questions]\n( _qi_ ) in the dataset, we include no golden document and only include distractor documents\n( _dk_ ). We then fine-tune the language model using standard supervised training (SFT)\ntechnique, training it to generate answers from the provided documents and question. Fig. 2\nillustrates the high-level design principal for RAFT .\n\n\nWe demonstrate that our RAG approach trains the model to perform better RAG on the set\n\n- f documents it is trained on _i.e., in-domain_ . By removing the golden documents in some\ninstances, we are compelling the model to memorize answers instead of deriving them from\nthe context. The training data for RAFT is as follows, and an example training data can be\nseen in Fig. 3:\n\n\n**P** % of data: **Q** + **D** _[\u2217]_ + **D** 1 + **D** 2 + . . . + **D** _k \u2192_ **A** _\u2217_\n\n\n(1 _\u2212_ **P** ) % of data: **Q** + **D** 1 + **D** 2 + . . . + **D** _k \u2192_ **A** _\u2217_\n\n\nSubsequently, for the test scenario, the model is provided with the Q and top-k documents\nretrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.\n\n\nA key factor in enhancing training quality is the generation of a reasoning process, such\nas Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we\ndemonstrate that creating a full reasoning chain and in-addition, clearly citing sources\nenhances the model\u2019s accuracy in answering questions. In Fig. 3, we illustrate this setup. Generating the training data in this fashion, involves presenting the model with a\nquestion, context, and verified answers, and then requesting it to form a reasoning chain\nthat appropriately references the original context.\n\n\nFor all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an"],"3":["6 5.0 4.6\nWQ 5.9 5.9 5.8 5.7\n\nCuratedTrec 6.9 7.2 6.3 5.9\n\nSQuAD 4.5 4.1 4.0 4.0\n\n\nTable 2: The Jaccard overlap between sparse retrieval\nresults and dense retrieval results.\n\n\nhybrid retrieval is less effective than dense retrieval in most settings, which is inconsistent with\n\n- ur experimental results. Instead, we found that\ndense\u2013sparse retrieval consistently beats sparse retrieval across all settings. The gains from both\nhybrid scoring techniques are statistically significant, with the exception of top-20 for CuratedTREC. Our results might be due to better BM25\neffectiveness, but we are unable to further diagnose these differences because, once again, the hybrid retrieval code is not provided in the DPR repo.\nFurther testing also found that the differences between the two hybrid techniques are not significant. Thus, there does not appear to be a strong\nbasis to prefer one hybrid technique over the other.\nIn Table 2, we report overlap when taking different top-k results from dense retrieval and sparse\nretrieval. Overlap is measured in terms of Jaccard overlap, which is computed by the intersection over the union. It is apparent that the overlap between dense and sparse results is quite small,\nwhich suggests that they are effective in very different ways. This provides an explanation of why\nhybrid retrieval is effective, i.e., they are exploiting very different signals. These results also justify the DPR design choice of retrieving k [\u2032] - k\nresults from dense and sparse retrieval and then\nrescoring the union to arrive at the final top-k.\n\n\n4.2 End-to-End QA\n\n\nTable 3 presents results for our end-to-end question answering replication experiments on the NQ\nand TriviaQA datasets in terms of the exact match\nscore. The original results are shown in the\n\u201corig\u201d column. The \u201crepl\u201d column reports our attempt to replicate exactly the span scoring technique described in the original paper, whereas the\n\u201cGAR\u201d column shows results from using the technique proposed by Mao et al. (2020). The version of each technique that incorporates retriever\nscores (see Section 2.3) is denoted with a * symbol, i.e., \u201crepl"," original DPR\nwork, Pyserini supports dense retrieval via integration with Facebook\u2019s Faiss library. Combining\ndense and sparse retrieval, our toolkit supports hybrid retrieval as well.\n\nTo be clear, we started with model checkpoint\nreleases in the DPR repo and did not retrain the\nquery and passage encoders from scratch. Otherwise, our implementation does not share any code\nwith the DPR repo, other than evaluation scripts to\nensure that results are comparable.\n\n\nSimilar to the original work, we calculated\nhybrid retrieval scores by linear combination of\ndense and sparse scores, as follows:\n\n\nSim(q, Dj) + \u03b1 \u00b7 BM25(q, Dj).\n\n\nNote that, contrary to the original work, we placed\nthe \u03b1 weight on the BM25 score because this\nyields a more natural way to answer the pertinent\nresearch question: Given dense retrieval as a starting point, does adding BM25 as an additional relevance signal provide any value? This question is\nanswered by a setting of \u03b1 = 0, which is equivalent to discarding BM25 results.\nFinally, there are a few more details of exactly\nhow to combine BM25 and DPR scores worth ex\nploring. As a baseline, we tried using the raw\nscores directly in the linear combination (exactly\nas above). However, we noticed that the range of\nscores from DPR and BM25 can be quite different. To potentially address this issue, we tried the\nfollowing normalization technique: If a document\nfrom sparse retrieval is not in the dense retrieval\nresults, we assign to it the the minimum dense retrieval score among the retrieved documents as its\ndense retrieval score, and vice versa for the sparse\nretrieval score.\n\nTo arrive at a final top-k ranking, the original\nDPR paper generated top k [\u2032] results from DPR and\ntop k [\u2032] results from BM25 (where k [\u2032] - k), before considering the union of the two result sets\nand combining the scores to arrive at the final top\nk. Karpukhin et al. (2020) set k [\u2032] = 2000, but after some preliminary experimentation, we decided\nto fix k [\u2032] = 1000 in our experiments since it is a\nmore common setting in information retrieval experiments (for example, k = 1000 is the default in\nmost TREC evaluations).\n\n\n2.2 Reader\n\n\nAs is standard in a retriever"," with this parlance,\nwe use \u201cdocument\u201d and \u201cpassage\u201d interchangeably throughout this paper. To add to the potential\nconfusion, results of the retriever are also referred\n\nto as \u201ccontexts\u201d that are fed to the reader.\n\nDense retrieval with DPR uses a query encoder\nand a passage encoder, which are both based on\nBERT. Queries and passages are encoded as dense\nrepresentation vectors as follows:\n\n\nq [\u2217] = BERTq(q), Dj [\u2217] [=][ BERT][D][(][D][j][)]\n\n\nwhere q [\u2217] and Dj [\u2217] [are low dimensional vectors]\n(768). The relevance score of a passage to a query\nis computed by dot product:\n\n\nSim(q, Dj) = \u27e8q [\u2217], Dj [\u2217][\u27e9]\n\n\nThus, the top k retrieval problem can be recast\nas a nearest neighbor search problem in vector\nspace. Operationally, this is accomplished via\nFacebook\u2019s Faiss library (Johnson et al., 2017).\n\nKarpukhin et al. (2020) also investigated hybrid\nretrieval, combining results from dense retrieval\n(DPR) and sparse retrieval (BM25) by computing\nthe linear combination of their respective scores to\nrerank the union of the two initial retrieved sets:\n\n\n\u03bb \u00b7 Sim(q, Dj) + BM25(q, Dj),\n\n\nwhere \u03bb = 1.1, an empirical value tuned on the\ndevelopment set. BM25 retrieval was performed\nusing Lucene with parameters b = 0.4 and k1 =\n0.9. However, the DPR repo does not appear to\ncontain code for reproducing the BM25 and hybrid\nfusion results.\n\nWe attempted to replicate the retriever results\nreported in Karpukhin et al. (2020) with Pyserini,\nan IR toolkit that our group has been developing\nsince 2019 (Lin et al., 2021). The toolkit supports\nsparse retrieval (i.e., BM25) via integration with\nanother toolkit called Anserini (Yang et al., 2017),\nwhich is built on Lucene. Like in the original DPR\nwork, Pyserini supports dense retrieval via integration with Facebook\u2019s Faiss library. Combining\ndense and sparse retrieval, our toolkit supports hybrid retrieval as well.\n\nTo be clear, we started with model checkpoint\nreleases in the",", our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar of text retrieval using learned dense representations, which has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using sparse bag-of-words\nrepresentations (Zhan et al., 2020; Xiong et al.,\n2020; Hofst\u00a8atter et al., 2020; Lin et al., 2020).\nOur experiments largely verify the claims of\nKarpukhin et al. (2020) regarding the effectiveness of their proposed techniques. However, we\narrived at two important additional findings, one\n\n- f which is inconsistent with the original work, the\n\n- ther of which presents an enhancement:\n\n\n1. Focusing on retrieval, we found that the effectiveness of the sparse retrieval (BM25)\nbaseline is higher than values reported by\nKarpukhin et al. (2020). Whereas they reported\nthat dense\u2013sparse hybrid results do not meaningfully improve over dense retrieval alone,\nwe arrived at the opposite conclusion, where\nhybrid techniques yield statistically significant\ngains. We are able to achieve on average\na three-point improvement in top-20 accuracy\n\n  - ver the best DPR results across five standard\nQA test collections.\n\n\n2. Focusing on end-to-end QA effectiveness, we\nexplored different techniques for evidence com\n\n[1Artifact Review and Badging](https:\/\/www.acm.org\/publications\/policies\/artifact-review-and-badging-current)\n\n\nbination to extract the final answer span.\nWhereas the original DPR paper only used\nscores from the reader to identify the final\nanswer span, we investigated combining retriever scores and further experimented with\nthe answer span selection technique described\nby Mao et al. (2020). In our best condition,\nwe were able to achieve statistically significant\nimprovements of around three points on exact\nmatch scores over the original DPR implementation, using the same exact models.\n\n\nThe main contribution of this work is the replication of DPR, where our experimental results\nadd a number of important refinements to the\n\n- riginal work. Code associated with our retrieval experiments is packaged in our Pyserini IR\ntoolkit [2] (Lin et al., 2021) and code associated with\n\n-","ing by emphasizing on _important_ terms and discarding\nmost of the terms without information content. Expansion allows\nto enrich documents, either by implicitly adding stemming effects\n(legs \u2192 leg) or by adding relevant topic words (e.g. treatment).\n\n\n**5** **CONCLUSION**\n\n\nRecently, dense retrieval based on BERT has demonstrated its superiority for first-stage retrieval, questioning the competitiveness of\ntraditional sparse models. In this work, we have proposed SPLADE,\na sparse model revisiting query\/document expansion. Our approach\nrelies on in-batch negatives, logarithmic activation and FLOPS regularization to learn effective and efficient sparse representations.\nSPLADE is an appealing candidate for initial retrieval: it rivals the\nlatest state-of-the-art dense retrieval models, its training procedure\nis straightforward, its sparsity\/FLOPS can be controlled explicitly\nthrough the regularization, and it can operate on inverted indexes.\nIn reason of its simplicity, SPLADE is a solid basis for further improvements in this line of research.\n\n\n\n33\n\n\n32\n\n\n31\n\n\n30\n\n\n29\n\n\n28\n\n\n27\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||a<br>a|a<br>a|vg q len <br>vg d len|_ \u2243_15<br>_ \u2243_58||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||y-T5<br>|y-T5<br>|\n||||||||||||doc2quer<br>|y-T5<br>|y-T5<br>|\n||||~~a~~<br>a|~~a~~<br>a|~~vg q len~~ <br>vg d len|~~_ \u2243_6~~<br>_ \u2243_18||<br> <br>|||~~SparTerm~~<br>ST exp-_\u2113F_<br>SPLADE-|~~ lexical~~<br>_LOPS_<br>_\u2113_~~1~~||\n||||~~a~~<br>a|~~a~~<br>a||||||||||\n||||||||||||SPLADE-|_\u2113FLOPS_|_\u2113FLO"],"4":[" the index\n\n- therwise. [8] As discussed in \u00a74.1, we use ColBERTL2 for end-toend retrieval, which employs negative squared L2 distance as its\nvector-similarity function. For its latency, we measure the time for\nfaiss-based candidate filtering and the subsequent re-ranking. In\nthis experiment, faiss uses all available CPU cores.\nLooking at Table 2, we first see Anserini\u2019s BM25 baseline at 18.7\nMRR@10, noticing its very low latency as implemented in Anserini\n(which extends the well-known Lucene system), owing to both\nvery cheap operations and decades of bag-of-words top- _k_ retrieval\n\n- ptimizations. Te three subsequent baselines, namely doc2query,\nDeepCT, and docTTTTquery, each brings a decisive enhancement\nto effectiveness. Tese improvements come at negligible overheads\nin latency, since these baselines ultimately rely on BM25-based\nretrieval. Te most effective among these three, docTTTTquery,\ndemonstrates a massive 9% gain over vanilla BM25 by fine-tuning\nthe recent language model T5.\n\n\n7htp:\/\/anserini.io\/\n8In practice, a myriad of reasons could still cause DeepCT\u2019s latency to differ\nslightly from BM25\u2019s. For instance, the top- _k_ pruning strategy employed, if any, could\ninteract differently with a changed distribution of scores.\n\n\nShifing our atention to ColBERT\u2019s end-to-end retrieval effectiveness, we see its major gains in MRR@10 over all of these end-toend models. In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due\nto the improved recall. Moving beyond MRR@10, we also see large\ngains in Recall@ _k_ for _k_ equals to 50, 200, and 1000. For instance,\nits Recall@50 actually exceeds the official BM25\u2019s Recall@1000 and\neven all but docTTTTTquery\u2019s Recall@200, emphasizing the value\n\n- f end-to-end retrieval (instead of just re-ranking) with ColBERT.\n\n|BERT [CLS]-based dot-product (5-layer) [A]|Col2|Col3|Col4|Col5|Col6","3Te public BERT implementations we saw simply pad to a pre-defined length.\n\n\n- ver all document embeddings. We map each of those to its document of origin, producing _Nq_ \u00d7 _k_ [\u2032] document IDs, only _K_ \u2264 _Nq_ \u00d7 _k_ [\u2032]\n\n- f which are unique. Tese _K_ documents likely contain one or more\nembeddings that are highly similar to the query embeddings. For\nthe second stage, we refine this set by exhaustively re-ranking _only_\nthose _K_ documents in the usual manner described in \u00a73.5.\nIn our faiss-based implementation, we use an IVFPQ index (\u201cinverted file with product quantization\u201d). Tis index partitions the\nembedding space into _P_ (e.g., _P_ = 1000) cells based on _k_ - means clustering and then assigns each document embedding to its nearest cell\nbased on the selected vector-similarity metric. For serving queries,\nwhen searching for the top- _k_ [\u2032] matches for a single query embedding, only the nearest _p_ (e.g., _p_ = 10) partitions are searched. To\nimprove memory efficiency, every embedding is divided into _s_ (e.g.,\n_s_ = 16) sub-vectors, each represented using one byte. Moreover,\nthe index conducts the similarity computations in this compressed\ndomain, leading to cheaper computations and thus faster search.\n\n\n**4** **EXPERIMENTAL EVALUATION**\n\n\nWe now turn our atention to empirically testing ColBERT, addressing the following research questions.\n**RQ** 1: In a typical re-ranking setup, how well can ColBERT bridge\nthe existing gap (highlighted in \u00a71) between highly-efficient and\nhighly-effective neural models? (\u00a74.2)\n**RQ** 2: Beyond re-ranking, can ColBERT effectively support endto-end retrieval directly from a large collection? (\u00a74.3)\n**RQ** 3: What does each component of ColBERT (e.g., late interaction, query augmentation) contribute to its quality? (\u00a74.4)\n**RQ** 4: What are ColBERT\u2019s indexing-related costs in terms of\n\n- ffline computation and memory overhead? (\u00a74.5)\n\n\n**4.1** **Methodology**\n\n\n_4.1","## **ColBERT: Efficient and Effective Passage Search via** **Contextualized Late Interaction over BERT**\n\n\n\nOmar Khatab\n\nStanford University\n\n         - khatab@stanford.edu\n\n\n**ABSTRACT**\n\n\nRecent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed\nto fine-tuning deep language models (LMs) for document ranking.\nWhile remarkably effective, the ranking models based on these LMs\nincrease computational cost by orders of magnitude over prior approaches, particularly as they must feed each query\u2013document pair\nthrough a massive neural network to compute a single relevance\nscore. To tackle this, we present ColBERT, a novel ranking model\nthat adapts deep LMs (in particular, BERT) for efficient retrieval.\nColBERT introduces a _late interaction_ architecture that independently encodes the query and the document using BERT and then\nemploys a cheap yet powerful interaction step that models their\nfine-grained similarity. By delaying and yet retaining this finegranular interaction, ColBERT can leverage the expressiveness of\ndeep LMs while simultaneously gaining the ability to pre-compute\ndocument representations offline, considerably speeding up query\nprocessing. Beyond reducing the cost of re-ranking the documents\nretrieved by a traditional model, ColBERT\u2019s _pruning-friendly_ interaction mechanism enables leveraging vector-similarity indexes\nfor end-to-end retrieval directly from a large document collection.\nWe extensively evaluate ColBERT using two recent passage search\ndatasets. Results show that ColBERT\u2019s effectiveness is competitive\nwith existing BERT-based models (and outperforms every nonBERT baseline), while executing two orders-of-magnitude faster\nand requiring four orders-of-magnitude fewer FLOPs per query.\n\n\n**ACM Reference format:**\nOmar Khatab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. In _Proceedings_\n\n_of Proceedings of the 43rd International ACM SIGIR Conference on Research_\n_and Development in Information Retrieval, Virtual Event, China, July 25\u201330,_\n_2020 (SIGIR \u201920),_ 10 pages.\nDOI: 10.1145\/3397271.3401075\n\n\n**1** **INTRODUCTION**\n\n\nOver the past few years, the Information Retrieval (IR) community\nhas witnessed the introduction of a host of neural"," original DPR\nwork, Pyserini supports dense retrieval via integration with Facebook\u2019s Faiss library. Combining\ndense and sparse retrieval, our toolkit supports hybrid retrieval as well.\n\nTo be clear, we started with model checkpoint\nreleases in the DPR repo and did not retrain the\nquery and passage encoders from scratch. Otherwise, our implementation does not share any code\nwith the DPR repo, other than evaluation scripts to\nensure that results are comparable.\n\n\nSimilar to the original work, we calculated\nhybrid retrieval scores by linear combination of\ndense and sparse scores, as follows:\n\n\nSim(q, Dj) + \u03b1 \u00b7 BM25(q, Dj).\n\n\nNote that, contrary to the original work, we placed\nthe \u03b1 weight on the BM25 score because this\nyields a more natural way to answer the pertinent\nresearch question: Given dense retrieval as a starting point, does adding BM25 as an additional relevance signal provide any value? This question is\nanswered by a setting of \u03b1 = 0, which is equivalent to discarding BM25 results.\nFinally, there are a few more details of exactly\nhow to combine BM25 and DPR scores worth ex\nploring. As a baseline, we tried using the raw\nscores directly in the linear combination (exactly\nas above). However, we noticed that the range of\nscores from DPR and BM25 can be quite different. To potentially address this issue, we tried the\nfollowing normalization technique: If a document\nfrom sparse retrieval is not in the dense retrieval\nresults, we assign to it the the minimum dense retrieval score among the retrieved documents as its\ndense retrieval score, and vice versa for the sparse\nretrieval score.\n\nTo arrive at a final top-k ranking, the original\nDPR paper generated top k [\u2032] results from DPR and\ntop k [\u2032] results from BM25 (where k [\u2032] - k), before considering the union of the two result sets\nand combining the scores to arrive at the final top\nk. Karpukhin et al. (2020) set k [\u2032] = 2000, but after some preliminary experimentation, we decided\nto fix k [\u2032] = 1000 in our experiments since it is a\nmore common setting in information retrieval experiments (for example, k = 1000 is the default in\nmost TREC evaluations).\n\n\n2.2 Reader\n\n\nAs is standard in a retriever"," of encoding documents offline and amortizing the cost of encoding the query once across\nall ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., [1, 15]) to retrieve the\ntop- _k_ results directly from a large document collection, substantially improving _recall_ - ver models that only re-rank the output of\nterm-based retrieval.\n\nAs Figure 1 illustrates, ColBERT can serve queries in tens or\nfew hundreds of milliseconds. For instance, when used for reranking as in \u201cColBERT (re-rank)\u201d, it delivers over 170\u00d7 speedup\n(and requires 14,000\u00d7 fewer FLOPs) relative to existing BERT-based\nmodels, while being more effective than every non-BERT baseline\n(\u00a74.2 & 4.3). ColBERT\u2019s indexing\u2014the only time it needs to feed\ndocuments through BERT\u2014is also practical: it can index the MS\nMARCO collection of 9M passages in about 3 hours using a single\nserver with four GPUs (\u00a74.5), retaining its effectiveness with a space\nfootprint of as litle as few tens of GiBs. Our extensive ablation\nstudy (\u00a74.4) shows that late interaction, its implementation via\nMaxSim operations, and crucial design choices within our BERTbased encoders are all essential to ColBERT\u2019s effectiveness.\nOur main contributions are as follows.\n\n\n(1) We propose _late interaction_ (\u00a73.1) as a paradigm for efficient\nand effective neural ranking.\n(2) We present ColBERT (\u00a73.2 & 3.3), a highly-effective model\nthat employs novel BERT-based query and document encoders within the late interaction paradigm.\n\n\n(3) We show how to leverage ColBERT both for re-ranking on\ntop of a term-based retrieval model (\u00a73.5) and for searching\na full collection using vector similarity indexes (\u00a73.6).\n(4) We evaluate ColBERT on MS MARCO and TREC CAR, two\nrecent passage search collections.\n\n\n**2** **RELATED WORK**\n\n\n**Neural Matching Models.** Over the past few years, IR researchers\nhave introduced numerous neural architectures for ranking. In\nthis work, we compare against KNRM [4, 36], Duet [20, 22], ConvKNRM [4], and fastText+Conv"],"5":[" we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\n\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70.0%, 72.5%, 75.0%, ..., 90.0%).\nEach split also represents a different mock RAG\nsystem. Since we know the success percentages of\neach dataset split, we know the appropriate ranking of each mock RAG system. This allows us to\n\n\ntest ARES success at both scoring and ranking the\nmock RAG systems appropriately across the three\nevaluation criteria.\n\n\n**4.3** **Metrics**\n\n\nTo calculate the correlation between the correct\n\nranking and the ARES ranking, we use the Kendall\nrank correlation coefficient or Kendall\u2019s _\u03c4_ :\n\n\n_\u03c4_ = [(#][ of concordant pairs][)] _[ \u2212]_ [(#][ of discordant pairs][)]\n\n# of pairs total\n\n\nConcordant pairs are defined as two ordinal values in the ranking where the earlier value in the\nsequence is lower than the later value in the sequence. Discordant pairs are defined as two ordinal\nvalues in the ranking where the earlier value in the\nsequence is greater than or equal to the later value\nin the sequence. A Kendall\u2019s _\u03c4_ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document preprocessing. We want to make sure that ARES has satisfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall\u2019s _\u03c4_ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pairwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in\ninformation retrieval, allowing developers to evaluate ranking systems empirically. Therefore, we\nbelieve Kendall\u2019s tau and prediction accuracy provide meaningful metrics for testing the efficacy of\nARES as a RAG evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges","## **Ragas: Automated Evaluation of Retrieval Augmented Generation**\n\n**Shahul Es** _[\u2020]_ **, Jithin James** _[\u2020]_ **, Luis Espinosa-Anke** _[\u2217\u2662]_ **, Steven Schockaert** _[\u2217]_\n\n_\u2020_ Exploding Gradients\n_\u2217_ CardiffNLP, Cardiff University, United Kingdom\n_\u2662_ AMPLYFI, United Kingdom\nshahules786@gmail.com,jamesjithin97@gmail.com\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n\n\n\n**Abstract**\n\n\nWe introduce **Ragas** ( **R** etrieval **A** ugmented\n**G** eneration **As** sessment), a framework for\nreference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG\nsystems are composed of a retrieval and an\nLLM based generation module, and provide\nLLMs with knowledge from a reference textual\ndatabase, which enables them to act as a natural language layer between a user and textual\ndatabases, reducing the risk of hallucinations.\nEvaluating RAG architectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to\nidentify relevant and focused context passages,\nthe ability of the LLM to exploit such passages\nin a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite\n\n  - f metrics which can be used to evaluate these\n\ndifferent dimensions _without having to rely on_\n_ground truth human annotations_ . We posit that\nsuch a framework can crucially contribute to\nfaster evaluation cycles of RAG architectures,\nwhich is especially important given the fast\nadoption of LLMs.\n\n\n**1** **Introduction**\n\n\nLanguage Models (LMs) capture a vast amount\n\n- f knowledge about the world, which allows them\nto answer questions without accessing any external sources. This idea of LMs as repositories of\nknowledge emerged shortly after the introduction\n\n- f BERT (Devlin et al., 2019) and became more\nfirmly established with the introduction of ever\nlarger LMs (Roberts et al., 2020). While the most\nrecent Large Language Models (LLMs) capture\nenough knowledge to rival human performance\nacross a wide variety"," 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of\nstate-of-the-art models, which are complex pipeline systems with domain-specific architectures and\nsubstantial engineering, trained using intermediate retrieval supervision, which RAG does not require.\n\n\n6\n\n\n**Document 1** : his works are considered classics of American\nliterature ... His wartime experiences formed the basis for his novel\n**\u201dA Farewell to Arms\u201d** (1929) ...\n\n\n**Document 2** : ... artists of the 1920s \u201dLost Generation\u201d expatriate\ncommunity. His debut novel, **\u201dThe Sun Also Rises\u201d**, was published\nin 1926.\n\n\n\nDoc 1\n\nDoc 2\n\nDoc 3\n\nDoc 4\n\nDoc 5\n\n\n\nFigure 2: RAG-Token document posterior _p_ ( _zi|x, yi, y\u2212i_ ) for each generated token for input \u201cHemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\nwhen generating \u201cA Farewell to Arms\" and for document 2 when generating \u201cThe Sun Also Rises\".\n\n\nTable 3: Examples from generation tasks. RAG models generate more specific and factually accurate\nresponses. \u2018?\u2019 indicates factually incorrect responses, * indicates partially correct responses.\n\n\nTask Input Model Generation\n\n\n\nBART ?The middle ear is the part of the ear between the middle ear and the nose.\nRAG-T The middle ear is the portion of the ear internal to the eardrum.\nRAG-S The middle ear includes the tympanic cavity and the three ossicles.\n\n\nBART The currency needed in Scotland is Pound sterling.\nRAG-T Pound is the currency needed in Scotland.\nRAG-S The currency needed in Scotland is the pound sterling.\n\n\n\nMS\nMARCO\n\n\nJeopardy\nQuestion\nGener\n\n- ation\n\n\n\ndefine middle\n\near\n\n\nwhat currency\nneeded in\n\nscotland\n\n\n\nThe Divine\nComedy\n\n\n\nBART ?This state has the largest number of counties in the U.S.\nWashington RAG-T It\u2019s the only U.S. state named for a U.S. president\nRAG-S It\u2019s the state where you\u2019ll find Mount Rainier National Park\n\n\n\nBART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio\n"," predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach. We also find that ARES consistently\ndistinguishes competitive RAG systems that are\n\n- nly a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop\n\n\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\n[available on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**2** **Related Work**\n\n\nRAG (Guu et al., 2020; Lewis et al., 2020; Khattab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combining them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual information (Shuster et al., 2021; Huo et al., 2023), and\n\n- ffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essential for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\n\n- ut of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. However, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min","\u201d, \u201cfaithfulness\u201d,\nand \u201canswer relevance\u201d (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can\nquantify relative performance for a given criterion by prompting the LLM to compare generations\nfrom two different competing models (LLM-as-a-judge, (Zheng et al., 2024)). In this work, we design criteria for evaluating RAG-generated answers to global sensemaking questions and evaluate\n\n- ur results using the comparative approach. We also validate results using statistics derived from\nLLM-extracted statements of verifiable facts, or \u201cclaims.\u201d\n\n\n**3** **Methods**\n\n\n**3.1** **GraphRAG Workflow**\n\n\nFigure 1 illustrates the high-level data flow of the GraphRAG approach and pipeline. In this section,\nwe describe the key design parameters, techniques, and implementation details for each step.\n\n\n**3.1.1** **Source Documents** _\u2192_ **Text Chunks**\n\n\nTo start, the documents in the corpus are split into text chunks. The LLM extracts information from\neach chunk for downstream processing. Selecting the size of the chunk is a fundamental design\ndecision; longer text chunks require fewer LLM calls for such extraction (which reduces cost) but\nsuffer from degraded recall of information that appears early in the chunk (Kuratov et al., 2024; Liu\net al., 2023). See Section A.1 for prompts and examples of the recall-precision trade-offs.\n\n\n**3.1.2** **Text Chunks** _\u2192_ **Entities & Relationships**\n\n\nIn this step, the LLM is prompted to extract instances of important _entities_ and the _relationships_\nbetween the entities from a given chunk. Additionally, the LLM generates short descriptions for the\nentities and relationships. To illustrate, suppose a chunk contained the following text:\n\n\n4\n\n\nNeoChip\u2019s (NC) shares surged in their first week of trading on the NewTech Exchange. However, market analysts caution that the chipmaker\u2019s public debut may\nnot reflect trends for other technology IPOs. NeoChip, previously a private entity,\nwas acquired by Quantum Systems in 2016. The innovative semiconductor firm\nspecializes in low-power processors for wearables and IoT devices.\n\n\nThe LLM is prompted such that it extracts the following:\n\n\n    - The entity `NeoChip`, with description \u201cNeoChip is a publicly"],"6":["\u223c_\n\n- verlaps between chunks ( 1 million tokens).\n\n\n**News articles** . A benchmark dataset comprised of news articles published from September 2013\nto December 2023 in a range of categories, including entertainment, business, sports, technology,\nhealth, and science (Tang and Yang, 2024). The corpus is divided into 3197 _\u00d7_ 600-token text\n\n_\u223c_\nchunks, with 100-token overlaps between chunks ( 1.7 million tokens).\n\n\n**4.1.2** **Conditions**\n\n\nWe compared six conditions including GraphRAG at four different graph community levels ( **C0**,\n**C1**, **C2**, **C3** ), a text summarization method that applies our map-reduce approach directly to source\ntexts ( **TS** ), and a vector RAG \u201csemantic search\u201d approach ( **SS** ):\n\n\n    - **CO** . Uses root-level community summaries (fewest in number) to answer user queries.\n\n\n    - **C1** . Uses high-level community summaries to answer queries. These are sub-communities\n\n     - f C0, if present, otherwise C0 communities projected downwards.\n\n\n    - **C2** . Uses intermediate-level community summaries to answer queries. These are subcommunities of C1, if present, otherwise C1 communities projected downwards.\n\n\n    - **C3** . Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected downwards.\n\n\n    - **TS** . The same method as in Section 3.1.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n\n\n    - **SS** . An implementation of vector RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\n\n\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\n\n\nThe graph index supporting conditions **C0** - **C3** was created using our generic prompts for entity and\nrelationship extraction, with entity types and few-shot examples tailored to the domain of the data.\n\n\n8\n\n\n**4","Microsoft, 2023)\nand intelligence analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an\nimplicit and interconnected set of concepts, an LLM can generate a summary that answers the query.\nThe challenge, however, arises when the volume of data requires a RAG approach, since vector RAG\napproaches are unable to support sensemaking over an entire corpus.\n\n\nIn this paper, we present **GraphRAG** - a graph-based RAG approach that enables sensemaking over\nthe entirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph,\nwhere nodes correspond to key entities in the corpus and edges represent relationships between those\nentities. Next, it partitions the graph into a hierarchy of communities of closely related entities,\nbefore using an LLM to generate community-level summaries. These summaries are generated in\na bottom-up manner following the hierarchical structure of extracted communities, with summaries\nat higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these\ncommunity summaries provide global descriptions and insights over the corpus. Finally, GraphRAG\nanswers queries through map-reduce processing of community summaries; in the map step, the\nsummaries are used to provide partial answers to the query independently and in parallel, then in the\nreduce step, the partial answers are combined and used to generate a final global answer.\n\n\nThe GraphRAG method and its ability to perform global sensemaking over an entire corpus form\nthe main contribution of this work. To demonstrate this ability, we developed a novel application\n\n- f the LLM-as-a-judge technique (Zheng et al., 2024) suitable for questions targeting broad issues\nand themes where there is no ground-truth answer. This approach first uses one LLM to generate\na diverse set of global sensemaking questions based on corpus-specific use cases, before using a\nsecond LLM to judge the answers of two different RAG systems using predefined criteria (defined\nin Section 3.3). We use this approach to compare GraphRAG to vector RAG on two representative\nreal-world text datasets. Results show GraphRAG strongly outperforms vector RAG when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https:\/\/github _._ [com\/microsoft\/graphrag. In ad-](https:\/\/github.com\/microsoft\/graphrag)\ndition, versions"," in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f"," a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that GraphRAG leads to substantial improvements over a conventional\nRAG baseline for both the comprehensiveness and diversity of generated answers.\n\n\n**1** **Introduction**\n\n\nRetrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using\nLLMs to answer queries based on data that is too large to contain in a language model\u2019s _context_\n_window_, meaning the maximum number of _tokens_ (units of text) that can be processed by the LLM\nat once (Kuratov et al., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to\na large external corpus of text records and retrieves a subset of records that are individually relevant\nto the query and collectively small enough to fit into the context window of the LLM. The LLM then\n\n\nPreprint. Under review.\n\n\ngenerates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang,\n2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call\n_vector RAG_, works well for queries that can be answered with information localized within a small\nset of records. However, vector RAG approaches do not support _sensemaking_ queries, meaning\nqueries that require global understanding of the entire dataset, such as \u201d _What are the key trends in_\n_how scientific discoveries are influenced by interdisciplinary research over the past decade?_ \u201d\n\n\n_Sensemaking_ tasks require reasoning over \u201c _connections (which can be among people, places, and_\n_events) in order to anticipate their trajectories and act effectively_ \u201d (Klein et al., 2006). LLMs such\nas GPT (Achiam et al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil\net al., 2023) excel at sensemaking in complex domains like scientific discovery (Microsoft, 2023)\nand intelligence analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an\nimplicit and interconnected set of concepts, an LLM can generate a summary that answers the query"," when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https:\/\/github _._ [com\/microsoft\/graphrag. In ad-](https:\/\/github.com\/microsoft\/graphrag)\ndition, versions of the GraphRAG approach are also available as extensions to multiple opensource libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), NebulaGraph (NebulaGraph, 2024), and Neo4J (Neo4J, 2024).\n\n\n**2** **Background**\n\n\n**2.1** **RAG Approaches and Systems**\n\n\nRAG generally refers to any system where a user query is used to retrieve relevant information from\nexternal data sources, whereupon this information is incorporated into the generation of a response\nto the query by an LLM (or other generative AI model, such as a multi-media model). The query and\nretrieved records populate a prompt template, which is then passed to the LLM (Ram et al., 2023).\nRAG is ideal when the total number of records in a data source is too large to include in a single\nprompt to the LLM, i.e. the amount of text in the data source exceeds the LLM\u2019s context window.\n\n\nIn canonical RAG approaches, the retrieval process returns a set number of records that are semantically similar to the query and the generated answer uses only the information in those retrieved\nrecords. A common approach to conventional RAG is to use text embeddings, retrieving records\nclosest to the query in vector space where closeness corresponds to semantic similarity (Gao et al.,\n2023). While some RAG approaches may use alternative retrieval mechanisms, we collectively refer\nto the family of conventional approaches as _vector RAG_ . GraphRAG contrasts with vector RAG in\nits ability to answer queries that require global sensemaking over the entire data corpus.\n\n\n2\n\n\nGraphRAG builds upon prior work on advanced RAG strategies. GraphRAG leverages summaries\n\n- ver large sections of the data source as a form of \u201dself-memory\u201d (described in Cheng et al. 2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao"],"7":[", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop\/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation"," in the context. The research findings reveal\nthat including irrelevant documents can unexpectedly increase\naccuracy by over 30%, contradicting the initial assumption\n\n- f reduced quality. These results underscore the importance\n\n- f developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\n\n\n_C. Hybrid Approaches_\n\n\nCombining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-toend joint training\u2014and how to harness both parameterized\n\n\n15\n\n\n\nTABLE IV\n\nSUMMARY OF EVALUATION FRAMEWORKS\n\n\n**Evaluation Framework** **Evaluation Targets** **Evaluation Aspects** **Quantitative Metrics**\n\n\n\nAccuracy\nEM\n\nAccuracy\nAccuracy\n\n\n\nRetrieval Quality\nRGB _[\u2020]_\nGeneration Quality\n\n\n\nNoise Robustness\n\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\n\n\n\nRECALL _[\u2020]_ Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\n\n\n\n    \n    \nCosine Similarity\n\n\nAccuracy\nAccuracy\nAccuracy\n\n\n    \n    \n    \n\nBLEU\n\nROUGE-L\n\nBertScore\n\nRAGQuestEval\n\n\n\nRetrieval Quality\nRAGAS _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nARES _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nTruLens _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nCRUD _[\u2020]_\nGeneration Quality\n\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nCreative Generation\n\nKnowledge-intensive QA\nError Correction\n\nSummarization\n\n\n\n\n_\u2020 represents a benchmark, and \u2021 represents a tool. * denotes customized quantitative metrics, which deviate from traditional_\n_metrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these_\n_metrics, as required._\n\n\n\nand non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific\nfunctionalities into RAG and fine-tuned by the results of RAG\nsystem. For example, CRAG [67] trains a lightweight retrieval\nevaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval\nactions based on confidence levels","ACL [56] [56]\nMath GSM8K [158] [73]\nMachine Translation JRC-Acquis [159] [17]\n\n\n14\n\n\n\nTABLE III\n\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\n\n\nContext Faithfulness Answer Noise Negative Information Counterfactual\nRelevance Relevance Robustness Rejection Integration Robustness\n\n\nAccuracy \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nEM \u2713\n\nRecall \u2713\n\nPrecision \u2713 \u2713\n\nR-Rate \u2713\n\nCosine Similarity \u2713\nHit Rate \u2713\n\nMRR \u2713\n\nNDCG \u2713\n\nBLEU \u2713 \u2713 \u2713\n\nROUGE\/ROUGE-L \u2713 \u2713 \u2713\n\n\n\nThe specific metrics for each evaluation aspect are summarized in Table III. It is essential to recognize that these\nmetrics, derived from related work, are traditional measures\nand do not yet represent a mature or standardized approach for\nquantifying RAG evaluation aspects. Custom metrics tailored\nto the nuances of RAG models, though not included here, have\nalso been developed in some evaluation studies.\n\n\n_D. Evaluation Benchmarks and Tools_\n\n\nA series of benchmark tests and tools have been proposed\nto facilitate the evaluation of RAG.These instruments furnish\n\nquantitative metrics that not only gauge RAG model performance but also enhance comprehension of the model\u2019s capabilities across various evaluation aspects. Prominent benchmarks\nsuch as RGB, RECALL and CRUD [167]\u2013[169] focus on\nappraising the essential abilities of RAG models. Concurrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens [8] employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\n\n\nVII. DISCUSSION AND FUTURE PROSPECTS\n\n\nDespite the considerable progress in RAG technology, several challenges persist that warrant in-depth research.This\nchapter will mainly introduce the current challenges and future\nresearch directions faced by RAG.\n\n\n_A. RAG vs Long Context_\n\n\nWith the deepening of related research, the context of LLMs\nis continuously expanding [170]\u2013[172]. Presently, LLMs can\neffortlessly manage contexts exceeding 200,000 tokens [9] . This\ncapability signifies that long-document question answering,\npreviously reliant on RAG, can now incorporate"," following CycleGAN. Moreover, inspired\nby GcGAN [18], FastCUT is trained with flip-equivariance augmentation, where\nthe input image to the generator is horizontally flipped, and the output features\nare flipped back before computing the PatchNCE loss. Our encoder _G_ enc is the\nfirst half of the CycleGAN generator [89]. In order to calculate our multi-layer,\npatch-based contrastive loss, we extract features from 5 layers, which are RGB\npixels, the first and second downsampling convolution, and the first and the\nfifth residual block. The layers we use correspond to receptive fields of sizes\n1 _\u00d7_ 1, 9 _\u00d7_ 9, 15 _\u00d7_ 15, 35 _\u00d7_ 35, and 99 _\u00d7_ 99. For each layer\u2019s features, we sample 256\nrandom locations, and apply 2-layer MLP to acquire 256-dim final features. For\n\n- ur baseline model that uses MoCo-style memory bank [24], we follow the setting\n\n- f MoCo, and used momentum value 0.999 with temperature 0.07. The size of\nthe memory bank is 16384 per layer, and we enqueue 256 patches per image per\niteration.\n\n\n**C.2** **Evaluation details**\n\n\nWe list the details of our evaluation protocol.\n\n\n**Fr\u00b4echet Inception Distance (FID [26])** throughout this paper is computed by\nresizing the images to 299-by-299 using bilinear sampling of PyTorch framework,\nand then taking the activations of the last average pooling layer of a pretrained\nInception V3 [70] using the weights provided by the TensorFlow framework.\nWe use the default setting of `[https:\/\/github.com\/mseitzer\/pytorch-fid](https:\/\/github.com\/mseitzer\/pytorch-fid)` . All\ntest set images are used for evaluation, unless noted otherwise.\n\n\n**Semantic segmentation metrics on the Cityscapes dataset** are computed\nas follows. First, we trained a semantic segmentation network using the DRN-D22 [83] architecture. We used the recommended setting from `[https:\/\/github.](https:\/\/github.com\/fyu\/drn)`\n`[com\/fyu\/drn](https:\/\/github.com\/fyu\/drn)`, with batch size 32 and learning rate 0.01, for 250 epochs at\n256x128"," we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\n\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70.0%, 72.5%, 75.0%, ..., 90.0%).\nEach split also represents a different mock RAG\nsystem. Since we know the success percentages of\neach dataset split, we know the appropriate ranking of each mock RAG system. This allows us to\n\n\ntest ARES success at both scoring and ranking the\nmock RAG systems appropriately across the three\nevaluation criteria.\n\n\n**4.3** **Metrics**\n\n\nTo calculate the correlation between the correct\n\nranking and the ARES ranking, we use the Kendall\nrank correlation coefficient or Kendall\u2019s _\u03c4_ :\n\n\n_\u03c4_ = [(#][ of concordant pairs][)] _[ \u2212]_ [(#][ of discordant pairs][)]\n\n# of pairs total\n\n\nConcordant pairs are defined as two ordinal values in the ranking where the earlier value in the\nsequence is lower than the later value in the sequence. Discordant pairs are defined as two ordinal\nvalues in the ranking where the earlier value in the\nsequence is greater than or equal to the later value\nin the sequence. A Kendall\u2019s _\u03c4_ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document preprocessing. We want to make sure that ARES has satisfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall\u2019s _\u03c4_ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pairwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in\ninformation retrieval, allowing developers to evaluate ranking systems empirically. Therefore, we\nbelieve Kendall\u2019s tau and prediction accuracy provide meaningful metrics for testing the efficacy of\nARES as a RAG evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges"],"8":[" sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n\n - think: To solve the task, I need to find and take a lettuce, then clean it with\nsinkbasin, then put it in diningtable.\n\nOK.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - go to fridge 1\nThe fridge 1 is closed.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n\n - think: I need to clean this lettuce (1) using sinkbasin 1.\n\nOK.\n\n - go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n\n - think: I need to clean this lettuce (1) using sinkbasin 1.\n\nOK.\n\n - clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n\n - think: I need to put this cleaned lettuce (1) in\/on diningtable 1.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup "," to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n\nD"," The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n_>_ go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n_>_ take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n_>_ go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n_>_ clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n_>_ think: Now I clean a lettuce (1). Next, I need to put it in\/on diningtable 1.\n\nOK.\n\n_>_ go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n_>_ put lettuce 1 in\/on diningtable 1\nYou put the lettuce 1 in\/on the diningtable 1.\n\n\n23\n\n\nPublished as a conference paper at ICLR 2023\n\n\nTable 8: An ReAct prompt on the ALFWorld clean task.\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n","11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n\n - think: To solve the task, I need to find and take a lettuce, then clean it with\nsinkbasin, then put it in diningtable.\n\nOK.\n\n - think: First I need to find a lettuce. A lettuce is more likely to appear in\nfridge (1), diningtable (1), sinkbasin (1), stoveburner (1-3), cabinet (1-13). I\ncan check one by one, starting with fridge 1.\n\nOK.\n\n - go to fridge 1\nThe fridge 1 is closed.\n\n - open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n - think: Now I find a lettuce (1). Next, I need to take it.\n\nOK.\n\n - take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n\n - think: Now I take a lettuce (1). Next, I need to go to sinkbasin"," a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n\n1.\n\nYour task is to: put a clean knife in countertop.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\n"],"9":["-free_ method for test-time\nadaptation.\n\n\n2\n\n\n**Contrastive learning.** Much research has focused on the effect of hard negatives on the performance\n\n- f contrastive learning methods Chen et al. (2020); Qu et al. (2021); Robinson et al. (2021); Wang\net al. (2023). (Zhang & Stratos, 2021) observe that harder negatives provide a better approximation\n\n- f the overall cross-entropy loss, but do not consider _batch_ - level optimizations for negative selection.\nHofstatter et al. (2021) cluster queries before training and show that this improves performance.\u00a8\nSachidananda et al. (2023) also consider contrastive batch sampling as a global optimization problem,\nbut do not apply their technique to state-of-the-art transformer-based text embedding models. (Ma\net al., 2024) use a clustering algorithm to partition a dataset into several sub-datasets, but train a\ndifferent model on each sub-dataset. Solatorio (2024) also use a pre-trained model to address the\nproblem of in-batch false negatives from randomly sampled batches. Our training algorithm aims to\nfind the hardest possible high-quality batches to train text embedding models.\n\n\n**Test-time adaptation.** Our method can be compared to other solutions to test-time adaptation, a\nproblem that has been well-studied across a variety of domains (Jang et al., 2023). In retrieval, one\nform of test-time adaptation is pseudo-relevance feedback (PRF) (Rocchio, 1971; Li et al., 2018;\nWang et al., 2021), where documents relevant to the query are used to construct a final, enhanced\nquery representation. The query side of our model can be seen as a form of pseudo-relevance\nfeedback; however, we train from scratch to support a more general form of PRF natively, on the\ndocument representation as well as the query.\n\n\n**Non-parametric modeling.** Our contextual document model can be seen as a form of nonparametric modeling. This shows connections with the a large body of deep learning research\nsuch as the non-parametric transformer (NPT) (Kossen et al., 2022) and the subfield of Neural\nProcesses (Garnelo et al., 2018; Kim et al., 201"," practice from [32], and first re-generate the claim, and\nthen classify using the representation of the final hidden state, before finally marginalizing across\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\nfirst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\nwe explore in the main paper. FEVER\u2019s other sub-task involves extracting sentences from Wikipedia\nas evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to\nus, directly tackling this task is not straightforward. We hope to address this in future work.\n\n\n**F** **Null Document Probabilities**\n\n\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order\nto model cases where no useful information could be retrieved for a given input. Here, if _k_ documents\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\ndocument, before marginalizing over _k_ + 1 predictions. We explored modelling this null document\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\n(iii) a neural network to predict the logit. We did not find that these improved performance, so in\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\ndocuments for questions that are less likely to benefit from retrieval, suggesting that null document\nmechanisms may not be necessary for RAG.\n\n\n**G** **Parameters**\n\n\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n\n\n18\n\n\nTable 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\n\n\nTask Train Development Test\n\n\nNatural Questions 79169 8758 3611\nTriviaQA 78786 8838 11314\nWebQuestions 3418 362 2033\nCuratedTrec 635 134 635\nJeopardy"," Tar Heels men\u2019s basketball. From\nthe standings, we can see that this team has 19 wins and 16 losses. 19-16\nis not good enough to make the NCAA tournament, which is mentioned\nin the context. From the standings, we can also see that this is the\nstanding for 2002-03 season. So the last time UNC did not make the\nNCAA tournament as of 2003 is 2002-03.\n\n\nTable 33: Illustration of wins of STEP-BACK PROMPTING on the SituatedQA dataset.\n\n\n37\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nStrategyQA **Original Question** : Could you drive a Rowe 550 to the 2008 Summer\nOlympics?\n**Original Answer [Incorrect]** : The 2008 Summer Olympics was held in\nBeijing, China. The Rowe 550 is an all-electric car. The Rowe 550 has a\nrange of 70 miles on a single charge and a top speed of about 30 mph.\nThe distance between Beijing and the U.K. is about 4000 miles.\n**Stepback Question** : What kind of vehicle is a Rowe 550?\n**Final Answer [Correct]** : The Roewe 550 was released in 2008; the\n2008 Summer Olympics were held from August 8-24, 2008. So you\ncould drive a Rowe 550 to the 2008 Summer Olympics.\n\n\nStrategyQA **Original Question** : what channel does thursday night football come on\nregular tv as of 2017?\n**Original Answer [Incorrect]** : Thursday Night Football is a weekly\nseries of National Football League (NFL) games broadcast on Thursday\nnights. It started in 2006 on NFL Network. Under a new agreement\nbetween the NFL and CBS, CBS will air 50% of the Thursday Night\nFootball games and NFL Network will air the other 50%.\n**Stepback Question** : which channel is thursday night football on as of\n2017?\n**Final Answer [Correct]** : In 2017, Thursday Night Football was aired\n\n            - n CBS, NBC, and NFL Network.\n\n\nStrategyQA **Original Question** : who is the present president of assam sahitya sabha\nas of 2018?\n**Original Answer [Incorrect]** : Dr. Nagen Sa","\ud835\udc68_ is manually set to 0,\ninstead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically\nto this setting and probe if the model has learned to automatically reset its state on episode boundaries.\n\n\n- Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of\nbeing an exponential moving average (EMA). They additionally make an interesting connection of the discretization step\n\n - f SSMs to an EMA _damping_ term. Contrary to findings in the original S4 papers, this was the first model to show that\nreal-valued SSMs are empirically effective in certain settings or when combined with different architectural components.\n\n\n- Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this\nperspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally\nand close to LTI.\n\n\n- SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox\n\n2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of\nS4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do\nfast autoregressive inference directly.\n\n\nNotably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually\nstrictly LTI (linear time invariant).\n\n\n**B.2** **SSM Architectures**\n\n\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating\n\n- ne of the previous SSMs as a black box layer.\n\n\n- GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated\nattention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most\nimportantly, its projection"," footmen; four grasshoppers into white horses. Next, the<br>Fairy touched Cinderella\u2019s rags, and they became rich satin robes, trimmed with point<br>lace. ... Alas! the clock had struck Eleven. She found no coach, only a pumpkin, and<br>the rat and mice ran quickly away when they saw her; while all her fne dress turned to<br>rags, and she had to run home alone. . . They told her a beautiful Princess had been at<br>the ball, with whom the Prince was delighted. They did not know it was Cinderella.|\n|**Question: How does Cinderella fnd a happy ending?**|**Question: How does Cinderella fnd a happy ending?**|\n|**RAPTOR**|The story of Cinderella involves her godmother, a fairy, who transforms a pumpkin<br>into a grand coach with her wand and allows Cinderella to attend the ball. However,<br>Cinderella must return home before the clock strikes eleven or her dress will turn back<br>into rags... Cinderella impresses the Prince at the ball but leaves before he can fnd<br>out who she is... The Prince searched for the owner of a lost glass slipper and found it<br>belonged to Cinderella. She forgave her sisters and the Prince was glad to have found<br>her.|\n|**DPR**|the clock had struck Eleven. . . The Prince was very much surprised when he missed<br>Cinderella again, and leaving the ball, went in search of her. . .<br>Fairy touched Cin-<br>derella\u2019s rags, and they became rich satin robes, trimmed with point lace... Her old<br>shoes became a charming pair of glass slippers, which shone like diamonds. \u201cNow go<br>to the ball, my love,\u201d she said, \u201cand enjoy yourself. But remember, you must leave the<br>room before the clock strikes eleven. If you do not your dress will return to its original<br>rags.\u201d|\n\n\n\nTable 13: Relevant excerpts from text retrieved by RAPTOR and DPR for the questions on the\nfairytale Cinderella.\n\n\nI ANALYSIS OF DIFFERENT LAYERS ON RAPTOR\u2019S PERFORMANCE\n\n\nI.1 HOW DO DIFFERENT LAYERS IMPACT PERFORMANCE ?\n\n\nIn this section, we present a detailed"],"10":["## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [","QA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20\nparagraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these\ndatasets into an open-domain setting, we make two\ncorpora, one for each dataset, by combining all\nsupporting and non-supporting paragraphs for all\nits questions in the train, development, and test\nsets. IIRC is originally a mix between reading\ncomprehension and an open-domain setting. Each\nquestion is grounded in one main paragraph, which\ncontains links to multiple Wikipedia pages with\nseveral paragraphs each. We create a corpus out\n\n- f all the paragraphs from all the Wikipedia pages\npresent in the dataset. [18] We do assume the availability of the main passage which doesn\u2019t need\nto be retrieved and is always present. We don\u2019t\nassume the availability of Wikipedia links in the\nmain passage, however, to keep the retrieval problem challenging. [19]\n\n\n**B** **Special Handling of Models for IIRC**\n\n\nIIRC is slightly different from the other datasets,\nin that the question is grounded in the main passage and other supporting paragraphs come from\nthe Wikipedia pages of entities mentioned in this\npassage. We modify the retrievers and readers to\naccount for this difference: (i) We always keep the\nmain passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all\nthe retrieval methods, we first prompt the model to\ngenerate a list of Wikipedia page titles using the\nmain passage and the question. We map these generated titles to the nearest Wikipedia page titles in\nthe corpus (found using BM25), and then the rest\n\n- f the paragraph retrieval queries are scoped within\n\n- nly those Wikipedia pages.\nTo prompt the model to generate Wikipedia page\ntitles using the main passage and the question for\n\n\n18Following are the corpus sizes for the datasets: HotpotQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e"," for GPT-2 [50] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or to automate the production of spam\/phishing\ncontent [54]. Advanced language models may also lead to the automation of various jobs in the\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to fight against\nmisleading content and automated spam\/phishing.\n\n\n**Acknowledgments**\n\n\nThe authors would like to thank the reviewers for their thoughtful and constructive feedback on this\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD\n\nprogram.\n\n\n**References**\n\n\n[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan\nMajumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina\nStoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. _arXiv:1611.09268 [cs]_, November 2016. URL `[http:](http:\/\/arxiv.org\/abs\/1611.09268)`\n`[\/\/arxiv.org\/abs\/1611.09268](http:\/\/arxiv.org\/abs\/1611.09268)` . arXiv: 1611.09268.\n\n\n[2] Petr Baudi\u0161 and Jan \u0160edivy. Modeling of the question answering task in the yodaqa system. In`\n_International Conference of the Cross-Language Evaluation Forum for European Languages_,\npages 222\u2013228. Springer, 2015. URL `[https:\/\/link.springer.com\/chapter\/10.1007%](https:\/\/link.springer.com\/chapter\/10.1007%2F978-3-319-24027-5_20)`\n`[2F978-3-319-24027-5_20](https:\/\/link.springer.com\/chapter\/"," time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine"],"11":["2022), Direct Prompting as proposed\nby Brown et al. (2020). For CoT prompting, we use\nthe same template as shown in \u00a73.2, but at test time\nwe ask the model to generate the full CoT from\nscratch. The final sentence of CoT is expected to\nbe of the form \u201canswer is: ...\u201d, so that the answer\ncan be extracted programmatically. If it\u2019s not in\nthat form, the full generation is returned as the\nanswer. For Direct Prompting, we use the same\ntemplate as CoT Prompting but the answer field\n(\u201cA: \u201d) contains only the final answer instead of\nCoT. See App. G for details.\n\n\n**4** **Experimental Setup**\n\n\nWe evaluate - ur method - n 4 multi-step\nQA datasets in the - pen-domain setting:\n**HotpotQA** (Yang et al., 2018), **2WikiMul-**\n**tihopQA** (Ho et al., 2020), answerable subset of\n**MuSiQue** (Trivedi et al., 2022), and answerable\nsubset of **IIRC** (Ferguson et al., 2020). For\nHotpotQA, we use the Wikipedia corpus that\ncomes with it for the open-domain setting. For\neach of the other three datasets, which originally\ncome in a reading comprehension or mixed setting,\nwe used the associated contexts to construct a\n\n\ncorpus for our open-domain setting (see App. A\nfor details). For each dataset, we use 100 randomly\nsampled questions from the original development\nset for tuning hyperparameters, and 500 other\nrandomly sampled questions as our test set.\n\n\n**4.1** **Models**\n\n\n**Retriever.** We use BM25 (Robertson et al., 2009)\nimplemented in Elasticsearch [6] as our base retriever.\nWe compare two retriever systems:\n\n(i) **One-step Retriever (OneR)** uses the question as a query to retrieve _K_ paragraphs. We select\n_K \u2208{_ 5 _,_ 7 _,_ 9 _,_ 11 _,_ 13 _,_ 15 _}_ that\u2019s best on the dev set.\n\n(ii) **IRCoT Retriever** is our method described in \u00a73. We use BM25 as its underlying retriever"," the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n\n\n**Selection of positive passages** Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA [6], we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the question will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. [7] We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the number of questions in training\/dev\/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n\n\n**5** **Experiments: Passage Retrieval**\n\n\nIn this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n\n\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Appendix A.\n\n\n**Training** **Retriever** **Top-20** **Top-100**\nNQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD\n\n\nNone BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0\n\n\nDPR 78.4 79.4 73.2 79.8 63.2 85.4 **85.0** 81.4 89.1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7"," run experiments.\n\n\n**Long-form QA** For \u201cWhere do the Philadelphia\nEagles play their home games?\u201d, the output we\naim to generate is \u201cWe need to consider the different possible locations or venues that could be\nconsidered the home field of the Philadelphia Eagles. These include the city, the sports complex,\n\n- r the stadium. Therefore, this question has 3 interpretations and the answers are: (1) The city is\nPhiladelphia. (2) The sports complex is the South\nPhiladelphia Sports Complex. (3) The stadium is\nthe Lincoln Financial Field stadium.\u201d For both the\n\n- riginal setting (ASQA) and the setting with hints\n(ASQA-hint), we manually annotate 8 exemplars\n(Prompt D.6 and D.8), use BM25 on the Wikipedia\ncorpus, and 3 retrieved documents to run experi\nments.\n\n\n**Open-domain Summarization** The original\nWikiAsp dataset is designed for multi-document\nsummarization and provides a list of references to\nsystems. We converted it into the open-domain\nsetting by removing the associated references and\ninstead gathering information from the open web.\nFor \u201cGenerate a summary about Echo School (Ore\n\ngon) including the following aspects: academics,\nhistory.\u201d, the output we aim to generate is \u201c# Academics. In 2008, 91% of the school\u2019s seniors received their high school diploma... # History. The\nclass of 2008 was the 100th class in the school\u2019s\n\nhistory.\u201d where # is used to indicate aspects. We\nmanually annotate 4 exemplars (Prompt D.10), and\nuse the Bing search engine to retrieve 5 documents\nfrom the open web. To avoid leaking, we exclude\nseveral Wikipedia-related domains listed in Table 8\nfrom Bing\u2019s search results.\n\n\n**C** **Hyperparameters**\n\n\nHyperparameters of FLARE on different datasets\nare listed in Table 9.\n\n\n**D** **Prompts and Few-shot exemplars**\n\n\nThe prompt used to linearize multiple documents\nis shown in Prompt D.1. The prompt used in selfask (Press et al., 2022) is shown in Prompt D.2.\nPrompts and exemplars of different tasks\/datasets\nare shown in Prompt D.3, D.4, D.5, D.6, D.8, and\nD.10","QA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20\nparagraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these\ndatasets into an open-domain setting, we make two\ncorpora, one for each dataset, by combining all\nsupporting and non-supporting paragraphs for all\nits questions in the train, development, and test\nsets. IIRC is originally a mix between reading\ncomprehension and an open-domain setting. Each\nquestion is grounded in one main paragraph, which\ncontains links to multiple Wikipedia pages with\nseveral paragraphs each. We create a corpus out\n\n- f all the paragraphs from all the Wikipedia pages\npresent in the dataset. [18] We do assume the availability of the main passage which doesn\u2019t need\nto be retrieved and is always present. We don\u2019t\nassume the availability of Wikipedia links in the\nmain passage, however, to keep the retrieval problem challenging. [19]\n\n\n**B** **Special Handling of Models for IIRC**\n\n\nIIRC is slightly different from the other datasets,\nin that the question is grounded in the main passage and other supporting paragraphs come from\nthe Wikipedia pages of entities mentioned in this\npassage. We modify the retrievers and readers to\naccount for this difference: (i) We always keep the\nmain passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all\nthe retrieval methods, we first prompt the model to\ngenerate a list of Wikipedia page titles using the\nmain passage and the question. We map these generated titles to the nearest Wikipedia page titles in\nthe corpus (found using BM25), and then the rest\n\n- f the paragraph retrieval queries are scoped within\n\n- nly those Wikipedia pages.\nTo prompt the model to generate Wikipedia page\ntitles using the main passage and the question for\n\n\n18Following are the corpus sizes for the datasets: HotpotQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e"," like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25\/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer"],"12":["QA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20\nparagraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these\ndatasets into an open-domain setting, we make two\ncorpora, one for each dataset, by combining all\nsupporting and non-supporting paragraphs for all\nits questions in the train, development, and test\nsets. IIRC is originally a mix between reading\ncomprehension and an open-domain setting. Each\nquestion is grounded in one main paragraph, which\ncontains links to multiple Wikipedia pages with\nseveral paragraphs each. We create a corpus out\n\n- f all the paragraphs from all the Wikipedia pages\npresent in the dataset. [18] We do assume the availability of the main passage which doesn\u2019t need\nto be retrieved and is always present. We don\u2019t\nassume the availability of Wikipedia links in the\nmain passage, however, to keep the retrieval problem challenging. [19]\n\n\n**B** **Special Handling of Models for IIRC**\n\n\nIIRC is slightly different from the other datasets,\nin that the question is grounded in the main passage and other supporting paragraphs come from\nthe Wikipedia pages of entities mentioned in this\npassage. We modify the retrievers and readers to\naccount for this difference: (i) We always keep the\nmain passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all\nthe retrieval methods, we first prompt the model to\ngenerate a list of Wikipedia page titles using the\nmain passage and the question. We map these generated titles to the nearest Wikipedia page titles in\nthe corpus (found using BM25), and then the rest\n\n- f the paragraph retrieval queries are scoped within\n\n- nly those Wikipedia pages.\nTo prompt the model to generate Wikipedia page\ntitles using the main passage and the question for\n\n\n18Following are the corpus sizes for the datasets: HotpotQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e"," in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f","rec (CT) [2]. As\nCT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG\nmodel. We use the same train\/dev\/test splits as prior work [31, 26] and report Exact Match (EM)\nscores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.\n\n\n**3.2** **Abstractive Question Answering**\n\n\nRAG models can go beyond simple extractive QA and answer questions with free-form, abstractive\ntext generation. To test RAG\u2019s natural language generation (NLG) in a knowledge-intensive setting,\nwe use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages\nretrieved from a search engine for each question, and a full sentence answer annotated from the\nretrieved passages. We do not use the supplied passages, only the questions and answers, to treat\n\n\n4\n\n\nMSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be\nanswered in a way that matches the reference answer without access to the gold passages, such as\n\u201cWhat is the weather in Volcano, CA?\u201d so performance will be lower without using gold passages.\nWe also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,\nRAG can rely on parametric knowledge to generate reasonable responses.\n\n\n**3.3** **Jeopardy Question Generation**\n\n\nTo evaluate RAG\u2019s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist\n\n- f short, simple questions, we propose the more demanding task of generating Jeopardy questions.\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\nFor example, \u201cThe World Cup\u201d is the answer to the question \u201cIn 1986 Mexico scored as the first\ncountry to host this international sports competition twice.\u201d As Jeopardy questions are precise,\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\nchallenging knowledge-intensive generation task.\n\n\nWe use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As\nthis is a new task,",". The answer relevance score, AR,\nfor question _q_ is then computed as:\n\n\n\nThis metric evaluates how closely the generated\nanswer aligns with the initial question or instruction.\n\n\n**Context relevance** The context _c_ ( _q_ ) is considered relevant to the extent that it exclusively contains information that is needed to answer the question. In particular, this metric aims to penalise the\n\n\n\nAR = [1]\n\n\n_n_\n\n\n\n_n_\n\n- sim( _q, qi_ ) (1)\n\n\n_i_ =1\n\n\ninclusion of redundant information. To estimate\n\ncontext relevance, given a question _q_ and its context _c_ ( _q_ ), the LLM extracts a subset of sentences,\n_Sext_, from _c_ ( _q_ ) that are crucial to answer _q_, using\nthe following prompt:\n\n\n_Please extract relevant sentences from_\n_the provided context that can potentially_\n_help answer the following question. If no_\n_relevant sentences are found, or if you_\n_believe the question cannot be answered_\n_from the given context, return the phrase_\n_\"Insufficient Information\". While extract-_\n_ing candidate sentences you\u2019re not al-_\n_lowed to make any changes to sentences_\n_from given context._\n\n\nThe context relevance score is then computed as:\n\n\nnumber of extracted sentences\nCR = (2)\ntotal number of sentences in _c_ ( _q_ )\n\n\n**4** **The WikiEval Dataset**\n\n\nTo evaluate the proposed framework, we ideally\nneed examples of question-context-answer triples\nwhich are annotated with human judgments. We\ncan then verify to what extent our metrics agree\nwith human assessments of faithfulness, answer\n\nrelevance and context relevance. Since we are not\n\naware of any publicly available datasets that could\nbe used for this purpose, we created a new dataset,\nwhich we refer to as _WikiEval_ [4] . To construct the\n\ndataset, we first selected 50 Wikipedia pages covering events that have happened since the start of\n2022 [5] . In selecting these pages, we prioritised\nthose with recent edits. For each of the 50 pages,\nwe then asked ChatGPT to suggest a question that\ncan be answered based on the introductory section\n\n- f the page, using the following prompt:\n\n\n_Your task is to formulate a question from_\n_given context satisfying the rules given_\n\n_below:_\n\n_1. The question should be fully answered_\n_from the given"," corresponding to Google\nsearch queries. The open-domain version of\nthis dataset is obtained by discarding answers\nwith more than 5 tokens.\n\n\n  - TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and quiz-league\n\n\nFigure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of retrieved passages.\n\n\n\nwebsites. The _unfiltered_ version of TriviaQA\nis used for open-domain question answering.\n\n\n  - SQuAD v1.1 (Rajpurkar et al., 2016) is a reading comprehension dataset. Given a paragraph\nextracted from Wikipedia, annotators were\nasked to write questions, for which the answer\nis a span from the corresponding paragraph.\n\n\nFollowing Lee et al. (2019) we use the validation as\ntest, and keep 10% of the training set for validation.\nWe use the Wikipedia dumps from Dec. 20, 2018\nfor NQ and TriviaQA and from Dec. 21, 2016 for\nSQuAD. We apply the same preprocessing as Chen\net al. (2017); Karpukhin et al. (2020), leading to\npassages of 100 words, which do not overlap.\n\n\n**Evaluation.** Predicted answers are evaluated\n\nwith the standard exact match metric (EM), as introduced by Rajpurkar et al. (2016). A generated\nanswer is considered correct if it matches any answer of the list of acceptable answers after normalization. This normalization step consists in lowercasing and removing articles, punctuation and\nduplicated whitespace.\n\n\n**Technical details.** We initialize our models with\n\nthe pretrained T5 models (Raffel et al., 2019), available in the HuggingFace Transformers library. [4] We\nconsider two model sizes, base and large, containing respectively 220M and 770M parameters. We\nfine-tune the models on each dataset independently,\nusing Adam (Kingma and Ba, 2014) with a constant learning rate of 10 _[\u2212]_ [4] and a dropout rate of\n10%. We train the model for 10k gradient steps,\nwith a batch size of 64, using 64 Tesla V100 32Gb.\nWe evaluate models every 500 steps and select the\nbest one on the validation set based on the Exact\n\n"],"13":[" to a _d_ dimensional real-valued vectors and builds an index\n\nfor all the _M_ passages that we will use for retrieval.\n\n\n3The ideal size and boundary of a text passage are functions of both the retriever and reader. We also experimented\nwith natural paragraphs in our preliminary trials and found that\nusing fixed-length passages performs better in both retrieval\nand final QA accuracy, as observed by Wang et al. (2019).\n4Exceptions include (Seo et al., 2019) and (Roberts et al.,\n2020), which _retrieves_ and _generates_ the answers, respectively.\n\n\nAt run-time, DPR applies a different encoder _EQ_ ( _\u00b7_ )\nthat maps the input question to a _d_ - dimensional\nvector, and retrieves _k_ passages of which vectors\nare the closest to the question vector. We define\nthe similarity between the question and the passage\nusing the dot product of their vectors:\n\n\nsim( _q, p_ ) = _EQ_ ( _q_ ) [\u22ba] _EP_ ( _p_ ) _._ (1)\n\n\nAlthough more expressive model forms for measuring the similarity between a question and a passage\ndo exist, such as networks consisting of multiple\nlayers of cross attentions, the similarity function\nneeds to be decomposable so that the representations of the collection of passages can be precomputed. Most decomposable similarity functions\nare some transformations of Euclidean distance\n\n(L2). For instance, cosine is equivalent to inner\nproduct for unit vectors and the Mahalanobis distance is equivalent to L2 distance in a transformed\nspace. Inner product search has been widely used\nand studied, as well as its connection to cosine\nsimilarity and L2 distance (Mussmann and Ermon,\n2016; Ram and Gray, 2012). As our ablation study\nfinds other similarity functions perform comparably (Section 5.2; Appendix B), we thus choose\nthe simpler inner product function and improve the\ndense passage retriever by learning better encoders.\n\n\n**Encoders** Although in principle the question and\npassage encoders can be implemented by any neural networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, uncased) and take the representation at the [CLS]\ntoken as the output, so _d_ = 768.\n\n\n"," positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically, we get top 100 passages for each\nquestion in a mini-batch and use the method similar\nto in-batch negative training: all retrieved passages\u2019\nvectors participate in the loss calculation for _all_\nquestions in a batch. Our training batch size is set\nto 16, which effectively gives 1,600 passages per\nquestion to calculate retriever loss. The reader still\nuses 24 passages per question, which are selected\n\n\n**Question** **Passage received by BM25** **Passage retrieved by DPR**\n\n\n\nWhat is the body of water\nbetween England and Ireland?\n\n\nWho plays Thoros of Myr in\nGame of Thrones?\n\n\n\nTitle:British Cycling Title: Irish Sea\n... **England** is not recognised as a region by the UCI, and . . . Annual traffic between Great Britain and **Ireland** amounts\nthere is no English cycling team outside the Commonwealth to over 12 million passengers and of traded goods. **The Irish**\nGames. For those occasions, British Cycling selects and sup- **Sea** is connected to the North Atlantic at both its northern\nports the **England** team. Cycling is represented on the Isle and southern ends. To the north, the connection is through\n\n- f Man by the Isle of Man Cycling Association. Cycling in the North Channel between Scotland and Northern **Ireland**\nNorthern **Ireland** is organised under Cycling Ulster, part of and the Malin Sea. The southern end is linked to the Atlantic\nthe all-Ireland governing **body** Cycling **Ireland** . Until 2006, through the St George\u2019s Channel between Ireland and Pema rival governing **body** existed, . . . brokeshire, and the Celtic Sea. . . .\n\n\n\nTitle: No One (Game of Thrones) Title: P\u02daal Sverre Hagen\n...He may be \u201dno one,\u201d but there\u2019s still enough of a person P\u02daal Sverre Valheim Hagen (born 6 November 1980) is a Norleft in him to respect, and admire who this girl is and what wegian stage and screen actor. He appeared in the Norweshe\u2019s become."," SpaCy. [2] In\nDPR, passages and questions are represented as\ndense vector representations, computed using two\nBERT networks. The ranking function is the dot\nproduct between the query and passage representations. Retrieval is performed using approximate\nnearest neighbors with the FAISS library. [3]\n\n\n**Reading.** Our generative model for open domain\nQA is based on a sequence-to-sequence network,\npretrained on unsupervised data, such as T5 or\nBART (Raffel et al., 2019; Lewis et al., 2019). The\nmodel takes as input the question, as well as the\nsupport passages, and generates the answer. More\nprecisely, each retrieved passage and its title are\nconcatenated with the question, and processed independently from other passages by the encoder.\nWe add special tokens question:, title: and\ncontext: before the question, title and text of\neach passage. Finally, the decoder performs atten\n\n1lucene.apache.org\n2spacy.io\n3github.com\/facebookresearch\/faiss\n\n\n\ntion over the concatenation of the resulting representations of all the retrieved passages. The model\nthus performs evidence fusion in the decoder only,\nand we refer to it as _Fusion-in-Decoder_ .\n\nBy processing passages independently in the encoder, but jointly in the decoder, this method differs from Min et al. (2020) and Lewis et al. (2020).\nProcessing passages independently in the encoder\nallows to scale to large number of contexts, as it\n\n- nly performs self attention over one context at a\ntime. This means that the computation time of the\nmodel grows linearly with the number of passages,\ninstead of quadratically. On the other hand, processing passages jointly in the decoder allows to\nbetter aggregate evidence from multiple passages.\n\n\n**4** **Experiments**\n\n\nIn this section, we report empirical evaluations of\nFusion-in-Decoder for open domain QA.\n\n\n**Datasets.** We consider the following datasets,\nand use the same setting as Lee et al. (2019):\n\n\n  - NaturalQuestions (Kwiatkowski et al., 2019)\ncontains questions corresponding to Google\nsearch queries. The open-domain version of\nthis dataset is obtained by discarding answers\nwith more than 5 tokens.\n\n\n  - TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and"," teacher, the resultant \u201cmode-covering\u201d behavior might cause the student\nto assign probability mass to tokens with low probability\nunder the teacher\u2019s distribution (cf. Figure 6 blue curve).\nThis mode-covering phenomenon can potentially lead to\nhallucinations and low-quality generations. Alternatively,\nmode-seeking divergences like reverse KL prioritize tokens\nwhere the teacher assigns high probabilities (cf. Figure 6\ngreen curve). This approach can mitigate the risk of lowquality outputs, fostering more accurate generations. However, it often does so at the cost of reduced diversity.\nGu et al. (2024) adopt reverse KL divergence to prevent\nstudents from overestimating low-probability regions of the\nteacher\u2019s distribution, employing Policy Gradient methods\nfor optimization. Both Agarwal et al. (2024) and Sason and\nVerd\u00b4u (2016) assess the effect of different divergence functions in LLM distillation, finding the optimal divergence to\nbe task-dependent. For instance, forward KL divergence is\nmore suitable for tasks like Machine Translation, where the\n\n- utput has fewer modes or variations, while reverse KL\ndivergence is preferable for tasks like dialogue generation\nand instruction tuning, which involve multiple modes and\na wider range of potential responses. Thus, the nature of the\ntask significantly influences the selection of the divergence\nfunction for optimal performance.\n\n\n_**Similarity.**_ Similarity-based methods in knowledge distillation aim to align the hidden states or features of the student\nmodel with those of the teacher. These methods use various\nsimilarity metrics to measure and optimize the congruence\n\n- f internal representations between the two models. The\n\n- bjective is to ensure that the student model not only\nproduces similar outputs to the teacher but also processes\ninformation in a comparable manner. The formulation for a\nsimilarity-based objective might look like this:\n\n\n_L_ Sim = E\n_x\u223cX_ _,y\u223cY_ [[] _[L][F]_ [ (\u03a6] _[T]_ [ (] _[f][T]_ [ (] _[x, y]_ [))] _[,]_ [ \u03a6] _[S]_ [ (] _[f][S]_ [(] _[x, y]_ [)))]] _[,]_ [ (11)]\n\n\nwhere _fT_ ( _x, y_ )","## **Leveraging Passage Retrieval with Generative Models** **for Open Domain Question Answering**\n\n**Gautier Izacard** [1] _[,]_ [2] _[,]_ [3] **Edouard Grave** [1]\n\n1 Facebook AI Research, Paris\n2 ENS, PSL University, Paris\n3 Inria, Paris\ngizacard|egrave@fb.com\n\n\n**Abstract**\n\n\n\nGenerative models for open domain question\nanswering have proven to be competitive, with\n  - ut resorting to external knowledge. While\npromising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we\ninvestigate how much these models can benefit from retrieving text passages, potentially\ncontaining evidence. We obtain state-of-theart results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that\nsequence-to-sequence models offers a flexible\nframework to efficiently aggregate and combine evidence from multiple passages.\n\n\n**1** **Introduction**\n\n\n\n\n\n\n\n\n\n\n\nRecently, several works have shown that factual\ninformation can be extracted from large scale\nlanguage models trained on vast quantities of\ndata (Radford et al., 2019; Petroni et al., 2019;\nJiang et al., 2019; Talmor et al., 2019). Building\n\n- n that observation and the advances in pretraining of natural language processing models, Roberts\net al. (2020) introduced a generative model for open\ndomain question answering. Without relying on\nexternal knowledge, this method obtained competitive results on several benchmarks. However, it\nrequires models containing billions of parameters,\nsince all the information needs to be stored in the\n\nweights. This makes models expensive to query\nand train. In this paper, we investigate how much\nthis method could benefit from having access to an\nexternal source of knowledge, such as Wikipedia.\nRetrieval based approaches were previously considered in the context of open domain question\nanswering with extractive models (Chen et al.,\n2017). In that case, systems start by retrieving\n\n\n\nFigure 1: A simple approach to open domain question\nanswering. First, it retrieves support text passages from\nan external source of knowledge such as Wikipedia.\nThen, a generative"],"14":["j]_ ) _\u2248_  _j_ _j_\n\n\n\nwhere _H_ is a set of examples used to approximate the normalizing constant. In implementation,\nin addition to these hard negative examples, other examples from the mini-batch are also used to\ncompute the normalizer since it requires no additional compute for calculating _\u03d5_ ( _d_ ).\n\n\n4 METHODS\n\n\nIn our work, we are interested in integrating contextual information into our embedding functions\n_\u03d5_ and _\u03c8_ . The standard neural _\u03d5_ is purely a function of the document _\u03d5_ ( _d_ ) and does not take into\naccount any notion of context. This contrasts with the statistical model _\u03d5_ ( _\u00b7_ ; _D_ ) and _\u03c8_ ( _\u00b7_ ; _D_ ). Arguably\nthis is not an issue if retrieval is completely in domain, as _\u03d5_ is capable of learning statistics such as\nIDF and average document length on the training set through gradient descent.\n\n\nHowever, in many retrieval benchmarks, models are trained over a single set of documents _D_ and\nthen tested in many other domains _D_ that differs significantly from _DT_ . In this setting, training on\n_DT_ alone may not be able to provide robust embeddings when used in contexts such as _D_ .\n\n\n4.1 CONTEXTUAL TRAINING WITH ADVERSARIAL CONTRASTIVE LEARNING\n\n\nReturning to the example from the introduction, we assume that in a general purpose training corpus\n_DT_, the term NFL is a rare word appearing in relatively few documents and a useful signal. However,\nif at test time _D_ is a corpus of sports articles, this word would be exceedingly common. Evaluation\nin this domain is, in a statistical sense, adversarial to the original dataset. To handle this issue,\nmeta-learning-style objectives have shown to be effective for training document embedders. In these\napproaches, instead of sampling documents-query pairs iid, the objective first sample a domain and\nthen sample a batch of examples. This ensures that the model mostly sees related training points in\neach domain.\n\n\nWe propose a training objective that synthesizes a large set of fine-grained domains to train the model\n\n- n. Formally, our aim is to partition the training dataset _DT_ into groups ( _B_ [1] _, . . . B_ _[","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice ["," time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine","\u2212]_ _i,n_ [)] (2)\n\n_e_ [sim(] _[q][i][,p]_ _i_ [+][)]\n\n= _\u2212_\nlog\n\n_e_ [sim(] _[q][i][,p]_ _i_ [+][)] + ~~[\ufffd]~~ _[n]_ _j_ =1 _[e]_ [sim(] _[q][i][,p]_ _i,j_ _[\u2212]_ [)] _[.]_\n\n\n**Positive and negative passages** For retrieval\nproblems, it is often the case that positive examples\nare available explicitly, while negative examples\nneed to be selected from an extremely large pool.\nFor instance, passages relevant to a question may\nbe given in a QA dataset, or can be found using the\nanswer. All other passages in the collection, while\nnot specified explicitly, can be viewed as irrelevant\nby default. In practice, how to select negative examples is often overlooked but could be decisive\nfor learning a high-quality encoder. We consider\nthree different types of negatives: (1) Random: any\nrandom passage from the corpus; (2) BM25: top\npassages returned by BM25 which don\u2019t contain\nthe answer but match most question tokens; (3)\nGold: positive passages paired with other questions\nwhich appear in the training set. We will discuss the\nimpact of different types of negative passages and\ntraining schemes in Section 5.2. Our best model\nuses gold passages from the same mini-batch and\n\n- ne BM25 negative passage. In particular, re-using\ngold passages from the same batch as negatives\ncan make the computation efficient while achieving great performance. We discuss this approach\nbelow.\n\n\n**In-batch negatives** Assume that we have _B_\nquestions in a mini-batch and each one is associated with a relevant passage. Let **Q** and **P** be the\n( _B_ _\u00d7d_ ) matrix of question and passage embeddings\nin a batch of size _B_ . **S** = **QP** _[T]_ is a ( _B \u00d7 B_ ) matrix of similarity scores, where each row of which\ncorresponds to a question, paired with _B_ passages.\nIn this way, we reuse computation and effectively\ntrain on _B_ [2] ( _qi_,"," positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically, we get top 100 passages for each\nquestion in a mini-batch and use the method similar\nto in-batch negative training: all retrieved passages\u2019\nvectors participate in the loss calculation for _all_\nquestions in a batch. Our training batch size is set\nto 16, which effectively gives 1,600 passages per\nquestion to calculate retriever loss. The reader still\nuses 24 passages per question, which are selected\n\n\n**Question** **Passage received by BM25** **Passage retrieved by DPR**\n\n\n\nWhat is the body of water\nbetween England and Ireland?\n\n\nWho plays Thoros of Myr in\nGame of Thrones?\n\n\n\nTitle:British Cycling Title: Irish Sea\n... **England** is not recognised as a region by the UCI, and . . . Annual traffic between Great Britain and **Ireland** amounts\nthere is no English cycling team outside the Commonwealth to over 12 million passengers and of traded goods. **The Irish**\nGames. For those occasions, British Cycling selects and sup- **Sea** is connected to the North Atlantic at both its northern\nports the **England** team. Cycling is represented on the Isle and southern ends. To the north, the connection is through\n\n- f Man by the Isle of Man Cycling Association. Cycling in the North Channel between Scotland and Northern **Ireland**\nNorthern **Ireland** is organised under Cycling Ulster, part of and the Malin Sea. The southern end is linked to the Atlantic\nthe all-Ireland governing **body** Cycling **Ireland** . Until 2006, through the St George\u2019s Channel between Ireland and Pema rival governing **body** existed, . . . brokeshire, and the Celtic Sea. . . .\n\n\n\nTitle: No One (Game of Thrones) Title: P\u02daal Sverre Hagen\n...He may be \u201dno one,\u201d but there\u2019s still enough of a person P\u02daal Sverre Valheim Hagen (born 6 November 1980) is a Norleft in him to respect, and admire who this girl is and what wegian stage and screen actor. He appeared in the Norweshe\u2019s become."],"15":[" the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n\n\n**Selection of positive passages** Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA [6], we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the question will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. [7] We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the number of questions in training\/dev\/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n\n\n**5** **Experiments: Passage Retrieval**\n\n\nIn this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n\n\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Appendix A.\n\n\n**Training** **Retriever** **Top-20** **Top-100**\nNQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD\n\n\nNone BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0\n\n\nDPR 78.4 79.4 73.2 79.8 63.2 85.4 **85.0** 81.4 89.1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7"," chain\nthat appropriately references the original context.\n\n\nFor all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an example of the generation step in Fig. 3, the detailed\nreasoning answer includes a citation from the original context inside `##begin_quote##` and\n`##end_quote##` as well as the detailed explanation on how to reach the conclusion based on\nthe citations. We demonstrate that adding detailed reasoning paragraphs can help boost the\nmodel\u2019s performance in our experiment section.\n\n\n**4** **Evaluation**\n\n\nWe design our experiments to study how well RAFT performs compared to various baselines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading\nand extracting information from in-domain documents, than domain-specific finetuned\nmodel, and general-purpose model with RAG. As an ablation, we also demonstrate how\nimportant it is for the model to learn with Chain-of-Thought responses. In this section,\nwe will first introduce all the datasets we used in the experiments, then all the baseline\nmodel\/fine-tuning techniques that we benchmark against.\n\n\n4\n\n\nPreprint, Under Review\n\n```\n Question: The Oberoi family is part of a hotel company that has a head office\n in what city?\n\n context: [The Oberoi family is an Indian family that is famous for its\n involvement in hotels, namely through The Oberoi Group]...[It is located in\n city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\n Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\n towers that comprises a hotel and the Airlangga Apartment respectively]...[The\n Oberoi Group is a hotel company with its head office in Delhi.]\n\n Instruction: Given the question, context and answer above, provide a logical\n reasoning for that answer. Please use the format of: ##Reason: {reason}\n ##Answer: {answer}.\n\n -------------------------------------------------------------------------------------\n CoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\n Indian family that is famous for its involvement in hotels, namely through The\n Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\n the Oberoi group, and the","forms BM25 and DPR by margins\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\n\n\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\nthe performance observed in Karpukhin et al. (2020), where dpr-multiset-base showed superior results.\n\n\n7\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTable 2: **QuALITY and QASPER Performance With + Without RAPTOR:** Performance comparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outperforms baselines of each respective retrieval method for both datasets.\n\n\n**Model** **Accuracy (QuALITY)** **Answer F1 (QASPER)**\n\n\n**SBERT with RAPTOR** **56.6%** **36.70%**\n\nSBERT without RAPTOR 54.9% 36.23%\n\n**BM25 with RAPTOR** **52.1%** **27.00%**\n\nBM25 without RAPTOR 49.9% 26.47%\n\n**DPR with RAPTOR** **54.7%** **32.23%**\n\nDPR without RAPTOR 53.1% 31.70%\n\n\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different language models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column \u201dTitle +\nAbstract\u201d reflects performance when only the title and abstract of the papers are used for context.\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\nSpecifically, RAPTOR\u2019s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\nhigher than BM25.\n\n\n**Retriever** **GPT-3 F-1 Match** **GPT-4 F-1 Match** **UnifiedQA F-1 Match**\n\n\nTitle + Abstract 25.2 ",".1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7 **81.3**\n\n\nDPR **79.4** 78.8 **75.0** **89.1** 51.6 **86.0** 84.7 **82.9** 93.9 67.6\nMulti\nBM25 + DPR 78.0 **79.9** 74.7 88.5 66.2 83.9 84.4 82.3 **94.1** 78.6\n\n\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20\/100 retrieved\npassages that contain the answer. _Single_ and _Multi_ denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.\n\n\n\ntraditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10 _[\u2212]_ [5] using Adam, linear\nscheduling with warm-up and dropout rate 0 _._ 1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be desirable to obtain a single retriever that works well\nacross the board. To this end, we train a _multi_ dataset encoder by combining training data from\nall datasets excluding SQuAD. [8] In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages"," BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25( _q_, _p_ ) + _\u03bb \u00b7_ sim( _q, p_ ) as the ranking function.\nWe used _\u03bb_ = 1 _._ 1 based on the retrieval accuracy in\nthe development set.\n\n\n**5.1** **Main Results**\n\n\nTable 2 compares different passage retrieval systems on five QA datasets, using the top- _k_ accuracy\n( _k \u2208{_ 20 _,_ 100 _}_ ). With the exception of SQuAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when _k_ is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\n\n- n Natural Questions). When training with mul\n\n8SQuAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\n\nmore in Section 5.1.\n\n9\n[Lucene implementation. BM25 parameters](https:\/\/lucene.apache.org\/) _b_ = 0 _._ 4 (document length normalization) and _k_ 1 = 0 _._ 9 (term frequency\nscaling) are tuned using development sets.\n\n\n\nFigure 1: Retriever top- _k_ accuracy with different numbers of training examples used in our dense passage retriever vs BM25. The results are measured on the de\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n\n\ntiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and"],"16":[" the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n\n\n**Selection of positive passages** Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA [6], we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the question will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. [7] We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the number of questions in training\/dev\/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n\n\n**5** **Experiments: Passage Retrieval**\n\n\nIn this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n\n\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Appendix A.\n\n\n**Training** **Retriever** **Top-20** **Top-100**\nNQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD\n\n\nNone BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0\n\n\nDPR 78.4 79.4 73.2 79.8 63.2 85.4 **85.0** 81.4 89.1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7","]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7 P_ end _,i_ ( _t_ ), and\na passage selection score of the _i_ - th passage as\n_P_ selected( _i_ ).\nDuring training, we sample one positive and\n\n\u02dc\n_m_ _\u2212_ 1 negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. \u02dc _m_ is a hyper-parameter and we\nuse \u02dc _m_ = 24 in all the experiments. The training objective is to maximize the marginal log-likelihood\n\n- f all the correct answer spans in the positive passage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\n\n- f the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQuAD)\nand 4 for small (TREC, WQ) datasets, and tune _k_\n\n- n the development set. For experiments on small\ndatasets under the _Multi_ setting, in which using\n\n- ther datasets is allowed, we fine-tune the reader\ntrained on Natural Questions to the target dataset.\nAll experiments were done on eight 32GB GPUs.\n\n\n**6.2** **Results**\n\n\nTable 4 summarizes our final end-to-end QA results, measured by _exact match_ with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can\n\n\n**Training** **Model** **NQ** **TriviaQA** **WQ** **TREC** **SQuAD**\n\n\nSingle BM25+BERT (Lee et al., 2019) 26.5 47.1 17.7 21.3 33.2\nSingle ORQA (Lee et al., 2019) 33.3 45.0 36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9    -    -    Single GraphRetriever (Min et al., 201"," the organizers discourage too many\nsubmissions by the same team.\n**TREC CAR.** Introduced by Dietz [6] _et al._ in 2017, TREC CAR\nis a synthetic dataset based on Wikipedia that consists of about\n29M passages. Similar to related work [25], we use the first four of\nfive pre-defined folds for training and the fifh for validation. Tis\namounts to roughly 3M queries generated by concatenating the\ntitle of a Wikipedia page with the heading of one of its sections.\nTat section\u2019s passages are marked as relevant to the corresponding\nquery. Our evaluation is conducted on the test set used in TREC\n2017 CAR, which contains 2,254 queries.\n\n\n_4.1.2_ _Implementation._ Our ColBERT models are implemented\nusing Python 3 and PyTorch 1. We use the popular transformers [5]\n\nlibrary for the pre-trained BERT model. Similar to [25], we fine-tune\nall ColBERT models with learning rate 3 \u00d7 10 [\u2212][6] with a batch size\n32. We fix the number of embeddings per query at _Nq_ = 32. We set\n\n- ur ColBERT embedding dimension _m_ to be 128; \u00a74.5 demonstrates\nColBERT\u2019s robustness to a wide range of embedding dimensions.\nFor MS MARCO, we initialize the BERT components of the ColBERT query and document encoders using Google\u2019s official pretrained BERTbase model. Further, we train all models for 200k iterations. For TREC CAR, we follow related work [2, 25] and use a different pre-trained model to the official ones. To explain, the official\nBERT models were pre-trained on Wikipedia, which is the source\n\n- f TREC CAR\u2019s training and test sets. To avoid leaking test data\ninto train, Nogueira and Cho\u2019s [25] pre-train a randomly-initialized\nBERT model on the Wiki pages corresponding to training subset of\nTREC CAR. Tey release their BERTlarge pre-trained model, which\nwe fine-tune for ColBERT\u2019s experiments on TREC CAR. Since finetuning this model is significantly slower than BERTbase, we train\n\n- n TREC CAR for only 125k iterations.\nIn our re-ranking results, unless stated otherwise, we use 4 bytes\nper dimension in",".4)\n**RQ** 4: What are ColBERT\u2019s indexing-related costs in terms of\n\n- ffline computation and memory overhead? (\u00a74.5)\n\n\n**4.1** **Methodology**\n\n\n_4.1.1_ _Datasets & Metrics._ Similar to related work [2, 27, 28],\nwe conduct our experiments on the MS MARCO Ranking [24]\n(henceforth, MS MARCO) and TREC Complex Answer Retrieval\n(TREC-CAR) [6] datasets. Both of these recent datasets provide\nlarge training data of the scale that facilitates training and evaluating deep neural networks. We describe both in detail below.\n**MS MARCO.** MS MARCO is a dataset (and a corresponding\ncompetition) introduced by Microsof in 2016 for reading comprehension and adapted in 2018 for retrieval. It is a collection of 8.8M\npassages from Web pages, which were gathered from Bing\u2019s results\nto 1M real-world queries. Each query is associated with _sparse_\nrelevance judgements of one (or very few) documents marked as\nrelevant and no documents explicitly indicated as irrelevant. Per\nthe official evaluation, we use MRR@10 to measure effectiveness.\nWe use three sets of queries for evaluation. Te official devel\n- pment and evaluation sets contain roughly 7k queries. However,\nthe relevance judgements of the evaluation set are held-out by Microsof and effectiveness results can only be obtained by submiting\nto the competition\u2019s organizers. We submited our main re-ranking\nColBERT model for the results in \u00a74.2. In addition, the collection\nincludes roughly 55k queries (with labels) that are provided as additional validation data. We re-purpose a random sample of 5k\nqueries among those (i.e., ones not in our development or training\n\n\n\nsets) as a \u201clocal\u201d evaluation set. Along with the official development set, we use this held-out set for testing our models as well as\nbaselines in \u00a74.3. We do so to avoid submiting multiple variants\n\n- f the same model at once, as the organizers discourage too many\nsubmissions by the same team.\n**TREC CAR.** Introduced by Dietz [6] _et al._ in 2017, TREC CAR\nis a synthetic dataset based on Wikipedia that consists of about","\/testing splitting method as in previous\nwork (Lee et al., 2019). Below we briefly describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\n**Natural Questions (NQ)** (Kwiatkowski et al.,\n2019) was designed for end-to-end question answering. The questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identified by annotators.\n**TriviaQA** (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web.\n\n**WebQuestions (WQ)** (Berant et al., 2013) consists\n\n- f questions selected using Google Suggest API,\nwhere the answers are entities in Freebase.\n**CuratedTREC (TREC)** (Baudis and\u02c7 Sediv [\u02c7] y`,\n2015) sources questions from TREC QA tracks\n\n\n5However, Wang et al. (2019) also propose splitting documents into overlapping passages, which we do not find advantageous compared to the non-overlapping version.\n\n\n\n**Dataset** **Train** **Dev** **Test**\n\n\nNatural Questions 79,168 58,880 8,757 3,610\nTriviaQA 78,785 60,413 8,837 11,313\nWebQuestions 3,417 2,474 361 2,032\nCuratedTREC 1,353 1,125 133 694\nSQuAD 78,713 70,096 8,886 10,570\n\n\nTable 1: Number of questions in each QA dataset. The\ntwo columns of **Train** denote the original training examples in the dataset and the actual questions used for\ntraining DPR after filtering. See text for more details.\n\n\nas well as various Web sources and is intended for\n\n- pen-domain QA from unstructured corpora.\n**SQuAD v1.1** (Rajpurkar et al., 2016) is a popular benchmark dataset for reading comprehension.\nAnnotators were presented with a Wikipedia paragraph, and asked to write questions that could be\nanswered from the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison"],"17":[" corresponding to Google\nsearch queries. The open-domain version of\nthis dataset is obtained by discarding answers\nwith more than 5 tokens.\n\n\n  - TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and quiz-league\n\n\nFigure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of retrieved passages.\n\n\n\nwebsites. The _unfiltered_ version of TriviaQA\nis used for open-domain question answering.\n\n\n  - SQuAD v1.1 (Rajpurkar et al., 2016) is a reading comprehension dataset. Given a paragraph\nextracted from Wikipedia, annotators were\nasked to write questions, for which the answer\nis a span from the corresponding paragraph.\n\n\nFollowing Lee et al. (2019) we use the validation as\ntest, and keep 10% of the training set for validation.\nWe use the Wikipedia dumps from Dec. 20, 2018\nfor NQ and TriviaQA and from Dec. 21, 2016 for\nSQuAD. We apply the same preprocessing as Chen\net al. (2017); Karpukhin et al. (2020), leading to\npassages of 100 words, which do not overlap.\n\n\n**Evaluation.** Predicted answers are evaluated\n\nwith the standard exact match metric (EM), as introduced by Rajpurkar et al. (2016). A generated\nanswer is considered correct if it matches any answer of the list of acceptable answers after normalization. This normalization step consists in lowercasing and removing articles, punctuation and\nduplicated whitespace.\n\n\n**Technical details.** We initialize our models with\n\nthe pretrained T5 models (Raffel et al., 2019), available in the HuggingFace Transformers library. [4] We\nconsider two model sizes, base and large, containing respectively 220M and 770M parameters. We\nfine-tune the models on each dataset independently,\nusing Adam (Kingma and Ba, 2014) with a constant learning rate of 10 _[\u2212]_ [4] and a dropout rate of\n10%. We train the model for 10k gradient steps,\nwith a batch size of 64, using 64 Tesla V100 32Gb.\nWe evaluate models every 500 steps and select the\nbest one on the validation set based on the Exact\n\n","\/testing splitting method as in previous\nwork (Lee et al., 2019). Below we briefly describe\neach dataset and refer readers to their paper for the\ndetails of data preparation.\n**Natural Questions (NQ)** (Kwiatkowski et al.,\n2019) was designed for end-to-end question answering. The questions were mined from real\nGoogle search queries and the answers were spans\nin Wikipedia articles identified by annotators.\n**TriviaQA** (Joshi et al., 2017) contains a set of trivia\nquestions with answers that were originally scraped\nfrom the Web.\n\n**WebQuestions (WQ)** (Berant et al., 2013) consists\n\n- f questions selected using Google Suggest API,\nwhere the answers are entities in Freebase.\n**CuratedTREC (TREC)** (Baudis and\u02c7 Sediv [\u02c7] y`,\n2015) sources questions from TREC QA tracks\n\n\n5However, Wang et al. (2019) also propose splitting documents into overlapping passages, which we do not find advantageous compared to the non-overlapping version.\n\n\n\n**Dataset** **Train** **Dev** **Test**\n\n\nNatural Questions 79,168 58,880 8,757 3,610\nTriviaQA 78,785 60,413 8,837 11,313\nWebQuestions 3,417 2,474 361 2,032\nCuratedTREC 1,353 1,125 133 694\nSQuAD 78,713 70,096 8,886 10,570\n\n\nTable 1: Number of questions in each QA dataset. The\ntwo columns of **Train** denote the original training examples in the dataset and the actual questions used for\ntraining DPR after filtering. See text for more details.\n\n\nas well as various Web sources and is intended for\n\n- pen-domain QA from unstructured corpora.\n**SQuAD v1.1** (Rajpurkar et al., 2016) is a popular benchmark dataset for reading comprehension.\nAnnotators were presented with a Wikipedia paragraph, and asked to write questions that could be\nanswered from the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison","]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7 P_ end _,i_ ( _t_ ), and\na passage selection score of the _i_ - th passage as\n_P_ selected( _i_ ).\nDuring training, we sample one positive and\n\n\u02dc\n_m_ _\u2212_ 1 negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. \u02dc _m_ is a hyper-parameter and we\nuse \u02dc _m_ = 24 in all the experiments. The training objective is to maximize the marginal log-likelihood\n\n- f all the correct answer spans in the positive passage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\n\n- f the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQuAD)\nand 4 for small (TREC, WQ) datasets, and tune _k_\n\n- n the development set. For experiments on small\ndatasets under the _Multi_ setting, in which using\n\n- ther datasets is allowed, we fine-tune the reader\ntrained on Natural Questions to the target dataset.\nAll experiments were done on eight 32GB GPUs.\n\n\n**6.2** **Results**\n\n\nTable 4 summarizes our final end-to-end QA results, measured by _exact match_ with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can\n\n\n**Training** **Model** **NQ** **TriviaQA** **WQ** **TREC** **SQuAD**\n\n\nSingle BM25+BERT (Lee et al., 2019) 26.5 47.1 17.7 21.3 33.2\nSingle ORQA (Lee et al., 2019) 33.3 45.0 36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9    -    -    Single GraphRetriever (Min et al., 201"," chain\nthat appropriately references the original context.\n\n\nFor all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an example of the generation step in Fig. 3, the detailed\nreasoning answer includes a citation from the original context inside `##begin_quote##` and\n`##end_quote##` as well as the detailed explanation on how to reach the conclusion based on\nthe citations. We demonstrate that adding detailed reasoning paragraphs can help boost the\nmodel\u2019s performance in our experiment section.\n\n\n**4** **Evaluation**\n\n\nWe design our experiments to study how well RAFT performs compared to various baselines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading\nand extracting information from in-domain documents, than domain-specific finetuned\nmodel, and general-purpose model with RAG. As an ablation, we also demonstrate how\nimportant it is for the model to learn with Chain-of-Thought responses. In this section,\nwe will first introduce all the datasets we used in the experiments, then all the baseline\nmodel\/fine-tuning techniques that we benchmark against.\n\n\n4\n\n\nPreprint, Under Review\n\n```\n Question: The Oberoi family is part of a hotel company that has a head office\n in what city?\n\n context: [The Oberoi family is an Indian family that is famous for its\n involvement in hotels, namely through The Oberoi Group]...[It is located in\n city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\n Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\n towers that comprises a hotel and the Airlangga Apartment respectively]...[The\n Oberoi Group is a hotel company with its head office in Delhi.]\n\n Instruction: Given the question, context and answer above, provide a logical\n reasoning for that answer. Please use the format of: ##Reason: {reason}\n ##Answer: {answer}.\n\n -------------------------------------------------------------------------------------\n CoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\n Indian family that is famous for its involvement in hotels, namely through The\n Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\n the Oberoi group, and the",".05)  - 90.1  - 95.0 [\u2021]\n\nHybrid (\u03b1 = 0.7)  - 89.6  - 94.6 [\u2021]\n\n\nSQuAD\n\nDPR 51.6 52.0 67.6 67.7\nBM25 68.8 71.1 [\u2020] 80.0 81.8 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 66.2  - 78.6  Hybridnorm (\u03b1 = 2.00)  - 75.1 [\u2021]  - 84.4 [\u2021]\n\nHybrid (\u03b1 = 28)  - 75.0 [\u2021]  - 84.0 [\u2021]\n\n\nTable 1: Retrieval effectiveness comparing results from\nthe original DPR paper (\u201corig\u201d) and our replication attempt (\u201crepl\u201d). The symbol [\u2020] - n a BM25 result indicates effectiveness that is significantly different from\nDPR. The symbol [\u2021] indicates that the hybrid technique\nis significantly better than BM25 (for SQuAD) or DPR\n(for all remaining collections).\n\n\nthat the effectiveness differences between DPR\n\nand BM25 in our replication results are statistically significant. In all datasets except for SQuAD,\nDPR outperforms BM25; this is consistent with\nthe original paper. We further confirmed that for\nSQuAD, DPR is significantly worse than BM25.\nAs Karpukhin et al. (2020) noted, RetrieverMulti\nwas trained by combining training data from all\ndatasets but excluding SQuAD; these poor results\nare expected, since SQuAD draws from a very\nsmall set of Wikipedia articles.\nThird, the effectiveness of hybrid dense\u2013sparse\nfusion appears to be understated in the original\nDPR paper. Karpukhin et al. (2020) found that\n\n\nCondition k = 20 100 500 1000\n\n\nNQ 6.1 5.2 4.4 4.2\nTriviaQA 9.2 6.6 5.0 4.6\nWQ 5.9 5.9 5.8 5.7\n\nCuratedTrec 6.9 7.2 6.3 5.9\n\n"],"18":[" like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25\/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer",".1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7 **81.3**\n\n\nDPR **79.4** 78.8 **75.0** **89.1** 51.6 **86.0** 84.7 **82.9** 93.9 67.6\nMulti\nBM25 + DPR 78.0 **79.9** 74.7 88.5 66.2 83.9 84.4 82.3 **94.1** 78.6\n\n\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20\/100 retrieved\npassages that contain the answer. _Single_ and _Multi_ denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.\n\n\n\ntraditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10 _[\u2212]_ [5] using Adam, linear\nscheduling with warm-up and dropout rate 0 _._ 1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be desirable to obtain a single retriever that works well\nacross the board. To this end, we train a _multi_ dataset encoder by combining training data from\nall datasets excluding SQuAD. [8] In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages","forms BM25 and DPR by margins\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\n\n\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\nthe performance observed in Karpukhin et al. (2020), where dpr-multiset-base showed superior results.\n\n\n7\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTable 2: **QuALITY and QASPER Performance With + Without RAPTOR:** Performance comparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outperforms baselines of each respective retrieval method for both datasets.\n\n\n**Model** **Accuracy (QuALITY)** **Answer F1 (QASPER)**\n\n\n**SBERT with RAPTOR** **56.6%** **36.70%**\n\nSBERT without RAPTOR 54.9% 36.23%\n\n**BM25 with RAPTOR** **52.1%** **27.00%**\n\nBM25 without RAPTOR 49.9% 26.47%\n\n**DPR with RAPTOR** **54.7%** **32.23%**\n\nDPR without RAPTOR 53.1% 31.70%\n\n\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different language models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column \u201dTitle +\nAbstract\u201d reflects performance when only the title and abstract of the papers are used for context.\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\nSpecifically, RAPTOR\u2019s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\nhigher than BM25.\n\n\n**Retriever** **GPT-3 F-1 Match** **GPT-4 F-1 Match** **UnifiedQA F-1 Match**\n\n\nTitle + Abstract 25.2 "," highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer. The passage\nselection model serves as a reranker through crossattention between the question and the passage. Although cross-attention is not feasible for retrieving\nrelevant passages in a large corpus due to its nondecomposable nature, it has more capacity than the\ndual-encoder model sim( _q, p_ ) as in Eq. (1). Applying it to selecting the passage from a small number\n\n- f retrieved candidates has been shown to work\n\nwell (Wang et al., 2019, 2018; Lin et al., 2018).\nSpecifically, let **P** _i \u2208_ R _[L][\u00d7][h]_ (1 _\u2264_ _i \u2264_ _k_ ) be\na BERT (base, uncased in our experiments) representation for the _i_ - th passage, where _L_ is the\nmaximum length of the passage and _h_ the hidden\ndimension. The probabilities of a token being the\nstarting\/ending positions of an answer span and a\npassage being selected are defined as:\n\n\n\n_P_ start _,i_ ( _s_ ) = softmax\ufffd **P** _i_ **w** start\ufffd\n\n\n\n_s_ _[,]_ (3)\n\n_t_ _[,]_ (4)\n\n\n\n_P_ end _,i_ ( _t_ ) = softmax\ufffd **P** _i_ **w** end\ufffd\n\n\n\n_P_ selected( _i_ ) = softmax\ufffd **\u02c6P** \u22ba **w** selected\ufffd\n\n\n\n_i_ _[,]_ [ (5)]\n\n\n\nwhere **P** **[\u02c6]** = [ **P** [[CLS]] 1 _, . . .,_ **P** [[CLS]] _k_ ] _\u2208_ R _[h][\u00d7][k]_ and\n**w** start _,_ **w** end _,_ **w** selected _\u2208_ R _[h]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7","r+\n\u03b3 \u00b7 R [\u20d7] )i \u00b7 softmax(\u20d7si)j.\nThus, to summarize, we explored four settings\nin our end-to-end QA replication: the original\nDPR span scoring technique, with and without retriever score fusion, and the answer span scoring\ntechnique of Mao et al. (2020), with and without\nretriever score fusion.\n\n\n3 Experimental Setup\n\n\nModels Our replication efforts began with\nmodel checkpoints provided in the DPR repo. Unfortunately, Karpukhin et al. (2020) did not appear\nto make available all models used in their experiments, and thus, to be precise, our experiments\nused the following models:\n\n\n- RetrieverNQ: DPR encoders trained using just\nthe NQ dataset (for the retriever).\n\n\n- RetrieverMulti: DPR encoders trained using a\ncombination of datasets (for the retriever).\n\n\n- ReaderNQ-Single: the DPR reader trained on NQ\nwith negative passages from retrieval results by\nRetrieverNQ.\n\n\n- ReaderTQA-Multi: the DPR reader trained on\nTriviaQA with negative passages from retrieval\nresults by RetrieverMulti.\n\n\n\nDatasets We evaluated retrieval effectiveness\n\n- n five standard benchmark QA datasets (NQ,\nTriviaQA, WQ, CuratedTREC, SQuAD), exactly\nthe same as Karpukhin et al. (2020). We used the\nRetrieverMulti model, which can be applied to all\nfive datasets. For end-to-end QA, we evaluated\n\n- n NQ and TriviaQA with the available models.\nMore precisely, we used the ReaderNQ-Single model\nto process the retrieved contexts from RetrieverNQ\nfor NQ and used the ReaderTQA-Multi model to process the retrieved contexts from RetrieverMulti for\nTriviaQA.\n\n\nMetrics For retrieval, we measured effectiveness in terms of top-k retrieval accuracy, defined\nas the fraction of questions that have a correct answer span in the top-k retrieved contexts at least\n\n- nce. End-to-end QA effectiveness is measured\nin terms of the exact match (EM) metric, defined\nas the fraction of questions that have an extracted\nanswer span exactly matching the ground truth an\nswer.\n\nMissing from the original DPR paper, we performed significance testing to assess the statistical\nsignificance of metric differences. In all cases,"],"19":[" like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25\/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer","]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7 P_ end _,i_ ( _t_ ), and\na passage selection score of the _i_ - th passage as\n_P_ selected( _i_ ).\nDuring training, we sample one positive and\n\n\u02dc\n_m_ _\u2212_ 1 negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. \u02dc _m_ is a hyper-parameter and we\nuse \u02dc _m_ = 24 in all the experiments. The training objective is to maximize the marginal log-likelihood\n\n- f all the correct answer spans in the positive passage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\n\n- f the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQuAD)\nand 4 for small (TREC, WQ) datasets, and tune _k_\n\n- n the development set. For experiments on small\ndatasets under the _Multi_ setting, in which using\n\n- ther datasets is allowed, we fine-tune the reader\ntrained on Natural Questions to the target dataset.\nAll experiments were done on eight 32GB GPUs.\n\n\n**6.2** **Results**\n\n\nTable 4 summarizes our final end-to-end QA results, measured by _exact match_ with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can\n\n\n**Training** **Model** **NQ** **TriviaQA** **WQ** **TREC** **SQuAD**\n\n\nSingle BM25+BERT (Lee et al., 2019) 26.5 47.1 17.7 21.3 33.2\nSingle ORQA (Lee et al., 2019) 33.3 45.0 36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9    -    -    Single GraphRetriever (Min et al., 201","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice ["," highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer. The passage\nselection model serves as a reranker through crossattention between the question and the passage. Although cross-attention is not feasible for retrieving\nrelevant passages in a large corpus due to its nondecomposable nature, it has more capacity than the\ndual-encoder model sim( _q, p_ ) as in Eq. (1). Applying it to selecting the passage from a small number\n\n- f retrieved candidates has been shown to work\n\nwell (Wang et al., 2019, 2018; Lin et al., 2018).\nSpecifically, let **P** _i \u2208_ R _[L][\u00d7][h]_ (1 _\u2264_ _i \u2264_ _k_ ) be\na BERT (base, uncased in our experiments) representation for the _i_ - th passage, where _L_ is the\nmaximum length of the passage and _h_ the hidden\ndimension. The probabilities of a token being the\nstarting\/ending positions of an answer span and a\npassage being selected are defined as:\n\n\n\n_P_ start _,i_ ( _s_ ) = softmax\ufffd **P** _i_ **w** start\ufffd\n\n\n\n_s_ _[,]_ (3)\n\n_t_ _[,]_ (4)\n\n\n\n_P_ end _,i_ ( _t_ ) = softmax\ufffd **P** _i_ **w** end\ufffd\n\n\n\n_P_ selected( _i_ ) = softmax\ufffd **\u02c6P** \u22ba **w** selected\ufffd\n\n\n\n_i_ _[,]_ [ (5)]\n\n\n\nwhere **P** **[\u02c6]** = [ **P** [[CLS]] 1 _, . . .,_ **P** [[CLS]] _k_ ] _\u2208_ R _[h][\u00d7][k]_ and\n**w** start _,_ **w** end _,_ **w** selected _\u2208_ R _[h]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7"," are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\n\n- pen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve comparable or better results on multiple QA datasets in the\n\n- pen-retrieval setting, compared to several, much\ncomplicated systems.\n\n\n**2** **Background**\n\n\nThe problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \u201c _Who first voiced Meg on Family_\n_Guy?_ \u201d or \u201c _Where was the 8th Dalai Lama born?_ \u201d, a\nsystem is required to answer it using a large corpus\n\n- f diversified topics. More specifically, we assume\n\n\n\nthe extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more passages in the corpus. Assume that our collection\ncontains _D_ documents, _d_ 1 _, d_ 2 _, \u00b7 \u00b7 \u00b7, dD_ . We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units [3] and get _M_\ntotal passages in our corpus _C_ = _{p_ 1 _, p_ 2 _, . . ., pM_ _}_,\nwhere each passage _pi_ can be viewed as a sequence\n\n- f tokens _w_ [(] _[i]_ [)]\n1 _[, w]_ 2 [(] _[i]_ [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _|_ [(] _p_ _[i]_ [)] _i|_ [. Given a question] _[ q]_ [,]\n\nthe task is to find a span _ws_ [(] _[i]_ [)] _[, w]_ _s_ [(] _[i]_ +1 [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _e_ [(] _[i]_ [)] from\n\n- ne of the passages _pi_ that can"],"20":["36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9    -    -    Single GraphRetriever (Min et al., 2019b) 34.5 56.0 36.4    -    Single PathRetriever (Asai et al., 2020) 32.6    -    -    - **56.5**\nSingle REALMWiki (Guu et al., 2020) 39.2    - 40.2 46.8    Single REALMNews (Guu et al., 2020) 40.4    - 40.7 42.9    \n\nBM25 32.6 52.4 29.9 24.9 38.1\n\nSingle DPR **41.5** 56.8 34.6 25.9 29.8\nBM25+DPR 39.0 57.0 35.2 28.0 36.7\n\n\nDPR **41.5** 56.8 **42.4** 49.4 24.1\nMulti\nBM25+DPR 38.8 **57.9** 41.1 **50.6** 35.8\n\n\nTable 4: End-to-end QA (Exact Match) Accuracy. The first block of results are copied from their cited papers.\nREALMWiki and REALMNews are the same model but pretrained on Wikipedia and CC-News, respectively. _Single_\nand _Multi_ denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets\n(all except SQuAD). For WQ and TREC in the _Multi_ setting, we fine-tune the reader trained on NQ.\n\n\n\nsee that higher retriever accuracy typically leads to\nbetter final QA results: in all cases except SQuAD,\nanswers extracted from the passages retrieved by\nDPR are more likely to be correct, compared to\nthose from BM25. For large datasets like NQ and\nTriviaQA, models trained using multiple datasets\n(Multi) perform comparably to those trained using\nthe individual training set (Single). Conversely,\n\n- n smaller datasets like WQ and TREC, the multidataset setting has a clear advantage. Overall, our\nD","I\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So\n\nthe answer is: 1894.\n\n\nQ: When was Neville A. Stanton's employer founded?\nA: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862.\n\nSo the answer is: 1862.\n\n\nQ: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to?\nA: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located\n\nin the Prince County. So the answer is: Prince County.\n\n\nQ: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America\u2212\n\nLite: How Imperial Academia Dismantled Our Culture's author?\nA: The author of America\u2212Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was\n\neducated at the Yale University. The city in Connecticut that has the highest number of Zagat\u2212rated restaurants is New\nHaven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale\nHerald.\n\n\nQ: What is the headquarters for the organization who sets the standards for ISO 21500?\nA: The standards for ISO 21500 were set by International Organization for Standardization. The International Organization\n\nfor Standardization has headquarters in Geneva. So the answer is: Geneva.\n\n\nQ: What did the publisher of Banjo\u2212Tooie rely primarily on for its support?\nA: The publisher of Banjo\u2212Tooie is Nintendo. Nintendo relied primarily for its support on first\u2212party games. So the answer is:\n\nfirst\u2212party games.\n\n\nQ: In which county was the birthplace of the Smoke in tha City performer?\nA: The performer of Smoke in tha City is MC Eiht. MC Eiht's birthplace is Compton. Compton is located in the county of Los\n\nAngeles County. So the answer is: Los Angeles County.\n\n\nQ: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?\n\n\nA: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central\n\nJakarta.\n\n\nQ:"," time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine","2023):\n\n\n   - search ~~q~~ uery\n\n   - search ~~d~~   - cument\n\n   - classification\n\n\n20\n\n\nquery document\n\n\npopulation of breckenridge mi breckenridge, michigan. breckenridge is a village\nin gratiot county in the u. s. state of michigan. the\npopulation was 1, 328 at the 2010 census. the village\nis located in wheeler township.\ncan a deposition be used in a criminal case depositions are commonly used in civil litigation\n(suits for money damages or equitable relief) [...]\nwhat cases require strict scrutiny the strict scrutiny standard is one of three employed\nby the courts in reviewing laws and government policies. the rational basis [...]\nfunction of state supreme courts it has also initiated several programs designed to\nimprove the effectiveness of the court system. a\nprimary function of the supreme court is to ensure\n\n[...]\nwhat is the population in idaho idaho \u2019 s population grows to nearly 1. 7 million.\nidaho \u2019 s population grew by 1. 2 percent between\nmid - 2014 and mid - 2015, the 12th strongest increase among the states and four - tenths of a percentage point ahead of the national growth rate.\nwhat is the population of manson, ia manson, iowa. manson is a city in calhoun county,\niowa, united states. the population was 1, 690 at the\n2010 census.\nwhat happens after a sentencing hearing find answers. sentencing. after a criminal defendant\nis convicted or pleads guilty, a judge will decide [...]\nflathead county population flathead county, montana. flathead county is a county\nlocated in the u. s. state of montana. as of the 2010\ncensus, the population was 90, 928, making it [...]\nwhiting, ks population the city of whiting had a population of 177 as of july\n1, 2017. whiting ranks in the lower quartile for population density and diversity index when compared to\nthe other cities, towns [...]\nwhat is the population of lewiston id lewiston, id population and races. as of 2010 - 2014,\nthe total population of lewiston is 32, 178, which is\n4. 12% more than it was"," the answer is: tuberculosis.\n\n\nQ: Which film has the director died earlier, When The Mad Aunts Arrive or The Miracle Worker (1962 Film)?\nA: When The Mad Aunts Arrive was directed by Franz Josef Gottlieb. The Miracle Worker (1962 film) was directed by\n\nArthur Penn. Franz Josef Gottlieb died on 23 July 2006. Arthur Penn died on September 28, 2010. Thus, of the two, the\ndirector to die earlier is Franz Josef Gottlieb, who directed When The Mad Aunts Arrive. So the answer is: When The\nMad Aunts Arrive.\n\n\nQ: Which album was released earlier, What'S Inside or Cassandra'S Dream (Album)?\nA: What's Inside was released in the year 1995. Cassandra's Dream (album) was released in the year 2008. Thus, of the two,\n\nthe album to release earlier is What's Inside. So the answer is: What's Inside.\n\n\nQ: Are both mountains, Serre Mourene and Monte Galbiga, located in the same country?\nA: Serre Mourene is located in Spain. Monte Galbiga is located in Italy. Thus, the two countries are not located in the same\n\n\ncountry. So the answer is: no.\n\n\nQ: What is the date of birth of the director of film Best Friends (1982 Film)?\nA: The film Best Friends was directed by Norman Jewison. Norman Jewison was born on July 21, 1926. So the answer is:\n\nJuly 21, 1926.\n\n\nQ: Which film has the director born first, Two Weeks With Pay or Chhailla Babu?\nA: Two Weeks with Pay was directed by Maurice Campbell. Chhailla Babu was directed by Joy Mukherjee. Maurice\n\nCampbell was born on November 28, 1919. Joy Mukherjee was born on 24 February 1939. Thus, from the two directors,\n\nChhailla Babu was born first, who directed Two Weeks With Pay. So the answer is: Two Weeks With Pay.\n\n\nQ: Who is the grandchild of Krishna Shah (Nepalese Royal)?\nA: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah has a\n\ngrandchild named Prithvipati Shah. So the answer is:"],"21":[" and the Family in May\n2002. She held this position until 2005.\n**Stepback Question** : What positions did Carmel Hanna hold?\n**Final Answer [Correct]** : From the context, we can see that Carmel\nHanna was named minister of Employment and Learning (2001). We\ncan then infer that she held the position of Minister for Employment and\nLearning in May 2002.\n\n\nTable 32: Illustration of wins of STEP-BACK PROMPTING on the TimeQA dataset.\n\n\n36\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nSituatedQA **Original Question** : when was the last time miss texas won miss america\nas of November 29, 2010\n**Original Answer [Incorrect]** : Laura Leigh Covington was the last Miss\nTexas to win the Miss America competition. This happened in 2000.\n**Stepback Question** : which years did miss texas win miss america\n**Final Answer [Correct]** : According to Wikipedia, the last time Miss\nTexas won Miss America was in 1975.\n\n\nSituatedQA **Original Question** : when is the last time congress passed a complete\nbudget as of 2021\n**Original Answer [Incorrect]** : The last time Congress passed a complete\nbudget as of 2021 was on January 19, 2018.\n**Stepback Question** : what are the years that congress passed a complete\nbudget as of 2021\n**Final Answer [Correct]** : The last time Congress passed a complete budget as of 2021 was December 27, 2020, the Consolidated Appropriations\nAct, 2021.\n\n\nSituatedQA **Original Question** : when was the last time unc did not make the ncaa\ntournament as of 2003\n**Original Answer [Incorrect]** : The last time UNC did not make the\nNCAA tournament was 1990.\n**Stepback Question** : which years did unc not make the ncaa tournament\nas of 2003\n**Final Answer [Correct]** : The correct answer is 2002-03. The context\nshows the standings of North Carolina Tar Heels men\u2019s basketball. From\nthe standings, we can see that this team has 19 wins and 16 losses. 19-16\nis not good enough to make the NCAA tournament, which is mentioned\nin the context."," fine-tuned by the results of RAG\nsystem. For example, CRAG [67] trains a lightweight retrieval\nevaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval\nactions based on confidence levels.\n\n\n_D. Scaling laws of RAG_\n\n\nEnd-to-end RAG models and pre-trained models based\n\n- n RAG are still - ne - f the focuses - f current re\nsearchers [173].The parameters of these models are one of\nthe key factors.While scaling laws [174] are established for\nLLMs, their applicability to RAG remains uncertain. Initial\nstudies like RETRO++ [44] have begun to address this, yet the\nparameter count in RAG models still lags behind that of LLMs.\nThe possibility of an Inverse Scaling Law [10], where smaller\nmodels outperform larger ones, is particularly intriguing and\nmerits further investigation.\n\n\n_E. Production-Ready RAG_\n\n\nRAG\u2019s practicality and alignment with engineering requirements have facilitated its adoption. However, enhancing retrieval efficiency, improving document recall in large knowledge bases, and ensuring data security\u2014such as preventing\n\n\n[10https:\/\/github.com\/inverse-scaling\/prize](https:\/\/github.com\/inverse-scaling\/prize)\n\n\n\ninadvertent disclosure of document sources or metadata by\nLLMs\u2014are critical engineering challenges that remain to be\naddressed [175].\nThe development of the RAG ecosystem is greatly impacted\nby the progression of its technical stack. Key tools like\nLangChain and LLamaIndex have quickly gained popularity\nwith the emergence of ChatGPT, providing extensive RAGrelated APIs and becoming essential in the realm of LLMs.The\nemerging technology stack, while not as rich in features as\nLangChain and LLamaIndex, stands out through its specialized\nproducts. For example, Flowise AI prioritizes a low-code\napproach, allowing users to deploy AI applications, including\nRAG, through a user-friendly drag-and-drop interface. Other\ntechnologies like HayStack, Meltano, and Cohere Coral are\nalso gaining attention for their unique contributions to the field.\nIn addition to AI-focused vendors, traditional software and\ncloud service providers are expanding their offerings to include\nRAG-centric services. Weaviate\u2019s Verba [11] is designed for\npersonal assistant applications, while Amazon\u2019s Kendra 12\n\n- ffers intelligent",". Next, we utilize search engines to retrieve relevant\nweb pages. Finally, a dense retrieval model is employed to\nre-rank the content of these web pages.\n\n\net al. 2019a,b; Peng et al. 2023), domain-specific question answering (Cui et al. 2023) and code generation (Zhou\net al. 2023b). Recently, with the development of large models, a series of retrieval-enhanced tools and products have\ngained widespread attention, such as ChatGPT retrieval plugin, Langchain, New Bing, etc. However, in real-world scenarios, the retrieved text inevitably contains noise. Therefore, in this paper we conducted a systematic evaluation and\nanalysis of retrieval-augmented generation in LLMs.\n\n\n**Evaluation of LLMs** Evaluating LLMs has received significant attention due to their remarkable general capability (Chang et al. 2023). It enables us to gain a deeper understanding of the specific abilities and limitations of LLMs,\nwhile also providing valuable guidance for future research.\nIn the past, benchmarks such as GLUE (Wang et al. 2019b)\nand SuperCLUE (Wang et al. 2019a) primarily focused on\nevaluating NLP tasks, particularly in natural language understanding. However, these evaluations often fail to fully\ncapture the capabilities of LLMs. MMLU (Hendrycks et al.\n2021) was then proposed to measure the knowledge acquired\nby language models when pre-training. Recently, with the\ndevelopment of LLMs, a series of general evaluation benchmarks have emerged, such as AGIEval (Zhong et al. 2023),\nC-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),\nOpenLLM Leaderboard (Edward Beeching 2023), etc. In\naddition to general abilities, there are also specific benchmarks that focus on evaluating the capabilities of models.\nFor example, CValues (Xu et al. 2023a) focuses on the safety\n\n\n\nand responsibility of LLMs, M3Exam (Zhang et al. 2023)\nfocuses on human exam and ToolBench (Qin et al. 2023)\nevaluates how well LLMs use external tools. Recently, Adlakha et al. (2023)","## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar","\n\n\n[6https:\/\/github.com\/aurelio-labs\/semantic-router](https:\/\/github.com\/aurelio-labs\/semantic-router)\n[7https:\/\/huggingface.co\/spaces\/mteb\/leaderboard](https:\/\/huggingface.co\/spaces\/mteb\/leaderboard)\n\n\n\n9\n\n\nto provide initial search results for training dense retrieval\nmodels. Additionally, pre-training language models (PLMs)\ncan be utilized to learn term weights to enhance sparse\nretrieval. Specifically, it also demonstrates that sparse retrieval\nmodels can enhance the zero-shot retrieval capability of dense\nretrieval models and assist dense retrievers in handling queries\ncontaining rare entities, thereby improving robustness.\n_2) Fine-tuning Embedding Model:_ In instances where the\ncontext significantly deviates from pre-training corpus, particularly within highly specialized disciplines such as healthcare,\nlegal practice, and other sectors replete with proprietary jargon,\nfine-tuning the embedding model on your own domain dataset\nbecomes essential to mitigate such discrepancies.\nIn addition to supplementing domain knowledge, another\npurpose of fine-tuning is to align the retriever and generator,\nfor example, using the results of LLM as the supervision signal\nfor fine-tuning, known as LSR (LM-supervised Retriever).\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\ngenerator to create task-specific retrievers, addressing challenges in supervised fine-tuning, particularly in data-scarce\ndomains. Another approach, LLM-Embedder [97], exploits\nLLMs to generate reward signals across multiple downstream\ntasks. The retriever is fine-tuned with two types of supervised\nsignals: hard labels for the dataset and soft rewards from\nthe LLMs. This dual-signal approach fosters a more effective\nfine-tuning process, tailoring the embedding model to diverse\ndownstream applications. REPLUG [72] utilizes a retriever\nand an LLM to calculate the probability distributions of the\nretrieved documents and then performs supervised training\nby computing the KL divergence. This straightforward and\neffective training method enhances the performance of the\nretrieval model by using an LM as the supervisory signal,\neliminating the need for specific cross-attention mechanisms.\nMoreover, inspired by RLHF (Reinforcement Learning from\nHuman Feedback), utilizing LM-based feedback to reinforce\nthe retriever through reinforcement learning.\n\n\n_E. Adapter_"],"22":["sufficient context. We constructed a sufficient context autorater, which enabled scalable insights into\nmodel performance on different types of instances. Our analysis revealed that even with sufficient\ncontext, LLMs frequently hallucinate answers. We also found, surprisingly, many cases where\na model will output a correct answer with access to only insufficient context. Qualitatively, we\ncategorized such instances, leading to a fuller picture of ways context can be useful. Finally, we\ndemonstrated a general-purpose selective generation method, which applies to Gemini, GPT, and\nGemma, and can reduce hallucinations by 2\u201310% on queries that the model answers.\n\n\n**Limitations.** Our analysis focuses on QA datasets, but summarization tasks also utilize context,\nwhich may or may not be sufficient. For example, models may behave differently on the prompt\n\u201cSummarize the reviews of 5-star hotels in Mallorca\u201d depending on whether the context mentions the\nhotel reviews, whether they are for 5-star hotels, etc. Another shortcoming is an exploration of how\n\n- ften different retrieval methods lead to sufficient context. Also to achieve the best performance, we\ncould have used our autorater to iteratively judge whether to retrieve more or answer the question.\n\n\n**Future Work.** One direction is a fine-grained sufficient context autorater, which outputs a score\ninstead of a binary label. This could be useful for ranking contexts after the retrieval step. Another\ndirection is to extend the definition of sufficient context to multi-modal RAG settings, such as for\nvisual QA (images) or document QA (pdf files). Finally, our selective generation results suggest that\nthere is room for improvement in reducing hallucinations by using auxiliary signals from the inputs.\n\n\n10\n\n\nPublished as a conference paper at ICLR 2025\n\n\nACKNOWLEDGMENTS\n\n\nWe thank Hrishikesh Garud, Vikram Gopali, Xun Sun, and Bruce Wang for annotating data. We\nthank Ranjay Krishna and Jacob Eisenstein for helpful discussions. We also thank Alyshia Olsen for\nhelp with the figure design and color palette. We thank the anonymous reviewers for suggestions to\nimprove the presentation.\n\n\nREFERENCES\n\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\nAlmeida, Janko Altenschmidt, Sam"," documents in datasets vary in how much information they\nprovide about the query, and whether they directly inform the ground truth answer or not. In other\nwords, while the goal seems to be to understand how LLMs behave when they do or do not have\nsufficient information to answer the query, prior work fails to address this head-on.\n\n\nAs our first contribution, we put forth a new notion of sufficient context. We divide instances into two\ncategories based on whether the context provides enough information to construct an answer to the\nquery. The sufficient context designation is a function of an input pair consisting of one question and\nthe associated context. Crucially, it does not require a ground truth answer. Figure 1 shows examples\nand a breakdown of model responses after splitting the data based on sufficient vs. insufficient context.\nTo divide the dataset, we use an LLM-based autorater to classify instances as sufficient or not. Here,\nan _autorater_ is a model that evaluates instances based on a property, e.g., a sufficient context autorater.\n\n\nUsing our sufficient context autorater, we uncover new insights into LLM behavior and into existing\nbenchmark datasets. First, we find models generate incorrect answers on a non-trivial fraction of\ninstances that have sufficient context to answer the query. In other words, open-book QA cannot\nbe solved by improving retrieval alone. Second, when given instances without sufficient context,\nmodels tend to hallucinate more than they abstain, especially for multi-hop questions. This finding\ncomplements prior work, which shows that LLMs are not robust to noisy retrieval (Yoran et al.,\n2024; Wu et al., 2024). Third, models generate correct answers in many cases, even when the\nprovided context is insufficient. Surprisingly, this remains true after we filter out questions that the\nmodel answers correctly in a closed book (w\/o RAG) setting. Together, our analysis deepens our\nunderstanding of RAG systems by revealing nuances in how models generate responses with retrieval.\n\n\nAs a final contribution, we explore ways to use sufficient context labels to reduce model hallucinations.\nWe implement a new selective generation framework that improves accuracy. We use a smaller,\nintervention model to determine when the model generates or abstains, providing a controllable\ntrade-off. Moreover, we can combine our method with any LLM, including proprietary models like\nGemini and GPT. Our main result is that using sufficient","ervention model to determine when the model generates or abstains, providing a controllable\ntrade-off. Moreover, we can combine our method with any LLM, including proprietary models like\nGemini and GPT. Our main result is that using sufficient context as an additional signal leads to\nmuch higher accuracy over the fraction of answered queries, for most coverage levels and across\nmultiple models\/datasets. We also find that fine-tuning open-source models with sufficient context\ninformation does not easily reduce the hallucination rate. Instead, for Mistral 3, fine-tuning can lead\nto a higher abstention rate at the cost of fewer correct answers. Key findings and the prompts used in\n\n[our autorater analysis are available on our github.](https:\/\/github.com\/hljoren\/sufficientcontext)\n\n\nTo summarize, our main contributions are\n\n\n1. We define the notion of sufficient context, unifying existing work on relevance for RAG systems.\nThen, we design a sufficient context autorater (achieving 93% accuracy), enabling us to label\ninstances scalably and to analyze model responses with or without sufficient context.\n\n\n2. Our analysis leads to several new findings about retrieval-augmented model performance. One\ntakeaway is that SOTA LLMs output correct responses 35\u201362% of the time with insufficient\ncontext. Hence, intervention strategies to increase accuracy should not solely rely on sufficiency.\n\n\n3. Building on our findings above, we develop an efficient and general method for selective generation,\nusing both confidence and sufficient context signals. Our method improves the fraction of correct\nanswers (among total model responses) by up to 2\u201310% for Gemini, GPT, and Gemma.\n\n\n2 RELATED WORK\n\n\nMany papers have shown that reaping the benefits of RAG (e.g., better factuality) will require a deeper\nunderstanding of how LLMs respond to variations in the queries and provided context (Asai et al.,\n2024; Fan et al., 2024; Ram et al., 2023; Rau et al., 2024). We review two main areas. First, much\nwork has evaluated RAG systems with poor retrieval, uncovering cases where LLMs are led astray by\nirrelevant context. Another line of study aims to reduce LLM hallucinations in RAG settings.\n\n\n**(Ir)relevant Context.** Prior studies uncover a lack of robustness to imperfect retrieval. However,\n","Published as a conference paper at ICLR 2025\n\n## SUFFICIENT CONTEXT: A NEW LENS ON RETRIEVAL AUGMENTED GENERATION SYSTEMS\n\n\n\n**Chun-Sung Ferng**\nGoogle\ncsferng@google.com\n\n\n**Cyrus Rashtchian**\nGoogle\ncyroid@google.com\n\n\n\n**Hailey Joren** _[\u2217]_\nUC San Diego\nhjoren@ucsd.edu\n\n\n**Da-Cheng Juan**\nGoogle\ndacheng@google.com\n\n\n\n**Jianyi Zhang** _[\u2020]_\nDuke University\njianyi.zhang@duke.edu\n\n\n**Ankur Taly**\nGoogle\nataly@google.com\n\n\n\nABSTRACT\n\n\nAugmenting LLMs with context leads to improved performance across many\napplications. Despite much research on Retrieval Augmented Generation (RAG)\nsystems, an open question is whether errors arise because LLMs fail to utilize the\ncontext from retrieval or the context itself is insufficient to answer the query. To\nshed light on this, we develop a new notion of sufficient context, along with a\nmethod to classify instances that have enough information to answer the query. We\nthen use sufficient context to analyze several models and datasets. By stratifying\nerrors based on context sufficiency, we find that larger models with higher baseline\nperformance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when\nthe context is sufficient, but often output incorrect answers instead of abstaining\nwhen the context is not. On the other hand, smaller models with lower baseline\nperformance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient\ncontext. We further categorize cases when the context is useful, and improves\naccuracy, even though it does not fully answer the query and the model errs without\nthe context. Building on our findings, we explore ways to reduce hallucinations in\nRAG systems, including a new selective generation method that leverages sufficient\ncontext information for guided abstention. Our method improves the fraction of\ncorrect answers among times where the model responds by 2\u201310% for Gemini,\nGPT, and Gemma. Key findings and the prompts used in our autorater analysis are\n[available on our github.](https:\/\/github.com\/hljoren\/sufficientcontext)\n\n\n1 INTRODUCTION\n\n\nProvid"," et al., 2024;\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\nrelevant documents, but simply uses \u201cdoes not contain the answer\u201d as a proxy for irrelevant context.\n\n\n**Reducing Hallucinations.** There have also been efforts to improve RAG factuality on open-book QA\ntasks (Asai et al., 2023; Mineiro, 2024; Simhi et al., 2024; Wang et al., 2024b; Zhang et al., 2024b).\nThe main theme is to improve both the generation and retrieval quality, often by fine-tuning one or\nmore components. Also, since RAG leads to very long contexts, another issue that arises is the \u201clost\nin the middle\u201d problem (Hsieh et al., 2024; Liu et al., 2024; Yu et al., 2024). These works start with\nthe premise that the provided query\/context should be precisely answerable by the LLM, and hence,\n\n- nly analyze their findings in the sufficient context scenario. Independent of RAG, many papers\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang\net al., 2024; Kadavath et al., 2022; Yin et al., 2023; Zhang et al., 2024a) and performance across\ndisaggregated subsets of data (Paes et al., 2022; Joren et al., 2023).\n\n\n3 SUFFICIENT CONTEXT\n\n\nAt a high level, our aim is to classify input instances based on whether the context contains enough\ninformation to answer the query. We split possible contexts into two cases: (1) **Sufficient Context.**\nThe context is sufficient to answer the query if it contains all the necessary information to provide a\ndefinitive answer. (2) **Insufficient Context.** Otherwise, a context is insufficient. A context may also\nbe insufficient if the query requires specialized knowledge that is not provided in the context or if\nthe information in the context is incomplete, inconclusive, or contradictory. In this section, we more\nthoroughly discuss sufficient context. Then, we show how to accurately and scalably label instances.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\n3.1 DEFIN"],"23":["## **Query Rewriting for Retrieval-Augmented Large Language Models**\n\n**Xinbei Ma** [1,2,] _[\u2217]_ **, Yeyun Gong** [3, #, \u2020] **, Pengcheng He** [4, #] **, Hai Zhao** [1,2,\u2020] **, Nan Duan** [3]\n\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\n3Microsoft Research Asia 4Microsoft Azure AI\n\nsjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,\n{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com\n\n\n\n**Abstract**\n\n\nLarge Language Models (LLMs) play powerful, black-box readers in the _retrieve-then-_\n_read_ pipeline, making remarkable progress\nin knowledge-intensive tasks. This work introduces a new framework, _Rewrite-Retrieve-_\n_Read_ instead of the previous _retrieve-then-read_\nfor the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior\nstudies focusing on adapting either the retriever\n\n  - r the reader, our approach pays attention to\nthe adaptation of the search query itself, for\nthere is inevitably a gap between the input text\nand the needed knowledge in retrieval. We\nfirst prompt an LLM to generate the query,\nthen use a web search engine to retrieve contexts. Furthermore, to better align the query\nto the frozen modules, we propose a trainable\nscheme for our pipeline. A small language\nmodel is adopted as a trainable rewriter to cater\nto the black-box LLM reader. The rewriter is\n\ntrained using the feedback of the LLM reader\nby reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA\nand multiple-choice QA. Experiments results\nshow consistent performance improvement, indicating that our framework is proven effective\nand scalable, and brings a new framework for\nretrieval-augmented LLM [1] .\n\n\n**1** **Introduction**\n\n\nLarge Language Models (LLMs) have shown remarkable abilities for human language processing\nand extraordinary scalability and adaptability in\nfew- or zero-shot settings.(Ouyang et al., 2022;\nBrown et al., 2020; Chowdh"," GenRead (Yu et al., 2023)\nprompts an LLM to generate context instead of\ndeploying a retriever, showing that LLMs can retrieve internal knowledge by prompting. ReAct\n\n\n\n(Yao et al., 2023) and Self-Ask (Press et al., 2022)\ncombines the Chain-of-Thought (CoT) (Wei et al.,\n2022; Wang et al., 2022) and inter-actions with web\nAPIs. Only relying on prompt construction, ReAct provides novel baselines for interactive tasks.\nDemonstrate\u2013Search\u2013Predict (DSP) (Khattab et al.,\n2022) defines a sophisticated pipeline between an\nLLM and a retriever. Unlike ReAct, DSP integrates\nprompts for demonstration bootstrap besides multihop breakdown and retrieval.\nDespite the promising performance in the zero or\nfew-shot setting, the behavior of LLMs sometimes\nneeds adjustments. A feasible approach is to append trainable small models in front of or after the\nLLM. The small models, as a part of the parameters\n\n - f the system, can be fine-tuned for optimization.\nRePlug (Shi et al., 2023) is proposed to fine-tune a\ndense retriever for the frozen LLM in the _retrieve-_\n\n_then-read_ pipeline. The retriever is trained under\nthe LLM\u2019s supervision to retrieve documents that\nare suitable for the LLM. With the same purpose,\nDirectional Stimulus Prompting (Li et al., 2023)\ndeploys a small model to provide the LLM with\nstimulus (e.g., keywords for summarization, or dialogue actions for response generation), which is\nupdated according to the LLM reward.\nDifferent from the inspiring work mentioned\nabove, our proposed pipeline contains a query\nrewriting step in front of the _retrieve-then-read_\nmodule. We further propose a trainable scheme\nwith a small rewriting model, which is a novel\nenhancement for retrieval-augmented LLM by re\n\nconstructing the search query.\n\n\n**3** **Methodology**\n\n\nWe present _Rewrite-Retrieve-Read_, a pipeline that\nimproves the retrieval-augmented LLM from the\nperspective of query rewriting. Figure 1 shows an\n\n- verview. This section first introduces the pipeline\nframework in section 3.1, then the trainable scheme\n\n","., legal or scientific documents,\nconversation histories, etc.) or when language models are augmented with external information (e.g.,\n\n\n*Work partially completed as an intern at Samaya AI.\n\n\n\n\n|Total|Col2|Ret|rievedDocuments( ~4Ktoken|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\n\n\nFigure 1: Changing the location of relevant information\n(in this case, the position of the passage that answers an\ninput question) within the language model\u2019s input context results in a U-shaped performance curve\u2014models\nare better at using relevant information that occurs at the\nvery beginning (primacy bias) or end of its input context\n(recency bias), and performance degrades significantly\nwhen models must access and use information located\n\nin the middle of its input context.\n\n\nrelevant documents from a search engine, database\nquery results, etc; Petroni et al., 2020; Ram et al.,\n2023; Shi et al., 2023; Mallen et al., 2023; Schick\net al., 2023, _inter alia_ ).\n\n\nHandling these use-cases requires language models to successfully operate over long sequences. Existing language models are generally implemented\nwith Transformers (Vaswani et al., 2017), which require memory and compute that increases quadratically in sequence length. As a result, Transformer language models were often trained with\nrelatively small context windows (between 5122048 tokens). Recent improvements in hardware\n(e.g., faster GPUs with more memory) and algorithms (Dai et al., 2019; Dao et al., 2022; Poli et al.,\n\n\n2023; Rubin and Berant, 2023, _inter alia_ ) have\nresulted in language models with larger context\nwindows (e.g., 4096, 32K, and even 100K tokens),\nbut it remains unclear how these extended-context\n\nlanguage models make use of their input contexts\nwhen performing downstream tasks.\n\n\nWe empirically investigate this question via\ncontrolled experiments with a variety of state-ofthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\n","## **Benchmarking Large Language Models in Retrieval-Augmented Generation**\n\n**Jiawei Chen** [1,3] **, Hongyu Lin** [1,*] **, Xianpei Han** [1,2,*] **, Le Sun** [1,2]\n\n1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n_{_ jiawei2020,hongyu,xianpei,sunle _}_ @iscas.ac.cn\n\n\n\n**Abstract**\n\n\nRetrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language\nmodels (LLMs). However, existing research lacks rigorous\nevaluation of the impact of retrieval-augmented generation\n\n  - n different large language models, which make it challenging to identify the potential bottlenecks in the capabilities\n\n  - f RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance\n\n  - f different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case.\nThen we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\n\n  - f noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n\n\n**Introduction**\n\n\nRecently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao"," the middle of the input context. We hypothesize that encoder-decoder models\nmay make better use of their context windows because their bidirectional encoder allows processing\neach document in the context of future documents,\npotentially improving relative importance estimation between documents.\n\n\n**4.2** **Effect of Query-Aware Contextualization**\n\n\nOur multi-document QA and key-value retrieval\nexperiments place the query (i.e., question to answer or key to retrieve) after the data to process\n(i.e., the documents or the key-value pairs). As a\nresult, decoder-only models cannot attend to query\ntokens when contextualizing documents or keyvalue pairs, since the query only appears at the end\n\n\n|reve o|cumens (~|\n|---|---|\n|||\n|||\n|||\n|||\n|||\n\n\n|a ereve o|Col2|cumen|s (~ o e|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n||||||\n\n\n\nFigure 8: When encoder-decoder models (Flan-UL2 and Flan-T5-XXL) evaluated on sequences that are _shorter_\nthan their encoder\u2019s training-time maximum sequence length (2048 and 512 tokens, respectively), they are relatively\nrobust to changes in the position of relevant information within their input context (left subplot). In contrast, when\nthese models are evaluated on sequences _longer_ than those seen during training (center and right subplots), we\n\n- bserve a U-shaped performance curve\u2014performance is higher when relevant information occurs at the beginning\n\n- r end of the input context, as opposed to the middle of the input context.\n\n\n\n\n|20 Total Retrieved Documen|Col2|\n|---|---|\n|20 Total Retrieved Docume<br>|20 Total Retrieved Docume<br>|\n|tokens, que|ry~~-~~aware context|\n|||\n|||\n|||\n|||\n\n\n\n\n\n\n\nFigure 9: Query-aware contextualization (placing the\nquery before _and_ after the documents) does not substantially improve robustness of language models to\nchanging the position of relevant information in multidocument QA; performance slightly increases when\nrelevant information occurs at the very beginning, but\n\n- therwise slightly decreases.\n\n\n- f the prompt and decoder-only models can only\nattend to prior tokens at each timestep. In contrast,\nencoder-decoder models (which seem more robust\nto changes in the"],"24":["## **Retrieval-Augmented Generation for** **Knowledge-Intensive NLP Tasks**\n\n**Patrick Lewis** _[\u2020\u2021]_ **, Ethan Perez** _[\u22c6]_ **,**\n\n\n**Aleksandra Piktus** _[\u2020]_ **, Fabio Petroni** _[\u2020]_ **, Vladimir Karpukhin** _[\u2020]_ **, Naman Goyal** _[\u2020]_ **, Heinrich K\u00fcttler** _[\u2020]_ **,**\n\n\n**Mike Lewis** _[\u2020]_ **, Wen-tau Yih** _[\u2020]_ **, Tim Rockt\u00e4schel** _[\u2020\u2021]_ **, Sebastian Riedel** _[\u2020\u2021]_ **, Douwe Kiela** _[\u2020]_\n\n\n_\u2020_ Facebook AI Research; _\u2021_ University College London; _\u22c6_ New York University;\n\n```\n                plewis@fb.com\n\n```\n\n**Abstract**\n\n\nLarge pre-trained language models have been shown to store factual knowledge\nin their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance\nlags behind task-specific architectures. Additionally, providing provenance for their\ndecisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric\nmemory have so far been only investigated for extractive downstream tasks. We\nexplore a general-purpose fine-tuning recipe for retrieval-augmented generation\n(RAG) \u2014 models which combine pre-trained parametric and non-parametric mem\n    - ry for language generation. We introduce RAG models where the parametric\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages\nacross the whole generated sequence, and another which can use different passages\nper token. We fine-tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks,\n\n     - utperforming parametric seq2seq models and task-specific retrieve-and-extract\narch","x_, we use\nMaximum Inner Product Search (MIPS) to find the top-K documents _zi_ . For final prediction _y_, we\ntreat _z_ as a latent variable and marginalize over seq2seq predictions given different documents.\n\n\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\nand non-parametric memory to the \u201cworkhorse of NLP,\u201d i.e. sequence-to-sequence (seq2seq) models.\n\n\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\na general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG).\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\ncan be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\n\n\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\nmemory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stackaugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\npresent without additional training.\n\n\nOur results highlight the benefits of combining parametric and non-parametric memory with generation for _knowledge-intensive tasks_ - tasks that humans could not reasonably be expected to perform\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\n\n- n open Natural Questions [",". These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\ncould represent promising future work.\n\n\n**6** **Discussion**\n\n\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\nfound that people prefer RAG\u2019s generation over purely parametric BART, finding RAG more factual\nand specific. We conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\nanother objective. Our work opens up new research directions on how parametric and non-parametric\nmemories interact and how to most effectively combine them, showing promise in being applied to a\nwide variety of NLP tasks.\n\n\n9\n\n\n**Broader Impact**\n\n\nThis work offers several positive societal benefits over previous work: the fact that it is more\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less\nwith generations that are more factual, and offers more control and interpretability. RAG could be\nemployed in a wide variety of scenarios with direct benefit to society, for example by endowing it\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\neffective at their jobs.\n\n\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or","\n\n\nNatural Questions 79169 8758 3611\nTriviaQA 78786 8838 11314\nWebQuestions 3418 362 2033\nCuratedTrec 635 134 635\nJeopardy Question Generation 97392 13714 26849\nMS-MARCO 153726 12468 101093*\nFEVER-3-way 145450 10000 10000\nFEVER-2-way 96966 6666 6666\n\n\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric\/nonparametric models require far fewer trainable parameters for strong open-domain QA performance.\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit floating\npoint precision to manage memory and disk footprints.\n\n\n**H** **Retrieval Collapse**\n\n\nIn preliminary experiments, we observed that for some tasks such as story generation [11], the\nretrieval component would \u201ccollapse\u201d and learn to retrieve the same documents regardless of the\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result\nin less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\n\n\n**I** **Number of instances per dataset**\n\n\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\n\n\n19\n\n\n","une and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks,\n\n     - utperforming parametric seq2seq models and task-specific retrieve-and-extract\narchitectures. For language generation tasks, we find that RAG models generate\nmore specific, diverse and factual language than a state-of-the-art parametric-only\nseq2seq baseline.\n\n\n**1** **Introduction**\n\n\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\u2019t straightforwardly provide insight into\ntheir predictions, and may produce \u201challucinations\u201d [38]. Hybrid models that combine parametric\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\ninspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\n\n\n|Col1|Col2|Col3|z4<br>z3<br>z2|Col5|Col6|\n|---|---|---|---|---|---|\n||||**z3**<br>**z2**|**z3**<br>**z2**|**z3**<br>**z2**|\n||||**z2**|**z2**|**z2**|\n|||**z**|**z**|||\n|||**1**|**1**|||\n|||||||\n\n\n\nFigure 1: Overview of our approach. We combine a pre-trained retriever ( _Query Encoder_ + _Document_\n_Index_ ) with a pre-trained seq2seq model ( _Generator_ ) and fine-tune end-to-end. For query _x_, we use\nMaximum Inner Product Search (MIPS) to find the top-K documents _zi_ . For final prediction _y_, we\ntreat _z_ as a latent variable and marginalize over seq2seq predictions given different"],"25":["## **A Survey on Retrieval-Augmented Text Generation**\n\n**Huayang Li** _[\u2665][,][\u2217]_ **Yixuan Su** _[\u2660][,][\u2217]_ **Deng Cai** _[\u2666][,][\u2217]_ **Yan Wang** _[\u2663][,][\u2217]_ **Lemao Liu** _[\u2663][,][\u2217]_\n\n_\u2665_ Nara Institute of Science and Technology _\u2660_ University of Cambridge\n_\u2666_ The Chinese University of Hong Kong _\u2663_ Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\n\n\n\n**Abstract**\n\n\nRecently, retrieval-augmented text generation\nattracted increasing attention of the computational linguistics community. Compared\nwith conventional generation models, retrievalaugmented text generation has remarkable advantages and particularly has achieved state-ofthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrievalaugmented text generation. It firstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks including\ndialogue response generation, machine translation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval technology, has achieved state-of-the-art (SOTA) performance in many NLP tasks and attracted the attention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counterpart, this new paradigm has some remarkable advantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation"," from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (Weston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n\n\n_\u2217_ All authors contributed equally.\n\n\n\nfirstly present the generic paradigm of retrievalaugmented generation as well as three key components under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are organized with respect to different tasks. Specifically,\n\n- n the dialogue response generation task, exemplar\/template retrieval as an intermediate step has\nbeen shown beneficial to informative response generation (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation exploring different forms of knowledge such as knowledge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summarize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we intensively highlight several popular methods to integrating translation memory to NMT models (Gu\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\nHe et al., 2021). We also review the applications\n\n- f retrieval-augmented generation in other generation tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b)."," for example,\ndata compression for the retrieval memory.\n\n\n**Local vs. Global Optimization** Theoretically, it\nseems promising to jointly learn retrieval metrics\nand generation models. However, in practice, there\nis an essential gap about the retrieval metric between the training and inference phrases. In the\ntraining phase, the loss is locally back-propagated\nto only a few retrieved examples while in the inference phase the metric is globally conducted among\nall examples in the memory. It would be interesting\nto narrow such a gap when learning a better metric\nfor generation tasks.\n\n\n**Multi-Modalities** With recent advancement in\n\nimage-text retrieval, directly associating images\nwith relevant text becomes possible. This urges\nresearchers to investigate the possibility of retrievalbased text generation in tasks that involve data from\ndifferent modalities. One typical task is image\ncaptioning. Beyond images, other tasks like speechto-text transcription could potentially benefit from\nretrieval-based generation methods as well.\n\n\n**Diverse & Controllable Retrieval** Most of the\n\nexisting approaches adopt a universal metric for\nretrieval, such as lexical similarities of sentences.\nFuture work should explore how to use customized\nmetrics for retrieval. This can be beneficial for\nmore controlled text generation. For example, instances with emotions and styles may be more desirable in the personalized dialogue generation, parallel data that contains specific terminologies is\nmore helpful in machine translation, and so on. On\nthe other hand, using a universal metric for retrieval\nmay lead to the lack of diversity of the retrieval results. Collecting a diverse set of retrieval results\ncan improve the coverage of useful information.\nThus, considering multiple different metrics for retrieval may lead to generation with higher quality\nin the future.\n\n\n**7** **Conclusion**\n\n\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different components of retrieval-augmented text generation including retrieval metrics, retrieval sources, and integration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other generation tasks. We also pointed out some future directions for retrieval-augmented text generation.\n\n\n**References**\n\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly\nlearning to align and translate. _"," question-answer pairs while referencing the documents in a simulated\nimperfect retrieval setting \u2014 thereby effectively preparing for the open-book exam setting.\n\n\nanswering questions. However, RAG based in-context learning methods fail to leverage\nthe learning opportunity afforded by the fixed domain setting and early access to the test\ndocuments. Alternatively, supervised fine-tuning offers the opportunity to learn more\ngeneral patterns in the documents and better align to end tasks and user preferences Zhou\net al. (2023). However, existing fine-tuning based approaches either fail to leverage the\ndocuments at test time (don\u2019t incorporate RAG) or fail to account for the imperfections in\nretrieval process during training.\n\n\nWe can draw an analogy to an open-book exam. Existing in-context retrieval methods are\nequivalent to taking an open-book exam without studying. Alternatively, existing finetuning based approaches implement \u201cstudying\" by either directly \u201cmemorizing\" Xiong\net al. (2023) the input documents or answering practice questions Wang et al. (2022) without\nreferencing the documents. While these approaches leverage in-domain learning they fail to\nprepare for the open-book nature of the test setting.\n\n\nIn this paper, we study how to combine instruction fine-tuning (IFT) with retrieval augmented generation (RAG). We propose a novel adaptation strategy \u2013 Retrieval-Augmented\nFine Tuning (RAFT). RAFT specifically addresses the challenge of fine-tuning LLMs to both\nincorporate domain knowledge while also improving in-domain RAG performance. RAFT\naims to not only enable models to learn domain-specific knowledge through fine-tuning,\nbut also to ensure robustness against distracting retrieved information. This is achieved\nby training the models to understand the dynamics between the question (prompt), the\ndomain-specific documents retrieved, and the right answer. Going back to our analogy to\nthe open book exam, our approach is analogous to studying for an open-book exam by\nrecognizing relevant, and irrelevant retrieved documents.\n\n\nIn RAFT, we train the model to answer the question (Q) from Document(s) (D*) to generate\nanswer (A*), where A* includes chain-of-thought reasoning Wei et al. (2022); Anthropic\n(2023), and in the presence of distractor documents ( _Dk_ ). We explain the methodology in\nSection 3 and analyze the sensitivity to the number of distractor documents (",", Large Language Models LLMs have achieved\nsignificant advances in a wide range of general knowledge reasoning tasks Brown et al.\n(2020); Wei et al. (2022). However, increasingly LLMs are being employed in specialized\ndomains to support tasks ranging from code completion for specific software frameworks\nto question answering on specific document collections (e.g., legal or medical documents).\nIn these settings, general knowledge reasoning is less critical and instead the primary goal\nis to maximize accuracy based on a given set of documents. Indeed, adapting LLMs to the\nspecialized domains (e.g., recent news, enterprise private documents, or program resources\nconstructed after the training cutoff) is essential to many emerging applications (Vu et al.,\n2023; Lazaridou et al., 2022) and is the focus of this work.\n\n\nThis paper studies the following question \u2013 _How do we adapt pre-trained LLMs for Retrieval_\n_Augmented Generation (RAG) in specialized domains?_\n\n\nWhen it comes to adapting LLMs to specialized domains, we consider the following two\ncandidates: in-context learning through Retrieval-Augmented Generation (RAG) and supervised fine-tuning. RAG based methods allow the LLM to reference the documents when\n\n\n_\u2217_ Corresponding author, personal website: `tianjunz.github.io`\n\n\n1\n\n\nPreprint, Under Review\n\n\n_Bake in Knowledge_\n\n\n_at Train Time_\n\n\nquery answer\n\n\n\u201cClosed book\u201d\n\n\n\n_Model can use_\n\n_External Docs at Test_\n\n\nquery answer\n\n\n\u201cOpen book\u201d\n\n\n\n_**Teach**_ _Model to_\n\n_use External Docs at Test_\n\n\nquery answer\n\n\nRAFT (Proposed)\n\n\n\nFigure 1: **How best to prepare for an Exam?** (a) Fine-tuning based approaches implement\n\"studying\" by either directly \"memorizing\" the input documents or answering practice\nQA without referencing the documents. (b) Alternatively, in-context retrieval methods fail\nto leverage the learning opportunity afforded by the fixed domain and are equivalent to\ntaking an open-book exam without studying. In contrast, our approach (c) RAFT leverages\nfine-tuning with question-answer pairs while referencing the documents in a simulated\nimperfect retrieval setting \u2014 thereby effectively preparing for the open-book exam setting.\n\n\nanswering questions. However, RAG based in-context learning methods fail to leverage\nthe learning opportunity afforded by the"],"26":["Published as a conference paper at ICLR 2025\n\n## - - VISRAG: VISION BASED RETRIEVAL AUGMENTED - GENERATION ON MULTI MODALITY DOCUMENTS\n\n\n**Shi Yu** [1] _[\u2217]_ **, Chaoyue Tang** [2] _[\u2217]_ **, Bokai Xu** [2] _[\u2217]_ **, Junbo Cui** [2] _[\u2217]_ **, Junhao Ran** [3] **, Yukun Yan** [1] _[\u2020]_ **,**\n**Zhenghao Liu** [4] **, Shuo Wang** [1] **, Xu Han** [1] **, Zhiyuan Liu** [1] _[\u2020]_ **, Maosong Sun** [1]\n\n1Department of Computer Science and Technology, Tsinghua University\n2ModelBest Inc. 3Rice University 4Northeastern University\nyus21@mails.tsinghua.edu.cn\n\n\nABSTRACT\n\n\nRetrieval-augmented generation (RAG) is an effective technique that enables large\nlanguage models (LLMs) to utilize external knowledge sources for generation.\nHowever, current RAG systems are solely based on text, rendering it impossible\nto utilize vision information like layout and images that play crucial roles in realworld multi-modality documents. In this paper, we introduce VisRAG, which\ntackles this issue by establishing a vision-language model (VLM)-based RAG\npipeline. In this pipeline, instead of first parsing the document to obtain text,\nthe document is directly embedded using a VLM as an image and then retrieved\nto enhance the generation of a VLM. Compared to traditional text-based RAG,\nVisRAG maximizes the retention and utilization of the data information in the\n\n     - riginal documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in\nVisRAG and explore a variety of generation methods. Experiments demonstrate\nthat VisRAG outperforms traditional RAG in both the retrieval and generation\nstages, achieving a 20\u201340% end-to-end performance gain over traditional textbased RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing\ntraining data and demonstrates strong generalization capability, positioning"," pri\n- ritize textual information over visual content. For each group, we calculate and plot the average\nperformance differences between VisRAG and TextRAG, as well as between VisRAG (SigLIP) and\nTextRAG, to compare how each model performs relative to TextRAG. We observe that, in general,\nthe relative performance of VisRAG and VisRAG (SigLIP) improves as the length of the relevant\ndocument decreases. This suggests that models with vision encoders can better understand documents that emphasize visual information. However, VisRAG (SigLIP) consistently underperforms\nVisRAG across all data subsets and, in some cases, even performs worse than TextRAG. In contrast,\nVisRAG outperforms TextRAG on most subsets, indicating that the underlying language model in\nVisRAG is crucial for better understanding the semantics conveyed through visual cues.\n\n\n6 CONCLUSION\n\n\nIn this paper, we propose VisRAG, a novel retrieval-augmented generation (RAG) paradigm that utilizes vision-language models (VLMs) to facilitate retrieval and generation within an RAG pipeline,\nthereby eliminating the parsing stage required in traditional text-based RAG. Our empirical results demonstrate that VisRAG consistently outperforms text-based RAG on retrieval and generation\nwhile maintaining a simpler pipeline. We hope that VisRAG will inspire future RAG development\nto incorporate VLMs for handling multi-modal documents.\n\n\n10\n\n\nPublished as a conference paper at ICLR 2025\n\n\nACKNOWLEDGMENTS\n\n\nThis work is supported by the Institute Guo Qiang at Tsinghua University. It is also partially supported by the National Natural Science Foundation of China under Grant No. 62206042.\n\n\nREFERENCES\n\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\nreport. _arXiv preprint arXiv:2303.08774_, 2023.\n\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Fl",", which can negatively impact the retrieval and generation phases. More\n- ver, TextRAG utilizes only textual information, overlooking potential information present in other\nmodalities like images. Although research has been conducted on image retrieval and multi-modal\n\n\n_\u2217_ Equal contribution.\n\n_\u2020_ Corresponding authors.\n\n\n1\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\nRAG, these approaches primarily focus on predefined scenarios wherein images and descriptive\ntexts are properly extracted and paired (Wei et al., 2023; Sharifymoghaddam et al., 2024; Zhou\net al., 2024), differing from real-world scenarios where texts and images (including figures) are\n\n- ften interleaved within a single document page.\n\n\nThe recent development of vision-language models (VLMs) has introduced a promising approach\nto understanding complex visual cues in images and documents (OpenBMB, 2024b; Wang et al.,\n2024). By integrating a language model with a vision encoder, VLMs demonstrate superior abilities in applications such as describing pictures (Alayrac et al., 2022), explaining figures (Bavishi\net al., 2023), and transcribing (printed and handwritten) text from document images (Laurenc\u00b8on\net al., 2024). Given the robust capabilities of VLMs in capturing multi-modal information present in\nimages, an intriguing question arises: can the basic language model in the retrieval and generation\ncomponents of TextRAG be substituted with a VLM, thus the parsing stage is bypassed and all the\ninformation of the document is preserved?\n\n\nIn this paper, we present **Vis** ion-based **R** etrieval- **a** ugmented **G** eneration (VisRAG), to study the feasibility of building a pure-vision RAG pipeline using VLMs. VisRAG is built with a VLM-based\nretriever VisRAG-Ret and generator VisRAG-Gen. Inherited the bi-encoder of text-based dense retriever (Karpukhin et al., 2020), VisRAG-Ret maps the query and the document into an embedding\nspace, but utilizing the document\u2019s image directly instead of relying on extracted textual content.\nThe embedding is obtained by applying weighted mean pooling on the final hidden states of the input text or vision tokens. After retrieving top- _k_","fect. Further analysis reveals that VisRAG possesses\nbetter training data efficiency and generalization ability than baseline models, and demonstrates robustness across both text-centric and vision-centric documents. VisRAG shows great promise in\nreplacing TextRAG as the next-generation standard for RAG pipelines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: TextRAG vs. VisRAG on final generation accuracy. In TextRAG, parsed text\nserves as the basis for both retrieval and generation processes. In contrast, VisRAG leverages the original document image directly by\nusing a VLM-based retriever and generator.\nDetails can be found in Sec. 5.1.\n\n\n\n2 RELATED WORK\n\n\n**Retrieval-augmented Generation (RAG).** RAG enhances large language models (LLMs) by\nincorporating retrieved information from external knowledge bases, which assists in addressing\nknowledge-intensive tasks (Guu et al., 2020), reducing hallucinations (Semnani et al., 2023), and\n\n\n2\n\n\nPublished as a conference paper at ICLR 2025\n\n\nacquiring new knowledge (Vu et al., 2023). An RAG pipeline typically comprises a text-based\nretriever that fetches relevant information from the knowledge base given the user query, and an\nLLM-based generator that reads the query along with the retrieved information to generate an answer (Shi et al., 2024b; Yu et al., 2023). Prior research on RAG primarily focuses on: a) improving\nthe retriever, which is typically a text encoder producing text embeddings, through generator feedback (Yu et al., 2023; Shi et al., 2024b); b) enhancing the generator via supervised fine-tuning (Lin\net al., 2024; Xu et al., 2024a), in-context pre-training (Shi et al., 2024a), or advanced prompting (Xu\net al., 2024c); and c) developing advanced RAG pipelines to handle long-form or multi-hop question answering (Jiang et al., 2023; Asai et al., 2024). However, research on RAG has predominantly\ntargeted cleaned text corpora like Wikipedia from an academic standpoint. Building effective RAG\npipelines for real-world, multi-modal documents remains a challenge.\n\n\n**Vision-language Models.** Recent advancements in vision-language models (VLMs"," the retrieval and generation\nstages, achieving a 20\u201340% end-to-end performance gain over traditional textbased RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing\ntraining data and demonstrates strong generalization capability, positioning it as a\npromising solution for RAG on multi-modality documents. Our code and data are\n[available at https:\/\/github.com\/openbmb\/visrag.](https:\/\/github.com\/openbmb\/visrag)\n\n\n1 INTRODUCTION\n\n\nTrained on massive data, large language models (LLMs) have shown strong abilities in common\nNLP tasks using their parametric knowledge (Wei et al., 2022; Zhao et al., 2023; Achiam et al.,\n2023). However, the issue of hallucination (Ji et al., 2023; Bang et al., 2023) and the challenge of\nupdating the parametric knowledge limit their real-world application in specific domains. Retrievalaugmented generation (RAG) alleviates this problem by supplying the LLM with information retrieved from a custom outer knowledge base (Guu et al., 2020; Lewis et al., 2020; Yu et al., 2023).\nOpen-source RAG frameworks like llamaindex (Liu, 2022) have been developed to facilitate the\nresearch and deployment of RAG.\n\n\nTypical retrieval-augmented generation (RAG) pipelines are _text-based_, operating on segmented\ntexts as retrieval units (Yu et al., 2023; Asai et al., 2024; Yan et al., 2024), which we refer to as\nTextRAG. In real-world scenarios, knowledge is often presented in multi-modality documents such\nas textbooks and manuals, which may have texts and figures intersected together. To acquire texts\nfrom such data sources, a _parsing_ stage is required, which typically involves a cascade of processes,\nincluding layout recognition, optical character recognition (OCR), and post-processing steps like\ntext joining (Zhang et al., 2024; Liu, 2022). While effective in most scenarios, the parsing process\ninevitably introduces errors, which can negatively impact the retrieval and generation phases. More\n- ver, TextRAG utilizes only textual information, overlooking potential information present in other\nmodalities like images. Although research has been conducted on image retrieval and multi-modal\n\n\n_\ufffd"],"27":["## **SPLADE: Sparse Lexical and Expansion Model** **for First Stage Ranking**\n\n\n\nThibault Formal\n\nNaver Labs Europe\nMeylan, France\nSorbonne Universit\u00e9, LIP6\n\nParis, France\nthibault.formal@naverlabs.com\n\n\n**ABSTRACT**\n\n\n\nBenjamin Piwowarski\nSorbonne Universit\u00e9, CNRS, LIP6\n\nParis, France\nbenjamin.piwowarski@lip6.fr\n\n\n\nSt\u00e9phane Clinchant\nNaver Labs Europe\nMeylan, France\nstephane.clinchant@naverlabs.com\n\n\n\nIn neural Information Retrieval, ongoing research is directed towards improving the first retriever in ranking pipelines. Learning\ndense embeddings to conduct retrieval using efficient approximate\nnearest neighbors methods has proven to work well. Meanwhile,\nthere has been a growing interest in learning _sparse_ representations\nfor documents and queries, that could inherit from the desirable\nproperties of bag-of-words models such as the exact matching of\nterms and the efficiency of inverted indexes. In this work, we present\na new first-stage ranker based on explicit sparsity regularization\nand a log-saturation effect on term weights, leading to highly sparse\nrepresentations and competitive results with respect to state-ofthe-art dense and sparse methods. Our approach is simple, trained\nend-to-end in a single stage. We also explore the trade-off between\neffectiveness and efficiency, by controlling the contribution of the\nsparsity regularization.\n\n\n**KEYWORDS**\n\n\nneural networks, indexing, sparse representations, regularization\n\n\n**ACM Reference Format:**\n\nThibault Formal, Benjamin Piwowarski, and St\u00e9phane Clinchant. 2021.\nSPLADE: Sparse Lexical and Expansion Model for First Stage Ranking.\nIn _Proceedings of ACM Conference (Conference\u201917)._ ACM, New York, NY,\n[USA, 5 pages. https:\/\/doi.org\/10.1145\/nnnnnnn.nnnnnnn](https:\/\/doi.org\/10.1145\/nnnnnnn.nnnnnnn)\n\n\n**1** **INTRODUCTION**\n\n\nThe release of large pre-trained language models like BERT [7]\nhas shaken-up Natural Language Processing and Information Retrieval. These models have shown a strong ability to adapt to various\ntasks by simple fine-tuning. At the beginning of 2019, _"," with approximate nearest neighbors search\nhas shown impressive results [8, 15, 26], but is still combined with\nBOW models because of its inability to explicitly model term matching. Hence, there has recently been a growing interest in learning\n_sparse representations_ for queries and documents [1, 4, 19, 28, 29].\nBy doing so, models can inherit from the desirable properties of\nBOW models like exact-match of (possibly latent) terms, efficiency\n\n- f inverted indexes and interpretability. Additionally, by modeling\nimplicit or explicit (latent, contextualized) _expansion_ mechanisms \u2013\nsimilarly to standard expansion models in IR \u2013 these models can\nreduce the vocabulary mismatch.\nThe contributions of this paper are threefold: (1) we build upon\nSparTerm [1], and show that a mild tuning of hyperparameters\nbrings improvements that largely outperform the results reported\nin the original paper; (2) we propose the SParse Lexical AnD Expansion (SPLADE) model, based on a logarithmic activation and\nsparse regularization. SPLADE performs an efficient document expansion [1, 16], with competitive results with respect to complex\ntraining pipelines for dense models like ANCE [26]; (3) finally, we\nshow how the sparsity regularization can be controlled to influence the trade-off between efficiency (in terms of the number of\nfloating-point operations) and effectiveness.\n\n\n**2** **RELATED WORKS**\n\n\nDense retrieval based on BERT Siamese models [22] has become\nthe standard approach for candidate generation in Question Answering and IR [8, 10, 12, 15, 25]. While the backbone of these models remains the same, recent works highlight the critical aspects\n\n- f the training strategy to obtain state-of-the-art results, ranging\nfrom improved negative sampling [8, 25] to distillation [11, 15].\nColBERT [13] pushes things further: the postponed token-level\ninteractions allow to efficiently apply the model for first-stage retrieval, benefiting of the effectiveness of modeling fine-grained\ninteractions, at the cost of storing embeddings for each (sub)term\n\n- raising concerns about the actual scalability of the approach for\nlarge collections. To the best of our knowledge, very few studies\nhave discussed the impact of using _approximate_ nearest neighbors\n\n\n(ANN) search on IR metrics [","_the bone alignment issue than you may be able to correct through bow legs_\n\n_correction exercises._\n\n\n**expansion terms**\n\n\n(leg, 1.62) (arrow, 0.7) (exercise, 0.64) (bones, 0.63) (problem, 0.41) (treatment,\n0.35) (happen, 0.29) (create, 0.22) (can, 0.14) (worse, 0.14) (effect, 0.08) (teeth,\n0.06) (remove, 0.03)\n\n\nSPLADE in terms of efficiency. In the meantime, SPLADE reaches efficiency levels equivalent to sparse BOW models, while outperforming doc2query-T5. Interestingly, strongly regularized models still\nshow competitive performance (e.g. FLOPS=0 _._ 05 _,_ MRR@10=0 _._ 296).\nFinally, the regularization effect brought by _\u2113_ FLOPS compared to _\u2113_ 1\nis clear: for the same level of efficiency, performance of the latter is\nalways lower.\n\n\n**The role of expansion.** Experiments show that the expansion\nbrings improvements w.r.t. to the purely lexical approach by increasing recall. Additionally, representations obtained from expansionregularized models are sparser: the models learn how to balance\nexpansion and compression, by both turning-off irrelevant dimensions and activating useful ones. On a set of 10k documents, the\nSPLADE- _\u2113_ FLOPS from Table 1 drops in average 20 terms per document, while adding 32 expansion terms. For one of our most efficient\nmodel (FLOPS=0 _._ 05), 34 terms are dropped in average, for only 5\nnew expansion terms. In this case, representations are extremely\nsparse: documents and queries contain in average 18 and 6 non-zero\nvalues respectively, and we need less that 1 _._ 4 GB to store the index\n\n- n disk. Table 2 shows an example where the model performs term\nre-weighting by emphasizing on _important_ terms and discarding\nmost of the terms without information content. Expansion allows\nto enrich documents, either by implicitly adding stemming effects\n(legs \u2192 leg) or by adding relevant topic words (e.g. treatment"," mechanism (roughly sum for\nSparTerm [1], max for EPIC [16] and SPARTA [29]), that allows to\ncompute an importance weight for each term of the vocabulary, for\nthe full document or query. However, EPIC and SPARTA (document)\nrepresentations are not sparse enough by construction \u2013 unless\nresorting on top- _\ud835\udc58_ pooling \u2013 contrary to SparTerm, for which fast\nretrieval is thus possible. Furthermore, the latter does not include\n(like SNRM) an _explicit_ sparsity regularization, which hinders its\nperformance. Our SPLADE model relies on such regularization, as\nwell as other key changes, that boost both the efficiency and the\neffectiveness of this type of models.\n\n\n**3** **SPARSE LEXICAL REPRESENTATIONS FOR**\n\n**FIRST-STAGE RANKING**\n\n\nIn this section, we first describe in details the SparTerm model [1],\nbefore presenting our model named SPLADE.\n\n\n**3.1** **SparTerm**\n\n\nSparTerm predicts term importance \u2013 in BERT WordPiece vocabulary (| _\ud835\udc49_ | = 30522) \u2013 based on the logits of the Masked Language Model (MLM) layer. More precisely, let us consider an input\nquery or document sequence (after WordPiece tokenization) _\ud835\udc61_ =\n( _\ud835\udc61_ 1 _,\ud835\udc61_ 2 _, ...,\ud835\udc61\ud835\udc41_ ), and its corresponding BERT embeddings ( _\u210e_ 1 _,\u210e_ 2 _, ...,\u210e\ud835\udc41_ ).\nWe consider the importance _\ud835\udc64\ud835\udc56\ud835\udc57_ - f the token _\ud835\udc57_ (vocabulary) for a\ntoken _\ud835\udc56_ (of the input sequence):\n\n\n_\ud835\udc64\ud835\udc56\ud835\udc57_ = transform( _\u210e\ud835\udc56_ ) _[\ud835\udc47]_ _\ud835\udc38_ _\ud835\udc57_ + _\ud835\udc4f_ _\ud835\udc57_ _\ud835\udc57_ \u2208{1 _, ...,_ | _\ud835\udc49_ |} (1)\n\n\n\nwhere _\ud835\udc38_ _\ud835\udc57_ denotes the BERT input embedding for token _\ud835\udc57_, _\ud835\udc4f_ _\ud835\udc57_ is a\ntoken-level bias, and transform("," a\npositive document _\ud835\udc51\ud835\udc56_ [+] [and a negative document] _[ \ud835\udc51]_ _\ud835\udc56_ [\u2212][, SparTerm is]\ntrained by minimzing the following loss:\n\n\n_\ud835\udc52_ _[\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc56_ [+][)]\nL _\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58_ = \u2212 log (3)\n\n_\ud835\udc52_ _[\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc56_ [+][)] + _\ud835\udc52_ _[\ud835\udc60]_ [(] _[\ud835\udc5e][\ud835\udc56][,\ud835\udc51]_ _\ud835\udc56_ [\u2212][)]\n\n\n**Limitations.** SparTerm expansion-aware gating is somewhat\nintricate, and the model cannot be trained end-to-end: the gating\nmechanism is learned beforehand, and _fixed_ while fine-tuning the\nmatching model with L _\ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc58_, therefore preventing the model to\nlearn the optimal sparsification strategy for the ranking task. More\n- ver, the two lexical and expansion-aware strategies do perform\nalmost equally well, questioning the actual benefits of expansion.\n\n\n**3.2** **SPLADE: SParse Lexical AnD Expansion**\n**model**\n\n\nIn the following, we propose slight, but essential changes to the\nSparTerm model that dramatically improve its performance.\n\n\n**Model.** We introduce a minor change in the importance estimation from Eq. 2, by introducing a log-saturation effect which\nprevents some terms to dominate and naturally ensures sparsity in\nrepresentations:\n\n\n_\ud835\udc64_ _\ud835\udc57_ = \u2211\ufe01 log [\ufffd] 1 + ReLU( _\ud835\udc64\ud835\udc56\ud835\udc57_ ) [\ufffd] (4)\n\n_\ud835\udc56_ \u2208 _\ud835\udc61_\n\n\nWhile it is intuitive that using a log-saturation prevents some terms\nfrom dominating \u2013 drawing a parallel with axiomatic approaches\nin IR and log(tf) models [9] \u2013 the implied sparsity can seem surprising at first, but, according to our experiments, it obtains better\nexperimental"],"28":[" (is the response generated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\n\n\nRAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\n\nMany existing RAG evaluation frameworks require substantial human annotations for scoring.\nARES significantly improves data efficiency during evaluation by only requiring three inputs: an indomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data generation.\n\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question\u2013answer\npairs, derived from the passages in the corpus. Sec\n- nd, it defines three separate judge models to perform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores\nthe different RAG systems being assessed using\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals\nfor RAG scoring. PPI utilizes a small set of human\nannotated datapoints for computing its confidence\nintervals; we designate this annotated set as our _hu-_\n_man preference validation set_, which is composed\n\n- f approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an\nswer relevance.\n\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating existing automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occurrences in the AIS attribution dataset (Rashkin et al.,\n2022), predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach."," LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab@stanford.edu\n\n\n**Matei Zaharia**\n\nDatabricks and UC Berkeley\nmatei@databricks.com\n\n\nThe best design for a RAG system is not necessarily universal across data domains, corpus sizes,\nand cost\/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand annotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alternatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strategy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the _relevance_ - f retrieved information and the _faithfulness_ and _accuracy_ - f generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accurately, we propose ARES, the **A** utomated **R** AG\n**E** valuation **S** ystem. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, leading to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RAGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered inference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response generated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\n\n\nRAG system finds relevant contexts and generates\nanswers that are both faithful and relevant"," 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RAGAS (James and Es, 2023). To evaluate RAG systems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several associated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\n\n- ffer little adaptability to new RAG evaluation set\n\ntings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n\n\n**3** **ARES**\n\n\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\n\n- r more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query\u2013passage\u2013answer\ntriples to train LLM judges. Subsequently, we apply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to estimate a confidence interval for the quality of each\nRAG system.\n\n\n**3.1** **LLM Generation of Synthetic Dataset**\n\n\nWe generate synthetic queries and answers from\nthe corpus","## **ARES: An Automated Evaluation Framework for Retrieval-Augmented** **Generation Systems**\n\n\n\n**Jon Saad-Falcon**\nStanford University _[\u2217]_\n\njonsaadfalcon@stanford.edu\n\n\n**Christopher Potts**\nStanford University\ncgpotts@stanford.edu\n\n\n**Abstract**\n\n\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an _Automated RAG Evaluation_\n_System_, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By creating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and\/or documents used in the evaluated RAG systems. We\nmake our code and datasets publicly available\n\n[on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to generate a response. This formulation admits a multitude\n\n- f choices: what retrieval model to use, how to di\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab","RES had strong generalizability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Furthermore, we found that even when the LLM judge\u2019s accuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\n\n\n\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\n\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when\nmaking more drastic shifts in domain, such as:\nswitching languages (e.g. English to Spanish, German, and other languages), switching from text to\ncode (e.g. questions + passages to coding functions\n+ documentation), and switching from retrieving\ntext to extraction of entities, webpages, or citations.\nTo test cross-lingual transfer, we used the\nXGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall\u2019s tau of 0.33\n\n- ver both context relevance and answer relevance\n\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall\u2019s tau\n\n- f 0.28 over both context relevance and answer\n\nrelevance scoring for CodeSearchNet. To test extraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall\u2019s\ntau of 0.38 over both context relevance and answer\n\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n\n\n**6** **Conclusion**\n\n\nIn this work, we present ARES, a novel automated\nevaluation framework for retrieval-augmented generation (RAG). ARES offers a novel training\npipeline for fine-tuning lightweight LLM judges\n\n- n synthetically generated queries and answers.\nARES can evaluate each component of a RAG system separately to help improve system understanding and create targeted solutions, and it requires\n\n- nly minimal human annotations. For the eight different datasets"],"29":["## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [","## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar"," like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25\/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer","Published as a conference paper at ICLR 2021\n\n## ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS\n### WITH MULTI-HOP DENSE RETRIEVAL\n\n\n**Wenhan Xiong** [1] _[\u2217]_ **Xiang Lorraine Li** [2] _[\u2217]_ **Srinivasan Iyer** _[\u2021]_ **Jingfei Du** _[\u2021]_\n\n\n**Patrick Lewis** _[\u2021\u2020]_ **William Wang** [1] **Yashar Mehdad** _[\u2021]_ **Wen-tau Yih** _[\u2021]_\n\n\n**Sebastian Riedel** _[\u2021\u2020]_ **Douwe Kiela** _[\u2021]_ **Barlas O\u02d8guz** _[\u2021]_\n\n\n_\u2021_ Facebook AI\n1University of California, Santa Barbara\n2University of Massachusetts Amherst\n\n_\u2020_ University College London\n_{_ xwhan, william _}_ @cs.ucsb.edu, xiangl@cs.umass.edu,\n_{_ sviyer, jingfeidu, plewis, mehdad, scottyih, sriedel, dkiela, barlaso _}_ @fb.com\n\n\nABSTRACT\n\n\nWe propose a simple and efficient multi-hop dense retrieval approach for answering\ncomplex open-domain questions, which achieves state-of-the-art performance on\ntwo multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previ\n     - us work, our method does not require access to any corpus-specific information,\nsuch as inter-document hyperlinks or human-annotated entity markers, and can\nbe applied to any unstructured text corpus. Our system also yields a much better\nefficiency-accuracy trade-off, matching the best published accuracy on HotpotQA\nwhile being 10 times faster at inference time. [1]\n\n\n1 INTRODUCTION\n\n\n_Open domain question answering_ is a challenging task where the answer to a given question needs to\nbe extracted from a large pool of documents. The prevailing approach (Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set","Published as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto finetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved significant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\n\n- f questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model\u2019s hidden states. However, their method requires an initial round\n\n- f TF-IDF\/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\n"],"30":["## **Precise Zero-Shot Dense Retrieval without Relevance Labels**\n\n**Luyu Gao** _[\u2217]_ _[\u2020]_ **Xueguang Ma** _[\u2217\u2021]_ **Jimmy Lin** _[\u2021]_ **Jamie Callan** _[\u2020]_\n\n_\u2020_ Language Technologies Institute, Carnegie Mellon University\n\n_\u2021_ David R. Cheriton School of Computer Science, University of Waterloo\n{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca\n\n\n\n**Abstract**\n\n\nWhile dense retrieval has been shown effec\ntive and efficient across tasks and languages,\nit remains difficult to create effective fully\nzero-shot dense retrieval systems when no relevance label is available. In this paper, we\nrecognize the difficulty of zero-shot learning\nand encoding relevance. Instead, we propose to pivot through Hypothetical Document\nEmbeddings (HyDE). Given a query, HyDE first\nzero-shot instructs an instruction-following\nlanguage model (e.g. InstructGPT) to generate a _hypothetical_ document. The document captures relevance patterns but is unreal\nand may contain false details. Then, an unsupervised contrastively learned encoder (e.g.\nContriever) encodes the document into an\nembedding vector. This vector identifies a\nneighborhood in the corpus embedding space,\nwhere similar _real_ documents are retrieved\n\nbased on vector similarity. This second step\nground the generated document to the actual\ncorpus, with the encoder\u2019s dense bottleneck\nfiltering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense\nretriever Contriever and shows strong performance comparable to fine-tuned retrievers,\nacross various tasks (e.g. web search, QA, fact\nverification) and languages (e.g. sw, ko, ja). [1]\n\n\n**1** **Introduction**\n\n\nDense retrieval (Lee et al., 2019; Karpukhin et al.,\n2020), the method of retrieving documents using\nsemantic embedding similarities, has been shown\nsuccessful across tasks like web search, question\nanswering, and fact verification. A variety of meth\n- ds such as negative mining (Xiong et al., 2021; Qu\net al., 2021"," we expect the encoder\u2019s dense bottleneck\nto serve a lossy compressor, where the extra\n(hallucinated) details are filtered out from the\nembedding. We use this vector to search against\nthe corpus embeddings. The most similar _real_\ndocuments are retrieved and returned. The retrieval\n\nleverages document-document similarity encoded\nin the inner-product during contrastive training.\nNote that, interestingly, with HyDE factorization,\nthe query-document similarity score is no longer\nexplicitly modeled nor computed. Instead, the\nretrieval task is cast into two NLU and NLG tasks.\n\nHyDE appears unsupervised. **No** model is trained\nin HyDE: both the generative model and the contrastive encoder remain intact. Supervision signals\nwere only involved in instruction learning of our\nbackbone LLM.\n\nIn our experiments, we show HyDE using InstructGPT (Ouyang et al., 2022) and Contriever (Izacard\net al., 2021) as backbone models significantly outperforms the previous state-of-the-art Contriever\n- nly zero-shot no-relevance system on 11 queries\n\n\n\nsets, covering tasks like Web Search, Question\nAnswering, Fact Verification and languages like\nSwahili, Korean, Japanese.\n\n\n**2** **Related Works**\n\n\n**Dense Retrieval** (Lee et al., 2019; Karpukhin\net al., 2020) has been extensively studied after the\nemergence of pre-trained Transformer language\nmodels (Devlin et al., 2019). Researchers studied the metric learning problems, such as training\nloss (Karpukhin et al., 2020) and negative sampling (Xiong et al., 2021; Qu et al., 2021), and also\nintroduced distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofst\u00e4tter et al., 2021). Later works studied\nthe second stage pre-training of language model\nspecifically for retrieval (Izacard et al., 2021; Gao\nand Callan, 2021; Lu et al., 2021; Gao and Callan,\n2022; Liu and Shao, 2022).\nThe popularity of dense retrieval can be partially\nattributed to the rich and successful research in very\nefficient minimum inner","ard et al. (2021), we also\ndo not assume access to the test time corpora for\ntraining. This is a more realistic setup and prevents\n\n- ver-engineering on the test corpora.\n\n\nBy the definition in Sachan et al. (2022), our\nsetup can be roughly considered as **\u201cunsuper-**\n**vised\u201d** . Strictly, as with Sachan et al. (2022), the\n\n- nly supervision resides in the LLM, in the processing of learning to follow instructions.\n\n\n**Generative Retrieval** Generative search is a new\n\nclass of retrieval methods that use neural generative\nmodels as search indices (Metzler et al., 2021; Tay\net al., 2022; Bevilacqua et al., 2022; Lee et al.,\n2022). These models use (constrained) decoding\nto generate document identifiers, such as id and\nsub-string, which map directly to _real_ documents.\nThey have to go through special training procedures\n\n- ver relevance data; effective search may also need\nto use novel forms of search indices (Bevilacqua\net al., 2022; Lee et al., 2022). In comparison, our\nmethod uses the standard MIPS index and requires\nno training or training data. Our generative model\nproduces an intermediate hypothetical document\nto be fed into a dense encoder, instead of a real\n\ndocument.\n\n\n**3** **Methodology**\n\n\nIn this section, we first formally define the problem of (zero-shot) dense retrieval. Then we will\nintroduce how HyDE is designed to solve it.\n\n\n\n**3.1** **Preliminaries**\n\n\nDense retrieval models similarity between query\nand document with inner product similarity. Given\na query _q_ and document _d_, it uses two encoder\nfunction enc _q_ and enc _d_ to map them into _d_ dimension vectors **vq** _,_ **vd**, whose inner product is used\nas similarity measurement.\n\n\nsim(q _,_ d) = _\u27e8_ enc _q_ (q) _,_ enc _d_ (d) _\u27e9_ = _\u27e8_ **vq** _,_ **vd** _\u27e9_ (1)\n\n\nFor zero-shot retrieval, we consider _L_ query sets\n_Q_ 1 _, Q_ 2 _, ..., Q","levance scores between a pair of query and docu\n\nment. Dense retrievers consider vector similarities\n\nwhile self-attentive re-rankers regression scores. In\ncomparison, the concept of relevance in HyDE is\ncaptured by an NLG model and the language generation process. We demonstrate in many cases, HyDE\ncan be as effective as dense retrievers that learn to\n\nmodel numerical relevance scores. So, is numerical relevance just a statistical artifact of language\nunderstanding? Will a weak retriever theoretically\nsuffice as the NLU & NLG models rapidly become\nstronger? Rushing to conclusions is not smart;\nmore works need to be done to get answers. With\nthis paper, we just want to raise these questions.\nConcretely in this paper, we introduce a new\nparadigm of interactions between LLM and dense\nencoder\/retriever. We demonstrate (part of) relevance modeling and instruction understanding\ncan be delegated to the more powerful and flexible LLM. As a consequence, the need for relevance labels is removed. We are excited to see\n\nhow this can be generalized further to more sophisticated tasks like multi-hop retrieval\/QA and\nconversational search.\n\nWe argue HyDE is also of practical use though not\nnecessarily over the entire lifespan of a search system. At the very beginning of the life of the search\nsystem, serving queries using HyDE offers performance comparable to a fine-tuned model, which\nno other relevance-free model can offer. As the\n\nsearch log grows, a supervised dense retriever can\nbe gradually rolled out. As the dense retriever\ngrows stronger, more queries will be routed to it,\nwith only less common and emerging ones going\nto HyDE backend.\n\n\n**References**\n\n\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wen-tau Yih. 2022. [Task-aware re-](https:\/\/doi.org\/10.48550\/ARXIV.2211.09260)\n[trieval with instructions.](https:\/\/doi.org\/10.48550\/ARXIV.2211.09260)\n\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder"," reaction is\nChemistry spontaneous?\n0) at equilibrium there are more products than reactants\n1) the value of \u2206 _G_ [0] is greater than zero\n2) the value of \u2206 _S_ [0] is greater than zero\n3) the value of Keq is less than one\n**Original Answer [Incorrect]** : 2) the value of \u2206S- is greater than zero.\nA reaction is spontaneous if the value of \u2206S- is greater than zero. This is\nbecause a positive \u2206S- indicates that the reaction results in an increase in\nentropy, which is a measure of disorder. A spontaneous reaction is one that\n\n          - ccurs without the input of energy, and an increase in entropy is a natural\ntendency of the universe.\n**Stepback Question** : What are the chemistry principles behind this question?\n**Principles** :\nSpontaneous reactions: A spontaneous reaction is a reaction that occurs without\nthe input of energy.\n**Final Answer** : A reaction is spontaneous if the change in Gibbs free energy is\nnegative (\u2206 _G <_ 0). The value of \u2206 _G_ [0] is greater than zero indicates that the\nreaction is not spontaneous.\n\n\nThe correct answer is 0) at equilibrium there are more products than reactants.\n\n\nMMLU- **Original Question** : Potassium-40 is a minor isotope found in naturally ocChemistry curring potassium. It is radioactive and can be detected on simple radiation\ncounters. How many protons, neutrons, and electrons does potassium-40 have\nwhen it is part of K2SO4?\n0) 21 neutrons, 19 protons, 18 electrons\n1) 20 neutrons, 19 protons, 19 electrons\n2) 21 neutrons, 19 protons, 19 electrons\n3) 19 neutrons, 19 protons, 19 electrons\n**Original Answer [Incorrect]** : The correct answer is 2).\nPotassium-40 has 19 protons and 21 neutrons. When it is part of K2SO4, it has\n19 electrons.\n\n**Stepback Question** : What are the chemistry principles behind this question?\n**Principles** :\nAtomic number: The atomic number of an element is the number of protons in\nthe nucleus of an atom of that element"],"31":[" et al. 2023)\nfocuses on human exam and ToolBench (Qin et al. 2023)\nevaluates how well LLMs use external tools. Recently, Adlakha et al. (2023) evaluate the RAG of LLMs in exist QA\ndataset. Different from their work, we focus on 4 required\nabilities of RAG and create Retrieval-Augmented Generation Benchmark to evaluate the LLMs.\n\n\n**Retrieval-Augmented Generation Benchmark**\n\n\nIn this section, we first introduce the specific retrievalaugmented generation abilities we aim to evaluate. Next, we\n\n- utline the process of constructing the RAG benchmark for\nevaluation. Lastly, we present the evaluation metrics.\n\n\n**Required abilities of RAG**\n\n\nExternal knowledge is the key to resolving the problems\n\n- f LLMs such as hallucination and outdated knowledge,\nwhich can make LLMs generate more accurate and reliable\nresponses through retrieval-augmented generation (RAG).\nHowever, LLMs cannot always response as expected with\nRAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating\nthese external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the\nuseful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the\nretrieval-augmented generation of LLMs, and we concern\nabout 4 specific abilities:\n**Noise Robustness** is the robustness of LLMs in noisy\ndocuments. As retrievers are not perfect, the external knowledge they retrieve often contains a significant amount of\nnoise, i.e., documents which are relevant to the question but\ndo not contain any information about the answer. To effectively answer user questions, LLMs must be able to extract\nthe necessary information from documents despite there are\nnoisy documents.\n**Negative Rejection** is a measure of whether LLMs can\ndecline to answer a question when none of the contexts provide useful information. In real-world situations, the search\nengine often fails to retrieve documents containing the answers. In these cases, it is important for the","## **Benchmarking Large Language Models in Retrieval-Augmented Generation**\n\n**Jiawei Chen** [1,3] **, Hongyu Lin** [1,*] **, Xianpei Han** [1,2,*] **, Le Sun** [1,2]\n\n1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n_{_ jiawei2020,hongyu,xianpei,sunle _}_ @iscas.ac.cn\n\n\n\n**Abstract**\n\n\nRetrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language\nmodels (LLMs). However, existing research lacks rigorous\nevaluation of the impact of retrieval-augmented generation\n\n  - n different large language models, which make it challenging to identify the potential bottlenecks in the capabilities\n\n  - f RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance\n\n  - f different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case.\nThen we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\n\n  - f noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n\n\n**Introduction**\n\n\nRecently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao"," question, LLMs often fail to provide accurate\nanswer. Finally, we found that even when the LLMs contain\nthe required knowledge and are given warnings about potential risks in the retrieved information through instruction,\nthey still tend to trust and prioritize the retrieved information\n\n- ver their own existing knowledge. The experimental results\nmentioned above highlight the need for further resolution of\nimportant issues in the existing RAG method. Therefore, it\nis crucial to exercise caution and carefully design its usage.\nGenerally speaking, the contributions of this paper are [1] :\n\n\n - We proposed to evaluate four capabilities for retrievalaugmented generation  - f LLMs and created the\nRetrieval-Augmented Generation Benchmark in both English and Chinese. To best of our knowledge, it is the first\nbenchmark designed to assess these four capabilities for\nretrieval-augmented generation of LLMs.\n\n - We evaluated the existing LLMs using RGB and found\nthe limitations of them in the four different abilities.\n\n - We analyzed the responses of LLMs in RGB and identified their current shortcomings as well as suggested directions for improvement.\n\n\n**Related work**\n\n**Retrieval-augmented models** The knowledge stored in\nlarge language models is commonly out-of-date (He, Zhang,\nand Roth 2022) and they also sometimes generate hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023) i.e., they may generate irrelevant or factually incorrect contents. By using external\nknowledge as guidance, retrieval-augmented models can\ngenerate more accurate and reliable responses (Guu et al.\n2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\net al. 2022; Shi et al. 2023; Ren et al. 2023). Retrievalaugmented models have achieved remarkable results in various tasks such as open-domain QA (Izacard and Grave\n2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\n\n\n1Our code&data: https:\/\/github.com\/chen700564\/RGB.\n\n\n|Retrieve using<br>search engine|Col2|\n|---|---|\n|**Retrieve using**<br>**search engine**|{\"link\": \"https"," measure of whether LLMs can\ndecline to answer a question when none of the contexts provide useful information. In real-world situations, the search\nengine often fails to retrieve documents containing the answers. In these cases, it is important for the model to have\nthe capability to reject recognition and avoid generating misleading content.\n**Information Integration** is a capacity to integrate answers from multiple documents. In many cases, the answer to a question may be contained in multiple documents.\nFor example, for the question _\u201cWho are the champions of_\n_the U.S. Open 2022 men\u2019s and women\u2019s singles?\u201d_, the two\nchampions may be mentioned in different documents. In order to provide better answers to complex questions, it is necessary for LLMs to have the ability to integrate information.\n**Counterfactual Robustness** refers to a capacity to handle errors in external knowledge. In the real world, there is\nan abundance of false information on the internet. Please\n\n\nnote that we only evaluate the situation that LLMs are given\nwarnings about potential risks in the retrieved information\nthrough instruction.\nIn real-world scenarios, it is not possible to obtain perfect documents with all the necessary external knowledge.\nTherefore, evaluating these four abilities of the model becomes essential in order to measure the RAG of LLMs.\n\n\n**Data construction**\n\nInspired by previous benchmarks for LLMs, RGB utilizes\na question-answering format for evaluation. We evaluate the\nLLMs by judging the retrieval-augmented responses of them\nto the questions. To simulate real-world scenarios, we construct question and answer data using actual news articles.\nDue to the abundance of knowledge contained within the\nLLMs there is a potential for bias when measuring the first\nthree abilities. To mitigate this, the instances of RGB are\nconstructed by latest news articles. Additionally, we retrieve\nexternal documents from Internet through search engines.\nFinally, we expand the corpus and divided it into 4 testbeds\nto evaluate the above basic abilities of LLMs. The overall\nprocedure of our data construction is illustrated in Figure 2.\n**QA instances generation.** We first collect latest news articles and use prompts to make ChatGPT generate events,\nquestions, and answers for each articles. For example, as\nshown in the Figure 2, for a report about \u201cThe 2022 Nobel\nPrize\u201d, ChatGPT will generate corresponding event, question and provide key",". These challenges result in LLMs being unable to\nconsistently generate reliable and accurate responses. Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how\ncould each model survives from these drawbacks and improvement their performance via information retrieval. As a\nresult, there is a pressing need for a comprehensive evaluation of LLMs on their ability to effectively utilize retrieved\ninformation, as well as their ability to withstand the various\ndrawbacks present in information retrieval.\nTo this end, this paper conducts a comprehensive evaluation of RAG for current LLMs. Specifically, we create a new\nRetrieval-Augmented Generation Benchmark, namely RGB,\nin both English and Chinese. In order to ensure that the internal knowledge of LLMs does not introduce bias into the\nevaluation results, RGB chooses to aggregate the latest news\ninformation and constructs queries based on the news information. Then, based on these queries, we use Search API to\nfetch relevant documents and select most relevant snippets\nfrom the content as external retrieved documents. Finally,\nbased on different compositions of query and document-set\npairs, we expand the corpus and divided it into 4 testbeds to\nevaluate the following basic abilities of LLMs according to\nthe common challenges in RAG, as shown in Figure 1:\n\n\n - **Noise Robustness**, which means a LLM can extract useful information from noisy documents. In this paper, we\ndefine noisy documents as those that are relevant to the\nquestion but do not contain any information of the answer. For the instance in Figure 1, the noisy documents\nrelated to the question \u201cWho was awarded the 2022 Nobel Prize in Literature\u201d include reports about the 2021\nNobel Prize in Literature. To this end, the testbed for\nnoise robustness contains instances whose external documents contain a certain number of noisy documents\nbased on the desired noise ratio.\n\n - **Negative Rejection**, which means that a LLM should reject to answer the question when the required knowledge\nis not present in any retrieved document. The testbed for\nnegative rejection contains instances whose external documents are only with noisy documents. LLMs are expected to indicate \u201cinsufficient information\u201d or other rejection signals.\n\n - **Information Integration**, which evaluates whether\nLLMs can answer complex questions that require integrating information from multiple documents. For the instance in Figure 1, for the question \u201cWhen were the ChatGPT app"],"32":[" in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f","-context transformers\nare recency-biased and do not effectively use long\nrange context.\n\n\n**6.3** **The Serial-Position Effect**\n\n\nThe U-shaped curve we observe in this work has\na connection in psychology known as the _serial-_\n_position effect_ (Ebbinghaus, 1913; Murdock Jr,\n1962), that states that in free-association recall\n\n- f elements from a list, humans tend to best remember the first and last elements of the list. The\nserial-position effect plays a role in understanding\nhow humans develop short- and long-term mem\n\n- ry. Observing a serial-position-like effect in language models is perhaps surprising, since the selfattention mechanisms underlying Transformer language models is technically equally capable of retrieving any token from their contexts.\n\n\n**7** **Conclusion**\n\n\nWe empirically study how language models use\nlong input contexts via a series of controlled experiments. We show that language model performance degrades significantly when changing the\nposition of relevant information, indicating that\nmodels struggle to robustly access and use information in long input contexts. In particular, performance is often lowest when models must use\n\ninformation in the middle of long input contexts.\nWe conduct a preliminary investigation of the role\n\n- f (i) model architecture, (ii) query-aware contextualization, and (iii) instruction fine-tuning to better\nunderstand how they affect how language models\nuse context. Finally, we conclude with a practical case study of open-domain question answering,\nfinding that the performance of language model\nreaders saturates far before retriever recall. Our\n\nresults and analysis provide a better understanding\n\n- f how language models use their input context\nand provides new evaluation protocols for future\nlong-context models.\n\n\n**Acknowledgments**\n\n\nWe would like to thank Luke Zettlemoyer, who\nserved as our TACL action editor, and the the\nanonymous reviewers for their comments and feedback. We also thank Claudiu Leoveanu-Condrei,\nMegan Leszczynski, Dmytro Okhonko, Maithra\nRaghu, Eric Wallace and Sang Michael Xie for\nfeedback and discussions that helped improve this\nwork. Further, we are grateful to Sewon Min for\nher help with the AmbigQA dataset. This work\nwas supported by the Stanford Center for Research\n\n- n Foundation Models (CRFM), by OpenAI via\nan API credits"," question via\ncontrolled experiments with a variety of state-ofthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\nAnthropic\u2019s Claude-1.3) language models in settings that require accessing and using information\nwithin an input context. In particular, our experiments make controlled changes to the input context\nsize and the position of the relevant information\nwithin the input context and study their effects on\nlanguage model performance. If language models\ncan robustly use information within long input contexts, then their performance should be _minimally_\n_affected_ by the position of the relevant information\nin the input context.\n\n\nWe first experiment with multi-document question answering, which requires models to reason\n\n- ver provided documents to find relevant information and use it to answer a given question; this task\nmimics the retrieval-augmented generation setup\nunderlying many commercial generative search and\nquestion answering applications (e.g., Bing Chat).\nIn this setting, we control (i) the input context\nlength by changing the number of documents in\nthe input context (akin to retrieving more or less\ndocuments in retrieval-augmented generation), and\n(ii) control the position of the relevant information\nwithin the input context by changing the order of\nthe documents to place the relevant document at\nthe beginning, middle or end of the context.\n\n\nWe find that changing the position of relevant\ninformation in the input context can substantially\naffect model performance, indicating that current\nlanguage models do not robustly access and use\ninformation in long input contexts. Furthermore,\nwe observe a distinctive U-shaped performance\ncurve (Figure 1); language model performance is\nhighest when relevant information occurs at the\nvery beginning (primacy bias) or end of its input context (recency bias), and performance significantly degrades when models must access and\nuse information in the middle of their input context (\u00a72.3). For example, when relevant information is placed in the middle of its input context, GPT-3.5-Turbo\u2019s performance on the multi\n\n\ndocument question task is lower than its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily","-4570-9f74-3624e77d6684\": \"d1ff29be-4e2a-4208-a182-0cea716be3d4\",\n\n\" **9f4a92b9-5f69-4725-ba1e-403f08dea695** \": \"703a7ce5-f17f-4e6d-b895-5836ba5ec71c\",\n\n\"52a9c80c-da51-4fc9-bf70-4a4901bc2ac3\": \"b2f8ea3d-4b1b-49e0-a141-b9823991ebeb\",\n\n\"f4eb1c53-af0a-4dc4-a3a5-c2d50851a178\": \"d733b0d2-6af3-44e1-8592-e5637fdb76fb\"}\n\n\nKey: \" **9f4a92b9-5f69-4725-ba1e-403f08dea695** \"\nCorresponding value:\n\n\nDesired Output\n\n\n703a7ce5-f17f-4e6d-b895-5836ba5ec71c\n\n\nFigure 6: Example of the key-value retrieval task, with an input context and the desired model output. Given a key,\nthe goal is to return the associated value. All keys and values are 128-bit UUIDs. The relevant key-value pair for\nanswering the query is bolded here within the input context for clarity.\n\n\n\nindicate that extended-context models are not nec\nessarily better than their non-extended counterparts\nat using their input context.\n\n\n**3** **How Well Can Language Models**\n**Retrieve From Input Contexts?**\n\n\nGiven that language models struggle to retrieve\nand use information from the middle of their input\ncontexts in the multi-document question answering\ntask, to what extent can they simply _retrieve_ from\ninput contexts? We study this question with a synthetic key-value retrieval task, which is designed to\nprovide a minimal testbed for the basic ability to\nretrieve matching tokens from an input context.\n\n\n**3.1** **Experimental Setup**\n\n\nIn our synthetic key-value retrieval task, the inputs\nare (i) a string-serialized JSON object with _k_ keyvalue pairs, where each of the keys and values are"," its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily\nbetter at using their input context (\u00a72.3).\nGiven that language models struggle to retrieve\nand use relevant information in the multi-document\n\nquestion answering task, to what extent can language models even _retrieve_ from their input contexts? We study this question with a synthetic keyvalue retrieval task, which is designed to be a minimal testbed for the basic ability to retrieve matching\ntokens from the input context. In this task, models\nare given a collection of JSON-formatted key-value\npairs and must return the value associated with a\nspecific key. Similar to the multi-document QA\ntask, the key-value retrieval task admits controlled\nchanges to the input context length (adding more\nkey-value pairs) and the position of relevant information. Although some models perform the\nsynthetic key-value retrieval task perfectly, other\nmodels struggle to simply retrieve matching tokens\nthat occur in the middle of their input context and\ncontinue to exhibit a U-shaped performance curve.\nTo better understand why language models struggle to robustly access and use information in their\ninput contexts, we study the role of model architecture (decoder-only vs. encoder-decoder), queryaware contextualization, and instruction fine-tuning\n(\u00a74). We find that:\n\n\n  - Encoder-decoder models are relatively robust\nto changes in the position of relevant information within their input context, but only when\nevaluated on sequences within its trainingtime sequence length. When evaluated on\nsequences longer than those seen during training, we observe a U-shaped performance\ncurve (\u00a74.1).\n\n\n  - Query-aware contextualization (placing the\nquery before _and_ after the documents or keyvalue pairs) enables near-perfect performance\n\n   - n the synthetic key-value task, but minimally\nchanges trends in multi-document QA (\u00a74.2).\n\n\n  - Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of\nrelevant information in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that"],"33":[" in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f","learn to use these long contexts from similarlyformatted data that may occur in Internet text seen\nduring pre-training, e.g., StackOverflow questions\n\n\n\nand answers.\n\nTo better understand the effect of additional finetuning and model scale, we also experimented\nwith Llama-2 models of varying sizes (7B, 13B,\nand 70B) with and without additional supervised\nfine-tuning and reinforcement learning from human feedback (Appendix E). We find that the Ushaped performance curve only appears in sufficiently large language models (with or without additional fine-tuning)\u2014the 7B Llama-2 models are\nsolely recency biased, while the 13B and 70B models exhibit a U-shaped performance curve. In addition, we see that the Llama-2 supervised fine-tuning\nand reinforcement learning from human feedback\nprocedure slightly mitigates the positional bias in\nsmaller models (13B, akin to trends shown when\ncomparing MPT-30B and MPT-30B-Instruct), but\nminimally affects trends on larger models (70B).\n\n\n**5** **Is More Context Is Always Better?**\n**A Case Study With Open-Domain QA**\n\n\nOur results indicate that prompting language models with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing\naccuracy. Even if a language model can take in\n16K tokens, is it actually beneficial to provide 16K\ntokens of context? The answer to this question\nis ultimately downstream task-specific since it depends on the marginal value of the added context\nand the model\u2019s ability to effectively use long input\ncontexts, but we perform a case study with opendomain question answering on NaturalQuestionsOpen to better understand this trade-off in existing\nlanguage models.\nWe use language models in a standard retrieverreader setup. A retrieval system (Contriever, finetuned on MS-MARCO) takes an input query from\nNaturalQuestions-Open and returns the _k_ documents from Wikipedia with the highest relevance\nscore. To condition language models on these retrieved documents, we simply include them in the\nprompt. We evaluate retriever recall and reader\naccuracy (whether any of the annotated answers\nappear in the predicted output) as a function of the\nnumber of"," its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily\nbetter at using their input context (\u00a72.3).\nGiven that language models struggle to retrieve\nand use relevant information in the multi-document\n\nquestion answering task, to what extent can language models even _retrieve_ from their input contexts? We study this question with a synthetic keyvalue retrieval task, which is designed to be a minimal testbed for the basic ability to retrieve matching\ntokens from the input context. In this task, models\nare given a collection of JSON-formatted key-value\npairs and must return the value associated with a\nspecific key. Similar to the multi-document QA\ntask, the key-value retrieval task admits controlled\nchanges to the input context length (adding more\nkey-value pairs) and the position of relevant information. Although some models perform the\nsynthetic key-value retrieval task perfectly, other\nmodels struggle to simply retrieve matching tokens\nthat occur in the middle of their input context and\ncontinue to exhibit a U-shaped performance curve.\nTo better understand why language models struggle to robustly access and use information in their\ninput contexts, we study the role of model architecture (decoder-only vs. encoder-decoder), queryaware contextualization, and instruction fine-tuning\n(\u00a74). We find that:\n\n\n  - Encoder-decoder models are relatively robust\nto changes in the position of relevant information within their input context, but only when\nevaluated on sequences within its trainingtime sequence length. When evaluated on\nsequences longer than those seen during training, we observe a U-shaped performance\ncurve (\u00a74.1).\n\n\n  - Query-aware contextualization (placing the\nquery before _and_ after the documents or keyvalue pairs) enables near-perfect performance\n\n   - n the synthetic key-value task, but minimally\nchanges trends in multi-document QA (\u00a74.2).\n\n\n  - Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of\nrelevant information in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that"," propose to apply a tuneable small language model the rewriter,\nwhich can be trained to cater to the frozen retriever\n\nand reader. The training implementation consists\n\n- f two stages, warm-up and reinforcement learning. Evaluation and analyses on open-domain QA\nand multiple-choice QA show the effectiveness\n\n- f query rewriting. Our work proposes a novel\nretrieval-augmented black-box LLM framework,\nproves that the retrieval augmentation can be enhanced from the aspect of query rewriting, and\nprovides a new method for integrating trainable\nmodules into black-box LLMs.\n\n\n**Limitations**\n\n\nWe acknowledge the limitations of this work. (i)\nThere is still a trade-off between generalization and\nspecialization among downstream tasks. Adding\na training process, the scalability to direct transfer\nis compromised, compared to few-shot in-context\nlearning. (ii) The research line of _LLM agent_ has\nshown impressive performance but relies on multiple calls to the LLM for each sample (Khattab\net al., 2022; Yao et al., 2023), where the LLM\nplays as an agent to flexibly call the retriever multiple times, reads the context in earlier hops, and\ngenerates follow-up questions. Different from\nthese studies, our motivation is to enhance the one\nturn retriever-then-read framework with a trainable\n\nquery rewriter. (iii) Using a web search engine as\nthe retriever also leads to some limitations. Neu\nral dense retrievers that are based on professional,\nfiltered knowledge bases may potentially achieve\nbetter and controllable retrieval. More discussion\n\nis included in the appendix.\n\n\n\n**References**\n\n\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. _arXiv preprint_\n_arXiv:2302.04023_ .\n\n\nParishad BehnamGhader, Santiago Miret, and Siva\nReddy. 2022. Can retriever-augmented language\nmodels reason? the blame game between the retriever and"," 2.0x for LLaMA, GPT-3.5 and PaLM\nrespectively. This indicates a tension between \u039b\nand \u03a6: propositional memory may be used when\navailable, and if not, the predicate pairing may be\nattended to more closely. Again, the \u03a6 effect is\n\n- bserved across the three model families, revealing its root in the large-scale pre-training process,\nrather than model peculiarities or fine-tuning.\n\n\n\n**8** **Impact of Bias on Performance**\n\n\nWe have demonstrated two sources of hallucination\n\nby LLMs on inference tasks. We now assess their\nimpact on model performance to quantify their risk.\nWe compare LLMs\u2019 performance between NLI\nsubsets that are _consistent_ - r _adversarial_ to each\n\nbias. A sample _P_ \u22a8 _H_ ? is _consistent_ with a bias\nwhen the prediction by the bias **agrees with** the\ngold entailment label; conversely, it is _adversarial_\nto a bias when the prediction by the bias **disagrees**\n**with** the label.\n\nFor example, \u201cGoogle bought YouTube \u22a8\nGoogle owns YouTube\u201d is _consistent_ with the attestation bias of every model, because the conclusion\n_Google owns YouTube_ is attested in every LLM\u2019s\ntraining data, and the sample label is Entail;\n\u201cApple owns Samsung \u22ad Apple bought Samsung\u201d\nis also _consistent_, because its conclusion is not attested and the sample label is No-Entail. The\nreverses of these two samples are _adversarial_, since\ntheir respective attestedness (unchanged) does not\nagree with the entailment labels (now flipped). For\neach subset, there is substantial representation in\nboth Levy\/Holt and RTE-1 (see appendix Table 9).\nWhile earlier experiments inspected model textual responses to characterize behavior change,\nwe now use area under the precision-recall curve\n(AUC) to summarize model performance over a\ntunable confidence threshold (scoring described in\n\u00a74.2), which is better for measuring practical discriminative power. Following Li et al. (2022), we\nre-scale AUC values to normalize over the label\n\ndistribution, yielding _AUCnorm_ values that assign\nrandom classifiers 0% and perfect classifiers 100%.\nWe report results in Table "],"34":[" confidence set of the ML\nmodel\u2019s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence\ninterval for the performance of the evaluated RAG\nsystem (e.g. its context relevance, answer faithfulness, or answer relevance accuracy individually)\ncompared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the human preference validation set with the much larger\nset of datapoints with ML predictions, PPI can develop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\n\n\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confidence bounds for the success and failure rates of the\n\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (probability) for our confidence interval.\n\n\nWith the accuracy confidence interval for each\ncomponent of the RAG, we find the midpoint of\neach confidence interval and use the midpoints to\nrank the RAG systems. With our ranking, we can\ncompare different RAG systems, as well as different configurations of the same RAG system, to find\nthe best-performing approach for a given domain.\n\n\n**4** **Experiments**\n\n\n**4.1** **Models**\n\n\nFor our fine-tuned judges, ARES relies on generating cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use OpenAI\u2019s _gpt-3.5-turbo-16k_, version 10\/23, (Brown\net al., 2020) in a zero\/few-shot setting. For similarity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand Open"," evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges). We compare against RAGAS (version 0.0.18) and a baseline few-shot prompted GPT3.5 judge ( _gpt-3.5-turbo-16k_ ). For the few-shot\nGPT-3.5 judge, we provide few-shot examples for\nguiding predictions; the prompts are included in\nAppendices A.2, A.3, and A.4. For both ARES\nand the GPT-3.5 judge baseline, we augment the\nLLM with PPI, using a 300-datapoint human preference validation set to rectify the ML predictions\nand produce confidence intervals.\n\n\n\nAcross almost all settings across the datasets\nfrom KILT and SuperGLUE, ARES provides a\nmore accurate ranking of RAG systems than RAGAS. ARES averages a Kendall\u2019s _\u03c4 0.065 higher_\n_for context relevance_ and _0.132 higher for answer_\n_relevance than RAGAS_ . Additionally, the LLMjudge is substantially more accurate than RAGAS\nat predicting context relevance and answer relevance of a query-passage-answer triple. For context relevance, ARES with a fine-tuned LLM-judge\nis _59.9 percentage points higher than RAGAS_ while\nfor answer relevance, our system is _14.4 percent-_\n_age points higher than RAGAS_ . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompting and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall\u2019s _\u03c4_ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall\u2019s _\u03c4_ for ARES is\n0.08 higher","## **ARES: An Automated Evaluation Framework for Retrieval-Augmented** **Generation Systems**\n\n\n\n**Jon Saad-Falcon**\nStanford University _[\u2217]_\n\njonsaadfalcon@stanford.edu\n\n\n**Christopher Potts**\nStanford University\ncgpotts@stanford.edu\n\n\n**Abstract**\n\n\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an _Automated RAG Evaluation_\n_System_, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By creating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and\/or documents used in the evaluated RAG systems. We\nmake our code and datasets publicly available\n\n[on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to generate a response. This formulation admits a multitude\n\n- f choices: what retrieval model to use, how to di\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab"," (is the response generated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\n\n\nRAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\n\nMany existing RAG evaluation frameworks require substantial human annotations for scoring.\nARES significantly improves data efficiency during evaluation by only requiring three inputs: an indomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data generation.\n\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question\u2013answer\npairs, derived from the passages in the corpus. Sec\n- nd, it defines three separate judge models to perform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores\nthe different RAG systems being assessed using\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals\nfor RAG scoring. PPI utilizes a small set of human\nannotated datapoints for computing its confidence\nintervals; we designate this annotated set as our _hu-_\n_man preference validation set_, which is composed\n\n- f approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an\nswer relevance.\n\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating existing automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occurrences in the AIS attribution dataset (Rashkin et al.,\n2022), predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach.","1344x1344) at any aspect ratio. We use MiniCPM-V 2.0 to\nbuild VisRAG-Ret (Table 2) and VisRAG-Gen (Table 3(b)), as well as the document parsing model.\n\n\n**MiniCPM-V 2.6** (OpenBMB, 2024b; Yao et al., 2024) is an upgrade of MiniCPM-V 2.0 and\nMiniCPM-Llama3-V 2.5 (Yao et al., 2024). It is built upon SigLIP-400M and Qwen2-7B (Yang\net al., 2024) with a total of 8.5B parameters, exihibiting a significant performance improvement\n\n- ver MiniCPM-Llama3-V 2.5 (Yao et al., 2024). Different from previous models, MiniCPM-V\n2.6 can accept multiple images as the input and perform multi-modal in-context learning. It also\ndemonstrates stronger OCR capabilities. We use MiniCPM-V 2.6 to build VisRAG-Gen (Table 3)\nand a text-based generation baseline MiniCPM-V 2.6 (OCR) (Figure 3, Figure 5).\n\n\nNote that, MiniCPM-Llama3-V 2.5 (Yao et al., 2024) is not used in this paper.\n\n\n**GPT-4o** (OpenAI, 2024) is OpenAI\u2019s latest multi-modal model, capable of processing any combination of text, audio, image, and video inputs and generating outputs in text, audio, and image\nformats. We use GPT-4o to construct VisRAG-Gen (Table 3) and to synthesize training data.\n\n\nD RETRIEVAL PERFORMANCE IN RECALL@10\n\n\nTable 6 presents the retrieval performance in Recall@10.\n\n\nE PROMPTS FOR GENERATION\n\n\nWe present the prompts of VisRAG-Gen and TextRAG-Gen in Table 7.\n\n\n2https:\/\/huggingface.co\/HuggingFaceM4\/siglip-so400m-14-980-flash-attn2-navit\n\n\n20\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 6: Overall retrieval performance in Recall@10.\n\n\nTable 7: Prompt templates for generation. \u201c"],"35":["LM judges\n\n- n synthetically generated queries and answers.\nARES can evaluate each component of a RAG system separately to help improve system understanding and create targeted solutions, and it requires\n\n- nly minimal human annotations. For the eight different datasets in KILT, SuperGLUE, and AIS requiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\n\n- n context relevance, answer faithfulness, and answer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\n\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing\n\n\nlogits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n\n\n**7** **Limitations**\n\n\nARES relies on a small set of annotations in the\n\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\n\n- ften require an annotator familiar with the RAG\nsystem\u2019s domain application. While these annotations can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our evaluation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\n\n- ther languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\n\n\n**References**\n\n\nMubashara Ak","## **ARES: An Automated Evaluation Framework for Retrieval-Augmented** **Generation Systems**\n\n\n\n**Jon Saad-Falcon**\nStanford University _[\u2217]_\n\njonsaadfalcon@stanford.edu\n\n\n**Christopher Potts**\nStanford University\ncgpotts@stanford.edu\n\n\n**Abstract**\n\n\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an _Automated RAG Evaluation_\n_System_, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By creating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and\/or documents used in the evaluated RAG systems. We\nmake our code and datasets publicly available\n\n[on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to generate a response. This formulation admits a multitude\n\n- f choices: what retrieval model to use, how to di\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab","ACL [56] [56]\nMath GSM8K [158] [73]\nMachine Translation JRC-Acquis [159] [17]\n\n\n14\n\n\n\nTABLE III\n\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\n\n\nContext Faithfulness Answer Noise Negative Information Counterfactual\nRelevance Relevance Robustness Rejection Integration Robustness\n\n\nAccuracy \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nEM \u2713\n\nRecall \u2713\n\nPrecision \u2713 \u2713\n\nR-Rate \u2713\n\nCosine Similarity \u2713\nHit Rate \u2713\n\nMRR \u2713\n\nNDCG \u2713\n\nBLEU \u2713 \u2713 \u2713\n\nROUGE\/ROUGE-L \u2713 \u2713 \u2713\n\n\n\nThe specific metrics for each evaluation aspect are summarized in Table III. It is essential to recognize that these\nmetrics, derived from related work, are traditional measures\nand do not yet represent a mature or standardized approach for\nquantifying RAG evaluation aspects. Custom metrics tailored\nto the nuances of RAG models, though not included here, have\nalso been developed in some evaluation studies.\n\n\n_D. Evaluation Benchmarks and Tools_\n\n\nA series of benchmark tests and tools have been proposed\nto facilitate the evaluation of RAG.These instruments furnish\n\nquantitative metrics that not only gauge RAG model performance but also enhance comprehension of the model\u2019s capabilities across various evaluation aspects. Prominent benchmarks\nsuch as RGB, RECALL and CRUD [167]\u2013[169] focus on\nappraising the essential abilities of RAG models. Concurrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens [8] employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\n\n\nVII. DISCUSSION AND FUTURE PROSPECTS\n\n\nDespite the considerable progress in RAG technology, several challenges persist that warrant in-depth research.This\nchapter will mainly introduce the current challenges and future\nresearch directions faced by RAG.\n\n\n_A. RAG vs Long Context_\n\n\nWith the deepening of related research, the context of LLMs\nis continuously expanding [170]\u2013[172]. Presently, LLMs can\neffortlessly manage contexts exceeding 200,000 tokens [9] . This\ncapability signifies that long-document question answering,\npreviously reliant on RAG, can now incorporate"," (is the response generated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\n\n\nRAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\n\nMany existing RAG evaluation frameworks require substantial human annotations for scoring.\nARES significantly improves data efficiency during evaluation by only requiring three inputs: an indomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data generation.\n\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question\u2013answer\npairs, derived from the passages in the corpus. Sec\n- nd, it defines three separate judge models to perform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores\nthe different RAG systems being assessed using\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals\nfor RAG scoring. PPI utilizes a small set of human\nannotated datapoints for computing its confidence\nintervals; we designate this annotated set as our _hu-_\n_man preference validation set_, which is composed\n\n- f approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an\nswer relevance.\n\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating existing automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occurrences in the AIS attribution dataset (Rashkin et al.,\n2022), predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach."," LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab@stanford.edu\n\n\n**Matei Zaharia**\n\nDatabricks and UC Berkeley\nmatei@databricks.com\n\n\nThe best design for a RAG system is not necessarily universal across data domains, corpus sizes,\nand cost\/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand annotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alternatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strategy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the _relevance_ - f retrieved information and the _faithfulness_ and _accuracy_ - f generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accurately, we propose ARES, the **A** utomated **R** AG\n**E** valuation **S** ystem. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, leading to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RAGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered inference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response generated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\n\n\nRAG system finds relevant contexts and generates\nanswers that are both faithful and relevant"],"36":[" (DisambigF1), ROUGE (Lin, 2004), and an overall score combining Disambig-F1 and ROUGE (DR).\n\n\n**Open-domain summarization** The goal of opendomain summarization is to generate a comprehensive summary about a topic by gathering information from open web (Giorgi et al., 2022). We use\nWikiAsp (Hayashi et al., 2021) which aims to generate aspect-based summaries about entities from\n20 domains in Wikipedia, e.g., \u201cGenerate a summary about Echo School (Oregon) including the\n\n\nNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp\n\n\nFigure 4: Comparision between FLARE and baselines across all tasks\/datasets. We report the primary metric for\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\n\n\n\nfollowing aspects: academics, history.\u201d Experimental setting details are included in Appendix B.\nMetrics include ROUGE, named entity-based F1,\nand UniEval (Zhong et al., 2022) which measures\nfactual consistency.\n\n\n**6** **Experimental Results**\n\n\nWe first report overall results across 4 tasks\/datasets\nand compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\n\nrun ablation experiments to study the efficacy of\nvarious design choices of our method.\n\n**6.1** **Comparison with Baselines**\n\n**Overall results.** The overall performance of\nFLARE and baseline across all tasks\/datasets are\n\nreported in Figure 4. FLARE outperforms all baseline on all tasks\/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks, multihop QA shows the\nmost significant improvement. This is largely due\nto the task\u2019s clear definition and specific objective\n\n- f producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to\ngenerate on-topic output. In contrast, ASQA and\nWikiAsp are more open-ended, which increases the\ndifficulty of both generation and evaluation. The\n"," factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n\n\n**6.2** **Ablation Study**\n\n\n**Importance of forward-looking retrieval.** We\nfirst validate that forward-looking retrieval is more\neffective than past-context-based retrieval. We run\nablation experiments on 2WikiMultihopQA and\nASQA-hint comparing retrieval using the previ\n- us versus the next sentence. Specifically, both\nmethods retrieve every sentence and directly use\nthe complete previous\/next sentence as queries. As\nshown in Table 3, using the next sentence to retrieve\nis clearly better than using the previous sentence,\nconfirming our hypothesis.\nWe also run previous-window approaches using\ndifferent numbers of past tokens as queries. As\nshown in Table 4, using too many tokens ( _>_ 32) in\n\n\n\nFigure 5: Performance (EM) of FLARE with respect\nto the percentage of steps\/sentences with retrieval on\n2WikiMultihopQA and StrategyQA.\n\n\nthe past hurts the performance, further confirming\n\n- ur hypothesis that previous context might not be\nrelevant to intent of future generations.\n\n\n**Importance of active retrieval.** Next, we investigate how active retrieval threshold _\u03b8_ affects performance. To alter our method from not retrieving\nto retrieving every sentence, we adjust the confidence threshold _\u03b8_ that determines when to trigger\nretrieval from 0 to 1. We then calculate the proportion of steps\/sentences where retrieval is activated, and present the performance based on it. As\nshown in Figure 5, on 2WikiMultihopQA, the performance plateaus when the retrieval percentage\nexceeds 60%, indicating that retrieval when LMs\nare confident is not necessary. On StrategyQA, the\nperformance drops when the retrieval percentage\nexceeds 50%, indicating that unnecessary retrieval\ncan introduce noise and impede the original generation process. We found triggering retrieval for\n40%-80% of sentences usually leads to a good performance across tasks\/datasets.\n\n\n**Effectiveness of different query formulation**\n**methods** We study implicit query formation by\nmasking and explicit query formulation through\nquestion generation. In Table 5, we compare the\nperformance of FLARE with different masking\n\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n","## **Active Retrieval Augmented Generation**\n\n**Zhengbao Jiang** [1] _[\u2217]_ **Frank F. Xu** [1] _[\u2217]_ **Luyu Gao** [1] _[\u2217]_ **Zhiqing Sun** [1] _[\u2217]_ **Qian Liu** [2]\n\n**Jane Dwivedi-Yu** [3] **Yiming Yang** [1] **Jamie Callan** [1] **Graham Neubig** [1]\n\n1Language Technologies Institute, Carnegie Mellon University\n2Sea AI Lab 3FAIR, Meta\n{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu\n\n\n\n**Abstract**\n\n\nDespite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one\npromising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate\nsetup that only retrieves information once based\n\n  - n the input. This is limiting, however, in\nmore general scenarios involving generation\n\n  - f long texts, where continually gathering information throughout generation is essential. In\nthis work, we provide a generalized view of _ac-_\n_tive retrieval augmented generation_, methods\nthat actively decide when and what to retrieve\nacross the course of the generation. We propose\n**F**  - rward- **L**  - oking **A** ctive **RE** trieval augmented\ngeneration ( **FLARE** ), a generic method which\niteratively uses a prediction of the upcoming\nsentence to anticipate future content, which is\nthen utilized as a query to retrieve relevant documents to regenerate the sentence if it contains\nlow-confidence tokens. We test FLARE along\nwith baselines comprehensively over 4 longform knowledge-intensive generation tasks\/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating\nthe effectiveness of our method. [1]\n\n\n**1** **Introduction**\n\n\nGenerative language models (LMs) (Brown et al.,\n2020; Ouyang et al., 2022; OpenAI, 2023; Chowdhery et al., 2022; Zhang et al., 2022; Touvron et al"," **RE** trieval augmented generation\n( **FLARE** ), as illustrated in Figure 1. FLARE iteratively generates _a temporary next sentence_, use\nit as the query to retrieve relevant documents _if it_\n_contains low-probability tokens_ and regenerate the\nnext sentence until reaches the end.\n\n\nFLARE is applicable to any existing LMs at\ninference time without additional training. Con\n\nsidering the impressive performance achieved by\nGPT-3.5 (Ouyang et al., 2022) on a variety of\ntasks, we examine the effectiveness of our meth\n- ds on text-davinci-003. We evaluate FLARE\n\n- n 4 diverse tasks\/datasets involving generating\nlong outputs, including multihop QA (2WikiMultihopQA), commonsense reasoning (StrategyQA),\nlong-form QA (ASQA), and open-domain summarization (WikiAsp) (Ho et al., 2020; Geva et al.,\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\nOver all tasks, FLARE achieves superior or competitive performance compared to single-time and\nmulti-time retrieval baselines, demonstrating the\neffectiveness and generalizability of our method.\n\n\n**2** **Retrieval Augmented Generation**\n\n\nWe formally define single-time retrieval augmented\ngeneration and propose the framework of active\nretrieval augmented generation.\n\n\n**2.1** **Notations and Definitions**\n\n\nGiven a user input _**x**_ and a document corpus _D_ =\n_{_ _**d**_ _i}_ _[|D|]_ _i_ =1 [(such as all Wikipedia articles), the goal of]\nretrieval augmented LMs is to generate the answer\n_**y**_ = [ _**s**_ 1 _,_ _**s**_ 2 _, ...,_ _**s**_ _m_ ] = [ _w_ 1 _, w_ 2 _, ..., wn_ ] containing\n_m_ sentences or _n_ tokens leveraging information\nretrieved from the corpus.\nIn retrieval augmented LM, the LM typically\npairs with a retriever that can retrieve a list of\ndocuments _D_ _**q**_ = ret( _**q**_ ) for a query _**q**_ ; the LM\nconditions on both the user input","-time\nretrieval augmented LMs, which is the most relevant area to this paper.\n\n\n**Iterative and adaptive retrieval** Iterative retrieval and refinement has been studied in both\ntext and code generation tasks (Peng et al., 2023;\nZhang et al., 2023; Zemlyanskiy et al., 2022; Yu\net al., 2023). FLARE differs from these methods in\nthe granularity of generation and retrieval strategies.\nAdaptive retrieval has been studied in single-time\nretrieval scenarios based on either question popularity or generation probabilities (Mallen et al.,\n2022; Li et al., 2023), while we focus on long-form\ngeneration requiring active information access.\n\n\n**Browser-enhanced LMs** WebGPT (Nakano\net al., 2021) and WebCPM (Qin et al., 2023) train\nLMs to interact with browser to enhance factuality\nusing reinforcement learning or supervised training where multiple queries can be triggered before\ngeneration. FLARE is built on text-based retrievers\nbut can be combined with a browser to potentially\nimprove retrieval quality.\n\n\n\n**8** **Conclusion**\n\n\nTo aid long-form generation with retrieval augmentation, we propose an active retrieval augmented generation framework that decides when\nand what to retrieve during generation. We implement this framework with forward-looking active\nretrieval that iteratively uses the upcoming sentence\nto retrieve relevant information if it contains low\nconfidence tokens and regenerates the next sentence. Experimental results on 4 tasks\/datasets\ndemonstrate the effectiveness of our methods. Fu\nture directions include better strategies for active\nretrieval and developing efficient LM architectures\nfor active information integration.\n\n\n**9** **Limitations**\n\n\nWe also conduct experiments on Wizard of\nWikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,\n\n2019), and found that FLARE did not provide significant gains. Wizard of Wikipedia is a knowledgeintensive dialogue generation dataset where the out\n_\u223c_\nput is relatively short ( 20 tokens on average) so\nretrieving multiple disparate pieces of information\nmight not be necessary. ELI5 (Fan et al., 2019)\nis a long-form QA dataset requiring in-depth answers to open-ended questions. Due to issues mentioned in Krishna et al. (2021) such as difficulties\n\n-"],"37":["\u223c_\n\n- verlaps between chunks ( 1 million tokens).\n\n\n**News articles** . A benchmark dataset comprised of news articles published from September 2013\nto December 2023 in a range of categories, including entertainment, business, sports, technology,\nhealth, and science (Tang and Yang, 2024). The corpus is divided into 3197 _\u00d7_ 600-token text\n\n_\u223c_\nchunks, with 100-token overlaps between chunks ( 1.7 million tokens).\n\n\n**4.1.2** **Conditions**\n\n\nWe compared six conditions including GraphRAG at four different graph community levels ( **C0**,\n**C1**, **C2**, **C3** ), a text summarization method that applies our map-reduce approach directly to source\ntexts ( **TS** ), and a vector RAG \u201csemantic search\u201d approach ( **SS** ):\n\n\n    - **CO** . Uses root-level community summaries (fewest in number) to answer user queries.\n\n\n    - **C1** . Uses high-level community summaries to answer queries. These are sub-communities\n\n     - f C0, if present, otherwise C0 communities projected downwards.\n\n\n    - **C2** . Uses intermediate-level community summaries to answer queries. These are subcommunities of C1, if present, otherwise C1 communities projected downwards.\n\n\n    - **C3** . Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected downwards.\n\n\n    - **TS** . The same method as in Section 3.1.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n\n\n    - **SS** . An implementation of vector RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\n\n\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\n\n\nThe graph index supporting conditions **C0** - **C3** was created using our generic prompts for entity and\nrelationship extraction, with entity types and few-shot examples tailored to the domain of the data.\n\n\n8\n\n\n**4","Microsoft, 2023)\nand intelligence analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an\nimplicit and interconnected set of concepts, an LLM can generate a summary that answers the query.\nThe challenge, however, arises when the volume of data requires a RAG approach, since vector RAG\napproaches are unable to support sensemaking over an entire corpus.\n\n\nIn this paper, we present **GraphRAG** - a graph-based RAG approach that enables sensemaking over\nthe entirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph,\nwhere nodes correspond to key entities in the corpus and edges represent relationships between those\nentities. Next, it partitions the graph into a hierarchy of communities of closely related entities,\nbefore using an LLM to generate community-level summaries. These summaries are generated in\na bottom-up manner following the hierarchical structure of extracted communities, with summaries\nat higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these\ncommunity summaries provide global descriptions and insights over the corpus. Finally, GraphRAG\nanswers queries through map-reduce processing of community summaries; in the map step, the\nsummaries are used to provide partial answers to the query independently and in parallel, then in the\nreduce step, the partial answers are combined and used to generate a final global answer.\n\n\nThe GraphRAG method and its ability to perform global sensemaking over an entire corpus form\nthe main contribution of this work. To demonstrate this ability, we developed a novel application\n\n- f the LLM-as-a-judge technique (Zheng et al., 2024) suitable for questions targeting broad issues\nand themes where there is no ground-truth answer. This approach first uses one LLM to generate\na diverse set of global sensemaking questions based on corpus-specific use cases, before using a\nsecond LLM to judge the answers of two different RAG systems using predefined criteria (defined\nin Section 3.3). We use this approach to compare GraphRAG to vector RAG on two representative\nreal-world text datasets. Results show GraphRAG strongly outperforms vector RAG when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https:\/\/github _._ [com\/microsoft\/graphrag. In ad-](https:\/\/github.com\/microsoft\/graphrag)\ndition, versions","hi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n(e.g., passages, tables) and edges encoding lexical and semantical similarity or structural relationships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of\ngraphs in this context: their inherent _modularity_ (Newman, 2006) and the ability to partition graphs\ninto nested modular communities of closely related nodes (e.g., Louvain, Blondel et al. 2008; Leiden, Traag et al. 2019). Specifically, GraphRAG recursively creates increasingly global summaries\nby using the LLM to create summaries spanning this community hierarchy.\n\n\n**2.3** **Adaptive benchmarking for RAG Evaluation**\n\n\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthese benchmarks are oriented towards vector RAG performance, i.e., they evaluate performance\n\n- n explicit fact retrieval. In this work, we propose an approach for generating a set of questions\nfor evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM\nmethods that use a corpus to generate questions whose answers would be summaries of the corpus,\nsuch as in Xu and Lapata (2021). However, in order to produce a fair evaluation, our method avoids\ngenerating the questions directly from the corpus itself (as an alternative implementation, one can\nuse a subset of the corpus held out from subsequent graph extraction and answer evaluation steps).\n\n\n_Adaptive benchmarking_ refers to the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases. Recent work has used LLMs for adaptive benchmarking\nto ensure relevance, diversity, and alignment with the target application or task (Yuan et al., 2024;\nZhang et al., 2024b). In this work, we propose an adaptive benchmarking approach to generating\nglobal sensemaking queries for the LLM. Our approach builds on prior work in LLM-based persona\ngeneration, where the LLM is used to generate diverse and authentic sets of personas (","2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao et al., 2023; Khattab et al., 2022; Shao et al., 2023; Su et al., 2020; Trivedi et al., 2022;\nWang et al., 2024). In particular, GraphRAG is similar to other approaches that use hierarchical\nindexing to create summaries (similar to Kim et al. 2023; Sarthi et al. 2024). GraphRAG contrasts\nwith these approaches by generating a graph index from the source data, then applying graph-based\ncommunity detection to create a thematic partitioning of the data.\n\n\n**2.2** **Using Knowledge Graphs with LLMs and RAG**\n\n\nApproaches to knowledge graph extraction from natural language text corpora include rulematching, statistical pattern recognition, clustering, and embeddings (Etzioni et al., 2004; Kim et al.,\n2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of\nresearch that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022;\nOpenAI, 2023; Tan et al., 2017; Trajanoska et al., 2023; Yao et al., 2023; Yates et al., 2007; Zhang\net al., 2024a). It also adds to a growing body of RAG approaches that use a knowledge graph as\nan index (Gao et al., 2023). Some techniques use subgraphs, elements of the graph, or properties\n\n- f the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023)\n\n- r as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n","Reflection**\n\n\n\nThe choice of prompt engineering techniques has a strong impact on the quality of knowledge graph\nextraction (Zhu et al., 2024), and different techniques have different costs in terms of tokens consumed and generated by the model. _Self-reflection_ is a prompt engineering technique where the\nLLM generates an answer, and is then prompted to evaluate its output for correctness, clarity, or\ncompleteness, then finally generate an improved response based on that evaluation (Huang et al.,\n2023; Madaan et al., 2024; Shinn et al., 2024; Wang et al., 2022). We leverage self-reflection in\nknowledge graph extraction, and explore ways how removing self-reflection affects performance\nand cost.\n\n\nUsing larger chunk size is less costly in terms of calls to the LLM. However, the LLM tends to\nextract few entities from chunks of larger size. For example, in a sample dataset (HotPotQA, Yang\net al., 2018), GPT-4 extracted almost twice as many entity references when the chunk size was 600\ntokens than when it was 2400. To address this issue, we deploy a self-reflection prompt engineering\napproach. After entities are extracted from a chunk, we provide the extracted entities back to the\nLLM, prompting it to \u201cglean\u201d any entities that it may have missed. This is a multi-stage process\nin which we first ask the LLM to assess whether all entities were extracted, using a logit bias of\n100 to force a yes\/no decision. If the LLM responds that entities were missed, then a continuation\nindicating that \u201cMANY entities were missed in the last extraction\u201d encourages the LLM to detect\nthese missing entities. This approach allows us to use larger chunk sizes without a drop in quality\n(Figure 3) or the forced introduction of noise. We interate self-reflection steps up to a specified\nmaximum number of times.\n\n\n30000\n\n\n\n20000\n\n\n\n\n\n10000\n\n\n0\n\n|Col1|Col2|600chunksize<br>1200chunksize|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||600 chunk size<br>1200 chunk size<br>|600 chunk size<br>1200 chunk size<br>|600 chunk size<br>1200 chunk size<br>|600 chunk"],"38":[" [51, 52] propose a single, pre-trained encoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nand generative tasks. Our work aims to expand the space of possible tasks with a single, unified\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\n\n\n**Learned Retrieval** There is significant work on learning to retrieve documents in information\nretrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\nwork optimizes the retrieval module to aid in a specific, downstream task such as question answering,\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\ncan be fine-tuned for strong performance on a variety of tasks.\n\n\n**Memory-based Architectures** Our document index can be seen as a large external memory for\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\nwork. Other work improves the ability of dialog models to generate factual text by attending over\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\u2019s\nmemory by editing the document index. This approach has also been used in knowledge-intensive\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\nrather than end-to-end learnt retrieval [9].\n\n\n**Retrieve-and-Edit approaches** Our method shares some similarities with retrieve-and-edit style\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\nto provide a final output. These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content",", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop\/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation"," the supervisory signal,\neliminating the need for specific cross-attention mechanisms.\nMoreover, inspired by RLHF (Reinforcement Learning from\nHuman Feedback), utilizing LM-based feedback to reinforce\nthe retriever through reinforcement learning.\n\n\n_E. Adapter_\n\n\nFine-tuning models may present challenges, such as integrating functionality through an API or addressing constraints arising from limited local computational resources.\nConsequently, some approaches opt to incorporate an external\nadapter to aid in alignment.\nTo optimize the multi-task capabilities of LLM, UPRISE [20] trained a lightweight prompt retriever that can\nautomatically retrieve prompts from a pre-built prompt pool\nthat are suitable for a given zero-shot task input. AAR\n(Augmentation-Adapted Retriver) [47] introduces a universal\nadapter designed to accommodate multiple downstream tasks.\nWhile PRCA [69] add a pluggable reward-driven contextual\nadapter to enhance performance on specific tasks. BGM [26]\nkeeps the retriever and LLM fixed,and trains a bridge Seq2Seq\nmodel in between. The bridge model aims to transform the\nretrieved information into a format that LLMs can work with\n\neffectively, allowing it to not only rerank but also dynamically select passages for each query, and potentially employ\nmore advanced strategies like repetition. Furthermore, PKG\n\n\nintroduces an innovative method for integrating knowledge\ninto white-box models via directive fine-tuning [75]. In this\napproach, the retriever module is directly substituted to generate relevant documents according to a query. This method\nassists in addressing the difficulties encountered during the\nfine-tuning process and enhances model performance.\n\n\nIV. GENERATION\n\n\nAfter retrieval, it is not a good practice to directly input all\nthe retrieved information to the LLM for answering questions.\nFollowing will introduce adjustments from two perspectives:\nadjusting the retrieved content and adjusting the LLM.\n\n\n_A. Context Curation_\n\n\nRedundant information can interfere with the final generation of LLM, and overly long contexts can also lead LLM\nto the \u201cLost in the middle\u201d problem [98]. Like humans, LLM\ntends to only focus on the beginning and end of long texts,\nwhile forgetting the middle portion. Therefore, in the RAG\nsystem, we typically need to further process the retrieved\n\ncontent.\n\n_1) Reranking:_ Reranking fundamentally reorders document\nchunks to highlight the most pertinent results","., 2018; Bapna and Firat,\n2019). Since the attention mechanism is becoming\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\nkey module in lots of NLP models, integrating\nretrieved memory through attention becomes a\nvery nature and efficient way.\n\n\n**Skeleton Extraction** In the previous two meth\n- ds, the downstream generation model learns how\nto filter out irrelevant or even harmful information from the retrieved examples implicitly. There\nalso exist some works that try to explicitly extract\nuseful information, i.e., _skeleton extraction_, from\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\n2019; Cai et al., 2019b). For example, one skeleton\nshould be a part of a whole utterance with irrelevant\ncontent masked, and the generation model only integrate this skeleton in the generation process.\n\n\n**3** **Dialogue Response Generation**\n\n\n**Background** Dialogue systems can be grouped\ninto two categories: chit-chat systems and task\n- riented systems. While task-oriented dialogue\nsystems are designed to accomplish specific user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and fluent response for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity\n\n- f possible responses to a single dialogue history\n(i.e., the _one-to-many_ problem). The dialogue history alone cannot decide a meaningful and specific\nresponse. Also, external knowledge that is not\npresent in the dialogue history are often necessary\nfor avoiding safe but boring responses. We focus\n\n- n recent efforts tackling the challenges to develop\nchit-chat dialogue systems.\nMost modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrievalbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from curated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa\n\ntions and possibly post-edited by a human. However, such systems perform poorly when a given\ndialogue history is substantially different from\nthose in the retrieval pool. On the","downstream models, our system can match the best published result while being 10x faster.\n\n\n2 METHOD\n\n\n2.1 PROBLEM DEFINITION\n\n\nThe retrieval task considered in this work can be described as follows (see also Figure 1). Given a\nmulti-hop question _q_ and a large text corpus _C_, the retrieval module needs to retrieve a sequence of\npassages _Pseq_ : _{p_ 1 _, p_ 2 _, ..., pn}_ that provide _sufficient_ information for answering _q_ . Practically, the\nretriever returns the _k_ best-scoring sequence candidates, _{Pseq_ [1] _[,][ P]_ _seq_ [2] _[, ...,][ P]_ _seq_ _[k]_ _[}]_ [ (] _[k][ \u226a|C|]_ [), with the]\nhope that at least one of them has the desired qualities. _k_ should be small enough for downstream\nmodules to process in a reasonable time while maintaining adequate recall. In general, retrieval also\nneeds to be efficient enough to handle real-world corpora containing millions of documents.\n\n\n2.2 MULTI-HOP DENSE RETRIEVAL\n\n\n**Model** Based on the sequential nature of the multi-hop retrieval problem, our system solves it in an\niterative fashion. We model the probability of selecting a certain passage sequence as follows:\n\n\n\n_P_ ( _Pseq|q_ ) =\n\n\n\n_n_\n\n- _P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) _,_\n\n\n_t_ =1\n\n\n2\n\n\nPublished as a conference paper at ICLR 2021\n\n\nwhere for _t_ = 1, we only condition on the original question for retrieval. At each retrieval step, we\nconstruct a new query representation based on previous results and the retrieval is implemented as\nmaximum inner product search over the dense representations of the whole corpus:\n\n\nexp ( _\u27e8_ _**p**_ _t,_ _**q**_ _t\u27e9_ )\n_P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) =\n\n~~\ufffd~~ _p\u2208C_ [exp (] _[\u27e8]_ _**[p]**_ _[,]_ _**[ q]**_ _[t][\u27e9]_ [)] _[,]_"],"39":["Ms) have seen improvements in\nvarious components: the retriever, the reader, and end-to-end system training. Retrieval methods\nhave transitioned from traditional term-based techniques like **TF-IDF** (Sp\u00a8arck Jones, 1972) and\n**BM25** (Robertson et al., 1995; Roberts et al., 2020) to deep learning\u2013based strategies (Karpukhin\net al., 2020; Khattab & Zaharia, 2020; Sachan et al., 2023). Some recent work proposes using\nlarge language models as retrievers due to their ability to memorize extensive knowledge (Yu et al.,\n2022; Sun et al., 2022). Research on the reader component includes **Fusion-in-Decoder (FiD)**\n(Izacard & Grave, 2022), which employs both DPR and BM25 for retrieval and processes passages\nindependently in the encoder and **RETRO** (Borgeaud et al., 2022; Wang et al., 2023), which utilizes\ncross-chunked attention and chunkwise retrieval to generate text grounded on retrieved context.\n\n\nEnd-to-end system training work includes **Atlas** (Izacard et al., 2022), which fine-tunes an encoderdecoder model in conjunction with the retriever; **REALM** (Guu et al., 2020), a bidirectional, masked\nLM fine-tuned for open-domain question answering; and **RAG (Retrieval-Augmented Genera-**\n**tion)** (Lewis et al., 2020), which integrates pre-trained sequence-to-sequence models with a neural\nretriever. Min et al. (2021) introduced **Joint Passage Retrieval (JPR)** model which uses a treedecoding algorithm to handle passage diversity and relevance in multi-answer retrieval. **Dense Hi-**\n**erarchical Retrieval (DHR)** and **Hybrid Hierarchical Retrieval (HHR)** represent advancements\nin retrieval accuracy by combining document and passage level retrievals and integrating sparse and\ndense retrieval methods, respectively (Liu et al., 2021; Arivazhagan et al., 2023).\n\n\n[1We will release the code of RAPTOR publicly here.](https:\/\/github.com\/parthsarthi03\/raptor)\n\n\n2\n\n\nPublished as a conference paper at",".1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7 **81.3**\n\n\nDPR **79.4** 78.8 **75.0** **89.1** 51.6 **86.0** 84.7 **82.9** 93.9 67.6\nMulti\nBM25 + DPR 78.0 **79.9** 74.7 88.5 66.2 83.9 84.4 82.3 **94.1** 78.6\n\n\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20\/100 retrieved\npassages that contain the answer. _Single_ and _Multi_ denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.\n\n\n\ntraditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10 _[\u2212]_ [5] using Adam, linear\nscheduling with warm-up and dropout rate 0 _._ 1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be desirable to obtain a single retriever that works well\nacross the board. To this end, we train a _multi_ dataset encoder by combining training data from\nall datasets excluding SQuAD. [8] In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages"," positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically, we get top 100 passages for each\nquestion in a mini-batch and use the method similar\nto in-batch negative training: all retrieved passages\u2019\nvectors participate in the loss calculation for _all_\nquestions in a batch. Our training batch size is set\nto 16, which effectively gives 1,600 passages per\nquestion to calculate retriever loss. The reader still\nuses 24 passages per question, which are selected\n\n\n**Question** **Passage received by BM25** **Passage retrieved by DPR**\n\n\n\nWhat is the body of water\nbetween England and Ireland?\n\n\nWho plays Thoros of Myr in\nGame of Thrones?\n\n\n\nTitle:British Cycling Title: Irish Sea\n... **England** is not recognised as a region by the UCI, and . . . Annual traffic between Great Britain and **Ireland** amounts\nthere is no English cycling team outside the Commonwealth to over 12 million passengers and of traded goods. **The Irish**\nGames. For those occasions, British Cycling selects and sup- **Sea** is connected to the North Atlantic at both its northern\nports the **England** team. Cycling is represented on the Isle and southern ends. To the north, the connection is through\n\n- f Man by the Isle of Man Cycling Association. Cycling in the North Channel between Scotland and Northern **Ireland**\nNorthern **Ireland** is organised under Cycling Ulster, part of and the Malin Sea. The southern end is linked to the Atlantic\nthe all-Ireland governing **body** Cycling **Ireland** . Until 2006, through the St George\u2019s Channel between Ireland and Pema rival governing **body** existed, . . . brokeshire, and the Celtic Sea. . . .\n\n\n\nTitle: No One (Game of Thrones) Title: P\u02daal Sverre Hagen\n...He may be \u201dno one,\u201d but there\u2019s still enough of a person P\u02daal Sverre Valheim Hagen (born 6 November 1980) is a Norleft in him to respect, and admire who this girl is and what wegian stage and screen actor. He appeared in the Norweshe\u2019s become."," 1) =\n\n~~\ufffd~~ _p\u2208C_ [exp (] _[\u27e8]_ _**[p]**_ _[,]_ _**[ q]**_ _[t][\u27e9]_ [)] _[,]_ [ where] _**[ q]**_ _[t]_ [ =] _[ g]_ [(] _[q, p]_ [1] _[, ..., p][t][\u2212]_ [1][)][ and] _**[ p]**_ _[t]_ [ =] _[ h]_ [(] _[p][t]_ [)] _[.]_\n\n\nHere _\u27e8\u00b7, \u00b7\u27e9_ is the inner product between the query and passage vectors. _h_ ( _\u00b7_ ) and and _g_ ( _\u00b7_ ) are\npassage and query encoders that produce the dense representations. In order to reformulate the\nquery representation to account for previous retrieval results at time step _t_, we simply concatenate\nthe question and the retrieved passages as the inputs to _g_ ( _\u00b7_ ). Note that our formulation for each\nretrieval step is similar to existing single-hop dense retrieval methods (Lee et al., 2019; Guu et al.,\n2020; Karpukhin et al., 2020) except that we add the query reformulation process conditioned on\nprevious retrieval results. Additionally, instead of using a bi-encoder architecture with separately\nparameterized encoders for queries and passages, we use a shared RoBERTa-base (Liu et al., 2019)\nencoder for both _h_ ( _\u00b7_ ) and _g_ ( _\u00b7_ ). In \u00a73.1.3, we show this simple modification yields considerable\nimprovements. Specifically, we apply layer normalization over the start token\u2019s representations from\nRoBERTa to get the final dense query\/passage vectors.\n\n\n**Training and Inference** The retriever model is trained as in Karpukhin et al. (2020), where each\ninput query (which at each step consists of a question and previously retrieved passages) is paired\nwith a positive passage and _m_ negative passages to approximate the softmax over all passages. The\npositive passage is the gold annotated evidence at step _t_ . Negative passages are a combination of\npassages in the current batch which correspond to other questions (","**\n\n\nBoth datasets assume 2 hops, so we fix _n_ = 2 for all experiments. Since HotpotQA does not provide\nthe order of the passage sequences, as a heuristic, we consider the passage that includes the answer\nspan as the final passage. [2] In \u00a73.1.3, we show that the order of the passages is important for effective\nretriever training. The hyperparameters can be found in Appendix B.1.\n\n\n3.1 EXPERIMENTS: RETRIEVAL\n\n\nWe evaluate our multi-hop dense retriever (MDR) in two different use cases: _direct_ and _reranking_,\nwhere the former outputs the top- _k_ results directly using the retriever scores and the latter applies a\ntask-specific reranking model to the initial results from MDR.\n\n\n3.1.1 DIRECT\n\n\nWe first compare MDR with several efficient retrieval methods that can directly find the top- _k_ passage\nsequences from a large corpus, including TF-IDF, TF-IDF + Linked, DrKIT and Entity Linking. **TF-**\n**IDF** is the standard term-matching baseline, while **TF-IDF + Linked** is a straightforward extension\nthat also extracts the hyperlinked passages from TF-IDF passages, and then reranks both TF-IDF\nand hyperlinked passages with BM25 [3] scores. **DrKIT** (Dhingra et al., 2020) is a recently proposed\ndense retrieval approach, which builds a entity-level (mentions of entities) dense index for retrieval.\nIt relies on hyperlinks to extract entity mentions and prunes the search space with a binary mask\nthat restricts the next hop to using hyperlinked entities. On FEVER, we additionally consider an\nentity linking baseline (Hanselowski et al., 2018) that is commonly used in existing fact verification\npipelines. This baseline first uses a constituency parser to extract potential entity mentions in the fact\nclaim and then uses the MediaWiki API to search documents with titles that match the mentions.\n\n\nTable 1 shows the performance of different retrieval methods. On HotpotQA the metric is recall at the\ntop _k_ paragraphs [4], while on FEVER the metrics are precision, recall and F1 in order to be consistent\nwith previous results. On both datasets, MDR substantially outperforms all baselines.\n\n\n3.1."],"40":[" in improving\nfactuality and citation accuracy for long-form generations relative to these models. [1]\n\n\n1 INTRODUCTION\n\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces **Self-Reflective Retrieval-augmented Generation (SELF-RAG)** to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., _reflection tokens_ ). Reflection tokens are categorized into _retrieval_ and _critique_ tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a **retrieval** token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and then **generating** corresponding task outputs (Step\n2). It then generates critique tokens to **criticize** its own output and choose best one (Step 3) in terms\n\n- f factuality and overall quality. This process differs from conventional RAG ("," question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self-reflection and customizable inference.\n\n\n**Training and generating with critics.** Training LLMs with reinforcement learning (e.g., Proximal\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\n\n- n retrieval and generation, we train our target LM on task examples augmented with reflection\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\nreflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on\nhuman preference alignment during training. Other works use general control tokens to guide LM\ngeneration (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluationguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\n\n\n3 SELF-RAG: LEARNING TO RETRIEVE, GENERATE AND CRITIQUE\n\n\nWe introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1.\nSELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM\u2019s original creativity and versatility. Our end-to-end training\nlets an LM _M_ **generate** text informed by **retrieved** passages, if needed, and **criticize** the output by\nlearning to generate special tokens. These _reflection tokens"," prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only\n\n- nce at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation\n\n- n top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named\nentities. Yet, the improved task performance of such approaches often comes at the expense of\nruntime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of\nattributions (Liu et al., 2023a; Gao et al., 2023). We introduce a method to train an arbitrary LM to\nlearn to use retrieval _on-demand_ for diverse instruction-following queries and introduce controlled\ngeneration guided by reflections tokens to further improve generation quality and attributions.\n\n\n**Concurrent RAG work.** A few concurrent works [2] - n RAG propose new training or prompting\nstrategies to improve widely-adopted RAG approaches. Lin et al. (2023) fine-tune both the retriever\nand LM on instruction-tuning datasets in two steps. While we also train our model on diverse\ninstruction-following datasets, SELF-RAG enables retrieval on demand and selection of the best\npossible model output via fine-grained self-reflection, making it widely applicable and more robust\nand controllable. Yoran et al. (2023) use a natural language inference model and Xu et al. (2023) use\na summarization model to filter out or compress retrieved passages before using them to prompt the\nLM to generate the output. SELF-RAG processes passages in parallel and filters out irrelevant ones\nthrough self-reflection, without relying on external models at inference. Moreover, our self-reflection\nmechanism also evaluates other aspects of the model output quality including factuality. LATS (Zhou\net al., 2023) prompt off-the-shelf LMs to search for relevant information for question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self","Preprint.\n\n## - SELF RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION\n\n\n**Akari Asai** _[\u2020]_ **, Zeqiu Wu** _[\u2020]_ **, Yizhong Wang** _[\u2020\u00a7]_ **, Avirup Sil** _[\u2021]_ **, Hannaneh Hajishirzi** _[\u2020\u00a7]_\n\n_\u2020_ University of Washington _\u00a7_ Allen Institute for AI _\u2021_ IBM Research AI\n_{_ akari,zeqiuwu,yizhongw,hannaneh _}_ @cs.washington.edu, avi@us.ibm.com\n\n\nABSTRACT\n\n\nDespite their remarkable capabilities, large language models (LLMs) often produce\nresponses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad\nhoc approach that augments LMs with retrieval of relevant knowledge, decreases\nsuch issues. However, indiscriminately retrieving and incorporating a fixed number\n\n     - f retrieved passages, regardless of whether retrieval is necessary, or passages are\nrelevant, diminishes LM versatility or can lead to unhelpful response generation.\nWe introduce a new framework called **Self-Reflective Retrieval-Augmented Gen-**\n**eration (SELF-RAG)** that enhances an LM\u2019s quality and factuality through retrieval\nand self-reflection. Our framework trains a single arbitrary LM that adaptively\nretrieves passages on-demand, and generates and reflects on retrieved passages\nand its own generations using special tokens, called _reflection_ tokens. Generating\nreflection tokens makes the LM controllable during the inference phase, enabling it\nto tailor its behavior to diverse task requirements. Experiments show that SELFRAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs\nand retrieval-augmented models on a diverse set of tasks. Specifically, SELF-RAG\n\n    - utperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA,\nreasoning and fact verification tasks, and it shows significant gains in improving\nfactuality and citation accuracy for long-form generations relative to these models. [1]\n\n\n1 INTRODUCTION\n\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al"," humans in performing complex tasks. Our work is also related\nto the recitation-augmented language models (Sun et al., 2022); however in contrast to their work, we\nexplicitly perform step-back and abstraction, with optional use of retrieval augmentation depending\n\n- n the nature of the task at hand.\n\n\n8.2 DECOMPOSITION\n\n\nDecomposing a task into simpler tasks and solving these tasks to complete the original task has\nbeen an effective way (Zhou et al., 2022; Patel et al., 2022; Khot et al., 2022; Press et al., 2022) to\nimprove model performance on complex tasks. Several prompting methods have been successful in\nthis regard. Our work STEP-BACK PROMPTING, in contrast, is on making the question more abstract\nand high-level, which is different from decomposition that is often a low-level breakdowns of the\n\n- riginal question. For instance, a generic question for _which employer did Steve Jobs work for in_\n_1990?_ could be _what is the employment history of Steve Jobs?_ While decomposition would lead\nto sub-questions such as _What was Steve Jobs doing in 1990?_, _Was Steve Jobs employed in 1990?_\nand _If Steve Jobs was employed, who was his employer?_ Furthermore, abstract questions such as\n_what is the employment history of Steve Jobs?_ are often generic in nature to have a many-to-one\nmapping since many questions (e.g. _which employer did Steve Jobs work for in 1990?_ and _which_\n_employer did Steve Jobs work for in 2000?_ ) can have the same abstract question. This is in contrast\nto decomposition where there is often a one-to-many mapping since there are multiple decomposed\nsub-problems necessary to solve a given question.\n\n\n9 CONCLUSION\n\n\nWe introduce STEP-BACK PROMPTING as a simple yet generic method to elicit deep reasoning via\nabstraction in large language models. Experimentation on LLMs across fact-seeking, commonsense\nreasoning and domain-specific reasoning benchmarks shows that STEP-BACK PROMPTING significantly improves model performance. We hypothesize that abstraction helps models to hallucinate\nless and reason better, probably reflecting the true nature of the model which are often hidden\nwhile responding to the original question without abstraction. We hope our work will inspire more\n"],"41":[" **RE** trieval augmented generation\n( **FLARE** ), as illustrated in Figure 1. FLARE iteratively generates _a temporary next sentence_, use\nit as the query to retrieve relevant documents _if it_\n_contains low-probability tokens_ and regenerate the\nnext sentence until reaches the end.\n\n\nFLARE is applicable to any existing LMs at\ninference time without additional training. Con\n\nsidering the impressive performance achieved by\nGPT-3.5 (Ouyang et al., 2022) on a variety of\ntasks, we examine the effectiveness of our meth\n- ds on text-davinci-003. We evaluate FLARE\n\n- n 4 diverse tasks\/datasets involving generating\nlong outputs, including multihop QA (2WikiMultihopQA), commonsense reasoning (StrategyQA),\nlong-form QA (ASQA), and open-domain summarization (WikiAsp) (Ho et al., 2020; Geva et al.,\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\nOver all tasks, FLARE achieves superior or competitive performance compared to single-time and\nmulti-time retrieval baselines, demonstrating the\neffectiveness and generalizability of our method.\n\n\n**2** **Retrieval Augmented Generation**\n\n\nWe formally define single-time retrieval augmented\ngeneration and propose the framework of active\nretrieval augmented generation.\n\n\n**2.1** **Notations and Definitions**\n\n\nGiven a user input _**x**_ and a document corpus _D_ =\n_{_ _**d**_ _i}_ _[|D|]_ _i_ =1 [(such as all Wikipedia articles), the goal of]\nretrieval augmented LMs is to generate the answer\n_**y**_ = [ _**s**_ 1 _,_ _**s**_ 2 _, ...,_ _**s**_ _m_ ] = [ _w_ 1 _, w_ 2 _, ..., wn_ ] containing\n_m_ sentences or _n_ tokens leveraging information\nretrieved from the corpus.\nIn retrieval augmented LM, the LM typically\npairs with a retriever that can retrieve a list of\ndocuments _D_ _**q**_ = ret( _**q**_ ) for a query _**q**_ ; the LM\nconditions on both the user input"," (DisambigF1), ROUGE (Lin, 2004), and an overall score combining Disambig-F1 and ROUGE (DR).\n\n\n**Open-domain summarization** The goal of opendomain summarization is to generate a comprehensive summary about a topic by gathering information from open web (Giorgi et al., 2022). We use\nWikiAsp (Hayashi et al., 2021) which aims to generate aspect-based summaries about entities from\n20 domains in Wikipedia, e.g., \u201cGenerate a summary about Echo School (Oregon) including the\n\n\nNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp\n\n\nFigure 4: Comparision between FLARE and baselines across all tasks\/datasets. We report the primary metric for\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\n\n\n\nfollowing aspects: academics, history.\u201d Experimental setting details are included in Appendix B.\nMetrics include ROUGE, named entity-based F1,\nand UniEval (Zhong et al., 2022) which measures\nfactual consistency.\n\n\n**6** **Experimental Results**\n\n\nWe first report overall results across 4 tasks\/datasets\nand compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\n\nrun ablation experiments to study the efficacy of\nvarious design choices of our method.\n\n**6.1** **Comparison with Baselines**\n\n**Overall results.** The overall performance of\nFLARE and baseline across all tasks\/datasets are\n\nreported in Figure 4. FLARE outperforms all baseline on all tasks\/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks, multihop QA shows the\nmost significant improvement. This is largely due\nto the task\u2019s clear definition and specific objective\n\n- f producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to\ngenerate on-topic output. In contrast, ASQA and\nWikiAsp are more open-ended, which increases the\ndifficulty of both generation and evaluation. The\n"," category:\n\n\n_**q**_ _t_ = _**y**_ _t\u2212_ 1 ( _t \u2265_ 2) _,_\n\n\n_**y**_ _t_ = _**s**_ _t._\n\n\n**Question decomposition** approaches manually\nannotated task-specific exemplars to guide LMs\nto generate decomposed sub-questions while producing outputs. For example, self-ask (Press et al.,\n2022), a method in this category, manually inserts\nsub-questions in exemplars using Prompt D.2. For\nthe test case, retrieval is triggered dynamically\nwhenever the model generates a sub-question.\nThe aforementioned approaches can retrieve additional information while generating. However,\nthey have notable drawbacks: (1) Using previously\ngenerated tokens as queries might not reflect what\nLMs intend to generate in the future. (2) Retrieving information at a fixed interval can be inefficient\nbecause it might occur at inappropriate points. (3)\nQuestion decomposition approaches require taskspecific prompt engineering, which restricts their\ngeneralizability in new tasks.\n\n\n**5** **Experimental Setup**\n\n\nWe evaluate the effectiveness of FLARE on 4 di\nverse knowledge-intensive tasks using few-shot incontext learning (Radford et al., 2019; Brown et al.,\n2020; Liu et al., 2023). We follow previous works\n(Trivedi et al., 2022) to sub-sample at most 500\nexamples from each dataset due to the cost of running experiments. Datasets, metrics, and settings\nare summarized in Table 7 of Appendix B. The\nhyperparameters of FLARE are selected based on\nthe development set and listed in Table 9. FLARE\nrefers to FLAREdirect if not specifically stated.\n\n\n**Multihop QA** The goal of multihop QA is to\nanswer complex questions through information retrieval and reasoning. We use 2WikiMultihopQA\n(Ho et al., 2020) which contains 2-hop complex\n\n\n4Since KNN-LM uses the contextualized representation\ncorresponding to the current decoding position to retrieve relevant information which encodes all previous tokens. Strictly\nspeaking, _**q**_ _t_ should be _**y**_ _<t_ .\n\n\n\nquestions sourced from Wikipedia articles that require composition, comparison, or inference, e.g.,\n\u201cWhy did the founder of Versus"," with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does not know, it can still refer to the context.\n\n\n4.1 Results\n\n\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\n\n- utperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes\/no question, we don\u2019t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.\n\n\nOverall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its\nanswering style not aligning with the ground truth. By applying domain-specific tuning,\nwe significantly enhance its performance. This process enables the model to learn and adopt\nthe appropriate style of answering. However, introducing RAG to a domain-specifically\nfine-tuned (DSF) model doesn\u2019t invariably lead to better outcomes. This might indicate that\nthe model lacks training in context processing and extracting useful information from it. By\nincorporating our method, RAFT, we train the model not only to match its answering style\nwith that required but also to improve its document processing capabilities. Consequently,\n\n- ur approach outperforms all others.\n\n\n4.2 Effect of CoT\n\n\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\nin enhancing the model\u2019s performance. As indicated in Table 2, simply providing the answer\nto a question","||||||DPR-rep<br>DPR-GAR<br>DPR-GAR*<br>Hybrid-GAR*|\n|||||||\n|||||||\n\n0 100 200 300 400 500\n\n\nnumber of retrieval results (k)\n\n\n\n53\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n||||||BM25-repl<br>BM25-GAR<br>BM25-GAR*<br>l|\n||||||BM25-repl<br>BM25-GAR<br>BM25-GAR*<br>l|\n|||||||\n|||||||\n|||||||\n||||||DPR-rep<br>DPR-GAR<br>DPR-GAR*<br>Hybrid-GAR*|\n|||||||\n|||||||\n\n0 100 200 300 400 500\n\n\nnumber of retrieval results (k)\n\n\n\nFigure 1: End-to-end question answering effectiveness (exact match score) varying the number of retrieval results\n(k) for NQ (left) and TriviaQA (right).\n\n\n\ntriever scores\u2014we observe statistically significant\ngains across all retrieval conditions, as can be\nseen in the \u201cGAR*\u201d vs. \u201crepl\u201d columns across all\nrows. Compared to the best replicated results, we\n\n- btained an improvement of approximately three\npoints in end-to-end QA effectiveness compared\nto the best answer extraction approach described\nin Karpukhin et al. (2020). Note that we were able\nto obtain these improvements using exactly the\nmodel checkpoints provided in the DPR repo\u2014we\nhave simply added two relatively simple tricks to\nimprove scoring and evidence combination.\nIn Figure 1, we plot exact match scores as a\nfunction of varying k retrieval results for NQ (left)\nand TriviaQA (right). That is, we show how endto-end QA effectiveness changes as the reader is\nprovided more contexts from the retriever to consider. There are two factors here at play: On the\n\n- ne hand, top-k accuracy increases monotonically,\ni.e., as k increases, so does the likelihood that the\nanswer appears in the contexts fed to the reader.\nOn the other hand, the reader is asked to con\nsider more contexts, and thus needs to discriminate the correct answer from a larger pool of candidate contexts, some of which"]},"response":{"0":"Retrieval-Augmented Generation (RAG) is a text generation paradigm that combines deep learning techniques with traditional retrieval methods to enhance various natural language processing (NLP) tasks. It offers distinct advantages over conventional generation models, primarily by allowing models to access external knowledge bases for generating text. This capability aids models in overcoming limitations such as implicit knowledge storage and improves the scalability and reliability of generated outputs.\n\nIn RAG, the generation process can be conceptualized as mapping an input sequence \\( x \\) to an output sequence \\( y \\) with the aid of relevant retrieved instances \\( z \\), which are drawn from original training data or external datasets. The retrieved data, which ideally contains information similar to the input, supports the generation process, making it more informed and contextually relevant (<document id=\"2202.01110_rag_survey_li:chunk_0\">).\n\nThe paradigm leverages several key components including the retrieval source, retrieval metrics, and integration methods, which collectively contribute to its effectiveness in tasks such as dialogue response generation and machine translation (<document id=\"2202.01110_rag_survey_li:chunk_900\">). The use of retrieved human-written references instead of generating content from scratch helps alleviate challenges associated with text generation (<document id=\"2202.01110_rag_survey_li:chunk_0\">). Overall, RAG represents a significant advancement in the capability of models to produce high-quality, contextually relevant text (<document id=\"2202.01110_rag_survey_li:chunk_0\">).","1":"The FiD (Fusion-in-Decoder) architecture was proposed by Gautier Izacard and \u00c9douard Grave <document id=\"2007.01282_fid_izacard:chunk_450\">.","2":"RAFT stands for \"Retrieval-Augmented Fine-Tuning\" based on the context provided in the materials. It describes a method aimed at enhancing language models\u2019 performance in domain-specific tasks by integrating retrieval-augmented generation techniques. Specifically, RAFT adapts language models to improve their capabilities in utilizing external knowledge effectively. This is reflected in various experimental results discussed in the document <2403.10131_raft_zhang:chunk_4500>.","3":"Hybrid retrieval (dense + sparse) outperforms either approach alone primarily because they exploit very different signals. The overlap in results between dense and sparse retrieval is minimal, indicating that they capture different aspects of relevance. As a result, combining these approaches capitalizes on the strengths of both dense retrieval (e.g., better semantic understanding) and sparse retrieval (e.g., effective traditional keyword matching), leading to improved overall effectiveness in retrieval tasks <document id=\"2104.05740_hybrid_retrieval_ma:chunk_5400\">.","4":"ColBERT differs from traditional dense retrieval primarily through its use of a **late interaction architecture**. This approach allows it to independently encode the query and document using BERT while implementing a cheaper interaction step to model their fine-grained similarity. This contrasts with traditional dense retrieval methods, which typically compute relevance scores for each query-document pair by processing them simultaneously through a neural network, resulting in significantly higher computational costs (<document 2004.12832_colbert_khattab:chunk_0>).\n\nAdditionally, ColBERT enables pre-computation of document representations and utilizes **vector-similarity indexes** for direct retrieval from large document collections, improving recall and efficiency compared to traditional models that usually only involve re-ranking (<document 2004.12832_colbert_khattab:chunk_2700>). Overall, ColBERT combines effectiveness with dramatically reduced computational demands, making it a more efficient choice for passage retrieval (<document 2004.12832_colbert_khattab:chunk_12150>).","5":"The trade-offs between long-context Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems primarily involve the balance between knowledge accessibility and generation quality.\n\n1. **Knowledge Accessibility**: Long-context LLMs are designed to encapsulate vast amounts of knowledge in a single model, allowing them to generate responses without needing external sources. However, this can lead to issues such as \"hallucinations,\" where models generate incorrect information due to the constraints of their training data or context limitations. Conversely, RAG systems address this by leveraging external databases to retrieve relevant information dynamically, thus grounding the generation in factual data which helps reduce hallucination risks (Es et al., 2023) <metadata>.\n\n2. **Generation Quality**: While long-context LLMs can generate coherent and contextually rich responses, RAG systems excel in producing factually accurate and specific outputs by combining retrieval and generation modules. RAG systems thus improve on the factual accuracy and specificity of answers compared to traditional LLMs, as evidenced by their performance benchmarks (Lewis et al., 2020) <metadata>.\n\n3. **Efficiency and Evaluation**: RAG systems require continuous evaluation of both the retrieval and generation components, making their evaluation more complex but also potentially faster in deployment due to less reliance on human annotation (Saad-Falcon et al., 2023) <metadata>. Long-context LLMs may be easier to implement but can face challenges in ensuring continuous accuracy without accessing updated external data.\n\nIn summary, long-context LLMs prioritize self-contained knowledge generation while RAG systems enhance factuality and relevance by integrating real-time information retrieval, albeit with a more intricate evaluation framework.","6":"GraphRAG and FiD (which is implied to be a conventional RAG approach based on vector retrieval) differ primarily in their methodology for multi-document reasoning.\n\n1. **GraphRAG Approach**:\n   - **Construction of Knowledge Graph**: GraphRAG first builds a knowledge graph using an LLM, where nodes represent key entities and edges represent the relationships between those entities. This graph is then partitioned into a hierarchy of communities related to these entities.\n   - **Community-Level Summaries**: It generates summaries at different community levels in a bottom-up manner, recursively integrating lower-level summaries into higher-level ones. This hierarchical summarization allows for a comprehensive understanding of the corpus.\n   - **Map-Reduce Processing**: When answering queries, GraphRAG employs a map-reduce process using these community summaries, providing a global understanding rather than just localized context. This enables it to tackle sensemaking queries that require understanding across the entire dataset (Edge et al., 2024).\n\n2. **FiD (Vector RAG Approach)**:\n   - **Local Context Reliance**: In contrast, FiD relies on retrieving a subset of documents that are semantically similar to the query. The LLM generates responses based solely on the information contained in these retrieved documents, which might be insufficient for queries needing global insights.\n   - **Limited Sensemaking Capability**: Vector RAG methods like FiD struggle with sensemaking tasks that require reasoning across a broader range of data due to their focus on localized information retrieval (Edge et al., 2024).\n\nIn summary, GraphRAG is designed for global sensemaking through a structured community-level summarization approach, while FiD relies on local context retrieval, potentially limiting its effectiveness for broader queries.","7":"For the evaluation of Retrieval-Augmented Generation (RAG) systems, several recommended metrics for both retrieval and generation include:\n\n### Retrieval Quality Metrics\n1. **Hit Rate** - Measures the proportion of relevant documents retrieved.\n2. **Mean Reciprocal Rank (MRR)** - Evaluates the average rank of the first relevant document.\n3. **Normalized Discounted Cumulative Gain (NDCG)** - Assesses the ranking quality of the retrieved documents.\n\n### Generation Quality Metrics\n1. **Exact Match (EM)** - Measures the percentage of generated answers that exactly match reference answers.\n2. **F1 Score** - Evaluates the balance between precision and recall.\n3. **BLEU** - Commonly used for evaluating the quality of generated text by comparing it to reference texts.\n4. **ROUGE** - Measures the overlap of n-grams between generated and reference texts, particularly useful in summarization tasks.\n5. **BertScore** - Utilizes contextual embeddings for semantic similarity between generated and reference text.\n\nBoth retrieval and generation quality assessments can be manually or automatically conducted (<document id=\"2312.10997_rag_survey_gao:chunk_11700\">).","8":"I don't have enough information in the provided materials to answer this question.","9":"I don't have enough information in the provided materials to answer this question.","10":"The authors of the paper \"Dense Passage Retrieval for Open-Domain Question Answering\" are Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih <document id=\"2004.04906_dpr_karpukhin:chunk_0\" \/>.","11":"BM25 is used in open-domain question answering (QA) as a retrieval method to identify relevant passages that could contain answers to user queries. Specifically, it serves as a base retriever to retrieve a certain number of paragraphs based on the question provided. In the context of the research, BM25 is implemented in Elasticsearch to select the top K paragraphs that best match the query, which helps in narrowing down the candidate passages that will be evaluated for answering the question <document id=\"2212.10509_cot_retrieval_trivedi:chunk_3600\">.","12":"The purpose of using Wikipedia in open-domain question answering is to provide a rich source of information that can be accessed to help answer various questions. Wikipedia serves as a corpus where questions can be grounded in main paragraphs that link to multiple Wikipedia pages, allowing models to retrieve relevant information from these sources efficiently. This enables the construction of corpora by combining supporting and non-supporting paragraphs, which can improve the performance of models in answering questions across diverse topics. Additionally, it allows for the retrieval of broad context and content that is necessary for generating accurate and comprehensive answers (Trivedi et al., 2022) <2212.10509_cot_retrieval_trivedi:chunk_13500>.","13":"Roberts et al. measure the similarity between questions and passages using the dot product of their dense vector representations. Specifically, they define the similarity with the following formula:\n\n\\[ \\text{sim}(q, p) = EQ(q)^\\top EP(p) \\]\n\nwhere \\(EQ\\) and \\(EP\\) are the encoder functions for the question and passage, respectively. This method focuses on using the inner product, as it allows for efficient computation and is well-suited for use with precomputed passage representations <document id=\"2004.04906_dpr_karpukhin:chunk_2250\">.","14":"The purpose of passage embeddings in machine learning, particularly in the context of dense passage retrieval for open-domain question answering, is to create compact and dense representations of passages that can be effectively used for retrieving relevant information. These embeddings allow a model to retrieve candidate contexts by comparing the similarity of passage embeddings to a given query embedding, improving the efficiency and accuracy of the retrieval process. This method simplifies the retrieval stage by relying on learned embeddings instead of traditional sparse vector space models, ultimately enhancing the performance of question-answering systems. According to a study, this approach has shown significant improvements over traditional methods in terms of passage retrieval accuracy <document id=\"2004.04906_dpr_karpukhin:chunk_0\">.","15":"BM25 is used in the context of question answering datasets primarily as a traditional retrieval method to rank passages based on their relevance to a given question. In experiments with datasets like TREC, WebQuestions, and TriviaQA, the highest-ranked passage from BM25 that contains the answer is selected as the positive passage. This selection process is crucial as it determines which passages are considered for answering a question. If none of the top 100 retrieved passages contain the answer, the question is discarded, highlighting the importance of BM25 in the retrieval phase of the question-answering process <document id=\"2004.04906_dpr_karpukhin:chunk_4500\">.","16":"TREC (Text Retrieval Conference) is used in the context of passage retrieval experiments primarily for its CuratedTREC dataset, which sources questions from TREC QA tracks. In passage retrieval tasks, TREC is utilized to evaluate systems by providing a set of questions and associated passages from which relevant information is to be retrieved <document id=\"2004.04906_dpr_karpukhin:chunk_4050\">. The dataset helps researchers benchmark their models against predefined queries to assess the retrieval performance effectively <document id=\"2004.04906_dpr_karpukhin:chunk_4500\">.","17":"According to Lee et al. (2019), the lower performance of SQuAD in open-domain question answering is attributed to the fact that many questions lack context in the absence of the provided paragraph. Although SQuAD has been used for open-domain QA research, it is not ideal for this purpose due to this limitation <document id=\"2004.04906_dpr_karpukhin:chunk_4050\">.","18":"The CPU model used to profile the passage retrieval speed for DPR was the Intel Xeon CPU E5-2698 v4 @ 2.20GHz <document id=\"2004.04906_dpr_karpukhin:chunk_7200\">.","19":"The purpose of a passage selection score in an end-to-end QA system is to evaluate and rank the relevance of candidate passages retrieved in response to a user's question. Specifically, after the retrieval component selects a set of passages, the neural reader assigns a passage selection score to each passage based on its relevance to the question. The passage with the highest selection score is then used to derive the final answer span, indicating the most likely passage that contains the answer to the question posed. This scoring mechanism helps in effectively filtering the passages, thereby improving the accuracy of the QA system <document id=\"2004.04906_dpr_karpukhin:chunk_7200\">.","20":"REALMWiki is a model that is part of the retrieval-augmented generation (RAG) framework, specifically pretrained on Wikipedia. It is designed to enhance the performance of question-answering systems by utilizing the vast information available in Wikipedia to improve retrieval and accuracy during the question-answering process. REALMWiki, along with REALMNews, which is pretrained on CC-News, reflects the importance of different training datasets in achieving better results in open-domain question answering tasks <document id=\"2004.04906_dpr_karpukhin:chunk_8550\">.","21":"I don't have enough information in the provided materials to answer this question.","22":"The sufficient context autorater helps reduce hallucinations in retrieval-augmented generation (RAG) systems by providing refined guidance on when a model should generate an answer or abstain from answering. Here's how it contributes to minimizing hallucinations:\n\n1. **Context Classification**: The autorater classifies instances based on whether the context provides sufficient information to construct a valid answer. By distinguishing between sufficient and insufficient context, the autorater enhances the understanding of the model's response dynamics, especially when the context is lacking <document id=\"2411.06037_sufficient_context_joren:chunk_900\">.\n\n2. **Selectively Generating Responses**: A new selective generation method is implemented that utilizes the sufficient context labels for a guided approach to answering. This approach allows models to abstain from generating answers when the context is insufficient, thus reducing the likelihood of hallucinations. The intervention model included in this method determines whether the model should generate a response or abstain, allowing for more controlled outputs <document id=\"2411.06037_sufficient_context_joren:chunk_1350\">.\n\n3. **Improving Accuracy**: The application of the sufficient context information has been shown to improve the accuracy of responses by 2\u201310% across various models and datasets. This increase occurs because the model is less likely to provide incorrect answers when abstaining is the alternative <document id=\"2411.06037_sufficient_context_joren:chunk_16650\">.\n\nThe combination of these strategies indicates that the sufficient context autorater effectively enhances model decision-making, allowing for higher quality outputs and fewer hallucinations.","23":"Query rewriting improves the performance of retrieval-augmented language models (LLMs) in handling factual information by addressing the gap between the search query and the needed knowledge. The proposed framework, termed _Rewrite-Retrieve-Read_, emphasizes adapting the search query itself rather than solely focusing on either the retriever or the reader. This approach involves prompting an LLM to generate a more suitable query, which is then used to retrieve relevant contexts from a web search engine.\n\nAdditionally, a trainable small language model is utilized as a rewriter. This model is trained using feedback from the LLM reader through reinforcement learning, allowing it to tailor the queries based on the specific capabilities of the LLM, thereby enhancing the retrieval process. Experimental results on tasks like open-domain QA show consistent performance improvements due to this novel incorporation of query rewriting, indicating that it effectively leads to better alignment between the search queries and the information sought by the LLMs <document id=\"2305.14283_query_rewriting_ma:chunk_0\">.","24":"The combination of parametric memory with non-parametric memory in Retrieval-Augmented Generation (RAG) models significantly improves their performance on knowledge-intensive NLP tasks due to several factors:\n\n1. **Enhanced Access to Knowledge**: RAG models utilize a pre-trained parametric memory (like a seq2seq model) alongside a non-parametric memory sourced from a dense vector index (e.g., Wikipedia). This setup allows the model to not only utilize stored knowledge but also retrieve explicit information as needed, thus overcoming limitations in solely parametric models which struggle to access and manipulate knowledge efficiently (<document 2005.11401_rag_lewis:chunk_0>).\n\n2. **Better Generalization**: By combining these two memory types, RAG models can generalize better across different tasks. This hybrid approach leads to generation outputs that are more specific, diverse, and factual compared to traditional parametric-only models, which may produce less accurate information and \"hallucinations\" (<document 2005.11401_rag_lewis:chunk_450>).\n\n3. **End-to-End Fine-Tuning**: Both the retriever and generator components of the RAG models are trained together in a joint learning framework. This allows for fine-tuning that optimally integrates the retrieval process with the generative capabilities of the model, further refining how knowledge is accessed and utilized during text generation (<document 2005.11401_rag_lewis:chunk_900>).\n\n4. **Adaptability**: The non-parametric memory can be easily updated without retraining the entire model, which means that RAG can efficiently incorporate new information as it becomes available, providing a flexible and up-to-date knowledge base for different applications (<document 2005.11401_rag_lewis:chunk_9000>).\n\nIn summary, the combination of parametric and non-parametric memories in RAG models allows them to perform better on knowledge-intensive tasks by enhancing knowledge retrieval, improving the quality of generated content, and allowing for easy updates to the knowledge base, which is crucial for tasks that require specific and accurate information.","25":"Notable approaches for retrieval-augmented text generation (RAG) tasks leverage deep learning technology in various ways:\n\n1. **Generic Paradigm**: The framework consists of three core components: retrieval sources, retrieval metrics, and generation models. This structure allows the combination of traditional retrieval techniques with modern deep learning models, enabling state-of-the-art performance in numerous NLP tasks such as dialogue response generation and machine translation <document id=\"2202.01110_rag_survey_li:chunk_0\">.\n\n2. **Dialogue Response Generation**: Approaches like exemplar\/ template retrieval improve the informativeness of responses. Furthermore, integrating knowledge from different sources, such as knowledge bases and external documents, facilitates knowledge-grounded generation <document id=\"2202.01110_rag_survey_li:chunk_450\">.\n\n3. **Machine Translation**: Initial works in integrating retrieval into machine translation (MT) emphasized the use of translation memory. More recent advancements have focused on integrating traditional statistical machine translation methods with neural networks to enhance accuracy (e.g., Gu et al., 2018) <document id=\"2202.01110_rag_survey_li:chunk_450\">.\n\n4. **Multi-Modal Learning**: Recent efforts have also ventured into multi-modal setups, linking texts with images, which opens new avenues for tasks like image captioning and speech-to-text transcription through retrieval-based methodologies <document id=\"2202.01110_rag_survey_li:chunk_8550\">.\n\n5. **RAFT (Retrieval-Augmented Fine Tuning)**: This novel strategy combines instruction fine-tuning with retrieval-augmented generation to better adapt LLMs to specific domains. RAFT achieves this by training models to handle domain-specific knowledge while also managing distractions from irrelevant retrieved documents <document id=\"2403.10131_raft_zhang:chunk_900\">.\n\nOverall, these approaches highlight the synergy between deep learning and retrieval mechanisms, allowing models to generate more accurate and contextually relevant outputs across various text generation tasks.","26":"VisRAG enhances the retrieval and generation stages of RAG by utilizing vision-language models (VLMs) in a way that directly incorporates multi-modal document images. Unlike traditional text-based RAG systems, which require a parsing stage to extract text, VisRAG embeds the entire document image using a VLM, allowing for the retention of critical visual information. This approach eliminates the potential information loss that can occur during parsing.\n\nIn the retrieval phase, VisRAG utilizes a VLM-based retriever that maps the query and document directly into an embedding space based on the document's image rather than on parsed text. The generation stage also employs a VLM, thereby enabling the generation process to draw from both visual and textual information effectively. The empirical results show that VisRAG achieves a 20\u201340% end-to-end performance gain over traditional text-based RAG systems, demonstrating greater efficiency in utilizing training data and a strong generalization capability across different document types (Yu et al., 2024) <document id=\"2410.10594_vision_rag_yu:chunk_0\">.","27":"The SPLADE model addresses the limitations of previous sparse lexical representation models by implementing explicit sparsity regularization and a log-saturation effect on term weights. These features lead to highly sparse representations that maintain desirable properties of bag-of-words (BOW) models, such as exact term matching and efficiency in inverted indexes, while achieving competitive results compared to both dense and sparse methods.\n\nKey improvements include:\n\n1. **Efficiency**: SPLADE can achieve efficiency levels comparable to sparse BOW models while outperforming more complex configurations like doc2query-T5. The model is also capable of controlling the trade-off between efficiency (in terms of floating-point operations) and effectiveness through adjustable sparsity regularization parameters (<document id=\"2107.05720_splade_formal:chunk_900\">).\n\n2. **End-to-End Training**: Unlike SparTerm, which has a complex gating mechanism that cannot be trained end-to-end, SPLADE allows for a simpler, unified training process that optimally learns sparsification strategies for ranking tasks (<document id=\"2107.05720_splade_formal:chunk_2700\">).\n\n3. **Improved Recall and Relevance**: The incorporation of implicit or explicit expansion mechanisms enriches document representations, which can lead to increased recall while maintaining a high level of sparsity (<document id=\"2107.05720_splade_formal:chunk_6300\">).\n\nIn summary, SPLADE effectively combines the advantages of traditional sparse models with enhancements that improve both efficiency and effectiveness in information retrieval tasks.","28":"ARES utilizes prediction-powered inference (PPI) to enhance model-based evaluation accuracy by leveraging a small set of human-annotated datapoints to compute confidence intervals for its evaluations. Specifically, ARES employs a human preference validation set, which consists of approximately 150 or more annotated datapoints indicating both positive and negative examples across context relevance, answer faithfulness, and answer relevance evaluations. This method enables ARES to effectively estimate the quality of different RAG systems while providing statistical confidence intervals for the scores it reports (Saad-Falcon et al., 2023) <document id=\"2311.09476_ares_saad_falcon:chunk_900\">.","29":"The introduction of dense passage retrieval (DPR) significantly improved open-domain question answering (QA) systems by utilizing learned dense representations for passage retrieval, which are more effective than traditional sparse vector space models like TF-IDF or BM25. Specifically, DPR captures semantic relationships and lexical variations more efficiently, resulting in a substantial improvement in retrieval accuracy. When evaluated on various open-domain QA datasets, DPR outperformed the strong Lucene-BM25 system by 9% to 19% in top-20 passage retrieval accuracy. This enhancement in retrieval led to state-of-the-art performance in end-to-end QA systems across multiple benchmarks (Karpukhin et al., 2020) <2004.04906_dpr_karpukhin:chunk_0>.\n\nAdditionally, dense retrieval systems streamline the process, allowing for quicker retrieval times\u2014DPR can process approximately 995 questions per second compared to just 23.7 for BM25, enhancing overall system efficiency (Karpukhin et al., 2020) <2004.04906_dpr_karpukhin:chunk_7200>. This combination of accuracy and speed represents a significant advancement in open-domain question answering capabilities.","30":"The core idea behind HyDE (Hypothetical Document Embeddings) is to enhance zero-shot dense retrieval systems by generating hypothetical documents that capture relevance patterns without requiring actual relevance labels. In this approach, a language model generates a hypothetical document based on a given query, which is then encoded by a contrastively learned encoder. This process helps identify a neighborhood in the corpus embedding space, allowing the retrieval of real documents similar to the generated hypothetical document, thereby filtering out incorrect or hallucinated details through the encoder's dense bottleneck.\n\nHyDE improves zero-shot dense retrieval by effectively bridging the gap between natural language understanding (NLU) and natural language generation (NLG), allowing for the retrieval task to operate in an unsupervised manner. This method not only produces results comparable to fine-tuned retrievers across various tasks and languages but also eliminates the need for training on relevance data, making it practical for scenarios where such labels are unavailable <document id=\"2212.10496_hyde_gao:chunk_0\">.","31":"The purpose of constructing the Retrieval-Augmented Generation Benchmark (RGB) is to evaluate the effectiveness of large language models (LLMs) in utilizing external knowledge to enhance response accuracy and reliability, particularly to mitigate issues like hallucinations and outdated information. The RGB aims to diagnose the challenges LLMs face when applying Retrieval-Augmented Generation (RAG) methods. \n\nThe benchmark specifically evaluates four fundamental abilities of LLMs in the context of RAG:\n\n1. **Noise Robustness**: The ability to extract useful information from noisy documents that may contain irrelevant or misleading content.\n2. **Negative Rejection**: The capability to decline answering a question when none of the retrieved documents provide useful information.\n3. **Information Integration**: The skill to synthesize answers from multiple documents, especially for complex questions requiring information from various sources.\n4. **Counterfactual Robustness**: The ability to handle inaccuracies in the external knowledge provided, especially in the presence of warnings about the reliability of retrieved information.\n\nThese abilities are crucial for effectively leveraging RAG in real-world applications, highlighting both the strengths and limitations of current LLMs <document id=\"2309.01431_rgb_benchmark_chen:chunk_0\">.","32":"Language models face several challenges when retrieving information from their input contexts in open-domain question answering:\n\n1. **Long Context Performance Trade-off**: Providing language models with longer input contexts can help improve performance, but it also increases the complexity of reasoning over more content, potentially decreasing accuracy. This trade-off means that models may not effectively utilize all available information, leading to suboptimal performance even when additional documents are retrieved <document id=\"2307.03172_lost_in_middle_liu:chunk_1800\">.\n\n2. **U-shaped Performance Curve**: Models typically exhibit a U-shaped performance curve, where performance is highest when relevant information appears at the beginning or end of the input context, and significantly worse when it is located in the middle. This suggests a bias in how models access information and indicates they struggle to robustly retrieve information positioned centrally <document id=\"2307.03172_lost_in_middle_liu:chunk_9450\">.\n\n3. **Reduced Effectiveness with Multiple Documents**: In the setting of multi-document question answering, model performance can saturate long before recall from retrieved documents saturates. For instance, even significant increases in the number of retrieved documents (e.g., from 20 to 50) yield only marginal improvements in performance <document id=\"2307.03172_lost_in_middle_liu:chunk_900\">.\n\n4. **Accessing Information**: When information is located in the middle of long input contexts, models may not only struggle to access it but could perform worse than when answering questions without any context (closed-book setting) <document id=\"2307.03172_lost_in_middle_liu:chunk_1350\">.\n\n5. **Architectural Limitations**: The architecture of language models plays a significant role in their effectiveness. For example, decoder-only models and those that have not been instruction-fine-tuned show a consistent U-shaped performance curve, indicating difficulties in retrieving information based on its position in long contexts <document id=\"2307.03172_lost_in_middle_liu:chunk_1350\">.\n\nThese challenges highlight the limitations of current language models in effectively utilizing extensive and complex input contexts for open-domain question answering tasks.","33":"The trade-off between providing more context and its impact on language model performance in open-domain question answering (QA) revolves around the balance between the potential benefits of increased information and the challenges of processing that information. \n\nLonger input contexts can help a language model perform better on downstream tasks, but they also increase the amount of content the model needs to reason over, which can potentially decrease accuracy. In open-domain QA, for instance, adding more retrieved documents does not significantly improve performance, as the models may fail to utilize this extra information effectively. For example, using 50 documents instead of 20 only marginally improved performance (approximately 1.5% for GPT-3.5-Turbo) <document id=\"2307.03172_lost_in_middle_liu:chunk_1800\">.\n\nFurthermore, the effectiveness of longer contexts varies depending on the model's architecture and training. Models may struggle to retrieve relevant information when it's positioned poorly in the context, often exhibiting a U-shaped performance curve where too much context can lead to reduced performance due to inefficiencies in information retrieval <document id=\"2307.03172_lost_in_middle_liu:chunk_1350\">. Thus, while additional context can be beneficial, it is crucial to consider how well the model can leverage that context without overwhelming it <document id=\"2307.03172_lost_in_middle_liu:chunk_8100\">.","34":"ARES used OpenAI\u2019s _gpt-3.5-turbo-16k_, version 10\/23, as its in-context learning baseline in a zero\/few-shot setting <document id=\"2311.09476_ares_saad_falcon:chunk_3600\">.","35":"Some limitations of using FLAN-T5-XXL in the ARES evaluation framework include:\n\n1. **Resource Intensity**: FLAN-T5-XXL, due to its large size (11.3 billion parameters), requires substantial GPU-based hardware with at least 32GB of memory. This can pose accessibility issues as such hardware may not be easily available to all NLP researchers and practitioners due to costs <document id=\"2311.09476_ares_saad_falcon:chunk_8550\">.\n\n2. **Dependence on Annotations**: The framework relies on a small set of human-annotated data points (approximately 150-300) for effective evaluation. While generating these annotations can be easy in general domains, specialized fields (e.g., law, medicine) may require annotators with domain-specific expertise <document id=\"2311.09476_ares_saad_falcon:chunk_8550\">.\n\n3. **Language Limitations**: All datasets used in the ARES evaluation are in English. This reliance on a singular well-resourced language limits the framework's applicability to other languages, necessitating future exploration with different LLMs for diverse languages <document id=\"2311.09476_ares_saad_falcon:chunk_8550\">. \n\nThese factors contribute to the challenges associated with the deployment of FLAN-T5-XXL in the ARES evaluation framework.","36":"The figure that shows the overall performance of FLARE and baseline methods across all tasks\/datasets is Figure 4. This figure illustrates a comparison between FLARE and the baseline methods, indicating that FLARE outperforms all baselines across the tasks\/datasets evaluated <document id=\"2305.06983_active_rag_jiang:chunk_6300\" metadata>.","37":"GraphRAG is a graph-based retrieval-augmented generation (RAG) approach designed to enable comprehensive sensemaking over large text corpora. It utilizes a language model to construct a knowledge graph, where nodes represent key entities, and edges capture the relationships between those entities. The process involves partitioning this graph into hierarchical communities, generating community-level summaries in a bottom-up manner, and ultimately using these summaries to answer queries through a map-reduce processing method <document id=\"2404.16130_graphrag_edge:chunk_900\">.\n\nIn contrast, vector RAG relies on semantic search techniques that retrieve text chunks for contextual input based on vector representations. This approach does not inherently support the same level of global sensemaking over an entire corpus, as it primarily focuses on explicit fact retrieval rather than on utilizing the structural relationships between data points <document id=\"2404.16130_graphrag_edge:chunk_2250\">. \n\nThe main difference lies in GraphRAG's ability to leverage the modularity of graphs and create thematic partitions, allowing for the generation of progressively global summaries to better inform and answer queries <document id=\"2404.16130_graphrag_edge:chunk_2250\">.","38":"To optimize the retrieval module for a specific downstream task like question answering, various strategies can be employed:\n\n1. **Task-Specific Learning**: By learning to retrieve documents tailored to the question-answering task, models can leverage techniques such as reinforcement learning, where the system is refined based on feedback specific to QA tasks. This can enhance the relevance and quality of the retrieved documents (<document id=\"2005.11401_rag_lewis:chunk_8550\">).\n\n2. **Context Curation**: After retrieving information, it is crucial to refine the input to avoid redundancy and to ensure the relevance of the content. Steps such as reranking documents to prioritize the most pertinent results can aid in filtering out less relevant information, thus improving the quality of the final answers (<document id=\"2312.10997_rag_survey_gao:chunk_9000\">).\n\n3. **Integration of Memory-Based Architectures**: Utilizing memory networks that maintain a large external memory helps in retrieving raw text rather than processed embeddings. This can aid in retaining interpretability and allowing dynamic updates to the system, which can be particularly beneficial for QA tasks that require current information (<document id=\"2005.11401_rag_lewis:chunk_8550\">).\n\n4. **Fine-Tuning with Multiple Tasks**: Implementing methods that allow for fine-tuning with a focus on multi-task capabilities can optimize performance for a range of downstream tasks, including QA. Techniques such as universal adapters or bridge models can be employed to help align retrieval with the needs of the QA system, ensuring it can effectively process the retrieved information (<document id=\"2312.10997_rag_survey_gao:chunk_9000\">).\n\n5. **Skeleton Extraction**: This involves explicitly extracting useful information fragments (or \"skeletons\") from retrieved documents, which helps in filtering out irrelevant content during the generation process, leading to more effective answers (<document id=\"2202.01110_rag_survey_li:chunk_2700\">).\n\nBy combining these strategies, the retrieval module can be optimized to significantly enhance its effectiveness for question answering tasks, improving both the relevance and accuracy of the answers generated.","39":"The researchers used the in-batch negative training methodology for their Dense Passage Retriever (DPR) model. They trained the question and passage encoders with a batch size of 128, including one additional BM25 negative passage per question. Specifically, they utilized a mini-batch approach where all retrieved passage vectors participated in the loss calculation for all questions in the batch, effectively leading to the use of a large number of passages to compute the retriever loss <document id=\"2004.04906_dpr_karpukhin:chunk_14400\">.","40":"Gao et al. introduced the **Self-Reflective Retrieval-Augmented Generation (SELF-RAG)** method to improve large language models' (LLMs) generation quality, specifically enhancing factual accuracy without compromising their versatility. This method employs on-demand retrieval and a self-reflection mechanism, allowing the model to evaluate its own output and decide whether to retrieve additional information. It utilizes special tokens, called reflection tokens, to indicate the need for retrieval and critique the quality of its own generations. This approach enables the model to enhance the relevancy of retrieved content and improve overall generation quality, making it more controllable and robust without sacrificing creativity (Asai et al., 2023) <document id=\"2310.11511_self_rag_asai:chunk_450\">.","41":"FLARE demonstrates superior effectiveness in multihop QA tasks compared to various baselines. It achieves competitive or superior performance across multiple tasks, emphasizing its generalizability and effectiveness in generating answers through iterative retrieval of information. In particular, it shows significant improvements in multihop QA, which is attributed to the task's clear definition and structured requirement for 2-hop reasoning, allowing language models to more easily generate relevant outputs.\n\nThe performance of FLARE is influenced by several factors:\n\n1. **Iterative Retrieval**: FLARE retrieves information dynamically during the generation process, particularly when the model generates low-probability tokens, optimizing the relevance of the retrieved data.\n\n2. **Task Characteristics**: Multihop QA's construction facilitates effective leveraging of FLARE\u2019s method as it requires structured reasoning that aligns with how the model generates content over multiple steps.\n\nOverall, FLARE's ability to continuously gather relevant information distinguishes it from single-time and fixed-interval retrieval baselines, which may not adapt to the evolving context of generated text or answer construction effectively (<document id=\"2305.06983_active_rag_jiang:chunk_5400\">)."},"reference":{"0":"Retrieval-Augmented Generation (RAG) is a method that combines parametric memory (a pre-trained language model) with non-parametric memory (a dense vector index of documents) for knowledge-intensive NLP tasks. RAG retrieves relevant documents from an external corpus and conditions the language model on both the input query and retrieved documents to generate responses. This approach allows models to access up-to-date knowledge without retraining and reduces hallucinations by grounding generations in retrieved evidence. The method was introduced by Lewis et al. in 2020.","1":"The FiD (Fusion-in-Decoder) architecture was proposed by Izacard and Grave in 2021. FiD processes multiple retrieved passages independently in the encoder and then fuses them in the decoder, allowing the model to effectively leverage information from many passages simultaneously while maintaining computational efficiency.","2":"RAFT stands for Retrieval-Augmented Fine-Tuning. It is a training method proposed by Zhang et al. in 2024 that fine-tunes language models to better utilize retrieved documents by training them to distinguish relevant from irrelevant retrieved passages and to extract correct information from the relevant ones.","3":"Hybrid retrieval combines dense (embedding-based) and sparse (keyword-based like BM25) retrieval methods, leveraging their complementary strengths. Sparse retrieval excels at exact keyword matching and is effective for queries with specific terminology, proper nouns, or rare terms. Dense retrieval captures semantic similarity and handles paraphrasing, synonyms, and conceptual queries better. By combining both approaches (typically using rank fusion methods like RRF), hybrid retrieval achieves higher recall by retrieving documents that either method would miss alone, leading to more comprehensive and robust retrieval performance across diverse query types.","4":"ColBERT (Contextualized Late Interaction over BERT) differs from traditional dense retrieval by using late interaction instead of single-vector representations. While traditional dense retrieval encodes queries and documents into single dense vectors and computes similarity via dot product, ColBERT preserves token-level representations and computes interactions between all query and document token embeddings. This approach captures fine-grained token-level matching signals while maintaining efficiency through maximum similarity (MaxSim) operations and precomputation of document embeddings. The late interaction enables better matching of specific terms and phrases compared to single-vector approaches.","5":"Long-context LLMs and RAG systems have different trade-offs. Long-context LLMs can process entire documents directly, avoiding retrieval errors and maintaining full context, but they face challenges including higher computational costs (quadratic attention complexity), difficulty focusing on relevant information in very long contexts (lost-in-the-middle problem), inability to update knowledge without retraining, and higher inference latency. RAG systems offer advantages including efficient access to large knowledge bases, easy knowledge updates by modifying the retrieval index, lower inference costs by processing only relevant chunks, and explicit source attribution. However, RAG systems depend on retrieval quality, may miss relevant information if retrieval fails, and require careful engineering of chunking and retrieval strategies. The choice depends on the use case: RAG is often preferred for knowledge-intensive tasks requiring up-to-date information, while long-context LLMs suit applications requiring deep understanding of specific documents.","6":"GraphRAG and FiD take fundamentally different approaches to multi-document reasoning. FiD (Fusion-in-Decoder) uses a sequence-to-sequence architecture where retrieved passages are processed independently in parallel by the encoder, and then their representations are concatenated and fused in the decoder during generation. This allows the model to implicitly learn to combine information from multiple passages through attention mechanisms, but relationships between passages are only captured during decoding. GraphRAG, on the other hand, explicitly constructs a knowledge graph from retrieved documents, creating structured representations of entities and their relationships across documents. It then uses graph-based reasoning methods to traverse and aggregate information from the graph structure. GraphRAG's explicit graph structure makes multi-hop reasoning paths more interpretable and can better capture complex relationships, while FiD's approach is more flexible and doesn't require structured knowledge extraction but is less interpretable.","7":"RAG systems should be evaluated on both retrieval and generation quality. For retrieval, recommended metrics include: Recall@K (what fraction of relevant documents are in top-K results), Mean Reciprocal Rank (MRR, measures ranking quality), and NDCG (Normalized Discounted Cumulative Gain, considers both relevance and ranking). For generation quality, recommended metrics include: answer correctness (semantic similarity to reference answer using LLM-based evaluation), faithfulness (whether the answer is grounded in retrieved context), answer relevancy (whether the answer addresses the question), and context precision\/recall (whether retrieved chunks are relevant and sufficient). Framework like RAGAS provides LLM-based metrics that avoid issues with incomplete ground truth. End-to-end metrics like exact match and F1 score on question answering benchmarks are also commonly used to measure overall RAG system performance.","8":"I cannot answer this question as it is outside the scope of the available RAG research papers. The corpus contains information about Retrieval-Augmented Generation systems and related NLP techniques, not culinary topics.","9":"While neural network training is a broad machine learning topic, the RAG research corpus focuses specifically on retrieval-augmented generation systems. For general neural network training information (backpropagation, gradient descent, loss functions, etc.), you would need to consult general deep learning resources. The corpus may contain some references to neural network training in the context of training retrieval models or fine-tuning language models for RAG, but does not provide comprehensive coverage of training neural networks from scratch.","10":"The authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering' are Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.","11":"BM25 is used for retrieval in open-domain QA, usually implemented using TF-IDF or BM25, which matches keywords efficiently with an inverted index and can be seen as representing the question and context in high-dimensional, sparse vectors (with weighting).","12":"Wikipedia is a large corpus of diversified topics that can be used to answer factoid questions in an extractive QA setting.","13":"Roberts et al. define the similarity between a question and a passage using the dot product of their vectors: sim(q, p) = EQ(q)\u22baEP (p).","14":"Passage embeddings are used to represent text passages as dense vectors, which can then be compared using similarity scores. This allows for efficient training on large batches of question-passage pairs by reusing computation and creating multiple positive and negative examples within each batch.","15":"BM25 is used to select positive passages by ranking them based on their relevance to the questions. The highest-ranked passage containing the answer is selected as the positive passage.","16":"TREC is used as one of the QA datasets to evaluate the retrieval performance of the Dense Passage Retriever (DPR) model.","17":"The lower performance on SQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage, resulting in high lexical overlap between passages and questions, which gives BM25 an advantage. Second, the data was collected from only 500+ Wikipedia articles, leading to a biased distribution of training examples.","18":"The Intel Xeon CPU E5-2698 v4 @ 2.20GHz was used to profile the passage retrieval speed for DPR.","19":"In an end-to-end QA system, a passage selection score assigns a value to each retrieved passage based on its relevance to the question. This score helps determine which passage is most likely to contain the correct answer.","20":"REALMWiki is a model pretrained on Wikipedia.","21":"Lee et al. made a significant contribution by jointly training the question encoder and reader, which outperformed the BM25 plus reader paradigm on multiple open-domain QA datasets in terms of QA accuracy. Their approach was further extended by REALM (Guu et al., 2020), which included tuning the passage encoder asynchronously by re-indexing the passages during training.","22":"The sufficient context autorater helps in reducing hallucinations in RAG systems by providing a binary label for whether the context is sufficient to answer the query. This label is then combined with model self-rated confidence scores to tune a selective accuracy-coverage trade-off, where 'coverage' denotes the portion of inputs on which the model does not abstain. By using these signals to train a simple logistic regression model, the approach leads to a better selective accuracy-coverage trade-off compared to using model confidence alone.","23":"Query rewriting improves the performance of retrieval-augmented language models by adapting the retrieval query to better match the input text, thus enhancing the reader's ability to retrieve relevant and accurate information. This leads to consistent improvements in the model's performance on downstream tasks such as open-domain QA and multiple-choice QA.","24":"The combination of parametric memory with non-parametric memory in RAG models improves their performance on knowledge-intensive NLP tasks by allowing for the expansion and revision of knowledge, as well as direct access to and inspection of accessed knowledge. This hybrid approach addresses some limitations of pre-trained neural language models, which cannot easily expand or revise their memory and may produce 'hallucinations'. RAG models achieve state-of-the-art results on open-domain extractive question answering tasks such as Natural Questions, WebQuestions, and CuratedTrec.","25":"Notable approaches for retrieval-augmented text generation tasks include exemplar\/template retrieval for dialogue response generation, integrating translation memory into NMT models for machine translation, and applying retrieval-augmented generation in other tasks such as abstractive summarization, code generation, paraphrase, and knowledge-intensive generation. These approaches leverage deep learning technology to enhance the scalability and effectiveness of text generation.","26":"VisRAG enhances the retrieval and generation stages of RAG by employing a vision-language model (VLM)-based retriever and generator. Instead of parsing documents to obtain text, VisRAG directly embeds the document page as an image using a VLM and retrieves it to enhance the generation process. This approach maximizes the retention and utilization of data information in the original documents, eliminating information loss introduced during the parsing process.","27":"The SPLADE model addresses the limitations of previous sparse lexical representation models by incorporating sparsity regularization, which boosts both the efficiency (in terms of the number of floating-point operations) and effectiveness of these types of models. Additionally, it relies on a logarithmic activation function and efficient document expansion, achieving competitive results with respect to complex training pipelines for dense models like ANCE.","28":"ARES utilizes prediction-powered inference (PPI) to improve model-based evaluation accuracy by leveraging a small set of human-annotated datapoints for computing its confidence intervals. This approach allows ARES to provide statistical confidence intervals for RAG scoring, enhancing the precision and reliability of its evaluations.","29":"Dense Passage Retrieval for Open-Domain Question Answering introduced a method that uses dense representations alone to perform efficient passage retrieval, outperforming traditional sparse vector space models like TF-IDF or BM25. This was achieved by learning embeddings from a small number of questions and passages using a simple dual-encoder framework. When evaluated on various open-domain QA datasets, the dense retriever demonstrated significant improvements in terms of top-20 passage retrieval accuracy compared to strong Lucene-BM25 systems.","30":"The core idea behind Hypothetical Document Embeddings (HyDE) is to pivot through generating a hypothetical document that captures relevance patterns but may contain false details. This document is then encoded into an embedding vector by an unsupervised contrastively learned encoder, which identifies a neighborhood in the corpus embedding space where similar real documents are retrieved based on vector similarity. HyDE improves zero-shot dense retrieval by grounding the generated document to the actual corpus and filtering out incorrect details with the encoder's dense bottleneck.","31":"The purpose of constructing a Retrieval-Augmented Generation Benchmark (RGB) is to evaluate the retrieval-augmented generation of LLMs. It evaluates four specific abilities: Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness.","32":"Language models struggle to retrieve and use relevant information in the multi-document question answering task, as evidenced by their performance on a synthetic key-value retrieval task. This challenge is particularly pronounced for models that rely on decoder-only architectures or do not employ query-aware contextualization techniques.","33":"The trade-off between providing more context and its impact on language model performance in open-domain QA is that while longer input contexts may help improve downstream task performance, they also increase the amount of content the model must reason over, potentially decreasing accuracy. The study found that reader model performance saturates long before retriever performance saturates, indicating that readers are not effectively using the extra context. Using more than 20 retrieved documents only marginally improves reader performance (\u223c1.5% for GPT-3.5-Turbo and \u223c1% for Claude-1.3), while significantly increasing the input context length (and thus latency and cost).","34":"ARES used OpenAI\u2019s gpt-3.5-turbo-16k, version 10\/23, (Brown et al., 2020) as its in-context learning baseline.","35":"The use of FLAN-T5-XXL in the ARES evaluation framework has several limitations. It requires substantial GPU-based hardware with about 32GB of memory to run, taking several hours for fine-tuning and generation. Additionally, all of the datasets used in the evaluation are in English, which may limit its applicability to other languages.","36":"Figure 4 shows the overall performance of FLARE and baseline methods across all tasks\/datasets, demonstrating that FLARE outperforms all baselines on all tasks\/datasets.","37":"GraphRAG is a method for knowledge graph extraction that uses advanced machine learning models, specifically leveraging summaries over large sections of the data source as a form of 'self-memory'. It contrasts with vector RAG by generating a graph index from the source data and applying graph-based community detection to create a thematic partitioning of the data. This approach allows GraphRAG to answer queries that require global sensemaking over the entire data corpus, making it more effective for complex knowledge extraction tasks.","38":"Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work.","39":"The researchers used the in-batch negative setting for training their Dense Passage Retriever model, with a batch size of 128 and one additional BM25 negative passage per question.","40":"Gao et al. introduced Self-Reflective Retrieval-augmented Generation (SELF-RAG) to improve an LLM's generation quality, including its factual accuracy without hurting its versatility, via on-demand retrieval and self-reflection.","41":"FLARE outperforms all baselines on multihop QA tasks by a large margin. This is largely due to the task's clear definition and specific objective of producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to generate on-topic output. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs stay on topic."},"answer_correctness":{"0":0.4396710402,"1":0.6111275602,"2":0.510255728,"3":0.852928384,"4":0.665618246,"5":0.6889173952,"6":0.5298559981,"7":0.7379476766,"8":0.4233486398,"9":0.1858465953,"10":0.997764637,"11":0.7357170858,"12":0.3433164924,"13":0.7848424304,"14":0.4106118368,"15":0.5036875436,"16":0.7294336776,"17":0.2203033218,"18":0.7425322641,"19":0.8166323175,"20":0.5333604573,"21":0.1843189687,"22":0.6250958064,"23":0.5789731398,"24":0.5406461727,"25":0.4193631573,"26":0.5612714404,"27":0.5241383767,"28":0.6201290014,"29":0.7962244262,"30":0.8705814314,"31":0.9895637646,"32":0.2933268209,"33":0.7636271632,"34":0.5393203121,"35":0.4041235092,"36":0.8423884206,"37":0.8255088941,"38":0.6489454838,"39":0.7371329351,"40":0.6346202942,"41":0.8706288754},"answer_relevancy":{"0":0.9186952053,"1":1.0,"2":0.903758711,"3":0.9836265911,"4":0.9952359044,"5":0.8897519438,"6":0.9959982287,"7":0.9556319475,"8":0.0,"9":0.0,"10":0.9997343957,"11":0.9581079826,"12":1.0,"13":0.9907355054,"14":0.9898650863,"15":0.9793538993,"16":0.9726948468,"17":0.9578971753,"18":1.0,"19":1.0,"20":0.9260332865,"21":0.0,"22":0.951689622,"23":0.9713827158,"24":0.95897989,"25":0.9639335794,"26":0.9063753851,"27":0.9577480613,"28":0.9231941658,"29":0.961568048,"30":0.9011036384,"31":0.9763757847,"32":1.0,"33":0.0,"34":0.9406745841,"35":1.0,"36":0.9740856654,"37":0.9374049524,"38":0.9584063879,"39":0.9583500573,"40":0.8161802289,"41":0.9572645525},"faithfulness":{"0":1.0,"1":0.0,"2":0.6666666667,"3":0.75,"4":1.0,"5":0.6111111111,"6":0.75,"7":1.0,"8":0.0,"9":0.0,"10":1.0,"11":1.0,"12":0.8571428571,"13":1.0,"14":0.6,"15":0.5714285714,"16":0.625,"17":1.0,"18":1.0,"19":1.0,"20":0.625,"21":1.0,"22":0.8571428571,"23":0.7142857143,"24":1.0,"25":1.0,"26":1.0,"27":0.7727272727,"28":1.0,"29":1.0,"30":1.0,"31":1.0,"32":0.9473684211,"33":0.9090909091,"34":1.0,"35":1.0,"36":1.0,"37":1.0,"38":1.0,"39":1.0,"40":0.9333333333,"41":1.0},"context_precision":{"0":0.6791666666,"1":0.5333333333,"2":0.0,"3":1.0,"4":0.5888888889,"5":0.6791666666,"6":0.5333333333,"7":0.95,"8":0.0,"9":0.0,"10":0.5,"11":0.8875,"12":0.8666666666,"13":0.7555555555,"14":0.5333333333,"15":0.8041666666,"16":0.8875,"17":0.8666666666,"18":0.9999999999,"19":1.0,"20":0.9999999999,"21":0.0,"22":1.0,"23":0.95,"24":1.0,"25":1.0,"26":1.0,"27":1.0,"28":1.0,"29":1.0,"30":1.0,"31":1.0,"32":1.0,"33":1.0,"34":0.9999999999,"35":0.7,"36":0.8333333333,"37":1.0,"38":1.0,"39":0.5,"40":1.0,"41":1.0},"context_recall":{"0":0.75,"1":0.5,"2":0.0,"3":0.75,"4":1.0,"5":1.0,"6":0.3333333333,"7":1.0,"8":1.0,"9":0.5,"10":1.0,"11":1.0,"12":1.0,"13":1.0,"14":1.0,"15":1.0,"16":1.0,"17":0.3333333333,"18":1.0,"19":1.0,"20":1.0,"21":0.5,"22":0.6666666667,"23":1.0,"24":1.0,"25":1.0,"26":1.0,"27":1.0,"28":1.0,"29":1.0,"30":1.0,"31":1.0,"32":1.0,"33":1.0,"34":1.0,"35":1.0,"36":1.0,"37":0.6666666667,"38":1.0,"39":1.0,"40":1.0,"41":0.6666666667}}