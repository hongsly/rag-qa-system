{"user_input":{"0":"What is Retrieval-Augmented Generation?","1":"Who proposed the FiD (Fusion-in-Decoder) architecture?","2":"What does RAFT stand for?","3":"Why does hybrid retrieval (dense + sparse) outperform either approach alone?","4":"How does ColBERT differ from traditional dense retrieval?","5":"What are the trade-offs between long-context LLMs and RAG systems?","6":"How do GraphRAG and FiD differ in their approach to multi-document reasoning?","7":"What evaluation metrics are recommended for both retrieval and generation in RAG?","8":"How to bake bread?","9":"How do you train a neural network from scratch?","10":"Who are the authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering'?","11":"What is BM25 used for in open-domain QA?","12":"What is the purpose of using Wikipedia in open-domain question answering?","13":"What method does Roberts et al. use to measure the similarity between questions and passages?","14":"What is the purpose of passage embeddings in machine learning?","15":"What is BM25 used for in the context of question answering datasets?","16":"What is TREC used for in the context of passage retrieval experiments?","17":"What is the reason for the lower performance of SQuAD according to Lee et al.?","18":"What CPU model was used to profile the passage retrieval speed for DPR?","19":"What is the purpose of a passage selection score in an end-to-end QA system?","20":"What is REALMWiki?","21":"What significant contributions did Lee et al. make to the field of information retrieval, particularly in the context of dense retrieval models?","22":"How does the sufficient context autorater help in reducing hallucinations in RAG systems?","23":"How does query rewriting improve the performance of retrieval-augmented language models in handling factual information?","24":"How does the combination of parametric memory with non-parametric memory in RAG models improve their performance on knowledge-intensive NLP tasks?","25":"What are some notable approaches for retrieval-augmented text generation tasks and how do they leverage deep learning technology?","26":"How does VisRAG enhance the retrieval and generation stages of RAG by utilizing vision-language models?","27":"How does the SPLADE model address the limitations of previous sparse lexical representation models in terms of efficiency and effectiveness?","28":"How does ARES utilize prediction-powered inference (PPI) to improve model-based evaluation accuracy?","29":"How did the introduction of dense passage retrieval improve open-domain question answering systems?","30":"What is the core idea behind HyDE and how does it improve zero-shot dense retrieval?","31":"What is the purpose of constructing a Retrieval-Augmented Generation Benchmark (RGB) and what specific abilities does it evaluate?","32":"What challenges do language models face when retrieving information from their input contexts in open-domain question answering?","33":"What is the trade-off between providing more context and its impact on language model performance in open-domain QA?","34":"What model did ARES use as its in-context learning baseline?","35":"What are some limitations of using FLAN-T5-XXL in the ARES evaluation framework?","36":"What figure shows the overall performance of FLARE and baseline methods across all tasks\/datasets?","37":"What is GraphRAG and how does it differ from vector RAG in terms of its approach to knowledge graph extraction?","38":"How can the retrieval module be optimized to aid in a specific downstream task such as question answering?","39":"What training methodology did the researchers use for their Dense Passage Retriever model?","40":"What method did Gao et al. introduce to improve LLMs' generation quality without hurting their versatility?","41":"How does the effectiveness of FLARE in multihop QA tasks compare to other baselines, and what factors contribute to its performance?"},"retrieved_contexts":{"0":[" generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n\n\n**2** **Retrieval-Augmented Paradigm**\n\n\nIn this section, we first give a general formulation\n\n- f retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrievalaugmented generation paradigm, including the re\n\nOutput\n\n\n\n\n\nFigure 1: The overview of this survey.\n\n\n\ntrieval source, retrieval metric and integration meth\n- ds.\n\n\n**2.1** **Formulation**\n\n\nMost text generation tasks can be formulated as a\nmapping from input sequence _**x**_ to output sequence\n_**y**_ : _**y**_ = _f_ ( _**x**_ ). For instance, _**x**_ and _**y**_ could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\n\n\n_**y**_ = _f_ ( _**x**_ _,_ _**z**_ ) (1)\n\n\nwhere _**z**_ = _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that _**y**_ _[r]_\n\nmay benefit the response generation, if _**x**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r","## **Benchmarking Large Language Models in Retrieval-Augmented Generation**\n\n**Jiawei Chen** [1,3] **, Hongyu Lin** [1,*] **, Xianpei Han** [1,2,*] **, Le Sun** [1,2]\n\n1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n_{_ jiawei2020,hongyu,xianpei,sunle _}_ @iscas.ac.cn\n\n\n\n**Abstract**\n\n\nRetrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language\nmodels (LLMs). However, existing research lacks rigorous\nevaluation of the impact of retrieval-augmented generation\n\n  - n different large language models, which make it challenging to identify the potential bottlenecks in the capabilities\n\n  - f RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance\n\n  - f different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case.\nThen we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\n\n  - f noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n\n\n**Introduction**\n\n\nRecently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao","## **A Survey on Retrieval-Augmented Text Generation**\n\n**Huayang Li** _[\u2665][,][\u2217]_ **Yixuan Su** _[\u2660][,][\u2217]_ **Deng Cai** _[\u2666][,][\u2217]_ **Yan Wang** _[\u2663][,][\u2217]_ **Lemao Liu** _[\u2663][,][\u2217]_\n\n_\u2665_ Nara Institute of Science and Technology _\u2660_ University of Cambridge\n_\u2666_ The Chinese University of Hong Kong _\u2663_ Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\n\n\n\n**Abstract**\n\n\nRecently, retrieval-augmented text generation\nattracted increasing attention of the computational linguistics community. Compared\nwith conventional generation models, retrievalaugmented text generation has remarkable advantages and particularly has achieved state-ofthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrievalaugmented text generation. It firstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks including\ndialogue response generation, machine translation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval technology, has achieved state-of-the-art (SOTA) performance in many NLP tasks and attracted the attention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counterpart, this new paradigm has some remarkable advantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation"," et al. 2023)\nfocuses on human exam and ToolBench (Qin et al. 2023)\nevaluates how well LLMs use external tools. Recently, Adlakha et al. (2023) evaluate the RAG of LLMs in exist QA\ndataset. Different from their work, we focus on 4 required\nabilities of RAG and create Retrieval-Augmented Generation Benchmark to evaluate the LLMs.\n\n\n**Retrieval-Augmented Generation Benchmark**\n\n\nIn this section, we first introduce the specific retrievalaugmented generation abilities we aim to evaluate. Next, we\n\n- utline the process of constructing the RAG benchmark for\nevaluation. Lastly, we present the evaluation metrics.\n\n\n**Required abilities of RAG**\n\n\nExternal knowledge is the key to resolving the problems\n\n- f LLMs such as hallucination and outdated knowledge,\nwhich can make LLMs generate more accurate and reliable\nresponses through retrieval-augmented generation (RAG).\nHowever, LLMs cannot always response as expected with\nRAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating\nthese external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the\nuseful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the\nretrieval-augmented generation of LLMs, and we concern\nabout 4 specific abilities:\n**Noise Robustness** is the robustness of LLMs in noisy\ndocuments. As retrievers are not perfect, the external knowledge they retrieve often contains a significant amount of\nnoise, i.e., documents which are relevant to the question but\ndo not contain any information about the answer. To effectively answer user questions, LLMs must be able to extract\nthe necessary information from documents despite there are\nnoisy documents.\n**Negative Rejection** is a measure of whether LLMs can\ndecline to answer a question when none of the contexts provide useful information. In real-world situations, the search\nengine often fails to retrieve documents containing the answers. In these cases, it is important for the",". These attempts\ninclude methods that passively use the past context\nto retrieve additional information at a fixed interval\n(Khandelwal et al., 2020; Borgeaud et al., 2022;\nRam et al., 2023; Trivedi et al., 2022) which might\nnot accurately reflect what LMs intend to generate in the future or retrieve at inappropriate points.\nSome works in multihop QA decompose the full\nquestion into sub-questions, each of which is used\nto retrieve extra information (Press et al., 2022; Yao\net al., 2022; Khot et al., 2022; Khattab et al., 2022).\n\n\nWe ask the following question: can we create a\nsimple and generic retrieval augmented LM that _ac-_\n_tively decides when and what to retrieve_ throughout\nthe generation process, and are applicable to a variety of long-form generation tasks? We provide a\ngeneralized view of active retrieval augmented generation. Our hypothesis regarding _when to retrieve_\nis that LMs should retrieve information only when\n\n\n\nthey lack the required knowledge to avoid unnecessary or inappropriate retrieval that occurs in passive\nretrieval augmented LMs (Khandelwal et al., 2020;\nBorgeaud et al., 2022; Ram et al., 2023; Trivedi\net al., 2022). Given the observation that large LMs\ntend to be well-calibrated and low probability\/confidence often indicates a lack of knowledge (Kadavath et al., 2022), we adopt an active retrieval\nstrategy that only retrieves when LMs generate lowprobability tokens. When deciding _what to retrieve_,\nit is important to consider what LMs intend to generate in the future, as the goal of active retrieval is to\nbenefit future generations. Therefore, we propose\nanticipating the future by generating a temporary\nnext sentence, using it as a query to retrieve relevant documents, and then regenerating the next\nsentence conditioning on the retrieved documents.\nCombining the two aspects, we propose **F** - rward**L** - oking **A** ctive **RE** trieval augmented generation\n( **FLARE** ), as illustrated in Figure 1. FLARE iteratively generates _a temporary next sentence_, use\nit as the query to retrieve relevant documents _if it_\n_contains low-prob"],"1":["score and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is twice as large as the extractive\nmodels and has 770M parameters. We reuse the hyperparameters as described in Izacard & Grave\n(2020). The original FiD uses the top 100 passages for NaturalQuestions. In our case, we use the top\n50 retrieved passage sequences and concatenate the passages in each sequence before feeding into T5.\nIn order to fit this model into GPU, we make use of PyTorch checkpoint [11] for training.\n\n\nB.2.3 MULTI-HOP RAG\n\n\nThe RAG model aims to generate answer _y_ given question _x_ and the retrieved documents _z_ . Similarly,\nthe goal of multi-hop RAG can be expressed as: generate answer _y_ given question _x_ and retrieved\ndocuments in hop one _z_ 1 and hop two _z_ 2 (Limiting to two hops for HotpotQA). The model has three\ncomponents:\n\n\n   - Hop-one retriever _p\u03b7_ 1( _z_ 1 _|x_ ) with parameter _\u03b7_ 1 to represent the retrieved top-k passage\ndistribution (top-k truncated distribution) given the input question _x_ .\n\n\n    - Hop-two retriever _p\u03b7_ 2( _z_ 2 _|x, z_ 1) with parameter _\u03b7_ 2 to represent the hop-two retrieved top-k\npassage distribution given not only the question _x_ but also the retrieved document _z_ 1 from\nhop-one.\n\n\n   - A generator _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _,, y_ 1: _i\u2212_ 1) to represent the next token distribution given input\nquestion _x_, hop-one retrieved document _z_ 1, hop-two retrieved document _z_ 2 and previous\npredicted token _y_ 1: _i\u2212_ 1 parametrized by _\u03b8_\n\n\n**Multi-Hop RAG Sequence Model** As the RAG Sequence model, this model generates the answer\nsequence given the fixed set of documents from hop-one retriever and hop-two retriever. In order","Test    -    - 0 _._ 02 8 _._ 33 257 _._ 6 0 _._ 19\n\n\n**4.5** **Speed and Memory Benchmarks**\n\n\n\nTable 5: ( **SC09 Model Ablations** ) Models with 6M parameters. In\nSaShiMi\u2019s U-Net backbone, there are 8 center blocks operating on\nsequence length 1000, sandwiched on each side by 8 outer blocks on\nsequence length 4000, sandwiched by 8 outer blocks on sequence\nlength 16000 (40 blocks total). The architecture of the 8 center\nblocks are ablated independently of the rest. Note that Transformers\n(MHA+MLP) were not tested in the more important outer blocks\nbecause of efficiency constraints.\n\n\nOuter Center NLL \u2193 FID \u2193 IS \u2191 mIS \u2191 AM \u2193\n\n\nS4+MLP MHA+MLP 1.859 1.45 5.06 47.03 0.70\n\nS4+MLP S4+MLP 1.867 1.43 5.42 53.54 0.65\n\nS4+MLP Mamba 1.859 1.42 5.71 56.51 0.64\n\nMamba MHA+MLP **1.850** 1.37 5.63 58.23 0.62\n\nMamba S4+MLP 1.853 1.07 6.05 73.34 0.55\n\nMamba Mamba 1.852 **0.94** **6.26** **88.54** **0.52**\n\n\n\nWe benchmark the speed of the SSM scan operation (state expansion _\ud835\udc41_ = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our efficient SSM scan is faster than the best attention implementation that we know of\n(FlashAttention-2 (Dao 2024)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard scan implementation\nin PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar size, since without the\nKV cache it can use much higher batch sizes. For example, a Mamba-6.","89], except that the _\u2113_ 1\ncycle-consistency loss is replaced with our contrastive loss. In detail, we used\nLSGAN [50] and Resnet-based generator [34] with PatchGAN [31]. We define our\nencoder as the first half of the generator, and accordingly extract our multilayer\nfeatures from five evenly distributed points of the encoder. For single image\ntranslation, we use a StyelGAN2-based generator [36]. To embed the encoder\u2019s\n\n\nContrastive Learning for Unpaired Image-to-Image Translation 9\n\n\n**Cityscapes** **Cat** _\u2192_ **Dog** **Horse** _\u2192_ **Zebra**\n**Method**\n\n**mAP** _\u2191_ **pixAcc** _\u2191_ **classAcc** _\u2191_ **FID** _\u2193_ **FID** _\u2193_ **FID** _\u2193_ **sec\/iter** _\u2193_ **Mem(GB)** _\u2193_\n\n\nCycleGAN [89] 20.4 55.9 25.4 76.3 85.9 77.2 0.40 4.81\nMUNIT [44] 16.9 56.5 22.5 91.4 104.4 133.8 0.39 3.84\nDRIT [41] 17.0 58.7 22.2 155.3 123.4 140.0 0.70 4.85\nDistance [4] 8.4 42.2 12.6 81.8 155.3 72.0 **0.15** 2.72\nSelfDistance [4] 15.3 56.9 20.6 78.8 144.4 80.8 0.16 2.72\nGCGAN [18] 21.2 63.2 26.6 105.2 96.6 86.7 0.26 2.67\nCUT **24.7** **68.8** **30.7** **56.4** **76.2** **45.5** 0.24 3.33\n\nFastCUT 19.1 59.9 24.3 68.8 94.0 "," generation models,\nnumerical comparison is still a bigger issue compared to extractive models.\n\n\nB MODEL DETAILS\n\n\nB.1 BEST MODEL HYPERPARAMETERS\n\n\nTable 11: Hyperparameters of Retriever\n\nlearning rate 2e-5\nbatch size 150\nmaximum passage length 300\nmaximum query length at initial hop 70\nmaximum query length at 2nd hop 350\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 50\nweight decay 0\n\n\nTable 12: Hyperparameters of Extractive Reader (ELECTRA)\n\n\nlearning rate 5e-5\nbatch size 128\nmaximum sequence length 512\nmaximum answer length 30\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 7\nweight decay 0\n# of negative context per question 5\nweight of SP sentence prediction loss 0.025\n\n\n16\n\n\n\nTable 10: Answer EM using top 50 retrieved passage chains\n\n\n\nComp Bridge\nModel Overall\n(20%) (80%)\n\n\n\nELECTRA 61.7 79.0 57.4\n\nFiD 61.7 75.3 58.3\n\n\nPublished as a conference paper at ICLR 2021\n\n\nB.2 FURTHER DETAILS ABOUT READER MODELS\n\n\nB.2.1 EXTRACTIVE READER\n\n\nThe extractive reader is trained with four loss functions. With the [CLS] token, we predict a\nreranking score based on whether the passage sequence match the groundtruth supporting passages.\nOn top of the representation of each token, we predict a answer start score and answer end score.\nFinally, we prepend each sentence with the [unused0] special token and predict whether the\nsentence is one of the supporting sentences using the representations of the special token. At training\ntime, we pair each question with 1 groundtruth passage sequence and 5 negative passage sequence\nwhich do not contain the answer. At inference time, we feed in the top 250 passage sequences from\nMDR. We rank the predicted answer for each sequence with a linear combination of the reranking\nscore and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is","The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of D = 64 and\npooling factor 4. The S4+MLP block has roughly 2 _\ud835\udc37_ [2] + 4 _\ud835\udc37_ [2] parameters (expansion factor 2 in the MLP). The Transformer\nblock has 4 _\ud835\udc37_ [2] + 2 _\ud835\udc37_ [2] parameters (expansion factor 1 in the MLP). The Mamba block has the usual \u2248 6 _\ud835\udc37_ [2] parameters. All\nmodels have roughly 6M total parameters.\n\n\n**E.5** **Efficiency Benchmark**\n\n\n**Scan Operation.** We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), against\nconvolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operations\n\n- utside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing the\nQKV projections in attention.\n\n\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing the\nparameters _\ud835\udc68, \ud835\udc69, \ud835\udc6a_ in HBM.\n\n\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the large\nparameters in HBM.\n\n\n35\n\n\nTable 15: ( **Memory benchmark** .) Mamba\u2019s memory footprint is comparable to the most optimized Transformer. Results for 125M\nmodels.\n\n\nBatch size Transformer (w\/ FlashAttention-2) Mamba\n\n\n1 4.6GB 4.8GB\n\n2 5.2GB 5.8GB\n\n4 6.9GB 7.3GB\n\n8 11.5GB 12.3GB\n\n16 20.7GB 23.1GB\n\n32 34.5GB 38.2GB\n\n\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and the\nfilters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexity\nis _\ud835\udc42_ ( _\ud835\udc3f_ log( _\ufffd"],"2":["begin_quote## The Oberoi family is an\n Indian family that is famous for its involvement in hotels, namely through The\n Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\n the Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel\n company with its head office in Delhi. ##end_quote## establishes the head\n office of The Oberoi Group. Therefore, the Oberoi family is part of a hotel\n company whose head office is in Delhi. ##Answer: Delhi\n\n```\n\nFigure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,\ncontrasting them with the correct reasoning and answers. The LLM is prompted to identify\nerrors in its reasoning and extract key insights for improvement. This figure specifically\nrepresents the \u2018GenerateExplanation\u2018 step in the RAFT algorithm (Section 3).\n\n\nTable 1: **RAFT improves RAG performance for all specialized domains** : Across PubMed,\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Finetuning improves significantly of the performance of the base model, RAFT consistently\n\n- utperforms the existing domain-specific finetuning method with or without RAG. This\nsuggests the need to train the model with context. We compare our model with LLaMA\nfinetuning receipes, and provide GPT-3.5 for reference.\n\n\nPubMed HotPot HuggingFace Torch Hub TensorFlow\n\n\nGPT-3.5 + RAG 71.60 **41.5** 29.08 60.21 65.59\n\n\nLLaMA2-7B 56.5 0.54 0.22 0 0\n\nLLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06\n\nDSF 59.7 6.38 61.06 84.94 86.56\n\nDSF + RAG 71.6 4.41 42.59 82.80 60.29\n\n\nRAFT (LLaMA2-7B) **73.30** 35.28 **74.00** **84.95** **86.86**\n\n\n**Datasets** In our experiments, we use the following datasets to evaluate our model and\nall baselines. We"," distractor documents (] _[d][k][\u2212]_ [1][). For][ (][1] _[ \u2212]_ _[P]_ [)][ fraction of the questions]\n( _qi_ ) in the dataset, we include no golden document and only include distractor documents\n( _dk_ ). We then fine-tune the language model using standard supervised training (SFT)\ntechnique, training it to generate answers from the provided documents and question. Fig. 2\nillustrates the high-level design principal for RAFT .\n\n\nWe demonstrate that our RAG approach trains the model to perform better RAG on the set\n\n- f documents it is trained on _i.e., in-domain_ . By removing the golden documents in some\ninstances, we are compelling the model to memorize answers instead of deriving them from\nthe context. The training data for RAFT is as follows, and an example training data can be\nseen in Fig. 3:\n\n\n**P** % of data: **Q** + **D** _[\u2217]_ + **D** 1 + **D** 2 + . . . + **D** _k \u2192_ **A** _\u2217_\n\n\n(1 _\u2212_ **P** ) % of data: **Q** + **D** 1 + **D** 2 + . . . + **D** _k \u2192_ **A** _\u2217_\n\n\nSubsequently, for the test scenario, the model is provided with the Q and top-k documents\nretrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.\n\n\nA key factor in enhancing training quality is the generation of a reasoning process, such\nas Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we\ndemonstrate that creating a full reasoning chain and in-addition, clearly citing sources\nenhances the model\u2019s accuracy in answering questions. In Fig. 3, we illustrate this setup. Generating the training data in this fashion, involves presenting the model with a\nquestion, context, and verified answers, and then requesting it to form a reasoning chain\nthat appropriately references the original context.\n\n\nFor all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an"," with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does not know, it can still refer to the context.\n\n\n4.1 Results\n\n\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\n\n- utperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes\/no question, we don\u2019t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.\n\n\nOverall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its\nanswering style not aligning with the ground truth. By applying domain-specific tuning,\nwe significantly enhance its performance. This process enables the model to learn and adopt\nthe appropriate style of answering. However, introducing RAG to a domain-specifically\nfine-tuned (DSF) model doesn\u2019t invariably lead to better outcomes. This might indicate that\nthe model lacks training in context processing and extracting useful information from it. By\nincorporating our method, RAFT, we train the model not only to match its answering style\nwith that required but also to improve its document processing capabilities. Consequently,\n\n- ur approach outperforms all others.\n\n\n4.2 Effect of CoT\n\n\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\nin enhancing the model\u2019s performance. As indicated in Table 2, simply providing the answer\nto a question","0.150<br>0.175<br>0.200<br>0.225<br>0.250<br>Final Accuracy<br>~~Train D*~~<br>Train D* + 1D<br>~~Train D* + 2D~~<br>Train D* + 3D||||||\n|2<br>4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.125<br>0.150<br>0.175<br>0.200<br>0.225<br>0.250<br>Final Accuracy<br>~~Train D*~~<br>Train D* + 1D<br>~~Train D* + 2D~~<br>Train D* + 3D||||||\n\n\n\nFigure 6: **Test-Time Documents Varying** : To analyze how robust RAFT is to varying number\n\n- f test-time documents, we study three domains \u2013 NQ, Trivia QA and HotPot QA. In NQ,\nwe find that training with 4 documents leads to optimal performance, and this changes to 3\nand 2 for for Trivia QA and HotPot QA respectively. However, we see that training with\n\n- nly _golden_ documents leads to poor performance.\n\n\ntraining with _D_ _[\u2217]_ + 3 _D_ and it is _D_ _[\u2217]_ + 1 _D_ documents with Hotpot QA. This insight has been\nparticularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ\na training setup consisting of one golden document alongside four distractor documents.\n\n\n**Generalization to a variable number of test-time documents.** We extended our research\nto examine the impact of different quantities of test-time documents on the model\u2019s performance. Specifically, our experiments focused on assessing how models, trained with\nvarying numbers of distractor documents, respond to changes in the number of documents\npresented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distractor documents during training indeed makes the model more resilient to fluctuations in the\nnumber of documents encountered during testing. This ability to maintain consistent performance despite variations in test-time document numbers further validates the robustness of\n\n- ur approach, RAFT . This finding underscores the importance of a well-calibrated training\nenvironment to prepare the model for a range of"," chain\nthat appropriately references the original context.\n\n\nFor all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an example of the generation step in Fig. 3, the detailed\nreasoning answer includes a citation from the original context inside `##begin_quote##` and\n`##end_quote##` as well as the detailed explanation on how to reach the conclusion based on\nthe citations. We demonstrate that adding detailed reasoning paragraphs can help boost the\nmodel\u2019s performance in our experiment section.\n\n\n**4** **Evaluation**\n\n\nWe design our experiments to study how well RAFT performs compared to various baselines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading\nand extracting information from in-domain documents, than domain-specific finetuned\nmodel, and general-purpose model with RAG. As an ablation, we also demonstrate how\nimportant it is for the model to learn with Chain-of-Thought responses. In this section,\nwe will first introduce all the datasets we used in the experiments, then all the baseline\nmodel\/fine-tuning techniques that we benchmark against.\n\n\n4\n\n\nPreprint, Under Review\n\n```\n Question: The Oberoi family is part of a hotel company that has a head office\n in what city?\n\n context: [The Oberoi family is an Indian family that is famous for its\n involvement in hotels, namely through The Oberoi Group]...[It is located in\n city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\n Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\n towers that comprises a hotel and the Airlangga Apartment respectively]...[The\n Oberoi Group is a hotel company with its head office in Delhi.]\n\n Instruction: Given the question, context and answer above, provide a logical\n reasoning for that answer. Please use the format of: ##Reason: {reason}\n ##Answer: {answer}.\n\n -------------------------------------------------------------------------------------\n CoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\n Indian family that is famous for its involvement in hotels, namely through The\n Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\n the Oberoi group, and the"],"3":["6 5.0 4.6\nWQ 5.9 5.9 5.8 5.7\n\nCuratedTrec 6.9 7.2 6.3 5.9\n\nSQuAD 4.5 4.1 4.0 4.0\n\n\nTable 2: The Jaccard overlap between sparse retrieval\nresults and dense retrieval results.\n\n\nhybrid retrieval is less effective than dense retrieval in most settings, which is inconsistent with\n\n- ur experimental results. Instead, we found that\ndense\u2013sparse retrieval consistently beats sparse retrieval across all settings. The gains from both\nhybrid scoring techniques are statistically significant, with the exception of top-20 for CuratedTREC. Our results might be due to better BM25\neffectiveness, but we are unable to further diagnose these differences because, once again, the hybrid retrieval code is not provided in the DPR repo.\nFurther testing also found that the differences between the two hybrid techniques are not significant. Thus, there does not appear to be a strong\nbasis to prefer one hybrid technique over the other.\nIn Table 2, we report overlap when taking different top-k results from dense retrieval and sparse\nretrieval. Overlap is measured in terms of Jaccard overlap, which is computed by the intersection over the union. It is apparent that the overlap between dense and sparse results is quite small,\nwhich suggests that they are effective in very different ways. This provides an explanation of why\nhybrid retrieval is effective, i.e., they are exploiting very different signals. These results also justify the DPR design choice of retrieving k [\u2032] - k\nresults from dense and sparse retrieval and then\nrescoring the union to arrive at the final top-k.\n\n\n4.2 End-to-End QA\n\n\nTable 3 presents results for our end-to-end question answering replication experiments on the NQ\nand TriviaQA datasets in terms of the exact match\nscore. The original results are shown in the\n\u201corig\u201d column. The \u201crepl\u201d column reports our attempt to replicate exactly the span scoring technique described in the original paper, whereas the\n\u201cGAR\u201d column shows results from using the technique proposed by Mao et al. (2020). The version of each technique that incorporates retriever\nscores (see Section 2.3) is denoted with a * symbol, i.e., \u201crepl",", our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar of text retrieval using learned dense representations, which has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using sparse bag-of-words\nrepresentations (Zhan et al., 2020; Xiong et al.,\n2020; Hofst\u00a8atter et al., 2020; Lin et al., 2020).\nOur experiments largely verify the claims of\nKarpukhin et al. (2020) regarding the effectiveness of their proposed techniques. However, we\narrived at two important additional findings, one\n\n- f which is inconsistent with the original work, the\n\n- ther of which presents an enhancement:\n\n\n1. Focusing on retrieval, we found that the effectiveness of the sparse retrieval (BM25)\nbaseline is higher than values reported by\nKarpukhin et al. (2020). Whereas they reported\nthat dense\u2013sparse hybrid results do not meaningfully improve over dense retrieval alone,\nwe arrived at the opposite conclusion, where\nhybrid techniques yield statistically significant\ngains. We are able to achieve on average\na three-point improvement in top-20 accuracy\n\n  - ver the best DPR results across five standard\nQA test collections.\n\n\n2. Focusing on end-to-end QA effectiveness, we\nexplored different techniques for evidence com\n\n[1Artifact Review and Badging](https:\/\/www.acm.org\/publications\/policies\/artifact-review-and-badging-current)\n\n\nbination to extract the final answer span.\nWhereas the original DPR paper only used\nscores from the reader to identify the final\nanswer span, we investigated combining retriever scores and further experimented with\nthe answer span selection technique described\nby Mao et al. (2020). In our best condition,\nwe were able to achieve statistically significant\nimprovements of around three points on exact\nmatch scores over the original DPR implementation, using the same exact models.\n\n\nThe main contribution of this work is the replication of DPR, where our experimental results\nadd a number of important refinements to the\n\n- riginal work. Code associated with our retrieval experiments is packaged in our Pyserini IR\ntoolkit [2] (Lin et al., 2021) and code associated with\n\n-"," with this parlance,\nwe use \u201cdocument\u201d and \u201cpassage\u201d interchangeably throughout this paper. To add to the potential\nconfusion, results of the retriever are also referred\n\nto as \u201ccontexts\u201d that are fed to the reader.\n\nDense retrieval with DPR uses a query encoder\nand a passage encoder, which are both based on\nBERT. Queries and passages are encoded as dense\nrepresentation vectors as follows:\n\n\nq [\u2217] = BERTq(q), Dj [\u2217] [=][ BERT][D][(][D][j][)]\n\n\nwhere q [\u2217] and Dj [\u2217] [are low dimensional vectors]\n(768). The relevance score of a passage to a query\nis computed by dot product:\n\n\nSim(q, Dj) = \u27e8q [\u2217], Dj [\u2217][\u27e9]\n\n\nThus, the top k retrieval problem can be recast\nas a nearest neighbor search problem in vector\nspace. Operationally, this is accomplished via\nFacebook\u2019s Faiss library (Johnson et al., 2017).\n\nKarpukhin et al. (2020) also investigated hybrid\nretrieval, combining results from dense retrieval\n(DPR) and sparse retrieval (BM25) by computing\nthe linear combination of their respective scores to\nrerank the union of the two initial retrieved sets:\n\n\n\u03bb \u00b7 Sim(q, Dj) + BM25(q, Dj),\n\n\nwhere \u03bb = 1.1, an empirical value tuned on the\ndevelopment set. BM25 retrieval was performed\nusing Lucene with parameters b = 0.4 and k1 =\n0.9. However, the DPR repo does not appear to\ncontain code for reproducing the BM25 and hybrid\nfusion results.\n\nWe attempted to replicate the retriever results\nreported in Karpukhin et al. (2020) with Pyserini,\nan IR toolkit that our group has been developing\nsince 2019 (Lin et al., 2021). The toolkit supports\nsparse retrieval (i.e., BM25) via integration with\nanother toolkit called Anserini (Yang et al., 2017),\nwhich is built on Lucene. Like in the original DPR\nwork, Pyserini supports dense retrieval via integration with Facebook\u2019s Faiss library. Combining\ndense and sparse retrieval, our toolkit supports hybrid retrieval as well.\n\nTo be clear, we started with model checkpoint\nreleases in the"," original DPR\nwork, Pyserini supports dense retrieval via integration with Facebook\u2019s Faiss library. Combining\ndense and sparse retrieval, our toolkit supports hybrid retrieval as well.\n\nTo be clear, we started with model checkpoint\nreleases in the DPR repo and did not retrain the\nquery and passage encoders from scratch. Otherwise, our implementation does not share any code\nwith the DPR repo, other than evaluation scripts to\nensure that results are comparable.\n\n\nSimilar to the original work, we calculated\nhybrid retrieval scores by linear combination of\ndense and sparse scores, as follows:\n\n\nSim(q, Dj) + \u03b1 \u00b7 BM25(q, Dj).\n\n\nNote that, contrary to the original work, we placed\nthe \u03b1 weight on the BM25 score because this\nyields a more natural way to answer the pertinent\nresearch question: Given dense retrieval as a starting point, does adding BM25 as an additional relevance signal provide any value? This question is\nanswered by a setting of \u03b1 = 0, which is equivalent to discarding BM25 results.\nFinally, there are a few more details of exactly\nhow to combine BM25 and DPR scores worth ex\nploring. As a baseline, we tried using the raw\nscores directly in the linear combination (exactly\nas above). However, we noticed that the range of\nscores from DPR and BM25 can be quite different. To potentially address this issue, we tried the\nfollowing normalization technique: If a document\nfrom sparse retrieval is not in the dense retrieval\nresults, we assign to it the the minimum dense retrieval score among the retrieved documents as its\ndense retrieval score, and vice versa for the sparse\nretrieval score.\n\nTo arrive at a final top-k ranking, the original\nDPR paper generated top k [\u2032] results from DPR and\ntop k [\u2032] results from BM25 (where k [\u2032] - k), before considering the union of the two result sets\nand combining the scores to arrive at the final top\nk. Karpukhin et al. (2020) set k [\u2032] = 2000, but after some preliminary experimentation, we decided\nto fix k [\u2032] = 1000 in our experiments since it is a\nmore common setting in information retrieval experiments (for example, k = 1000 is the default in\nmost TREC evaluations).\n\n\n2.2 Reader\n\n\nAs is standard in a retriever","ing by emphasizing on _important_ terms and discarding\nmost of the terms without information content. Expansion allows\nto enrich documents, either by implicitly adding stemming effects\n(legs \u2192 leg) or by adding relevant topic words (e.g. treatment).\n\n\n**5** **CONCLUSION**\n\n\nRecently, dense retrieval based on BERT has demonstrated its superiority for first-stage retrieval, questioning the competitiveness of\ntraditional sparse models. In this work, we have proposed SPLADE,\na sparse model revisiting query\/document expansion. Our approach\nrelies on in-batch negatives, logarithmic activation and FLOPS regularization to learn effective and efficient sparse representations.\nSPLADE is an appealing candidate for initial retrieval: it rivals the\nlatest state-of-the-art dense retrieval models, its training procedure\nis straightforward, its sparsity\/FLOPS can be controlled explicitly\nthrough the regularization, and it can operate on inverted indexes.\nIn reason of its simplicity, SPLADE is a solid basis for further improvements in this line of research.\n\n\n\n33\n\n\n32\n\n\n31\n\n\n30\n\n\n29\n\n\n28\n\n\n27\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||a<br>a|a<br>a|vg q len <br>vg d len|_ \u2243_15<br>_ \u2243_58||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||y-T5<br>|y-T5<br>|\n||||||||||||doc2quer<br>|y-T5<br>|y-T5<br>|\n||||~~a~~<br>a|~~a~~<br>a|~~vg q len~~ <br>vg d len|~~_ \u2243_6~~<br>_ \u2243_18||<br> <br>|||~~SparTerm~~<br>ST exp-_\u2113F_<br>SPLADE-|~~ lexical~~<br>_LOPS_<br>_\u2113_~~1~~||\n||||~~a~~<br>a|~~a~~<br>a||||||||||\n||||||||||||SPLADE-|_\u2113FLOPS_|_\u2113FLO"],"4":[" of encoding documents offline and amortizing the cost of encoding the query once across\nall ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., [1, 15]) to retrieve the\ntop- _k_ results directly from a large document collection, substantially improving _recall_ - ver models that only re-rank the output of\nterm-based retrieval.\n\nAs Figure 1 illustrates, ColBERT can serve queries in tens or\nfew hundreds of milliseconds. For instance, when used for reranking as in \u201cColBERT (re-rank)\u201d, it delivers over 170\u00d7 speedup\n(and requires 14,000\u00d7 fewer FLOPs) relative to existing BERT-based\nmodels, while being more effective than every non-BERT baseline\n(\u00a74.2 & 4.3). ColBERT\u2019s indexing\u2014the only time it needs to feed\ndocuments through BERT\u2014is also practical: it can index the MS\nMARCO collection of 9M passages in about 3 hours using a single\nserver with four GPUs (\u00a74.5), retaining its effectiveness with a space\nfootprint of as litle as few tens of GiBs. Our extensive ablation\nstudy (\u00a74.4) shows that late interaction, its implementation via\nMaxSim operations, and crucial design choices within our BERTbased encoders are all essential to ColBERT\u2019s effectiveness.\nOur main contributions are as follows.\n\n\n(1) We propose _late interaction_ (\u00a73.1) as a paradigm for efficient\nand effective neural ranking.\n(2) We present ColBERT (\u00a73.2 & 3.3), a highly-effective model\nthat employs novel BERT-based query and document encoders within the late interaction paradigm.\n\n\n(3) We show how to leverage ColBERT both for re-ranking on\ntop of a term-based retrieval model (\u00a73.5) and for searching\na full collection using vector similarity indexes (\u00a73.6).\n(4) We evaluate ColBERT on MS MARCO and TREC CAR, two\nrecent passage search collections.\n\n\n**2** **RELATED WORK**\n\n\n**Neural Matching Models.** Over the past few years, IR researchers\nhave introduced numerous neural architectures for ranking. In\nthis work, we compare against KNRM [4, 36], Duet [20, 22], ConvKNRM [4], and fastText+Conv"," the index\n\n- therwise. [8] As discussed in \u00a74.1, we use ColBERTL2 for end-toend retrieval, which employs negative squared L2 distance as its\nvector-similarity function. For its latency, we measure the time for\nfaiss-based candidate filtering and the subsequent re-ranking. In\nthis experiment, faiss uses all available CPU cores.\nLooking at Table 2, we first see Anserini\u2019s BM25 baseline at 18.7\nMRR@10, noticing its very low latency as implemented in Anserini\n(which extends the well-known Lucene system), owing to both\nvery cheap operations and decades of bag-of-words top- _k_ retrieval\n\n- ptimizations. Te three subsequent baselines, namely doc2query,\nDeepCT, and docTTTTquery, each brings a decisive enhancement\nto effectiveness. Tese improvements come at negligible overheads\nin latency, since these baselines ultimately rely on BM25-based\nretrieval. Te most effective among these three, docTTTTquery,\ndemonstrates a massive 9% gain over vanilla BM25 by fine-tuning\nthe recent language model T5.\n\n\n7htp:\/\/anserini.io\/\n8In practice, a myriad of reasons could still cause DeepCT\u2019s latency to differ\nslightly from BM25\u2019s. For instance, the top- _k_ pruning strategy employed, if any, could\ninteract differently with a changed distribution of scores.\n\n\nShifing our atention to ColBERT\u2019s end-to-end retrieval effectiveness, we see its major gains in MRR@10 over all of these end-toend models. In fact, using ColBERT in the end-to-end setup is superior in terms of MRR@10 to re-ranking with the same model due\nto the improved recall. Moving beyond MRR@10, we also see large\ngains in Recall@ _k_ for _k_ equals to 50, 200, and 1000. For instance,\nits Recall@50 actually exceeds the official BM25\u2019s Recall@1000 and\neven all but docTTTTTquery\u2019s Recall@200, emphasizing the value\n\n- f end-to-end retrieval (instead of just re-ranking) with ColBERT.\n\n|BERT [CLS]-based dot-product (5-layer) [A]|Col2|Col3|Col4|Col5|Col6"," and another for the document, extracted from\nBERT\u2019s [CLS] contextualized embedding and expanded through a\nlinear layer to dimension 4096 (which equals _Nq_ \u00d7 128 = 32 \u00d7 128).\nRelevance is estimated as the inner product of the query\u2019s and the\ndocument\u2019s embeddings, which we found to perform beter than\ncosine similarity for single-vector re-ranking. As the results show,\nthis model is considerably less effective than ColBERT, reinforcing\nthe importance of late interaction.\nSubsequently, we ask if our MaxSim-based late interaction is better than other simple alternatives. We test a model [B] that replaces\nColBERT\u2019s maximum similarity with _average_ similarity. Te results\nsuggest the importance of individual terms in the query paying\nspecial atention to particular terms in the document. Similarly,\nthe figure emphasizes the importance of our query augmentation\nmechanism: without query augmentation [C], ColBERT has a noticeably lower MRR@10. Lastly, we see the impact of end-to-end\nretrieval not only on recall but also on MRR@10. By retrieving\ndirectly from the full collection, ColBERT is able to retrieve to the\ntop-10 documents missed entirely from BM25\u2019s top-1000.\n\n\n\n|Basic ColBERT Indexing<br>+multi-GPU document processing<br>+per-batch maximum sequence length<br>+length-based bucketing<br>+multi-core pre-processing|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic","ation. Finally, we sort the _k_ documents by their total scores.\nRelative to existing neural rankers (especially, but not exclusively, BERT-based ones), this computation is very cheap that, in\nfact, its cost is dominated by the cost of gathering and transferring\nthe pre-computed embeddings. To illustrate, ranking _k_ documents\nvia typical BERT rankers requires feeding BERT _k_ different inputs\neach of length _l_ = | _q_ | + | _di_ | for query _q_ and documents _di_, where\natention has quadratic cost in the length of the sequence. In contrast, ColBERT feeds BERT only a single, much shorter sequence of\nlength _l_ = | _q_ |. Consequently, ColBERT is not only cheaper, it also\nscales much beter with _k_ as we examine in \u00a74.2.\n\n\n**3.6** **End-to-end Top-** _k_ **Retrieval with ColBERT**\n\nAs mentioned before, ColBERT\u2019s late-interaction operator is specifically designed to enable end-to-end retrieval from a large collection,\nlargely to improve recall relative to term-based retrieval approaches.\nTis section is concerned with cases where the number of docu\nments to be ranked is too large for exhaustive evaluation of each\npossible candidate document, particularly when we are only interested in the highest scoring ones. Concretely, we focus here on\nretrieving the top- _k_ results directly from a large document collection with _N_ (e.g., _N_ = 10, 000, 000) documents, where _k_ \u226a _N_ .\nTo do so, we leverage the pruning-friendly nature of the MaxSim\n\n- perations at the backbone of late interaction. Instead of applying MaxSim between one of the query embeddings and all of one\ndocument\u2019s embeddings, we can use fast vector-similarity data\nstructures to efficiently conduct this search between the query\nembedding and _all_ document embeddings across the full collection. For this, we employ an off-the-shelf library for large-scale\nvector-similarity search, namely faiss [15] from Facebook. [4] In particular, at the end of offline indexing (\u00a73.4), we maintain a mapping\nfrom each embedding to its document of origin and then index all\ndocument embeddings into faiss.\nSubsequently"," emphasizing the value\n\n- f end-to-end retrieval (instead of just re-ranking) with ColBERT.\n\n|BERT [CLS]-based dot-product (5-layer) [A]|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|**ColBERT via average similarity (5-layer) [B]**||||||||||\n|**ColBERT without query augmentation (5-layer) [C]**||||||||||\n|**ColBERT (5-layer) [D]**||||||||||\n|**ColBERT (12-layer) [E]**||||||||||\n|**ColBERT + e2e retrieval (12-layer) [F]**||||||||||\n\n\n\n0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.36\n\n\nMRR@10\n\n\n**Figure 5: Ablation results on MS MARCO (Dev). Between**\n**brackets is the number of BERT layers used in each model.**\n\n\nTe results from \u00a74.2 indicate that ColBERT is highly effective\ndespite the low cost and simplicity of its late interaction mechanism.\nTo beter understand the source of this effectiveness, we examine a\nnumber of important details in ColBERT\u2019s interaction and encoder\narchitecture. For this ablation, we report MRR@10 on the validation\nset of MS MARCO in Figure 5, which shows our main _re-ranking_\nColBERT model [E], with MRR@10 of 34.9%.\nDue to the cost of training all models, we train a copy of our\nmain model that retains only the first 5 layers of BERT out of 12\n(i.e., model [D]) and similarly train all our ablation models for 200k\niterations with five BERT layers. To begin with, we ask if the finegranular _interaction_ in late interaction is necessary. Model [A]\ntackles this question: it uses BERT to produce a single embedding\nvector for the query and another for the document, extracted from\nBERT\u2019s [CLS] contextualized embedding and expanded through a\nlinear layer to dimension 4096 (which equals _Nq_ \u00d7 128 = 32 \u00d7 128).\nRelevance is estimated"],"5":[", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop\/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation"," LLM without the correct corresponding context at times can be\nbeneficial for the downstream task of answering questions related to the documents. In our\ntraining setup, we include four distractor documents alongside the golden document, and at\ntest time, we maintain this format by providing the golden document with four distractors.\nOur findings suggest that, for domain-specific RAG tasks, including a certain percentage of\ntraining data without the golden documents in the context proves to be advantageous.\n\n\n**5** **RAFT Generalizes to Top-K RAG**\n\n\nWe now study another important problem: How does the number of distractor documents\nin RAFT affect the model\u2019s performance when augmented with top-k RAG results during\nevaluation? Previous research has highlighted the vulnerability of LLMs to irrelevant text\n(see studies (Shi et al., 2023a; Weston & Sukhbaatar, 2023; Liu et al., 2023)). This issue is\nparticularly critical for LLMs + RAG since top-k RAG is frequently employed at test time to\nensure high recall. Such a scenario necessitates the model to have the ability to discern and\ndisregard irrelevant content, focusing solely on pertinent information.\n\n\n5.1 Making Model Robust to top-K RAG\n\n\nTo tackle the challenge of enhancing large language models\u2019 (LLMs) ability to sift through\nirrelevant text within the retrieval pipeline, our analysis revealed that training solely with\ngolden (highly relevant) documents can inadvertently diminish the model\u2019s ability to discern and disregard irrelevant information. To address this, our algorithm, RAFT, adopts\na strategy that integrates golden documents with a mix of irrelevant ones. This method\n- logy prompts us to investigate the ideal fraction of distractor (irrelevant) documents to\nincorporate throughout the training process and to assess how well this training approach\nadapts to different volumes of documents encountered by the Retrieval-Augmented Generation (RAG) during the test phase. Our aim is to refine the balance between relevant\nand irrelevant information to strenghten the model\u2019s efficiency in identifying and utilizing\npertinent content. Notice that Sec 4.4 looked what what P% of training data should include\ndistractors, while in this section, we study test-time scenarios.\n\n\n**Training with Distractor Documents** To enhance the robustness of LLMs against irrelevant\ntext in retrieved documents, we adopted a fin"," 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RAGAS (James and Es, 2023). To evaluate RAG systems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several associated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\n\n- ffer little adaptability to new RAG evaluation set\n\ntings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n\n\n**3** **ARES**\n\n\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\n\n- r more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query\u2013passage\u2013answer\ntriples to train LLM judges. Subsequently, we apply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to estimate a confidence interval for the quality of each\nRAG system.\n\n\n**3.1** **LLM Generation of Synthetic Dataset**\n\n\nWe generate synthetic queries and answers from\nthe corpus"," we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\n\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70.0%, 72.5%, 75.0%, ..., 90.0%).\nEach split also represents a different mock RAG\nsystem. Since we know the success percentages of\neach dataset split, we know the appropriate ranking of each mock RAG system. This allows us to\n\n\ntest ARES success at both scoring and ranking the\nmock RAG systems appropriately across the three\nevaluation criteria.\n\n\n**4.3** **Metrics**\n\n\nTo calculate the correlation between the correct\n\nranking and the ARES ranking, we use the Kendall\nrank correlation coefficient or Kendall\u2019s _\u03c4_ :\n\n\n_\u03c4_ = [(#][ of concordant pairs][)] _[ \u2212]_ [(#][ of discordant pairs][)]\n\n# of pairs total\n\n\nConcordant pairs are defined as two ordinal values in the ranking where the earlier value in the\nsequence is lower than the later value in the sequence. Discordant pairs are defined as two ordinal\nvalues in the ranking where the earlier value in the\nsequence is greater than or equal to the later value\nin the sequence. A Kendall\u2019s _\u03c4_ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document preprocessing. We want to make sure that ARES has satisfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall\u2019s _\u03c4_ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pairwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in\ninformation retrieval, allowing developers to evaluate ranking systems empirically. Therefore, we\nbelieve Kendall\u2019s tau and prediction accuracy provide meaningful metrics for testing the efficacy of\nARES as a RAG evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges"," [170]\u2013[172]. Presently, LLMs can\neffortlessly manage contexts exceeding 200,000 tokens [9] . This\ncapability signifies that long-document question answering,\npreviously reliant on RAG, can now incorporate the entire\ndocument directly into the prompt. This has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n\n\n[8https:\/\/www.trulens.org\/trulens eval\/core concepts rag triad\/](https:\/\/www.trulens.org\/trulens_eval\/core_concepts_rag_triad\/)\n[9https:\/\/kimi.moonshot.cn](https:\/\/kimi.moonshot.cn)\n\n\n\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quickly locate the original\nreferences for LLMs to help users verify the generated answers. The entire retrieval and reasoning process is observable,\nwhile generation solely relying on long context remains a\nblack box. Conversely, the expansion of context provides new\n\n- pportunities for the development of RAG, enabling it to\naddress more complex problems and integrative or summary\nquestions that require reading a large amount of material to\nanswer [49]. Developing new RAG methods in the context of\nsuper-long contexts is one of the future research trends.\n\n\n_B. RAG Robustness_\n\n\nThe presence of noise or contradictory information during\nretrieval can detrimentally affect RAG\u2019s output quality. This\nsituation is figuratively referred to as \u201cMisinformation can\nbe worse than no information at all\u201d. Improving RAG\u2019s\nresistance to such adversarial or counterfactual inputs is gaining research momentum and has become a key performance\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\ntype of documents should be retrieved, evaluate the relevance\n\n- f the documents to the prompt, their position, and the\nnumber included in the context. The research findings reveal\nthat including irrelevant documents can unexpectedly increase\naccuracy by over 30%, contradicting the initial assumption\n\n- f reduced quality. These results underscore the importance\n\n- f developing specialized strategies to integrate retrieval with\n"],"6":["hi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n(e.g., passages, tables) and edges encoding lexical and semantical similarity or structural relationships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of\ngraphs in this context: their inherent _modularity_ (Newman, 2006) and the ability to partition graphs\ninto nested modular communities of closely related nodes (e.g., Louvain, Blondel et al. 2008; Leiden, Traag et al. 2019). Specifically, GraphRAG recursively creates increasingly global summaries\nby using the LLM to create summaries spanning this community hierarchy.\n\n\n**2.3** **Adaptive benchmarking for RAG Evaluation**\n\n\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthese benchmarks are oriented towards vector RAG performance, i.e., they evaluate performance\n\n- n explicit fact retrieval. In this work, we propose an approach for generating a set of questions\nfor evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM\nmethods that use a corpus to generate questions whose answers would be summaries of the corpus,\nsuch as in Xu and Lapata (2021). However, in order to produce a fair evaluation, our method avoids\ngenerating the questions directly from the corpus itself (as an alternative implementation, one can\nuse a subset of the corpus held out from subsequent graph extraction and answer evaluation steps).\n\n\n_Adaptive benchmarking_ refers to the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases. Recent work has used LLMs for adaptive benchmarking\nto ensure relevance, diversity, and alignment with the target application or task (Yuan et al., 2024;\nZhang et al., 2024b). In this work, we propose an adaptive benchmarking approach to generating\nglobal sensemaking queries for the LLM. Our approach builds on prior work in LLM-based persona\ngeneration, where the LLM is used to generate diverse and authentic sets of personas ("," when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https:\/\/github _._ [com\/microsoft\/graphrag. In ad-](https:\/\/github.com\/microsoft\/graphrag)\ndition, versions of the GraphRAG approach are also available as extensions to multiple opensource libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), NebulaGraph (NebulaGraph, 2024), and Neo4J (Neo4J, 2024).\n\n\n**2** **Background**\n\n\n**2.1** **RAG Approaches and Systems**\n\n\nRAG generally refers to any system where a user query is used to retrieve relevant information from\nexternal data sources, whereupon this information is incorporated into the generation of a response\nto the query by an LLM (or other generative AI model, such as a multi-media model). The query and\nretrieved records populate a prompt template, which is then passed to the LLM (Ram et al., 2023).\nRAG is ideal when the total number of records in a data source is too large to include in a single\nprompt to the LLM, i.e. the amount of text in the data source exceeds the LLM\u2019s context window.\n\n\nIn canonical RAG approaches, the retrieval process returns a set number of records that are semantically similar to the query and the generated answer uses only the information in those retrieved\nrecords. A common approach to conventional RAG is to use text embeddings, retrieving records\nclosest to the query in vector space where closeness corresponds to semantic similarity (Gao et al.,\n2023). While some RAG approaches may use alternative retrieval mechanisms, we collectively refer\nto the family of conventional approaches as _vector RAG_ . GraphRAG contrasts with vector RAG in\nits ability to answer queries that require global sensemaking over the entire data corpus.\n\n\n2\n\n\nGraphRAG builds upon prior work on advanced RAG strategies. GraphRAG leverages summaries\n\n- ver large sections of the data source as a form of \u201dself-memory\u201d (described in Cheng et al. 2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao"," a\nclass of global sensemaking questions over datasets in the 1 million token range,\nwe show that GraphRAG leads to substantial improvements over a conventional\nRAG baseline for both the comprehensiveness and diversity of generated answers.\n\n\n**1** **Introduction**\n\n\nRetrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using\nLLMs to answer queries based on data that is too large to contain in a language model\u2019s _context_\n_window_, meaning the maximum number of _tokens_ (units of text) that can be processed by the LLM\nat once (Kuratov et al., 2024; Liu et al., 2023). In the canonical RAG setup, the system has access to\na large external corpus of text records and retrieves a subset of records that are individually relevant\nto the query and collectively small enough to fit into the context window of the LLM. The LLM then\n\n\nPreprint. Under review.\n\n\ngenerates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang,\n2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call\n_vector RAG_, works well for queries that can be answered with information localized within a small\nset of records. However, vector RAG approaches do not support _sensemaking_ queries, meaning\nqueries that require global understanding of the entire dataset, such as \u201d _What are the key trends in_\n_how scientific discoveries are influenced by interdisciplinary research over the past decade?_ \u201d\n\n\n_Sensemaking_ tasks require reasoning over \u201c _connections (which can be among people, places, and_\n_events) in order to anticipate their trajectories and act effectively_ \u201d (Klein et al., 2006). LLMs such\nas GPT (Achiam et al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil\net al., 2023) excel at sensemaking in complex domains like scientific discovery (Microsoft, 2023)\nand intelligence analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an\nimplicit and interconnected set of concepts, an LLM can generate a summary that answers the query","\u201d, \u201cfaithfulness\u201d,\nand \u201canswer relevance\u201d (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can\nquantify relative performance for a given criterion by prompting the LLM to compare generations\nfrom two different competing models (LLM-as-a-judge, (Zheng et al., 2024)). In this work, we design criteria for evaluating RAG-generated answers to global sensemaking questions and evaluate\n\n- ur results using the comparative approach. We also validate results using statistics derived from\nLLM-extracted statements of verifiable facts, or \u201cclaims.\u201d\n\n\n**3** **Methods**\n\n\n**3.1** **GraphRAG Workflow**\n\n\nFigure 1 illustrates the high-level data flow of the GraphRAG approach and pipeline. In this section,\nwe describe the key design parameters, techniques, and implementation details for each step.\n\n\n**3.1.1** **Source Documents** _\u2192_ **Text Chunks**\n\n\nTo start, the documents in the corpus are split into text chunks. The LLM extracts information from\neach chunk for downstream processing. Selecting the size of the chunk is a fundamental design\ndecision; longer text chunks require fewer LLM calls for such extraction (which reduces cost) but\nsuffer from degraded recall of information that appears early in the chunk (Kuratov et al., 2024; Liu\net al., 2023). See Section A.1 for prompts and examples of the recall-precision trade-offs.\n\n\n**3.1.2** **Text Chunks** _\u2192_ **Entities & Relationships**\n\n\nIn this step, the LLM is prompted to extract instances of important _entities_ and the _relationships_\nbetween the entities from a given chunk. Additionally, the LLM generates short descriptions for the\nentities and relationships. To illustrate, suppose a chunk contained the following text:\n\n\n4\n\n\nNeoChip\u2019s (NC) shares surged in their first week of trading on the NewTech Exchange. However, market analysts caution that the chipmaker\u2019s public debut may\nnot reflect trends for other technology IPOs. NeoChip, previously a private entity,\nwas acquired by Quantum Systems in 2016. The innovative semiconductor firm\nspecializes in low-power processors for wearables and IoT devices.\n\n\nThe LLM is prompted such that it extracts the following:\n\n\n    - The entity `NeoChip`, with description \u201cNeoChip is a publicly","\u223c_\n\n- verlaps between chunks ( 1 million tokens).\n\n\n**News articles** . A benchmark dataset comprised of news articles published from September 2013\nto December 2023 in a range of categories, including entertainment, business, sports, technology,\nhealth, and science (Tang and Yang, 2024). The corpus is divided into 3197 _\u00d7_ 600-token text\n\n_\u223c_\nchunks, with 100-token overlaps between chunks ( 1.7 million tokens).\n\n\n**4.1.2** **Conditions**\n\n\nWe compared six conditions including GraphRAG at four different graph community levels ( **C0**,\n**C1**, **C2**, **C3** ), a text summarization method that applies our map-reduce approach directly to source\ntexts ( **TS** ), and a vector RAG \u201csemantic search\u201d approach ( **SS** ):\n\n\n    - **CO** . Uses root-level community summaries (fewest in number) to answer user queries.\n\n\n    - **C1** . Uses high-level community summaries to answer queries. These are sub-communities\n\n     - f C0, if present, otherwise C0 communities projected downwards.\n\n\n    - **C2** . Uses intermediate-level community summaries to answer queries. These are subcommunities of C1, if present, otherwise C1 communities projected downwards.\n\n\n    - **C3** . Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected downwards.\n\n\n    - **TS** . The same method as in Section 3.1.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n\n\n    - **SS** . An implementation of vector RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\n\n\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\n\n\nThe graph index supporting conditions **C0** - **C3** was created using our generic prompts for entity and\nrelationship extraction, with entity types and few-shot examples tailored to the domain of the data.\n\n\n8\n\n\n**4"],"7":[", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop\/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation"," in the context. The research findings reveal\nthat including irrelevant documents can unexpectedly increase\naccuracy by over 30%, contradicting the initial assumption\n\n- f reduced quality. These results underscore the importance\n\n- f developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\n\n\n_C. Hybrid Approaches_\n\n\nCombining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-toend joint training\u2014and how to harness both parameterized\n\n\n15\n\n\n\nTABLE IV\n\nSUMMARY OF EVALUATION FRAMEWORKS\n\n\n**Evaluation Framework** **Evaluation Targets** **Evaluation Aspects** **Quantitative Metrics**\n\n\n\nAccuracy\nEM\n\nAccuracy\nAccuracy\n\n\n\nRetrieval Quality\nRGB _[\u2020]_\nGeneration Quality\n\n\n\nNoise Robustness\n\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\n\n\n\nRECALL _[\u2020]_ Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\n\n\n\n    \n    \nCosine Similarity\n\n\nAccuracy\nAccuracy\nAccuracy\n\n\n    \n    \n    \n\nBLEU\n\nROUGE-L\n\nBertScore\n\nRAGQuestEval\n\n\n\nRetrieval Quality\nRAGAS _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nARES _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nTruLens _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nCRUD _[\u2020]_\nGeneration Quality\n\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nCreative Generation\n\nKnowledge-intensive QA\nError Correction\n\nSummarization\n\n\n\n\n_\u2020 represents a benchmark, and \u2021 represents a tool. * denotes customized quantitative metrics, which deviate from traditional_\n_metrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these_\n_metrics, as required._\n\n\n\nand non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific\nfunctionalities into RAG and fine-tuned by the results of RAG\nsystem. For example, CRAG [67] trains a lightweight retrieval\nevaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval\nactions based on confidence levels","ACL [56] [56]\nMath GSM8K [158] [73]\nMachine Translation JRC-Acquis [159] [17]\n\n\n14\n\n\n\nTABLE III\n\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\n\n\nContext Faithfulness Answer Noise Negative Information Counterfactual\nRelevance Relevance Robustness Rejection Integration Robustness\n\n\nAccuracy \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713\nEM \u2713\n\nRecall \u2713\n\nPrecision \u2713 \u2713\n\nR-Rate \u2713\n\nCosine Similarity \u2713\nHit Rate \u2713\n\nMRR \u2713\n\nNDCG \u2713\n\nBLEU \u2713 \u2713 \u2713\n\nROUGE\/ROUGE-L \u2713 \u2713 \u2713\n\n\n\nThe specific metrics for each evaluation aspect are summarized in Table III. It is essential to recognize that these\nmetrics, derived from related work, are traditional measures\nand do not yet represent a mature or standardized approach for\nquantifying RAG evaluation aspects. Custom metrics tailored\nto the nuances of RAG models, though not included here, have\nalso been developed in some evaluation studies.\n\n\n_D. Evaluation Benchmarks and Tools_\n\n\nA series of benchmark tests and tools have been proposed\nto facilitate the evaluation of RAG.These instruments furnish\n\nquantitative metrics that not only gauge RAG model performance but also enhance comprehension of the model\u2019s capabilities across various evaluation aspects. Prominent benchmarks\nsuch as RGB, RECALL and CRUD [167]\u2013[169] focus on\nappraising the essential abilities of RAG models. Concurrently, state-of-the-art automated tools like RAGAS [164],\nARES [165], and TruLens [8] employ LLMs to adjudicate the\nquality scores. These tools and benchmarks collectively form\na robust framework for the systematic evaluation of RAG\nmodels, as summarized in Table IV.\n\n\nVII. DISCUSSION AND FUTURE PROSPECTS\n\n\nDespite the considerable progress in RAG technology, several challenges persist that warrant in-depth research.This\nchapter will mainly introduce the current challenges and future\nresearch directions faced by RAG.\n\n\n_A. RAG vs Long Context_\n\n\nWith the deepening of related research, the context of LLMs\nis continuously expanding [170]\u2013[172]. Presently, LLMs can\neffortlessly manage contexts exceeding 200,000 tokens [9] . This\ncapability signifies that long-document question answering,\npreviously reliant on RAG, can now incorporate","fulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation methods [29], [161], [163].\n\n\n_C. Evaluation Aspects_\n\nContemporary evaluation practices of RAG models emphasize three primary quality scores and four essential abilities,\nwhich collectively inform the evaluation of the two principal\ntargets of the RAG model: retrieval and generation.\n_1) Quality Scores:_ Quality scores include context relevance, answer faithfulness, and answer relevance. These quality scores evaluate the efficiency of the RAG model from\ndifferent perspectives in the process of information retrieval\nand generation [164]\u2013[166].\n_Context Relevance_ evaluates the precision and specificity\n\n- f the retrieved context, ensuring relevance and minimizing\nprocessing costs associated with extraneous content.\n_Answer Faithfulness_ ensures that the generated answers\nremain true to the retrieved context, maintaining consistency\nand avoiding contradictions.\n_Answer Relevance_ requires that the generated answers are\ndirectly pertinent to the posed questions, effectively addressing\nthe core inquiry.\n_2) Required Abilities:_ RAG evaluation also encompasses\nfour abilities indicative of its adaptability and efficiency:\nnoise robustness, negative rejection, information integration,\nand counterfactual robustness [167], [168]. These abilities are\ncritical for the model\u2019s performance under various challenges\nand complex scenarios, impacting the quality scores.\n_Noise Robustness_ appraises the model\u2019s capability to manage noise documents that are question-related but lack substantive information.\n\n_Negative Rejection_ assesses the model\u2019s discernment in\nrefraining from responding when the retrieved documents do\nnot contain the necessary knowledge to answer a question.\n_Information Integration_ evaluates the model\u2019s proficiency in\nsynthesizing information from multiple documents to address\ncomplex questions.\n_Counterfactual Robustness_ tests the model\u2019s ability to rec\n- gnize and disregard known inaccuracies within documents,\neven when instructed about potential misinformation.\nContext relevance and noise robustness are important for\nevaluating the quality of retrieval, while answer faithfulness,\nanswer relevance, negative rejection, information integration,\nand counterfactual robustness are important for evaluating the\nquality of generation.\n\n\n13\n\n\nTABLE II\n\nDOWNSTREAM TASKS AND DATASETS OF RAG\n\n\nTask Sub Task Dataset Method\n\n\n[26], [30], [",". This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, and\ndelve into their synergies, elucidating how these components intricately collaborate to form a cohesive and\neffective RAG framework.\n\n\n_\u2022_ We have summarized the current assessment methods of\n\nRAG, covering 26 tasks, nearly 50 datasets, outlining\nthe evaluation objectives and metrics, as well as the\ncurrent evaluation benchmarks and tools. Additionally,\nwe anticipate future directions for RAG, emphasizing\npotential enhancements to tackle current challenges.\n\n\nThe paper unfolds as follows: Section II introduces the\nmain concept and current paradigms of RAG. The following\nthree sections explore core components\u2014\u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, respectively. Section III focuses\n\n- n optimization methods in retrieval,including indexing, query\nand embedding optimization. Section IV concentrates on postretrieval process and LLM fine-tuning in generation. Section V\nanalyzes the three augmentation processes. Section VI focuses\n\n- n RAG\u2019s downstream tasks and evaluation system. Section VII mainly discusses the challenges that RAG currently\n\n\n\nfaces and its future development directions. At last, the paper\nconcludes in Section VIII.\n\n\nII. OVERVIEW OF RAG\n\n\nA typical application of RAG is illustrated in Figure 2.\nHere, a user poses a question to ChatGPT about a recent,\nwidely discussed news. Given ChatGPT\u2019s reliance on pretraining data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information\ngap by sourcing and incorporating knowledge from external\ndatabases. In this case, it gathers relevant news articles related\nto the user\u2019s query. These articles, combined with the original\nquestion, form a comprehensive prompt that empowers LLMs\nto generate a well-informed answer.\nThe RAG research paradigm is continuously evolving, and\nwe categorize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\n\n- f the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response"],"8":[" sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n\n - think: To solve the task, I need to find and take a lettuce, then clean it with\nsinkbasin, then put it in diningtable.\n\nOK.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - go to fridge 1\nThe fridge 1 is closed.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n\n - think: I need to clean this lettuce (1) using sinkbasin 1.\n\nOK.\n\n - go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n\n - think: I need to clean this lettuce (1) using sinkbasin 1.\n\nOK.\n\n - clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n\n - think: I need to put this cleaned lettuce (1) in\/on diningtable 1.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup "," The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n_>_ go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n_>_ take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n_>_ go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n_>_ clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n_>_ think: Now I clean a lettuce (1). Next, I need to put it in\/on diningtable 1.\n\nOK.\n\n_>_ go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n_>_ put lettuce 1 in\/on diningtable 1\nYou put the lettuce 1 in\/on the diningtable 1.\n\n\n23\n\n\nPublished as a conference paper at ICLR 2023\n\n\nTable 8: An ReAct prompt on the ALFWorld clean task.\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n"," a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n\n1.\n\nYour task is to: put a clean knife in countertop.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\n"," to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n\nD"," it.\n\nOK.\n\n - take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n\n - think: Now I take a lettuce (1). Next, I need to go to sinkbasin (1) and clean\n\nit.\n\nOK.\n\n - go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n\n - clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n\n - think: Now I clean a lettuce (1). Next, I need to put it in\/on diningtable 1.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n\n - put lettuce 1 in\/on diningtable 1\nYou put the lettuce 1 in\/on the diningtable 1.\n\n\n24\n\n\nPublished as a conference paper at ICLR 2023\n\n\nTable 9: An ReAct-IM prompt on the ALFWorld clean task.\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n1, a coffeemachine 1, a countertop 1, a diningtable 1, a drawer 1, a fridge 1, a\ngarbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean"],"9":[" a large, complex model (teacher) to a\nsmaller, more efficient model (student) (Gou et al., 2021).\nThis technique is pivotal in mitigating the challenges posed\nby the computational demands and resource constraints of\ndeploying large-scale models in practical applications.\nHistorically, knowledge distillation techniques, prior to\nthe era of LLMs, primarily concentrated on transferring\nknowledge from complex, often cumbersome neural networks to more compact and efficient architectures (Sanh\net al., 2019; Kim and Rush, 2016). This process was largely\ndriven by the need to deploy machine learning models in\nresource-constrained environments, such as mobile devices\n\n- r edge computing platforms, where the computational\npower and memory are limited. The focus was predominantly on ad-hoc neural architecture selection and training\n\n- bjectives tailored for single tasks. These earlier methods\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 3: Taxonomy of Knowledge Distillation of Large Language Models. The detailed taxonomy of Verticalization\nDistillation is shown in Figure 7.\n\n\ninvolved training a smaller student network to mimic the\n\n- utput of a larger teacher network, often through techniques\nlike soft target training, where the student learns from\nthe softened softmax output of the teacher. Please refer to\nthe survey (Gou et al., 2021) for more details on general\nknowledge distillation techniques in AI and DL.\nIn contrast, the advent of LLMs has revolutionized\nthe knowledge distillation landscape. The current era of\nknowledge distillation in LLMs shifts the focus from mere\narchitecture compression to knowledge elicitation and transfer (Taori et al., 2023; Chaudhary, 2023; Tunstall et al., 2023).\nThis paradigm change is largely due to the expansive and\ndeep-seated knowledge that LLMs like GPT-4 and Gemini\npossess. And the inaccessible parameters of LLMs make it\nhard to compress them by using pruning (Han et al., 2016) or\nquantization (Liu et al., 2023a) techniques. Unlike the earlier\nera, where the goal was to replicate the output behavior of\nthe teacher model or reduce the model size, the current focus\nin LLM-based knowledge distillation is to elicit the specific\nknowledge these models have.\nThe key to this modern approach lies in","-free_ method for test-time\nadaptation.\n\n\n2\n\n\n**Contrastive learning.** Much research has focused on the effect of hard negatives on the performance\n\n- f contrastive learning methods Chen et al. (2020); Qu et al. (2021); Robinson et al. (2021); Wang\net al. (2023). (Zhang & Stratos, 2021) observe that harder negatives provide a better approximation\n\n- f the overall cross-entropy loss, but do not consider _batch_ - level optimizations for negative selection.\nHofstatter et al. (2021) cluster queries before training and show that this improves performance.\u00a8\nSachidananda et al. (2023) also consider contrastive batch sampling as a global optimization problem,\nbut do not apply their technique to state-of-the-art transformer-based text embedding models. (Ma\net al., 2024) use a clustering algorithm to partition a dataset into several sub-datasets, but train a\ndifferent model on each sub-dataset. Solatorio (2024) also use a pre-trained model to address the\nproblem of in-batch false negatives from randomly sampled batches. Our training algorithm aims to\nfind the hardest possible high-quality batches to train text embedding models.\n\n\n**Test-time adaptation.** Our method can be compared to other solutions to test-time adaptation, a\nproblem that has been well-studied across a variety of domains (Jang et al., 2023). In retrieval, one\nform of test-time adaptation is pseudo-relevance feedback (PRF) (Rocchio, 1971; Li et al., 2018;\nWang et al., 2021), where documents relevant to the query are used to construct a final, enhanced\nquery representation. The query side of our model can be seen as a form of pseudo-relevance\nfeedback; however, we train from scratch to support a more general form of PRF natively, on the\ndocument representation as well as the query.\n\n\n**Non-parametric modeling.** Our contextual document model can be seen as a form of nonparametric modeling. This shows connections with the a large body of deep learning research\nsuch as the non-parametric transformer (NPT) (Kossen et al., 2022) and the subfield of Neural\nProcesses (Garnelo et al., 2018; Kim et al., 201"," practice from [32], and first re-generate the claim, and\nthen classify using the representation of the final hidden state, before finally marginalizing across\ndocuments to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The\nfirst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task\nwe explore in the main paper. FEVER\u2019s other sub-task involves extracting sentences from Wikipedia\nas evidence supporting the classification prediction. As FEVER uses a different Wikipedia dump to\nus, directly tackling this task is not straightforward. We hope to address this in future work.\n\n\n**F** **Null Document Probabilities**\n\n\nWe experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order\nto model cases where no useful information could be retrieved for a given input. Here, if _k_ documents\nwere retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null\ndocument, before marginalizing over _k_ + 1 predictions. We explored modelling this null document\nlogit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or\n(iii) a neural network to predict the logit. We did not find that these improved performance, so in\nthe interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents\ncannot always be retrieved, we observe that the model learns to always retrieve a particular set of\ndocuments for questions that are less likely to benefit from retrieval, suggesting that null document\nmechanisms may not be necessary for RAG.\n\n\n**G** **Parameters**\n\n\nOur RAG models contain the trainable parameters for the BERT-base query and document encoder of\nDPR, with 110M parameters each (although we do not train the document encoder ourselves) and\n406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable\n\n\n18\n\n\nTable 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\n\n\nTask Train Development Test\n\n\nNatural Questions 79169 8758 3611\nTriviaQA 78786 8838 11314\nWebQuestions 3418 362 2033\nCuratedTrec 635 134 635\nJeopardy","-specific knowledge distillation in\nachieving superior performance on specialized topics.\n\n\n_**Chemistry and Materials Science.**_ The integration of LLMs\ninto Chemistry and Materials Science has revolutionized\nthe way researchers approach the discovery and development of new compounds and materials. By analyzing vast\ndatasets and scientific literature, LLMs can predict the properties and behaviors of substances, significantly accelerating\nthe innovation cycle.\nGIMLET (Zhao et al., 2023f), Graph Instruction based\nMolecuLe zEro-shoT learning, is a novel approach to\nmolecule property prediction that integrates graph and text\ndata within a single language model framework, aiming\nto improve instruction-based zero-shot learning for molecular tasks. By leveraging a transformer mechanism with\ngeneralized position embedding and decoupled attention,\nGIMLET significantly outperforms traditional molecule-text\nbaselines in zero-shot learning scenarios, demonstrating\nthe model\u2019s effectiveness in generalizing from instructions\nto a broad range of molecule-related tasks without prior\nexplicit task-specific training. LLM-Prop (Rubungo et al.,\n2023), leveraging the T5 model, showcases how LLMs can\n\n- utperform SoTA graph neural networks in predicting the\n\n\n\n25\n\n\nphysical and electronic properties of crystalline solids from\ntext descriptions. This approach underscores the potential of\ntext-based methods in materials science, offering significant\nimprovements in prediction accuracy while also contributing a benchmark dataset, TextEdge, to foster further research in this emerging field. InstructMol (Cao et al., 2023a)\nintegrates multi-modal data, aligning molecular structures\nwith natural language instructions for drug discovery tasks.\nThrough a novel two-stage instruction-tuning approach,\nit significantly enhances performance in molecule-related\ntasks, establishing a reliable molecular assistant that outperforms existing LLMs and reduces the performance gap with\nspecialized models. This demonstrates the value of multimodal integration in developing versatile tools for complex\ndomains like drug discovery.\n\n\n_**Biology.**_ In the field of Biology, particularly in the study\n\n- f proteins, DNA, and RNA, LLMs are revolutionizing our\nunderstanding of the fundamental molecules of life. By analyzing vast datasets of biological sequences and structures,\nLLMs can predict the three-dimensional shapes of proteins,\npotential functions, and interactions at a scale and speed\nbeyond traditional computational methods. This capability\nis critical for unraveling the complexities"," Tar Heels men\u2019s basketball. From\nthe standings, we can see that this team has 19 wins and 16 losses. 19-16\nis not good enough to make the NCAA tournament, which is mentioned\nin the context. From the standings, we can also see that this is the\nstanding for 2002-03 season. So the last time UNC did not make the\nNCAA tournament as of 2003 is 2002-03.\n\n\nTable 33: Illustration of wins of STEP-BACK PROMPTING on the SituatedQA dataset.\n\n\n37\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nStrategyQA **Original Question** : Could you drive a Rowe 550 to the 2008 Summer\nOlympics?\n**Original Answer [Incorrect]** : The 2008 Summer Olympics was held in\nBeijing, China. The Rowe 550 is an all-electric car. The Rowe 550 has a\nrange of 70 miles on a single charge and a top speed of about 30 mph.\nThe distance between Beijing and the U.K. is about 4000 miles.\n**Stepback Question** : What kind of vehicle is a Rowe 550?\n**Final Answer [Correct]** : The Roewe 550 was released in 2008; the\n2008 Summer Olympics were held from August 8-24, 2008. So you\ncould drive a Rowe 550 to the 2008 Summer Olympics.\n\n\nStrategyQA **Original Question** : what channel does thursday night football come on\nregular tv as of 2017?\n**Original Answer [Incorrect]** : Thursday Night Football is a weekly\nseries of National Football League (NFL) games broadcast on Thursday\nnights. It started in 2006 on NFL Network. Under a new agreement\nbetween the NFL and CBS, CBS will air 50% of the Thursday Night\nFootball games and NFL Network will air the other 50%.\n**Stepback Question** : which channel is thursday night football on as of\n2017?\n**Final Answer [Correct]** : In 2017, Thursday Night Football was aired\n\n            - n CBS, NBC, and NFL Network.\n\n\nStrategyQA **Original Question** : who is the present president of assam sahitya sabha\nas of 2018?\n**Original Answer [Incorrect]** : Dr. Nagen Sa"],"10":["## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice ["," shown effective in passage\n\n- r dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates\nthe feasibility of full dense retrieval in IR tasks.\nInstead of employing the dual-encoder framework,\nthey introduced a late-interaction operator on top\n\n- f the BERT encoders.\n\n\nDense retrieval for open-domain QA has been\nexplored by Das et al. (2019), who propose to retrieve relevant passages iteratively using reformulated question vectors. As an alternative approach\nthat skips passage retrieval, Seo et al. (2019) propose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input questions efficiently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al. (2019) jointly train\nthe question encoder and reader. Their approach\n\n- utperforms the BM25 plus reader paradigm on\nmultiple open-domain QA datasets in QA accuracy,\nand is further extended by REALM (Guu et al.,\n2020), which includes tuning the passage encoder\nasynchronously by re-indexing the passages during training. The pretraining objective has also\nrecently been improved by Xiong et al. (2020b).\nIn contrast, our model provides a simple and yet\n\n\n\neffective solution that shows stronger empirical performance, without relying on additional pretraining\n\n- r complex joint training schemes.\nDPR has also been used as an important module in very recent work. For instance, extending\nthe idea of leveraging hard negatives, Xiong et al.\n(2020a) use the retrieval model trained in the previous iteration to discover new negatives and construct a different set of examples in each training\niteration. Starting from our trained DPR model,\nthey show that the retrieval performance can be\nfurther improved. Recent work (Izacard and Grave,\n2020; Lewis et al., 2020b) have also shown that\nDPR can be combined with generation models\nsuch as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2019), achieving good performance on\n\n- pen-domain QA and other knowledge-intensive\ntasks.\n\n\n**8** **Conclusion**\n\n\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the","QA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20\nparagraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these\ndatasets into an open-domain setting, we make two\ncorpora, one for each dataset, by combining all\nsupporting and non-supporting paragraphs for all\nits questions in the train, development, and test\nsets. IIRC is originally a mix between reading\ncomprehension and an open-domain setting. Each\nquestion is grounded in one main paragraph, which\ncontains links to multiple Wikipedia pages with\nseveral paragraphs each. We create a corpus out\n\n- f all the paragraphs from all the Wikipedia pages\npresent in the dataset. [18] We do assume the availability of the main passage which doesn\u2019t need\nto be retrieved and is always present. We don\u2019t\nassume the availability of Wikipedia links in the\nmain passage, however, to keep the retrieval problem challenging. [19]\n\n\n**B** **Special Handling of Models for IIRC**\n\n\nIIRC is slightly different from the other datasets,\nin that the question is grounded in the main passage and other supporting paragraphs come from\nthe Wikipedia pages of entities mentioned in this\npassage. We modify the retrievers and readers to\naccount for this difference: (i) We always keep the\nmain passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all\nthe retrieval methods, we first prompt the model to\ngenerate a list of Wikipedia page titles using the\nmain passage and the question. We map these generated titles to the nearest Wikipedia page titles in\nthe corpus (found using BM25), and then the rest\n\n- f the paragraph retrieval queries are scoped within\n\n- nly those Wikipedia pages.\nTo prompt the model to generate Wikipedia page\ntitles using the main passage and the question for\n\n\n18Following are the corpus sizes for the datasets: HotpotQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e",":2002.08909 [cs.CL]](https:\/\/arxiv.org\/abs\/2002.08909)\n\n[11] Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, and\nAllan Hanbury. 2020. Improving Efficient Neural Ranking Models with Cross[Architecture Knowledge Distillation. arXiv:2010.02666 [cs.IR]](https:\/\/arxiv.org\/abs\/2010.02666)\n\n[12] Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen tau Yih. 2020. Dense Passage Retrieval for Open[Domain Question Answering. arXiv:2004.04906 [cs.CL]](https:\/\/arxiv.org\/abs\/2004.04906)\n\n[13] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\nSearch via Contextualized Late Interaction over BERT. In _Proceedings of the 43rd_\n_International ACM SIGIR Conference on Research and Development in Information_\n_Retrieval_ (Virtual Event, China) _(SIGIR \u201920)_ . Association for Computing Machinery,\n[New York, NY, USA, 39\u201348. https:\/\/doi.org\/10.1145\/3397271.3401075](https:\/\/doi.org\/10.1145\/3397271.3401075)\n\n[14] Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. 2015. Numba: A llvm-based\npython jit compiler. In _Proceedings of the Second Workshop on the LLVM Compiler_\n_Infrastructure in HPC_ . 1\u20136.\n\n[15] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Repre[sentations for Ranking using Tightly-Coupled Teachers. arXiv:2010.11386 [cs.IR]](https:\/\/arxiv.org\/abs\/2010.11386)\n\n[16] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\nGoharian, and Ophir Frieder. 2020. Expansion via Prediction"],"11":[" like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25\/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice ["," question and contexts. Dense retrieval methods have thus never\n\nbe shown to outperform TF-IDF\/BM25 for opendomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\n\n - bjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then finetuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain\n\n\nQA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot fine-tuned using pairs of questions and answers,\nthe corresponding representations could be suboptimal.\n\nIn this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), _with-_\n\n_out_ additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\n\n- f question and passage pairs. Through a series\n\n- f careful ablation studies, our final solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective comparing all pairs of questions and passages in a batch.\nOur _Dense Passage Retriever_ (DPR) is exceptionally strong. It not only outperforms BM25 by a\nlarge margin (65.2% vs. 42.9% in Top-5 accuracy),\nbut also results in a substantial improvement on\nthe end-to-end QA accuracy compared to ORQA\n(41.5% vs. 33.3%) in the open Natural Questions\nsetting (Lee et al., 2019; Kwiatkowski et al., 2019).\n\nOur contributions are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso"," BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25( _q_, _p_ ) + _\u03bb \u00b7_ sim( _q, p_ ) as the ranking function.\nWe used _\u03bb_ = 1 _._ 1 based on the retrieval accuracy in\nthe development set.\n\n\n**5.1** **Main Results**\n\n\nTable 2 compares different passage retrieval systems on five QA datasets, using the top- _k_ accuracy\n( _k \u2208{_ 20 _,_ 100 _}_ ). With the exception of SQuAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when _k_ is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\n\n- n Natural Questions). When training with mul\n\n8SQuAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\n\nmore in Section 5.1.\n\n9\n[Lucene implementation. BM25 parameters](https:\/\/lucene.apache.org\/) _b_ = 0 _._ 4 (document length normalization) and _k_ 1 = 0 _._ 9 (term frequency\nscaling) are tuned using development sets.\n\n\n\nFigure 1: Retriever top- _k_ accuracy with different numbers of training examples used in our dense passage retriever vs BM25. The results are measured on the de\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n\n\ntiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and","2022), Direct Prompting as proposed\nby Brown et al. (2020). For CoT prompting, we use\nthe same template as shown in \u00a73.2, but at test time\nwe ask the model to generate the full CoT from\nscratch. The final sentence of CoT is expected to\nbe of the form \u201canswer is: ...\u201d, so that the answer\ncan be extracted programmatically. If it\u2019s not in\nthat form, the full generation is returned as the\nanswer. For Direct Prompting, we use the same\ntemplate as CoT Prompting but the answer field\n(\u201cA: \u201d) contains only the final answer instead of\nCoT. See App. G for details.\n\n\n**4** **Experimental Setup**\n\n\nWe evaluate - ur method - n 4 multi-step\nQA datasets in the - pen-domain setting:\n**HotpotQA** (Yang et al., 2018), **2WikiMul-**\n**tihopQA** (Ho et al., 2020), answerable subset of\n**MuSiQue** (Trivedi et al., 2022), and answerable\nsubset of **IIRC** (Ferguson et al., 2020). For\nHotpotQA, we use the Wikipedia corpus that\ncomes with it for the open-domain setting. For\neach of the other three datasets, which originally\ncome in a reading comprehension or mixed setting,\nwe used the associated contexts to construct a\n\n\ncorpus for our open-domain setting (see App. A\nfor details). For each dataset, we use 100 randomly\nsampled questions from the original development\nset for tuning hyperparameters, and 500 other\nrandomly sampled questions as our test set.\n\n\n**4.1** **Models**\n\n\n**Retriever.** We use BM25 (Robertson et al., 2009)\nimplemented in Elasticsearch [6] as our base retriever.\nWe compare two retriever systems:\n\n(i) **One-step Retriever (OneR)** uses the question as a query to retrieve _K_ paragraphs. We select\n_K \u2208{_ 5 _,_ 7 _,_ 9 _,_ 11 _,_ 13 _,_ 15 _}_ that\u2019s best on the dev set.\n\n(ii) **IRCoT Retriever** is our method described in \u00a73. We use BM25 as its underlying retriever"],"12":[" in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f","QA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20\nparagraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these\ndatasets into an open-domain setting, we make two\ncorpora, one for each dataset, by combining all\nsupporting and non-supporting paragraphs for all\nits questions in the train, development, and test\nsets. IIRC is originally a mix between reading\ncomprehension and an open-domain setting. Each\nquestion is grounded in one main paragraph, which\ncontains links to multiple Wikipedia pages with\nseveral paragraphs each. We create a corpus out\n\n- f all the paragraphs from all the Wikipedia pages\npresent in the dataset. [18] We do assume the availability of the main passage which doesn\u2019t need\nto be retrieved and is always present. We don\u2019t\nassume the availability of Wikipedia links in the\nmain passage, however, to keep the retrieval problem challenging. [19]\n\n\n**B** **Special Handling of Models for IIRC**\n\n\nIIRC is slightly different from the other datasets,\nin that the question is grounded in the main passage and other supporting paragraphs come from\nthe Wikipedia pages of entities mentioned in this\npassage. We modify the retrievers and readers to\naccount for this difference: (i) We always keep the\nmain passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all\nthe retrieval methods, we first prompt the model to\ngenerate a list of Wikipedia page titles using the\nmain passage and the question. We map these generated titles to the nearest Wikipedia page titles in\nthe corpus (found using BM25), and then the rest\n\n- f the paragraph retrieval queries are scoped within\n\n- nly those Wikipedia pages.\nTo prompt the model to generate Wikipedia page\ntitles using the main passage and the question for\n\n\n18Following are the corpus sizes for the datasets: HotpotQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e","Raffel et al., 2019), achieving good performance on\n\n- pen-domain QA and other knowledge-intensive\ntasks.\n\n\n**8** **Conclusion**\n\n\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the traditional sparse retrieval component in open-domain\nquestion answering. While a simple dual-encoder\napproach can be made to work surprisingly well,\nwe showed that there are some critical ingredients\nto training a dense retriever successfully. Moreover,\n\n- ur empirical analysis and ablation studies indicate\nthat more complex model frameworks or similarity\nfunctions do not necessarily provide additional values. As a result of improved retrieval performance,\nwe obtained new state-of-the-art results on multiple\n\n- pen-domain question answering benchmarks.\n\n\n**Acknowledgments**\n\n\nWe thank the anonymous reviewers for their helpful\ncomments and suggestions.\n\n\n**References**\n\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over Wikipedia graph\nfor question answering. In _International Conference_\n\n_on Learning Representations (ICLR)_ .\n\n\nPetr Baudi\u02c7s and Jan Sediv`y. 2015. [\u02c7] Modeling of the\nquestion answering task in the yodaqa system. In _In-_\n_ternational Conference of the Cross-Language Eval-_\n_uation Forum for European Languages_, pages 222\u2013\n228. Springer.\n\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\n\n\nquestion-answer pairs. In _Empirical Methods in Nat-_\n_ural Language Processing (EMNLP)_ .\n\n\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard\nS\u00a8ackinger, and Roopak Shah. 1994. Signature verification using a \u201cSiamese\u201d time delay neural network.\nIn _NIPS_, pages 737\u2013744.\n\n\nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,\nMatt Deeds, Nicole Hamilton, and Greg Hullender.\n2005. Learning to rank using gradient descent. In\n_Proceedings of the 22nd international conference on_\n_Machine learning_, pages 89\u201396.\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to"," only receive the question\/claim as input without access to support\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\nan external environment to support reasoning.\n\n\n**Action Space** We design a simple Wikipedia web API with three types of actions to support\ninteractive information retrieval: (1) **search** [entity], which returns the first 5 sentences from\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\nWikipedia search engine, (2) **lookup** [string], which would return the next sentence in the page\ncontaining string, simulating Ctrl+F functionality on the browser. (3) **finish** [answer], which\nwould finish the current task with answer. We note that this action space mostly can only retrieve a\nsmall part of a passage based on exact passage name, which is significantly weaker than state-of-theart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\nand force models to retrieve via explicit reasoning in language.\n\n\n3.2 METHODS\n\n\n**ReAct Prompting** For HotpotQA and Fever, we randomly select 6 and 3 cases [2] from the training\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\nthought), where free-form thoughts are used for various purposes. Specifically, we use a combination\n\n- f thoughts that decompose questions (\u201cI need to search x, find y, then find z\u201d), extract information\nfrom Wikipedia observations (\u201cx was started in 1844\u201d, \u201cThe paragraph does not tell x\u201d), perform\ncommonsense (\u201cx is not y, so z must instead be...\u201d) or arithmetic reasoning (\u201c1844 < 1989\u201d), guide\n\n\n2We find more examples do not improve performance.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n**HotpotQA** **Fever**\n**Prompt Method** _[a]_\n(EM) (Acc)\n\n\nStandard 28.7 57.1\nCoT (Wei et al., 2022) 29.4 56.3\nCoT-SC (Wang et al., 2022a) 33.4 60.4\n\n\nAct 25.7 58.9\n\nReAct "," corresponding to Google\nsearch queries. The open-domain version of\nthis dataset is obtained by discarding answers\nwith more than 5 tokens.\n\n\n  - TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and quiz-league\n\n\nFigure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of retrieved passages.\n\n\n\nwebsites. The _unfiltered_ version of TriviaQA\nis used for open-domain question answering.\n\n\n  - SQuAD v1.1 (Rajpurkar et al., 2016) is a reading comprehension dataset. Given a paragraph\nextracted from Wikipedia, annotators were\nasked to write questions, for which the answer\nis a span from the corresponding paragraph.\n\n\nFollowing Lee et al. (2019) we use the validation as\ntest, and keep 10% of the training set for validation.\nWe use the Wikipedia dumps from Dec. 20, 2018\nfor NQ and TriviaQA and from Dec. 21, 2016 for\nSQuAD. We apply the same preprocessing as Chen\net al. (2017); Karpukhin et al. (2020), leading to\npassages of 100 words, which do not overlap.\n\n\n**Evaluation.** Predicted answers are evaluated\n\nwith the standard exact match metric (EM), as introduced by Rajpurkar et al. (2016). A generated\nanswer is considered correct if it matches any answer of the list of acceptable answers after normalization. This normalization step consists in lowercasing and removing articles, punctuation and\nduplicated whitespace.\n\n\n**Technical details.** We initialize our models with\n\nthe pretrained T5 models (Raffel et al., 2019), available in the HuggingFace Transformers library. [4] We\nconsider two model sizes, base and large, containing respectively 220M and 770M parameters. We\nfine-tune the models on each dataset independently,\nusing Adam (Kingma and Ba, 2014) with a constant learning rate of 10 _[\u2212]_ [4] and a dropout rate of\n10%. We train the model for 10k gradient steps,\nwith a batch size of 64, using 64 Tesla V100 32Gb.\nWe evaluate models every 500 steps and select the\nbest one on the validation set based on the Exact\n\n"],"13":[" to a _d_ dimensional real-valued vectors and builds an index\n\nfor all the _M_ passages that we will use for retrieval.\n\n\n3The ideal size and boundary of a text passage are functions of both the retriever and reader. We also experimented\nwith natural paragraphs in our preliminary trials and found that\nusing fixed-length passages performs better in both retrieval\nand final QA accuracy, as observed by Wang et al. (2019).\n4Exceptions include (Seo et al., 2019) and (Roberts et al.,\n2020), which _retrieves_ and _generates_ the answers, respectively.\n\n\nAt run-time, DPR applies a different encoder _EQ_ ( _\u00b7_ )\nthat maps the input question to a _d_ - dimensional\nvector, and retrieves _k_ passages of which vectors\nare the closest to the question vector. We define\nthe similarity between the question and the passage\nusing the dot product of their vectors:\n\n\nsim( _q, p_ ) = _EQ_ ( _q_ ) [\u22ba] _EP_ ( _p_ ) _._ (1)\n\n\nAlthough more expressive model forms for measuring the similarity between a question and a passage\ndo exist, such as networks consisting of multiple\nlayers of cross attentions, the similarity function\nneeds to be decomposable so that the representations of the collection of passages can be precomputed. Most decomposable similarity functions\nare some transformations of Euclidean distance\n\n(L2). For instance, cosine is equivalent to inner\nproduct for unit vectors and the Mahalanobis distance is equivalent to L2 distance in a transformed\nspace. Inner product search has been widely used\nand studied, as well as its connection to cosine\nsimilarity and L2 distance (Mussmann and Ermon,\n2016; Ram and Gray, 2012). As our ablation study\nfinds other similarity functions perform comparably (Section 5.2; Appendix B), we thus choose\nthe simpler inner product function and improve the\ndense passage retriever by learning better encoders.\n\n\n**Encoders** Although in principle the question and\npassage encoders can be implemented by any neural networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, uncased) and take the representation at the [CLS]\ntoken as the output, so _d_ = 768.\n\n\n","; and _comparison_\nquestions where two entities are mentioned simultaneously and compared in some way. In Figure 2,\nwe show the retrieval performance of both question\ntypes. The case of _comparison_ questions proves easier, since both entities needed for retrieval are present\nin the question.\n\n\nThis case appears almost solved, confirming recent\nwork demonstrating that dense retrieval is very effective at entity linking (Wu et al., 2019).\n\n\n\nFigure 2: The retrieval performance gap between comparison and bridge questions. Left:\nrecall of groundtruth passage sequences with\n- ut reranking. Right: Top-1 chain exact\nmatch after reranking.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the case of _bridge_ questions, we manually in\nrecall of groundtruth passage sequences with\nspect 50 randomly sampled erroneous examples after\n\n                          - ut reranking. Right: Top-1 chain exact\n\nreranking. We find that in half of these cases, our re\nmatch after reranking.\n\ntrieval model predicts an alternative passage sequence\nthat is also valid (see Appendix A.1 for examples).\nThis gives an estimated top-1 passage sequence accuracy of about 90%. Other remaining errors are\ndue to the dense method\u2019s inability to capture the exact n-gram match between the question and\npassages. This is a known issue (Lee et al., 2019; Karpukhin et al., 2020) of dense retrieval methods\nwhen dealing with questions that have high lexical overlap with the passages. To this end, a hybrid\nmulti-hop retrieval method with both term and dense index might be used to further improve the\nperformance on _bridge_ questions.\n\n\n\n**Retriever Ablation Study** In Table 3, we examine our model with different variations on HotpotQA\nto show the effectiveness of each proposed component. We see that further training with a memory\nbank results in modest gains, while using a shared encoder is crucial for the best performance.\nRespecting the ordering of passages in two hops is essential - training in an order-agnostic manner\nhardly works at all, and underperforms even the single-hop baseline. Finally, not using hyperlinked\nparagraphs from TF-IDF passages as additional negatives has only a minor impact on performance.\n\n\n**Question Decomposition for Retrieval** As multi-hop questions have more complex structures\nthan simple questions, recent studies (Min et al., 2019; Perez et al., 2020) propose to use"," SpaCy. [2] In\nDPR, passages and questions are represented as\ndense vector representations, computed using two\nBERT networks. The ranking function is the dot\nproduct between the query and passage representations. Retrieval is performed using approximate\nnearest neighbors with the FAISS library. [3]\n\n\n**Reading.** Our generative model for open domain\nQA is based on a sequence-to-sequence network,\npretrained on unsupervised data, such as T5 or\nBART (Raffel et al., 2019; Lewis et al., 2019). The\nmodel takes as input the question, as well as the\nsupport passages, and generates the answer. More\nprecisely, each retrieved passage and its title are\nconcatenated with the question, and processed independently from other passages by the encoder.\nWe add special tokens question:, title: and\ncontext: before the question, title and text of\neach passage. Finally, the decoder performs atten\n\n1lucene.apache.org\n2spacy.io\n3github.com\/facebookresearch\/faiss\n\n\n\ntion over the concatenation of the resulting representations of all the retrieved passages. The model\nthus performs evidence fusion in the decoder only,\nand we refer to it as _Fusion-in-Decoder_ .\n\nBy processing passages independently in the encoder, but jointly in the decoder, this method differs from Min et al. (2020) and Lewis et al. (2020).\nProcessing passages independently in the encoder\nallows to scale to large number of contexts, as it\n\n- nly performs self attention over one context at a\ntime. This means that the computation time of the\nmodel grows linearly with the number of passages,\ninstead of quadratically. On the other hand, processing passages jointly in the decoder allows to\nbetter aggregate evidence from multiple passages.\n\n\n**4** **Experiments**\n\n\nIn this section, we report empirical evaluations of\nFusion-in-Decoder for open domain QA.\n\n\n**Datasets.** We consider the following datasets,\nand use the same setting as Lee et al. (2019):\n\n\n  - NaturalQuestions (Kwiatkowski et al., 2019)\ncontains questions corresponding to Google\nsearch queries. The open-domain version of\nthis dataset is obtained by discarding answers\nwith more than 5 tokens.\n\n\n  - TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and"," positive passages selected from\nthe retriever model, and correct spans and passages\nselected from the reader model. Since the passage\nencoder is fixed, we could use larger amount of\nretrieved passages when calculating the retriever\nloss. Specifically, we get top 100 passages for each\nquestion in a mini-batch and use the method similar\nto in-batch negative training: all retrieved passages\u2019\nvectors participate in the loss calculation for _all_\nquestions in a batch. Our training batch size is set\nto 16, which effectively gives 1,600 passages per\nquestion to calculate retriever loss. The reader still\nuses 24 passages per question, which are selected\n\n\n**Question** **Passage received by BM25** **Passage retrieved by DPR**\n\n\n\nWhat is the body of water\nbetween England and Ireland?\n\n\nWho plays Thoros of Myr in\nGame of Thrones?\n\n\n\nTitle:British Cycling Title: Irish Sea\n... **England** is not recognised as a region by the UCI, and . . . Annual traffic between Great Britain and **Ireland** amounts\nthere is no English cycling team outside the Commonwealth to over 12 million passengers and of traded goods. **The Irish**\nGames. For those occasions, British Cycling selects and sup- **Sea** is connected to the North Atlantic at both its northern\nports the **England** team. Cycling is represented on the Isle and southern ends. To the north, the connection is through\n\n- f Man by the Isle of Man Cycling Association. Cycling in the North Channel between Scotland and Northern **Ireland**\nNorthern **Ireland** is organised under Cycling Ulster, part of and the Malin Sea. The southern end is linked to the Atlantic\nthe all-Ireland governing **body** Cycling **Ireland** . Until 2006, through the St George\u2019s Channel between Ireland and Pema rival governing **body** existed, . . . brokeshire, and the Celtic Sea. . . .\n\n\n\nTitle: No One (Game of Thrones) Title: P\u02daal Sverre Hagen\n...He may be \u201dno one,\u201d but there\u2019s still enough of a person P\u02daal Sverre Valheim Hagen (born 6 November 1980) is a Norleft in him to respect, and admire who this girl is and what wegian stage and screen actor. He appeared in the Norweshe\u2019s become.","clweb.org\/anthology\/D18-1255](https:\/\/www.aclweb.org\/anthology\/D18-1255)` .\n\n\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\nsystems. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language_\n_Processing_, pages 3950\u20133959, Brussels, Belgium, October-November 2018. Association for\nComputational Linguistics. doi: 10.18653\/v1\/D18-1429. URL `[https:\/\/www.aclweb.org\/](https:\/\/www.aclweb.org\/anthology\/D18-1429)`\n`[anthology\/D18-1429](https:\/\/www.aclweb.org\/anthology\/D18-1429)` .\n\n\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne, editors,\n_Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic_\n\n\n13\n\n\n_approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing_\n_Systems (NIPS 2016), Barcelona, Spain, December 9, 2016_, volume 1773 of _CEUR Workshop_\n_Proceedings_ . CEUR-WS.org, 2016. URL `[http:\/\/ceur-ws.org\/Vol-1773\/CoCoNIPS_](http:\/\/ceur-ws.org\/Vol-1773\/CoCoNIPS_2016_paper9.pdf)`\n`[2016_paper9.pdf](http:\/\/ceur-ws.org\/Vol-1773\/CoCoNIPS_2016_paper9.pdf)` .\n\n\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. _arXiv preprint_\n_arXiv:1901.04085_, 2019. URL `[https:\/\/arxiv.org\/abs\/1901.04085](https:\/\/arxiv.org\/abs\/1901.04085)` .\n\n\n[45] Myle Ott"],"14":["## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [","4], moving from simple web search (text-only) to advanced\ninstruction-following and multi-modal representations. These models generally followed advances in\nlanguage models, such as pre-trained LMs [Hoffmann et al., 2022], multi-modal LMs [Li et al., 2024,\nTeam, 2024], and advances in instruction-following [Zhou et al., 2023, Ouyang et al., 2022]. Some\n\n- f the prominent examples in retrieval include CoPali [Faysse et al., 2024] and DSE [Ma et al., 2024]\nwhich focus on multimodal embeddings, Instructor [Su et al., 2022] and FollowIR [Weller et al.,\n2024a] for instruction following, and GritLM [Muennighoff et al., 2024] and Gemini Embeddings\n\n[Lee et al., 2025] for pre-trained LMs turned embedders.\n\n\nOur work, though focused solely on textual representations for simplicity, **applies to all modalities**\n\n**of single vector embeddings for any domain of dataset** . As the space of things to represent grows\n(through instructions or multi-modality) they will increasingly run into these theoretical limitations.\n\n\n**2.2. Empirical tasks pushing the limits of dense retrieval**\n\n\nRetrieval models have been pushed beyond their initial use cases to handle a broad variety of areas.\nNotable works include efforts to represent a wide group of domains [Thakur et al., 2021, Lee et al.,\n2024], a diverse set of instructions [Weller et al., 2024a, Zhou et al., 2024, Oh et al., 2024], and to\nhandle reasoning over the queries [Xiao et al., 2024, Su et al., 2024]. This has pushed the focus of\nembedding models from basic keyword matching to embeddings that can represent the full semantic\nmeaning of language. As such, it is more common than ever to connect what were previously unrelated\ndocuments into the top- _\ud835\udc58_ relevant set, [2] increasing the number of combinations that models must be\nable to represent. This has motivated our interest in understanding the limits of what embeddings\ncan represent, as current work expects it to handle _every_ task.\n\n\nPrevious work has explored empirically the limits of models: Reimers and G"," networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, uncased) and take the representation at the [CLS]\ntoken as the output, so _d_ = 768.\n\n\n**Inference** During inference time, we apply the\npassage encoder _EP_ to all the passages and index\nthem using FAISS (Johnson et al., 2017) offline.\nFAISS is an extremely efficient, open-source library for similarity search and clustering of dense\nvectors, which can easily be applied to billions of\nvectors. Given a question _q_ at run-time, we derive\nits embedding _vq_ = _EQ_ ( _q_ ) and retrieve the top _k_\npassages with embeddings closest to _vq_ .\n\n\n**3.2** **Training**\n\n\nTraining the encoders so that the dot-product similarity (Eq. (1)) becomes a good ranking function\nfor retrieval is essentially a _metric learning_ problem (Kulis, 2013). The goal is to create a vector\nspace such that relevant pairs of questions and passages will have smaller distance (i.e., higher simi\n\n\nlarity) than the irrelevant ones, by learning a better\nembedding function.\nLet _D_ = _{\u27e8qi, p_ [+] _i_ _[, p][\u2212]_ _i,_ 1 _[,][ \u00b7 \u00b7 \u00b7][, p][\u2212]_ _i,n_ _[\u27e9}]_ _i_ _[m]_ =1 [be the]\ntraining data that consists of _m_ instances. Each\ninstance contains one question _qi_ and one relevant\n(positive) passage _p_ [+] _i_ [, along with] _[ n]_ [ irrelevant (neg-]\native) passages _p_ _[\u2212]_ _i,j_ [. We optimize the loss function]\nas the negative log likelihood of the positive pas\nsage:\n\n\n_L_ ( _qi, p_ [+] _i_ _[, p][\u2212]_ _i,_ 1 _[,][ \u00b7 \u00b7 \u00b7][, p][\u2212]_ _i,n_ [)] (2)\n\n_e_ [sim(] _[q][i][,p]_ _i_ [+][)]\n\n= _\u2212_\nlog\n\n_e_ [sim(] _[q][","j]_ ) _\u2248_  _j_ _j_\n\n\n\nwhere _H_ is a set of examples used to approximate the normalizing constant. In implementation,\nin addition to these hard negative examples, other examples from the mini-batch are also used to\ncompute the normalizer since it requires no additional compute for calculating _\u03d5_ ( _d_ ).\n\n\n4 METHODS\n\n\nIn our work, we are interested in integrating contextual information into our embedding functions\n_\u03d5_ and _\u03c8_ . The standard neural _\u03d5_ is purely a function of the document _\u03d5_ ( _d_ ) and does not take into\naccount any notion of context. This contrasts with the statistical model _\u03d5_ ( _\u00b7_ ; _D_ ) and _\u03c8_ ( _\u00b7_ ; _D_ ). Arguably\nthis is not an issue if retrieval is completely in domain, as _\u03d5_ is capable of learning statistics such as\nIDF and average document length on the training set through gradient descent.\n\n\nHowever, in many retrieval benchmarks, models are trained over a single set of documents _D_ and\nthen tested in many other domains _D_ that differs significantly from _DT_ . In this setting, training on\n_DT_ alone may not be able to provide robust embeddings when used in contexts such as _D_ .\n\n\n4.1 CONTEXTUAL TRAINING WITH ADVERSARIAL CONTRASTIVE LEARNING\n\n\nReturning to the example from the introduction, we assume that in a general purpose training corpus\n_DT_, the term NFL is a rare word appearing in relatively few documents and a useful signal. However,\nif at test time _D_ is a corpus of sports articles, this word would be exceedingly common. Evaluation\nin this domain is, in a statistical sense, adversarial to the original dataset. To handle this issue,\nmeta-learning-style objectives have shown to be effective for training document embedders. In these\napproaches, instead of sampling documents-query pairs iid, the objective first sample a domain and\nthen sample a batch of examples. This ensures that the model mostly sees related training points in\neach domain.\n\n\nWe propose a training objective that synthesizes a large set of fine-grained domains to train the model\n\n- n. Formally, our aim is to partition the training dataset _DT_ into groups ( _B_ [1] _, . . . B_ _["," Special Publication Sp_ [, 109:109, 1995. URL https:\/\/www.](https:\/\/www.microsoft.com\/en-us\/research\/publication\/okapi-at-trec-3\/)\n[microsoft.com\/en-us\/research\/publication\/okapi-at-trec-3\/.](https:\/\/www.microsoft.com\/en-us\/research\/publication\/okapi-at-trec-3\/)\n\n\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\nZaheer. Questions are all you need to train a dense passage retriever. _Transactions of the As-_\n_sociation for Computational Linguistics_, 11:600\u2013616, 2023. doi: 10.1162\/tacl ~~a 0~~ 0564. URL\n[https:\/\/aclanthology.org\/2023.tacl-1.35.](https:\/\/aclanthology.org\/2023.tacl-1.35)\n\n\nGideon Schwarz. Estimating the Dimension of a Model. _The annals of statistics_, pp. 461\u2013464,\n[1978. URL https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/](https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/aos\/1176344136.full)\n[volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/](https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/aos\/1176344136.full)\n[aos\/1176344136.full.](https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/aos\/1176344136.full)\n\n\nKaren Sp\u00a8arck Jones. A Statistical Interpretation of Term Specificity and its Application in Retrieval. _Journal of documentation_ [, 28(1):11\u201321, 1972. URL https:\/\/doi.org\/10.1108\/](https:\/\/doi.org\/10.1108\/eb026526)\n[eb026526"],"15":[" question and contexts. Dense retrieval methods have thus never\n\nbe shown to outperform TF-IDF\/BM25 for opendomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\n\n - bjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then finetuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain\n\n\nQA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot fine-tuned using pairs of questions and answers,\nthe corresponding representations could be suboptimal.\n\nIn this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), _with-_\n\n_out_ additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\n\n- f question and passage pairs. Through a series\n\n- f careful ablation studies, our final solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective comparing all pairs of questions and passages in a batch.\nOur _Dense Passage Retriever_ (DPR) is exceptionally strong. It not only outperforms BM25 by a\nlarge margin (65.2% vs. 42.9% in Top-5 accuracy),\nbut also results in a substantial improvement on\nthe end-to-end QA accuracy compared to ORQA\n(41.5% vs. 33.3%) in the open Natural Questions\nsetting (Lee et al., 2019; Kwiatkowski et al., 2019).\n\nOur contributions are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso"," BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25( _q_, _p_ ) + _\u03bb \u00b7_ sim( _q, p_ ) as the ranking function.\nWe used _\u03bb_ = 1 _._ 1 based on the retrieval accuracy in\nthe development set.\n\n\n**5.1** **Main Results**\n\n\nTable 2 compares different passage retrieval systems on five QA datasets, using the top- _k_ accuracy\n( _k \u2208{_ 20 _,_ 100 _}_ ). With the exception of SQuAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when _k_ is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\n\n- n Natural Questions). When training with mul\n\n8SQuAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\n\nmore in Section 5.1.\n\n9\n[Lucene implementation. BM25 parameters](https:\/\/lucene.apache.org\/) _b_ = 0 _._ 4 (document length normalization) and _k_ 1 = 0 _._ 9 (term frequency\nscaling) are tuned using development sets.\n\n\n\nFigure 1: Retriever top- _k_ accuracy with different numbers of training examples used in our dense passage retriever vs BM25. The results are measured on the de\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n\n\ntiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and"," the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n\n\n**Selection of positive passages** Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA [6], we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the question will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. [7] We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the number of questions in training\/dev\/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n\n\n**5** **Experiments: Passage Retrieval**\n\n\nIn this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n\n\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Appendix A.\n\n\n**Training** **Retriever** **Top-20** **Top-100**\nNQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD\n\n\nNone BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0\n\n\nDPR 78.4 79.4 73.2 79.8 63.2 85.4 **85.0** 81.4 89.1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7"," graph comprehension\nand question-answering capabilities through soft prompting\n\n- f the LLM, and employs the Prize-Collecting Steiner Tree\n(PCST) optimization problem for targeted graph retrieval. On\nthe contrary, it requires additional effort to build, validate,\nand maintain structured databases. On the contrary, it requires\nadditional effort to build, validate, and maintain structured\n\ndatabases.\n\n_LLMs-Generated Content._ Addressing the limitations of\nexternal auxiliary information in RAG, some research has\nfocused on exploiting LLMs\u2019 internal knowledge. SKR [58]\nclassifies questions as known or unknown, applying retrieval\nenhancement selectively. GenRead [13] replaces the retriever\nwith an LLM generator, finding that LLM-generated contexts\n\n- ften contain more accurate answers due to better alignment\nwith the pre-training objectives of causal language modeling.\nSelfmem [17] iteratively creates an unbounded memory pool\nwith a retrieval-enhanced generator, using a memory selector to choose outputs that serve as dual problems to the\n\n- riginal question, thus self-enhancing the generative model.\nThese methodologies underscore the breadth of innovative\ndata source utilization in RAG, striving to improve model\nperformance and task effectiveness.\n_2) Retrieval Granularity:_ Another important factor besides\nthe data format of the retrieval source is the granularity of\nthe retrieved data. Coarse-grained retrieval units theoretically\ncan provide more relevant information for the problem, but\nthey may also contain redundant content, which could distract\nthe retriever and language models in downstream tasks [50],\n\n[87]. On the other hand, fine-grained retrieval unit granularity\nincreases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge. Choosing\n\n\nthe appropriate retrieval granularity during inference can be\na simple and effective strategy to improve the retrieval and\ndownstream task performance of dense retrievers.\nIn text, retrieval granularity ranges from fine to coarse,\nincluding Token, Phrase, Sentence, Proposition, Chunks, Document. Among them, DenseX [30]proposed the concept of\nusing propositions as retrieval units. Propositions are defined\nas atomic expressions in the text, each encapsulating a unique\nfactual segment and presented in a concise, self-contained natural language format. This approach aims to enhance retrieval\nprecision and relevance. On the Knowledge Graph (KG),\nretrieval granularity includes Entity, Triplet, and sub-Graph"," chain\nthat appropriately references the original context.\n\n\nFor all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an example of the generation step in Fig. 3, the detailed\nreasoning answer includes a citation from the original context inside `##begin_quote##` and\n`##end_quote##` as well as the detailed explanation on how to reach the conclusion based on\nthe citations. We demonstrate that adding detailed reasoning paragraphs can help boost the\nmodel\u2019s performance in our experiment section.\n\n\n**4** **Evaluation**\n\n\nWe design our experiments to study how well RAFT performs compared to various baselines. We find that the RAFT-7B model (a finetuned version of LlaMA-2) is better at reading\nand extracting information from in-domain documents, than domain-specific finetuned\nmodel, and general-purpose model with RAG. As an ablation, we also demonstrate how\nimportant it is for the model to learn with Chain-of-Thought responses. In this section,\nwe will first introduce all the datasets we used in the experiments, then all the baseline\nmodel\/fine-tuning techniques that we benchmark against.\n\n\n4\n\n\nPreprint, Under Review\n\n```\n Question: The Oberoi family is part of a hotel company that has a head office\n in what city?\n\n context: [The Oberoi family is an Indian family that is famous for its\n involvement in hotels, namely through The Oberoi Group]...[It is located in\n city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott\n Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two\n towers that comprises a hotel and the Airlangga Apartment respectively]...[The\n Oberoi Group is a hotel company with its head office in Delhi.]\n\n Instruction: Given the question, context and answer above, provide a logical\n reasoning for that answer. Please use the format of: ##Reason: {reason}\n ##Answer: {answer}.\n\n -------------------------------------------------------------------------------------\n CoT Answer: ##Reason: The document ##begin_quote## The Oberoi family is an\n Indian family that is famous for its involvement in hotels, namely through The\n Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\n the Oberoi group, and the"],"16":["]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7 P_ end _,i_ ( _t_ ), and\na passage selection score of the _i_ - th passage as\n_P_ selected( _i_ ).\nDuring training, we sample one positive and\n\n\u02dc\n_m_ _\u2212_ 1 negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. \u02dc _m_ is a hyper-parameter and we\nuse \u02dc _m_ = 24 in all the experiments. The training objective is to maximize the marginal log-likelihood\n\n- f all the correct answer spans in the positive passage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\n\n- f the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQuAD)\nand 4 for small (TREC, WQ) datasets, and tune _k_\n\n- n the development set. For experiments on small\ndatasets under the _Multi_ setting, in which using\n\n- ther datasets is allowed, we fine-tune the reader\ntrained on Natural Questions to the target dataset.\nAll experiments were done on eight 32GB GPUs.\n\n\n**6.2** **Results**\n\n\nTable 4 summarizes our final end-to-end QA results, measured by _exact match_ with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can\n\n\n**Training** **Model** **NQ** **TriviaQA** **WQ** **TREC** **SQuAD**\n\n\nSingle BM25+BERT (Lee et al., 2019) 26.5 47.1 17.7 21.3 33.2\nSingle ORQA (Lee et al., 2019) 33.3 45.0 36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9    -    -    Single GraphRetriever (Min et al., 201"," = 1000 in our experiments since it is a\nmore common setting in information retrieval experiments (for example, k = 1000 is the default in\nmost TREC evaluations).\n\n\n2.2 Reader\n\n\nAs is standard in a retriever\u2013reader design, the retriever in Karpukhin et al. (2020) returns k candidate passages (i.e., splits from Wikipedia) for each\nquery q. The reader extracts the final answer span\nfrom these candidate contexts, where each context\nCi is comprised of the Wikipedia article title Ci [title]\nand the content text Ci [text] .\nThe reader in DPR uses BERT-base and takes\n\nas input each candidate context Ci concatenated to\nthe question q. Answer extraction is treated as a labeling task, and the reader identifies the answer by\npredicting the start and end tokens of the answer\n\n\n\nspan in the contexts. To do so, the DPR reader\nadds a linear layer on top of BERT to predict the\nstart logit and end logit for each token from the final hidden layer representations. The score of an\nanswer span is calculated by adding the start logit\n\n- f the first token and the end logit of the last token. The reader returns the m highest scoring answer spans. In addition, the reader uses the learned\nrepresentation of [CLS] to predict the overall relevance of the context to the question.\n\nIn more detail, the reader operates as follows:\n\n\nri, S = Reader([CLS] q [SEP] Ci [title] [SEP] Ci [text] )\n\n\nwhere ri is the overall relevance score for context\nCi, and S comprises m potential (answer span,\nspan score) pairs extracted from context Ci:\n\n\n{(Si,1, si,1), (Si,2, si,2), . . . (Si,m, si,m)}.\n\n\nIn the original paper, the final answer span is the\ncandidate with the maximum span score from the\ncontext with the highest relevance score.\n\nWe attempted to replicate exactly the DPR implementation of answer extraction using our opensource PyGaggle neural reranking library, which\nholds the code to many of our other search-related\nprojects. Once again, we began with reader checkpoints released in the DPR repo, but otherwise our\nimplementation is completely independent (other\nthan, again, the evaluation code).\n\nIn addition to the answer extraction algorithm above, we also implemented the normalized answer span scoring technique described\n"," of the gold paragraphs among the retrieved set of paragraphs. We search for the hyperparameter _K_ (and _M_ for IRCoT) that maximizes\nthe recall on the dev set and use it on the test set.\n\n\n[6https:\/\/www.elastic.co\/](https:\/\/www.elastic.co\/)\n\n\n\nThe reported metric can thus be viewed as the _fixed-_\n_budget optimal recall_ for each system considered. [7]\n\n\n**QA Reader.** To implement the reader, we use\nthe same LMs as used in the reason-step of\nIRCoT Retriever. We found that QA readers implemented with Flan-T5-* perform better with the\nDirect Prompting strategy and GPT3 performs better with CoT Prompting strategy (see App. E).\nHence we use Direct prompting strategy for QA\nwith Flan-T5-* and CoT with GPT3 for the experiments. [8]\n\n\nThe QA reader has one hyperparameter _M_ : the\nnumber of distractor paragraphs in the in-context\ndemonstrations. We search for _M_ in _{_ 1 _,_ 2 _,_ 3 _}_ .\nWhen used in conjunction with IRCoT retriever\n_M_ is tied for the CoT generator and the reader.\n\n\n**Open-Domain QA (ODQA) Models.** Putting retrievers and readers together, we experiment with\nODQA models constructed from the various language models denoted as **OneR QA** and **IRCoT**\n**QA** . For IRCoT QA, the choice of LM for the CoT\ngenerator and the reader is kept the same. We also\nexperiment with retriever-less QA readers **NoR QA**\nto assess how well LMs can answer the question\nfrom their parametric knowledge alone. To select\nthe best hyperparameters for the ODQA model,\nwe search for the hyperparameters _K_ and _M_ that\nmaximize the answer F1 on the development set.\nIIRC is structured slightly differently from the\n\n- ther datasets, in that its questions are grounded\nin a main passage and other supporting paragraphs\ncome from the Wikipedia pages of entities mentioned in this passage. We slightly modify the retrievers and readers to account for this (see App. B).\n\n\n**5** **Results**\n\n\n**IRCoT retrieval is better than one-step.** Fig. 3\ncompares OneR with"," the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n\n\n**Selection of positive passages** Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA [6], we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the question will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. [7] We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the number of questions in training\/dev\/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n\n\n**5** **Experiments: Passage Retrieval**\n\n\nIn this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n\n\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Appendix A.\n\n\n**Training** **Retriever** **Top-20** **Top-100**\nNQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD\n\n\nNone BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0\n\n\nDPR 78.4 79.4 73.2 79.8 63.2 85.4 **85.0** 81.4 89.1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7",". Murdock Jr. 1962. The serial position\neffect of free recall. _Journal of experimental_\n_psychology_, 64(5):482.\n\n\nJoe O\u2019Connor and Jacob Andreas. 2021. What con\ntext features can Transformer language models\nuse? In _Proc. of ACL_ .\n\n\nDimitris Papailiopoulos, Kangwook Lee, and Jyyong Sohn. 2023. A little retrieval test for\n[large language models. https:\/\/github.com\/](https:\/\/github.com\/anadim\/the-little-retrieval-test)\n\n[anadim\/the-little-retrieval-test.](https:\/\/github.com\/anadim\/the-little-retrieval-test)\n\n\n[Bo Peng. 2023. RWKV-LM. https:\/\/github.](https:\/\/github.com\/BlinkDL\/RWKV-LM)\n\n[com\/BlinkDL\/RWKV-LM.](https:\/\/github.com\/BlinkDL\/RWKV-LM)\n\n\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong.\n2021. Random feature attention. In _Proc. of_\n\n_ICLR_ .\n\n\nFabio Petroni, Patrick Lewis, Aleksandra Piktus,\nTim Rockt\u00e4schel, Yuxiang Wu, Alexander H\nMiller, and Sebastian Riedel. 2020. How context\naffects language models\u2019 factual predictions. In\n_Proc. of AKBC_ .\n\n\nMichael Poli, Stefano Massaroli, Eric Nguyen,\nDaniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua\nBengio, Stefano Ermon, and Christopher R\u00e9.\n2023. Hyena hierarchy: Towards larger convolutional language models. In _Proc. of ICML_ .\n\n\nOfir Press, Noah A. Smith, and Mike Lewis. 2021.\nShortformer: Better language modeling using\nshorter inputs. In _Proc. of ACL_ .\n\n\nOfir Press, Noah A. Smith, and Mike Lewis. 2022.\nTrain short, test long: Attention with linear biases enables input length extrapolation. In _Proc._\n\n_of ICLR_ .\n\n\nGuanghui Qin, Yukun Feng, and Benjamin\nVan Durme. 2023. The NLP task effectiveness\n\n - f long-range transformers. In _"],"17":[".05)  - 90.1  - 95.0 [\u2021]\n\nHybrid (\u03b1 = 0.7)  - 89.6  - 94.6 [\u2021]\n\n\nSQuAD\n\nDPR 51.6 52.0 67.6 67.7\nBM25 68.8 71.1 [\u2020] 80.0 81.8 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 66.2  - 78.6  Hybridnorm (\u03b1 = 2.00)  - 75.1 [\u2021]  - 84.4 [\u2021]\n\nHybrid (\u03b1 = 28)  - 75.0 [\u2021]  - 84.0 [\u2021]\n\n\nTable 1: Retrieval effectiveness comparing results from\nthe original DPR paper (\u201corig\u201d) and our replication attempt (\u201crepl\u201d). The symbol [\u2020] - n a BM25 result indicates effectiveness that is significantly different from\nDPR. The symbol [\u2021] indicates that the hybrid technique\nis significantly better than BM25 (for SQuAD) or DPR\n(for all remaining collections).\n\n\nthat the effectiveness differences between DPR\n\nand BM25 in our replication results are statistically significant. In all datasets except for SQuAD,\nDPR outperforms BM25; this is consistent with\nthe original paper. We further confirmed that for\nSQuAD, DPR is significantly worse than BM25.\nAs Karpukhin et al. (2020) noted, RetrieverMulti\nwas trained by combining training data from all\ndatasets but excluding SQuAD; these poor results\nare expected, since SQuAD draws from a very\nsmall set of Wikipedia articles.\nThird, the effectiveness of hybrid dense\u2013sparse\nfusion appears to be understated in the original\nDPR paper. Karpukhin et al. (2020) found that\n\n\nCondition k = 20 100 500 1000\n\n\nNQ 6.1 5.2 4.4 4.2\nTriviaQA 9.2 6.6 5.0 4.6\nWQ 5.9 5.9 5.8 5.7\n\nCuratedTrec 6.9 7.2 6.3 5.9\n\n"," BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25( _q_, _p_ ) + _\u03bb \u00b7_ sim( _q, p_ ) as the ranking function.\nWe used _\u03bb_ = 1 _._ 1 based on the retrieval accuracy in\nthe development set.\n\n\n**5.1** **Main Results**\n\n\nTable 2 compares different passage retrieval systems on five QA datasets, using the top- _k_ accuracy\n( _k \u2208{_ 20 _,_ 100 _}_ ). With the exception of SQuAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when _k_ is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\n\n- n Natural Questions). When training with mul\n\n8SQuAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\n\nmore in Section 5.1.\n\n9\n[Lucene implementation. BM25 parameters](https:\/\/lucene.apache.org\/) _b_ = 0 _._ 4 (document length normalization) and _k_ 1 = 0 _._ 9 (term frequency\nscaling) are tuned using development sets.\n\n\n\nFigure 1: Retriever top- _k_ accuracy with different numbers of training examples used in our dense passage retriever vs BM25. The results are measured on the de\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n\n\ntiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and","-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\n\n- nly 500+ Wikipedia articles and thus the distribution of training examples is extremely biased, as\nargued previously by Lee et al. (2019).\n\n\n**5.2** **Ablation Study on Model Training**\n\n\nTo understand further how different model training\n\n- ptions affect the results, we conduct several additional experiments and discuss our findings below.\n\n\n\n\n\n\n**Sample efficiency** We explore how many training examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top- _k_\nretrieval accuracy with respect to different numbers of training examples, measured on the devel\n- pment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 examples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question\u2013passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently.\n\n\n**In-batch negative training** We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3.\nThe top block is the standard 1-of- _N_ training setting, where each question in the batch is paired\nwith a positive passage and its own set of _n_ negative passages (Eq. (2)). We find that the choice\n\n- f negatives \u2014 random, BM25 or gold passages\n(positive passages from other questions) \u2014 does\nnot impact the top- _k_ accuracy much in this setting\nwhen _k \u2265_ 20.\n\nThe middle bock is the in-batch negative training\n(Section 3.2) setting. We find that using a similar\nconfiguration (7 gold negative passages), in-batch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\n\n- r from the whole training set. Effectively, in-batch\nnegative training is an easy and memory"," the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n\n\n**Selection of positive passages** Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA [6], we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the question will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. [7] We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the number of questions in training\/dev\/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n\n\n**5** **Experiments: Passage Retrieval**\n\n\nIn this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n\n\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Appendix A.\n\n\n**Training** **Retriever** **Top-20** **Top-100**\nNQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD\n\n\nNone BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0\n\n\nDPR 78.4 79.4 73.2 79.8 63.2 85.4 **85.0** 81.4 89.1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7"," corresponding to Google\nsearch queries. The open-domain version of\nthis dataset is obtained by discarding answers\nwith more than 5 tokens.\n\n\n  - TriviaQA (Joshi et al., 2017) contains questions gathered from trivia and quiz-league\n\n\nFigure 3: Performance of Fusion-in-Decoder (base) on valid sets as a function of the number of retrieved passages.\n\n\n\nwebsites. The _unfiltered_ version of TriviaQA\nis used for open-domain question answering.\n\n\n  - SQuAD v1.1 (Rajpurkar et al., 2016) is a reading comprehension dataset. Given a paragraph\nextracted from Wikipedia, annotators were\nasked to write questions, for which the answer\nis a span from the corresponding paragraph.\n\n\nFollowing Lee et al. (2019) we use the validation as\ntest, and keep 10% of the training set for validation.\nWe use the Wikipedia dumps from Dec. 20, 2018\nfor NQ and TriviaQA and from Dec. 21, 2016 for\nSQuAD. We apply the same preprocessing as Chen\net al. (2017); Karpukhin et al. (2020), leading to\npassages of 100 words, which do not overlap.\n\n\n**Evaluation.** Predicted answers are evaluated\n\nwith the standard exact match metric (EM), as introduced by Rajpurkar et al. (2016). A generated\nanswer is considered correct if it matches any answer of the list of acceptable answers after normalization. This normalization step consists in lowercasing and removing articles, punctuation and\nduplicated whitespace.\n\n\n**Technical details.** We initialize our models with\n\nthe pretrained T5 models (Raffel et al., 2019), available in the HuggingFace Transformers library. [4] We\nconsider two model sizes, base and large, containing respectively 220M and 770M parameters. We\nfine-tune the models on each dataset independently,\nusing Adam (Kingma and Ba, 2014) with a constant learning rate of 10 _[\u2212]_ [4] and a dropout rate of\n10%. We train the model for 10k gradient steps,\nwith a batch size of 64, using 64 Tesla V100 32Gb.\nWe evaluate models every 500 steps and select the\nbest one on the validation set based on the Exact\n\n"],"18":[".1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7 **81.3**\n\n\nDPR **79.4** 78.8 **75.0** **89.1** 51.6 **86.0** 84.7 **82.9** 93.9 67.6\nMulti\nBM25 + DPR 78.0 **79.9** 74.7 88.5 66.2 83.9 84.4 82.3 **94.1** 78.6\n\n\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20\/100 retrieved\npassages that contain the answer. _Single_ and _Multi_ denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.\n\n\n\ntraditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10 _[\u2212]_ [5] using Adam, linear\nscheduling with warm-up and dropout rate 0 _._ 1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be desirable to obtain a single retriever that works well\nacross the board. To this end, we train a _multi_ dataset encoder by combining training data from\nall datasets excluding SQuAD. [8] In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages"," like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25\/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer","forms BM25 and DPR by margins\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\n\n\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\nthe performance observed in Karpukhin et al. (2020), where dpr-multiset-base showed superior results.\n\n\n7\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTable 2: **QuALITY and QASPER Performance With + Without RAPTOR:** Performance comparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outperforms baselines of each respective retrieval method for both datasets.\n\n\n**Model** **Accuracy (QuALITY)** **Answer F1 (QASPER)**\n\n\n**SBERT with RAPTOR** **56.6%** **36.70%**\n\nSBERT without RAPTOR 54.9% 36.23%\n\n**BM25 with RAPTOR** **52.1%** **27.00%**\n\nBM25 without RAPTOR 49.9% 26.47%\n\n**DPR with RAPTOR** **54.7%** **32.23%**\n\nDPR without RAPTOR 53.1% 31.70%\n\n\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different language models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column \u201dTitle +\nAbstract\u201d reflects performance when only the title and abstract of the papers are used for context.\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\nSpecifically, RAPTOR\u2019s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\nhigher than BM25.\n\n\n**Retriever** **GPT-3 F-1 Match** **GPT-4 F-1 Match** **UnifiedQA F-1 Match**\n\n\nTitle + Abstract 25.2 "," highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer. The passage\nselection model serves as a reranker through crossattention between the question and the passage. Although cross-attention is not feasible for retrieving\nrelevant passages in a large corpus due to its nondecomposable nature, it has more capacity than the\ndual-encoder model sim( _q, p_ ) as in Eq. (1). Applying it to selecting the passage from a small number\n\n- f retrieved candidates has been shown to work\n\nwell (Wang et al., 2019, 2018; Lin et al., 2018).\nSpecifically, let **P** _i \u2208_ R _[L][\u00d7][h]_ (1 _\u2264_ _i \u2264_ _k_ ) be\na BERT (base, uncased in our experiments) representation for the _i_ - th passage, where _L_ is the\nmaximum length of the passage and _h_ the hidden\ndimension. The probabilities of a token being the\nstarting\/ending positions of an answer span and a\npassage being selected are defined as:\n\n\n\n_P_ start _,i_ ( _s_ ) = softmax\ufffd **P** _i_ **w** start\ufffd\n\n\n\n_s_ _[,]_ (3)\n\n_t_ _[,]_ (4)\n\n\n\n_P_ end _,i_ ( _t_ ) = softmax\ufffd **P** _i_ **w** end\ufffd\n\n\n\n_P_ selected( _i_ ) = softmax\ufffd **\u02c6P** \u22ba **w** selected\ufffd\n\n\n\n_i_ _[,]_ [ (5)]\n\n\n\nwhere **P** **[\u02c6]** = [ **P** [[CLS]] 1 _, . . .,_ **P** [[CLS]] _k_ ] _\u2208_ R _[h][\u00d7][k]_ and\n**w** start _,_ **w** end _,_ **w** selected _\u2208_ R _[h]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7","r+\n\u03b3 \u00b7 R [\u20d7] )i \u00b7 softmax(\u20d7si)j.\nThus, to summarize, we explored four settings\nin our end-to-end QA replication: the original\nDPR span scoring technique, with and without retriever score fusion, and the answer span scoring\ntechnique of Mao et al. (2020), with and without\nretriever score fusion.\n\n\n3 Experimental Setup\n\n\nModels Our replication efforts began with\nmodel checkpoints provided in the DPR repo. Unfortunately, Karpukhin et al. (2020) did not appear\nto make available all models used in their experiments, and thus, to be precise, our experiments\nused the following models:\n\n\n- RetrieverNQ: DPR encoders trained using just\nthe NQ dataset (for the retriever).\n\n\n- RetrieverMulti: DPR encoders trained using a\ncombination of datasets (for the retriever).\n\n\n- ReaderNQ-Single: the DPR reader trained on NQ\nwith negative passages from retrieval results by\nRetrieverNQ.\n\n\n- ReaderTQA-Multi: the DPR reader trained on\nTriviaQA with negative passages from retrieval\nresults by RetrieverMulti.\n\n\n\nDatasets We evaluated retrieval effectiveness\n\n- n five standard benchmark QA datasets (NQ,\nTriviaQA, WQ, CuratedTREC, SQuAD), exactly\nthe same as Karpukhin et al. (2020). We used the\nRetrieverMulti model, which can be applied to all\nfive datasets. For end-to-end QA, we evaluated\n\n- n NQ and TriviaQA with the available models.\nMore precisely, we used the ReaderNQ-Single model\nto process the retrieved contexts from RetrieverNQ\nfor NQ and used the ReaderTQA-Multi model to process the retrieved contexts from RetrieverMulti for\nTriviaQA.\n\n\nMetrics For retrieval, we measured effectiveness in terms of top-k retrieval accuracy, defined\nas the fraction of questions that have a correct answer span in the top-k retrieved contexts at least\n\n- nce. End-to-end QA effectiveness is measured\nin terms of the exact match (EM) metric, defined\nas the fraction of questions that have an extracted\nanswer span exactly matching the ground truth an\nswer.\n\nMissing from the original DPR paper, we performed significance testing to assess the statistical\nsignificance of metric differences. In all cases,"],"19":[" highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer. The passage\nselection model serves as a reranker through crossattention between the question and the passage. Although cross-attention is not feasible for retrieving\nrelevant passages in a large corpus due to its nondecomposable nature, it has more capacity than the\ndual-encoder model sim( _q, p_ ) as in Eq. (1). Applying it to selecting the passage from a small number\n\n- f retrieved candidates has been shown to work\n\nwell (Wang et al., 2019, 2018; Lin et al., 2018).\nSpecifically, let **P** _i \u2208_ R _[L][\u00d7][h]_ (1 _\u2264_ _i \u2264_ _k_ ) be\na BERT (base, uncased in our experiments) representation for the _i_ - th passage, where _L_ is the\nmaximum length of the passage and _h_ the hidden\ndimension. The probabilities of a token being the\nstarting\/ending positions of an answer span and a\npassage being selected are defined as:\n\n\n\n_P_ start _,i_ ( _s_ ) = softmax\ufffd **P** _i_ **w** start\ufffd\n\n\n\n_s_ _[,]_ (3)\n\n_t_ _[,]_ (4)\n\n\n\n_P_ end _,i_ ( _t_ ) = softmax\ufffd **P** _i_ **w** end\ufffd\n\n\n\n_P_ selected( _i_ ) = softmax\ufffd **\u02c6P** \u22ba **w** selected\ufffd\n\n\n\n_i_ _[,]_ [ (5)]\n\n\n\nwhere **P** **[\u02c6]** = [ **P** [[CLS]] 1 _, . . .,_ **P** [[CLS]] _k_ ] _\u2208_ R _[h][\u00d7][k]_ and\n**w** start _,_ **w** end _,_ **w** selected _\u2208_ R _[h]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7"," we began with reader checkpoints released in the DPR repo, but otherwise our\nimplementation is completely independent (other\nthan, again, the evaluation code).\n\nIn addition to the answer extraction algorithm above, we also implemented the normalized answer span scoring technique described\nby Mao et al. (2020). Each answer span in each\ncandidate context Ci is rescored by:\n\n\ns [\u2032] i,j [=][ softmax][(][\u20d7r][)][i] [\u00b7][ softmax][(][\u20d7s][i][)][j]\n\n\nwhere \u20d7r = {r1, \u00b7 \u00b7 \u00b7, rk} is the set of relevance scores of all candidate contexts and \u20d7si =\n{si,1, \u00b7 \u00b7 \u00b7, si,m} is the set of all span scores within\ncontext Ci. Duplicate answer spans across all contexts are scored by accumulating their individual\nscores. The answer span with the maximum final\nscore is selected as the final prediction.\n\nIn summary, we compared two answer span\nscoring techniques in the reader: the \u201coriginal\u201d answer span scoring technique described\nby Karpukhin et al. (2020), and the span scoring\ntechnique described by Mao et al. (2020).\n\n\n2.3 Final Evidence Fusion\n\n\nIn the original DPR paper, the final answer span is\n\n- nly selected based on scores from the reader. In\n\n- ur replication attempt, we additionally tried to exploit scores from the retriever to improve answer\nspan selection. Our intuition is that predictions\nfrom both the retriever and the reader should con\ntribute to the final answer. Concretely, instead of\njust using the relevance score ri from the reader to\nscore contexts, we fuse ri with the retriever score\nRi, calculated by:\n\n\n\u03b2 \u00b7 ri + \u03b3 \u00b7 Ri\n\n\nDepending on the retrieval method, Ri can be the\nsparse retrieval score, the dense retrieval score,\n\n- r the score after hybrid fusion. This final fused\nscore replaces ri as the relevance score for each\ncontext in the answer span scoring step. For example, with fusion, the answer span scoring technique of Mao et al. (2020) becomes softmax(\u03b2 - \u20d7r+\n\u03b3 \u00b7 R [\u20d7] )i \u00b7 softmax(\u20d7si)j.\nThus, to summarize, we explored four settings\nin our end-to-end QA replication: the original\nDPR span scoring technique, with","r+\n\u03b3 \u00b7 R [\u20d7] )i \u00b7 softmax(\u20d7si)j.\nThus, to summarize, we explored four settings\nin our end-to-end QA replication: the original\nDPR span scoring technique, with and without retriever score fusion, and the answer span scoring\ntechnique of Mao et al. (2020), with and without\nretriever score fusion.\n\n\n3 Experimental Setup\n\n\nModels Our replication efforts began with\nmodel checkpoints provided in the DPR repo. Unfortunately, Karpukhin et al. (2020) did not appear\nto make available all models used in their experiments, and thus, to be precise, our experiments\nused the following models:\n\n\n- RetrieverNQ: DPR encoders trained using just\nthe NQ dataset (for the retriever).\n\n\n- RetrieverMulti: DPR encoders trained using a\ncombination of datasets (for the retriever).\n\n\n- ReaderNQ-Single: the DPR reader trained on NQ\nwith negative passages from retrieval results by\nRetrieverNQ.\n\n\n- ReaderTQA-Multi: the DPR reader trained on\nTriviaQA with negative passages from retrieval\nresults by RetrieverMulti.\n\n\n\nDatasets We evaluated retrieval effectiveness\n\n- n five standard benchmark QA datasets (NQ,\nTriviaQA, WQ, CuratedTREC, SQuAD), exactly\nthe same as Karpukhin et al. (2020). We used the\nRetrieverMulti model, which can be applied to all\nfive datasets. For end-to-end QA, we evaluated\n\n- n NQ and TriviaQA with the available models.\nMore precisely, we used the ReaderNQ-Single model\nto process the retrieved contexts from RetrieverNQ\nfor NQ and used the ReaderTQA-Multi model to process the retrieved contexts from RetrieverMulti for\nTriviaQA.\n\n\nMetrics For retrieval, we measured effectiveness in terms of top-k retrieval accuracy, defined\nas the fraction of questions that have a correct answer span in the top-k retrieved contexts at least\n\n- nce. End-to-end QA effectiveness is measured\nin terms of the exact match (EM) metric, defined\nas the fraction of questions that have an extracted\nanswer span exactly matching the ground truth an\nswer.\n\nMissing from the original DPR paper, we performed significance testing to assess the statistical\nsignificance of metric differences. In all cases,"," like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25\/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer",", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop\/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation"],"20":["36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9    -    -    Single GraphRetriever (Min et al., 2019b) 34.5 56.0 36.4    -    Single PathRetriever (Asai et al., 2020) 32.6    -    -    - **56.5**\nSingle REALMWiki (Guu et al., 2020) 39.2    - 40.2 46.8    Single REALMNews (Guu et al., 2020) 40.4    - 40.7 42.9    \n\nBM25 32.6 52.4 29.9 24.9 38.1\n\nSingle DPR **41.5** 56.8 34.6 25.9 29.8\nBM25+DPR 39.0 57.0 35.2 28.0 36.7\n\n\nDPR **41.5** 56.8 **42.4** 49.4 24.1\nMulti\nBM25+DPR 38.8 **57.9** 41.1 **50.6** 35.8\n\n\nTable 4: End-to-end QA (Exact Match) Accuracy. The first block of results are copied from their cited papers.\nREALMWiki and REALMNews are the same model but pretrained on Wikipedia and CC-News, respectively. _Single_\nand _Multi_ denote that our Dense Passage Retriever (DPR) is trained using individual or combined training datasets\n(all except SQuAD). For WQ and TREC in the _Multi_ setting, we fine-tune the reader trained on NQ.\n\n\n\nsee that higher retriever accuracy typically leads to\nbetter final QA results: in all cases except SQuAD,\nanswers extracted from the passages retrieved by\nDPR are more likely to be correct, compared to\nthose from BM25. For large datasets like NQ and\nTriviaQA, models trained using multiple datasets\n(Multi) perform comparably to those trained using\nthe individual training set (Single). Conversely,\n\n- n smaller datasets like WQ and TREC, the multidataset setting has a clear advantage. Overall, our\nD",") **73.30** 35.28 **74.00** **84.95** **86.86**\n\n\n**Datasets** In our experiments, we use the following datasets to evaluate our model and\nall baselines. We selected these datasets to represent both popular and diverse domains\nincluding Wikipedia, Coding\/API documents, and question-answering on medical documents. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and\nHotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,\nmainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,\nand TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.\nThese benchmarks measure how to generate the correct, functional, and executable API\ncalls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering\ndataset tailored only for biomedical-research question-answering. It mainly focuses on\nanswering medical and biology questions based on a given set of documents. We would\n\n\n5\n\n\nPreprint, Under Review\n\n\nlike to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas\nthe latter two domains are on domain-specific documents.\n\n\n**Baselines** We consider the following baselines for our experiments:\n\n\n   - LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used\ninstruction-finetuned model for QA tasks, where we provide clearly written instructions, but no reference documentation.\n\n   - LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,\nexcept here we include reference documents. This is a popular technique when\ndealing with domain-specific QA tasks.\n\n   - Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervisedfinetuning, without documents in context. We find that its mostly useful to align\nthe answering style of the model as well as get familiar with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does","I\u2212FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So\n\nthe answer is: 1894.\n\n\nQ: When was Neville A. Stanton's employer founded?\nA: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862.\n\nSo the answer is: 1862.\n\n\nQ: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to?\nA: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located\n\nin the Prince County. So the answer is: Prince County.\n\n\nQ: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America\u2212\n\nLite: How Imperial Academia Dismantled Our Culture's author?\nA: The author of America\u2212Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was\n\neducated at the Yale University. The city in Connecticut that has the highest number of Zagat\u2212rated restaurants is New\nHaven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale\nHerald.\n\n\nQ: What is the headquarters for the organization who sets the standards for ISO 21500?\nA: The standards for ISO 21500 were set by International Organization for Standardization. The International Organization\n\nfor Standardization has headquarters in Geneva. So the answer is: Geneva.\n\n\nQ: What did the publisher of Banjo\u2212Tooie rely primarily on for its support?\nA: The publisher of Banjo\u2212Tooie is Nintendo. Nintendo relied primarily for its support on first\u2212party games. So the answer is:\n\nfirst\u2212party games.\n\n\nQ: In which county was the birthplace of the Smoke in tha City performer?\nA: The performer of Smoke in tha City is MC Eiht. MC Eiht's birthplace is Compton. Compton is located in the county of Los\n\nAngeles County. So the answer is: Los Angeles County.\n\n\nQ: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?\n\n\nA: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central\n\nJakarta.\n\n\nQ:"," and\nmethod of leveraging meta-information. UltraChat (Ding\net al., 2023b) effectively demonstrates the process of curating\nboth high-quality and diverse data by distilled knowledge.\nThey collect extensive meta-information across three domains: _Questions about the World, Creation and Generation_,\nand _Assistance on Existing Materials_ . For example, under\n_Questions about the World_, they explore 30 meta-topics like\n\u201dTechnology\u201d and \u201dFood and Drink.\u201d the teacher LLMs\nthen use this meta-information to distill a broad array\n\n- f instructions and conversations, achieving a substantial\nscale of 1.5 million instances. UltraChat stands out with its\nlexical and topical diversity. The UltraLLaMA model, finetuned on this data, consistently surpasses other open-source\nmodels. Another notable series, **phi** (Gunasekar et al., 2023;\nLi et al., 2023a; Mar, 2023), focuses on distilling smaller,\nhigh-quality datasets akin to \u201dtextbooks.\u201d **Phi-1** (Gunasekar\net al., 2023) experiments with synthesizing \u201dtextbook quality\u201d data in the coding domain. Their approach involves\ndistilling clear, self-contained, instructive, and balanced content from LLMs, guided by random topics or function names\nto enhance diversity. The distilled data is a synthesis of 1\nbillion tokens of Python textbooks, complete with natural\nlanguage explanations and code snippets, as well as 180 million tokens of Python exercises with solutions. Remarkably,\nthe **phi-1** model, despite its smaller size, outperforms nearly\nall open-source models on coding benchmarks like HumanEval and MBPP while being 10 times smaller in model\nsize and 100 times smaller in dataset size. MFTCoder (Liu\net al., 2023d) utilizes hundreds of Python knowledge points\nas meta-information to create a CodeExercise Dataset. In\ncontrast, Magicoder (Wei et al., 2023) and WaveCoder (Yu\net al., 2024) get raw code collections from open-source\ncode datasets, using this as meta-information for generating\ninstructional data. In the context of NLU tasks, certain\nstudies (Ye et al., 2022; Gao et al., 2023a; Wang et al., 2021a)\n"," time a team from which years did a team from canada\ncanada won the stanley cup as of 2002 won the stanley cup as of 2002\n\n\nSituatedQA when did england last get to the semi which years did england get to the semi\nfinal in a world cup as of 2019 final in a world cup as of 2019?\n\n\nSituatedQA what is the biggest hotel in las vegas nv what is the size of the hotels in las vegas\nas of November 28, 1993 nv as of November 28, 1993\n\n\nSituatedQA who has scored most runs in t20 What are the runs of players in t20\nmatches as of 2017 matches as of 2017\n\n\nSituatedQA who is the highest paid player in the nba what is the salary of the high paid playthis season as of 2017 ers in the nba this season as of 2017\n\n\nTable 13: Few-shot demonstration exemplars for asking step-back questions in TimeQA and SituatedQA.\n\n\n19\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nKnowledge QA Final-Answer Prompt\n\n\nYou are an expert of world knowledge. I am going to ask you a question.\nYour response should be comprehensive and not contradicted with the\nfollowing context if they are relevant. Otherwise, ignore them if they are\nnot relevant.\n\n\n_<_ Passage from original retrieval augmentation _>_\n_<_ Passage from step-back retrieval augmentation _>_\n\n\nOriginal Question: _<_ Original Question _>_\nAnswer:\n\n\nTable 14: Prompt of querying the model for final answer with additional contexts from original and\nstep-back retrieval augmentations in TimeQA and SituatedQA\n\n\ndataset Original Question Step-back Question\n\n\n\nMuSiQue at year saw the creation of the region\nwhere the county of Hertfordshire is\nlocated?\n\n\n\nwhich region is the county of Hertfordshire located?\n\n\n\nMuSiQue Jan Sindel\u2019s was born in what coun- [\u02c7] what is Jan Sindel\u2019s personal his- [\u02c7]\ntry? tory?\n\n\nMuSiQue When was the abolishment of the which studio distributed The Game?\nstudio that distributed The Game?\n\n\n\nMuSiQue What city is the person who broadened the doctrine of philosophy of\nlanguage from?\n\n\nMuSiQue When was the baseball team winning the world series in 2015 baseball created?\n\n\n\nwho broadened the doctrine"],"21":["## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar","., 2019, Craswell et al., 2020, Izacard et al., 2021, Wang et al., 2022].\nThese neural models are predominantly used in a single vector capacity, where they output a single\n_embedding_ representing the entire input (also known as _dense retrieval_ ). These embedding models\nare capable of generalizing to new retrieval datasets and have been tasked with solving increasingly\ncomplicated retrieval problems [Thakur et al., 2021, Enevoldsen et al., 2025, Lee et al., 2025].\n\n\nIn recent years this has been pushed even further with the rise of instruction-following retrieval\nbenchmarks, where models are asked to represent **any relevance definition** for **any query** [Weller\net al., 2025a,b, Song et al., 2025, Xiao et al., 2024, Su et al., 2024]. For example, the QUEST dataset\n\n[Malaviya et al., 2023] uses logical operators to combine different concepts, studying the difficulty\n\n- f retrieval for complex queries (e.g., \u201cMoths or Insects or Arthropods of Guadeloupe\u201d). On the\n\n- ther hand, datasets like BRIGHT [Su et al., 2024] explore the challenges stemming from different\ndefinitions of relevance by defining relevance in ways that require reasoning. One subtask includes\nreasoning over a given Leetcode problem (the query) to find other Leetcode problems that share a\nsub-task (e.g. others problems using dynamic programming). Although models cannot solve these\nbenchmarks yet, the community has proposed these problems in order to push the boundaries of\nwhat dense retrievers are capable of\u2014which is now implicitly _every task_ that could be defined.\n\n\nRather than proposing empirical benchmarks to gauge what embedding models can achieve, we\nseek to understand at a more fundamental level what the limitations are. Since embedding models use\n\n\n\u2217Work done during internship at GDM.\nData and code are available at `[https:\/\/github.com\/google-deepmind\/limit](https:\/\/github.com\/google-deepmind\/limit)`\n\n\n\u00a9 2025 Google DeepMind. All rights reserved\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n##### Query-Document Relevance LIMIT : A real-world instantiation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeslie Laham likes Apples and Candy.\n\n\n\n\n\n\n\n\n|Col1","Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set. Until recently, retrieval models were dependent\n\n- n traditional term-based information retrieval (IR) methods, which fail to capture the semantics of\nthe question beyond lexical matching and remain a major performance bottleneck for the task. Recent\nwork on dense retrieval methods instead uses pretrained encoders to cast the question and documents\ninto dense representations in a vector space and relies on fast maximum inner-product search (MIPS)\nto complete the retrieval. These approaches (Lee et al., 2019; Guu et al., 2020; Karpukhin et al.,\n2020) have demonstrated significant retrieval improvements over traditional IR baselines.\n\n\nHowever, such methods remain limited to _simple_ questions, where the answer to the question\nis explicit in a single piece of text evidence. In contrast, _complex_ questions typically involve\naggregating information from multiple documents, requiring logical reasoning or sequential (multihop) processing in order to infer the answer (see Figure 1 for an example). Since the process for\nanswering such questions might be sequential in nature, single-shot approaches to retrieval are\ninsufficient. Instead, iterative methods are needed to recursively retrieve new information at each\nstep, conditioned on the information already at hand. Beyond further expanding the scope of existing\ntextual open-domain QA systems, answering more complex questions usually involves _multi-hop_\n_reasoning_, which poses unique challenges for existing neural-based AI systems. With its practical\n\n\n_\u2217_ Equal Contribution\n1\n[https:\/\/github.com\/facebookresearch\/multihop_dense_retrieval.](https:\/\/github.com\/facebookresearch\/multihop_dense_retrieval)\n\n\n1\n\n\nPublished as a conference paper at ICLR 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: An overview of the multi-hop dense retrieval approach.\n\n\nand research values, multi-hop QA has been extensively studied recently (Talmor & Berant, 2018;\nYang et al., 2018; Welbl et al., 2018) and remains an active research area in NLP (Qi et al., 2019; Nie\net al., 2019; Min et al., 2019; Zhao et al., 2020; Asai"," and the Family in May\n2002. She held this position until 2005.\n**Stepback Question** : What positions did Carmel Hanna hold?\n**Final Answer [Correct]** : From the context, we can see that Carmel\nHanna was named minister of Employment and Learning (2001). We\ncan then infer that she held the position of Minister for Employment and\nLearning in May 2002.\n\n\nTable 32: Illustration of wins of STEP-BACK PROMPTING on the TimeQA dataset.\n\n\n36\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\ndataset example\n\n\nSituatedQA **Original Question** : when was the last time miss texas won miss america\nas of November 29, 2010\n**Original Answer [Incorrect]** : Laura Leigh Covington was the last Miss\nTexas to win the Miss America competition. This happened in 2000.\n**Stepback Question** : which years did miss texas win miss america\n**Final Answer [Correct]** : According to Wikipedia, the last time Miss\nTexas won Miss America was in 1975.\n\n\nSituatedQA **Original Question** : when is the last time congress passed a complete\nbudget as of 2021\n**Original Answer [Incorrect]** : The last time Congress passed a complete\nbudget as of 2021 was on January 19, 2018.\n**Stepback Question** : what are the years that congress passed a complete\nbudget as of 2021\n**Final Answer [Correct]** : The last time Congress passed a complete budget as of 2021 was December 27, 2020, the Consolidated Appropriations\nAct, 2021.\n\n\nSituatedQA **Original Question** : when was the last time unc did not make the ncaa\ntournament as of 2003\n**Original Answer [Incorrect]** : The last time UNC did not make the\nNCAA tournament was 1990.\n**Stepback Question** : which years did unc not make the ncaa tournament\nas of 2003\n**Final Answer [Correct]** : The correct answer is 2002-03. The context\nshows the standings of North Carolina Tar Heels men\u2019s basketball. From\nthe standings, we can see that this team has 19 wins and 16 losses. 19-16\nis not good enough to make the NCAA tournament, which is mentioned\nin the context.","eng, X. A. Lin, L. He, and\nD. Jiang, \u201cHyper: Multitask hyper-prompted training enables large-scale retrieval generalization,\u201d in _The Eleventh_\n\n\n_International Conference on Learning Representations_, 2022.\nC. Liu, C. Tao, X. Geng, T. Shen, D. Zhao, C. Xu, B. Jiao,\nand D. Jiang, \u201cAdam: Dense retrieval distillation with\nadaptive dark examples,\u201d _arXiv preprint arXiv:2212.10192_,\n2022.\nJ. Feng, C. Tao, X. Geng, T. Shen, C. Xu, G. Long, D. Zhao,\nand D. Jiang, \u201cKnowledge refinement via interaction between search engines and large language models,\u201d _arXiv_\n_preprint arXiv:2305.07402_, 2023.\nT. Shen, G. Long, X. Geng, C. Tao, T. Zhou, and D. Jiang,\n\u201cLarge language models are strong zero-shot retriever,\u201d\n_arXiv preprint arXiv:2304.14233_, 2023.\nX. Ma, X. Zhang, R. Pradeep, and J. Lin, \u201cZero-shot listwise\ndocument reranking with a large language model,\u201d 2023.\nZ. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, J. Shen,\nT. Liu, J. Liu, D. Metzler, X. Wang, and M. Bendersky,\n\u201cLarge language models are effective text rankers with\npairwise ranking prompting,\u201d 2023.\nX. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, \u201cQuery\nrewriting in retrieval-augmented large language models,\u201d\nin _Proceedings of the 2023 Conference on Empirical Methods_\n_in Natural Language Processing_, H. Bouamor, J. Pino, and\nK. Bali, Eds. Singapore: Association for Computational\nLinguistics, Dec. 2023, pp. 5303\u20135315. [Online]. Available:\n[https:\/\/aclanthology.org\/2023.emnlp-main.322](https:\/\/aclanthology.org\/2023.emnlp-main.322)\nD."],"22":[" have seen that models may hallucinate rather than abstain and that\nthis happens more with RAG than in a closed-book setting. A natural next question is whether we\ncan prompt or fine-tune a model to perform closer to the ideal case. Can we steer the model to either\n\n- utput the correct answer or abstain, while hallucinating an incorrect answer as little as possible?\n\n\n5.1 SELECTIVE RAG USING SUFFICIENT CONTEXT SIGNAL\n\n\nOne simple solution to improving RAG performance would be to use the sufficient context autorater\nto abstain given insufficient context. However, this heavy-handed approach can lower overall\nperformance, since all models answer some questions correctly even with insufficient context, as\ndescribed in Table 2 and demonstrated in Figure 3. Instead, we propose a method for combining\n\n\n8\n\n\nPublished as a conference paper at ICLR 2025\n\n\nthe sufficient context autorater outputs with model self-rated confidence scores to tune a selective\naccuracy-coverage trade-off, where \u201ccoverage\u201d denotes the portion of inputs on which the model does\nnot abstain. Specifically, we use these signals to train a simple linear model to predict hallucinations,\nand then use it to set coverage-accuracy trade-off thresholds.\n\n\nThis mechanism differs from other strategies for improving abstention in two key ways. First, because\nit operates independently from generation, it mitigates unintended downstream effects, whereas\nstrategies like fine-tuning to improve abstention can inadvertently worsen performance on certain\ninputs (see Section 5.2). Second, it offers a _controllable_ mechanism for tuning abstention, which\nallows for different operating settings in differing applications, such as strict accuracy compliance in\nmedical domains or maximal coverage on creative generation tasks.\n\n\n**Abstention Signals** We utilize two main signals for abstention: the self-rated probabilities as\nin Li et al. (2024); Kadavath et al. (2022) and the sufficient context autorater. For the self-rated\nprobabilities, we use two strategies: P(True) and P(Correct). P(True) requires sampling answers\nfrom the model multiple times, and then prompting the model multiple times to label each model as\ncorrect or incorrect, resulting in a final probability of correctness associated with each question as in\nKadavath et al. (2022). For proprietary models, where extensive querying is prohibitively expensive,\nwe use P(Correct) instead."," et al., 2024;\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\nrelevant documents, but simply uses \u201cdoes not contain the answer\u201d as a proxy for irrelevant context.\n\n\n**Reducing Hallucinations.** There have also been efforts to improve RAG factuality on open-book QA\ntasks (Asai et al., 2023; Mineiro, 2024; Simhi et al., 2024; Wang et al., 2024b; Zhang et al., 2024b).\nThe main theme is to improve both the generation and retrieval quality, often by fine-tuning one or\nmore components. Also, since RAG leads to very long contexts, another issue that arises is the \u201clost\nin the middle\u201d problem (Hsieh et al., 2024; Liu et al., 2024; Yu et al., 2024). These works start with\nthe premise that the provided query\/context should be precisely answerable by the LLM, and hence,\n\n- nly analyze their findings in the sufficient context scenario. Independent of RAG, many papers\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang\net al., 2024; Kadavath et al., 2022; Yin et al., 2023; Zhang et al., 2024a) and performance across\ndisaggregated subsets of data (Paes et al., 2022; Joren et al., 2023).\n\n\n3 SUFFICIENT CONTEXT\n\n\nAt a high level, our aim is to classify input instances based on whether the context contains enough\ninformation to answer the query. We split possible contexts into two cases: (1) **Sufficient Context.**\nThe context is sufficient to answer the query if it contains all the necessary information to provide a\ndefinitive answer. (2) **Insufficient Context.** Otherwise, a context is insufficient. A context may also\nbe insufficient if the query requires specialized knowledge that is not provided in the context or if\nthe information in the context is incomplete, inconclusive, or contradictory. In this section, we more\nthoroughly discuss sufficient context. Then, we show how to accurately and scalably label instances.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\n3.1 DEFIN","ervention model to determine when the model generates or abstains, providing a controllable\ntrade-off. Moreover, we can combine our method with any LLM, including proprietary models like\nGemini and GPT. Our main result is that using sufficient context as an additional signal leads to\nmuch higher accuracy over the fraction of answered queries, for most coverage levels and across\nmultiple models\/datasets. We also find that fine-tuning open-source models with sufficient context\ninformation does not easily reduce the hallucination rate. Instead, for Mistral 3, fine-tuning can lead\nto a higher abstention rate at the cost of fewer correct answers. Key findings and the prompts used in\n\n[our autorater analysis are available on our github.](https:\/\/github.com\/hljoren\/sufficientcontext)\n\n\nTo summarize, our main contributions are\n\n\n1. We define the notion of sufficient context, unifying existing work on relevance for RAG systems.\nThen, we design a sufficient context autorater (achieving 93% accuracy), enabling us to label\ninstances scalably and to analyze model responses with or without sufficient context.\n\n\n2. Our analysis leads to several new findings about retrieval-augmented model performance. One\ntakeaway is that SOTA LLMs output correct responses 35\u201362% of the time with insufficient\ncontext. Hence, intervention strategies to increase accuracy should not solely rely on sufficiency.\n\n\n3. Building on our findings above, we develop an efficient and general method for selective generation,\nusing both confidence and sufficient context signals. Our method improves the fraction of correct\nanswers (among total model responses) by up to 2\u201310% for Gemini, GPT, and Gemma.\n\n\n2 RELATED WORK\n\n\nMany papers have shown that reaping the benefits of RAG (e.g., better factuality) will require a deeper\nunderstanding of how LLMs respond to variations in the queries and provided context (Asai et al.,\n2024; Fan et al., 2024; Ram et al., 2023; Rau et al., 2024). We review two main areas. First, much\nwork has evaluated RAG systems with poor retrieval, uncovering cases where LLMs are led astray by\nirrelevant context. Another line of study aims to reduce LLM hallucinations in RAG settings.\n\n\n**(Ir)relevant Context.** Prior studies uncover a lack of robustness to imperfect retrieval. However,\n","Ab|rrect<br>Ab|sta|in<br>Hal|in<br>Hal|rect<br>Abst|rect<br>Abst|ain<br>Hall|\n\n\n\nFigure 3: **Model Performance on Datasets Stratified by Sufficient Context.** Given sufficient\ncontext, models have a higher correct percentage on these challenging datasets. Performance drops,\nbut the models are still able to answer a large portion of questions correct without sufficient context.\nOne prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient\ncontext. The smallest model Gemma 27B struggles to avoid hallucinations given insufficient context.\n\n\nappropriate. Without RAG, Claude 3.5 Sonnet abstains on 84.1% questions, while with RAG, the\nfraction of abstentions drops to 52%. Similarly, GPT 4o\u2019s abstention fraction moves from 34.4%\nto 31.2% and Gemini 1.5 Pro\u2019s drops from 100% to 18.6%. This phenomenon may arise from the\nmodel\u2019s increased confidence in the presence of any contextual information, leading to a higher\npropensity for hallucination rather than abstention.\n\n\n**Models Hallucinate with Both Sufficient and Insufficient Context.** Considering Figure 3, models\ngenerally achieve higher accuracy with sufficient context (higher **green bars**, top row) than without\nsufficient context (lower **green bars**, bottom row). However, looking at each row separately, we\ndiscover several findings. First, in the sufficient context case (top row), we see that models hallucinate\nmore than they abstain ( **red bars** are higher than **blue bars**, usually). The trend holds across all\nthree datasets. Moving to insufficient context (bottom row), we find a different distribution of model\nresponses, with more abstentions and hallucinations. This tendency varies notably across different\nmodels. For instance, Claude abstains more (higher **blue bars** ) with insufficient context, but answers\nfewer questions correctly (lower **green bars** ) than Gemini and GPT. These differences underscore\nthe potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\nmuch more hallucinations (higher **red bars** ) than the other models, except for HotPotQA, where we\nattribute the higher accuracy to the smaller retrieved contexts.\n\n\n4.3 QUALITATIVELY ANALYZING"," LLMs are led astray by\nirrelevant context. Another line of study aims to reduce LLM hallucinations in RAG settings.\n\n\n**(Ir)relevant Context.** Prior studies uncover a lack of robustness to imperfect retrieval. However,\nthese studies vary in terms of how they evaluate retrieval quality, without anchoring to a precise\n\u201crelevance\u201d definition. Shi et al. (2023a) adds sentences to math questions (based on GSM8K) which\n\n\n2\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: **New insights into RAG systems by looking at whether instances have sufficient context.**\nOn the left, we show examples of sufficient context; on the right, a breakdown of model responses on\nthe Musique dataset. Adding RAG improves the percentage of correct answers. Unfortunately, with\nRAG, models hallucinate more than abstain, and the insufficiency of the context does not account for\nthis major issue. Also, standard datasets have many instances with insufficient context (here, 55.4%).\nWe include results for other datasets (FreshQA, HotPotQA) in Appendix B, showing similar trends.\n\n\nshould not impact the answer at all (and GSM8K is designed to have sufficient context by definition).\nXie et al. (2024) looks at having counter-memory context, by either replacing the entity name with\nan erroneous one or using an LLM to generate a synthetic context supporting the erroneous entity.\nRet-Robust (Yoran et al., 2024) trains a model to be robust to irrelevant context, with an NLI-based\nentailment to determine relevance, and only uses the relevance scores to influence the training mixture\n\n- f relevant vs. irrelevant documents. Wu et al. (2024) looks at questions where the LLM gets the\nanswer correct without retrieval and is non-robust to changes in the retrieval. Multiple methods use\na model to predict relevance scores (as part of a larger pipeline), without calibration to a formal\ndefinition (Wang et al., 2024a; Zhou et al., 2024), including for iterative retrieval (Jiang et al., 2024;\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\nrelevant documents, but simply uses \u201cdoes not contain the answer\u201d"],"23":["## **Query Rewriting for Retrieval-Augmented Large Language Models**\n\n**Xinbei Ma** [1,2,] _[\u2217]_ **, Yeyun Gong** [3, #, \u2020] **, Pengcheng He** [4, #] **, Hai Zhao** [1,2,\u2020] **, Nan Duan** [3]\n\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\n3Microsoft Research Asia 4Microsoft Azure AI\n\nsjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,\n{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com\n\n\n\n**Abstract**\n\n\nLarge Language Models (LLMs) play powerful, black-box readers in the _retrieve-then-_\n_read_ pipeline, making remarkable progress\nin knowledge-intensive tasks. This work introduces a new framework, _Rewrite-Retrieve-_\n_Read_ instead of the previous _retrieve-then-read_\nfor the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior\nstudies focusing on adapting either the retriever\n\n  - r the reader, our approach pays attention to\nthe adaptation of the search query itself, for\nthere is inevitably a gap between the input text\nand the needed knowledge in retrieval. We\nfirst prompt an LLM to generate the query,\nthen use a web search engine to retrieve contexts. Furthermore, to better align the query\nto the frozen modules, we propose a trainable\nscheme for our pipeline. A small language\nmodel is adopted as a trainable rewriter to cater\nto the black-box LLM reader. The rewriter is\n\ntrained using the feedback of the LLM reader\nby reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA\nand multiple-choice QA. Experiments results\nshow consistent performance improvement, indicating that our framework is proven effective\nand scalable, and brings a new framework for\nretrieval-augmented LLM [1] .\n\n\n**1** **Introduction**\n\n\nLarge Language Models (LLMs) have shown remarkable abilities for human language processing\nand extraordinary scalability and adaptability in\nfew- or zero-shot settings.(Ouyang et al., 2022;\nBrown et al., 2020; Chowdh"," as the retriever, which\navoids the maintenance of the search index and\n\ncan access up-to-date knowledge (Lazaridou et al.,\n2022). Different from previous studies (Khattab\net al., 2022; Yao et al., 2023) that require the mem\n- ry of multiple interaction rounds between the retriever and the LLM for each sample, the motivation of our rewriting step is to clarify the retrieval\nneed from the input text.\n\nWe also propose a trainable scheme for our\n_rewrite-retrieve-read_ framework (Figure 1 (c)).\nThe black-box retriever and the reader form a\n\nfrozen system. To further smooth the steps of\n\n- ur pipeline, we apply a small, trainable language\nmodel to perform the rewriting step, denoted as the\n_rewriter_ . The rewriter is trained by reinforcement\nlearning using the LLM performance as a reward,\nlearning to adapt the retrieval query to improve the\nreader on downstream tasks.\n\nOur proposed methods are evaluated  - n\nknowledge-intensive downstream tasks including\n\n- pen-domain QA (HotpoQA (Yang et al., 2018),\nAmbigNQ (Min et al., 2020), PopQA (Mallen\net al., 2022)) and multiple choice QA (MMLU\n(Hendrycks et al., 2021)). The experiments are\nimplemented on T5-large (Raffel et al., 2020) as\nthe rewriter, ChatGPT (Ouyang et al., 2022) and\n\n\n\nVicuna-13B (Chiang et al., 2023) as the LLM\nreader. The results show that query rewriting consistently improves the retrieve-augmented LLM\nperformance. The results also indicate that the\nsmaller language model can be competent for query\nrewriting.\nTo sum up, our proposed novel retrievalaugmentation method, _rewrite-retrieve-read_ is the\nfirst framework where the input text is adapted for\nthe frozen retriever and LLM reader. We introduce\n\na tuneable scheme with a small, trainable model,\nachieving performance gains with less resource\nconsumption.\n\n\n**2** **Related Work**\n\n\n**2.1** **Retrieval Augmentation**\n\n\nLanguage models require external knowledge to alleviate the factuality drawbacks. Retrieval augmentation has been regarded as the standard effective\nsolution. With a retrieval module","\nimproves the retrieval-augmented LLM from the\nperspective of query rewriting. Figure 1 shows an\n\n- verview. This section first introduces the pipeline\nframework in section 3.1, then the trainable scheme\n\nin section 3.2.\n\n\n**3.1** _**Rewrite-Retrieve-Read**_\n\n\nA task with retrieval augmentation can be denoted as follows. Given a dataset of a knowledgeintensive task (e.g., open-domain QA), _D_ =\n_{_ ( _x, y_ ) _i}, i_ = 0 _,_ 1 _,_ 2 _, . . ., N_, _x_ (e.g., a question)\nis the input to the pipeline, _y_ is the expected output\n(e.g., the correct answer). Our pipeline consists of\nthree steps. (i) Query rewrite: generate a query \u02dc _x_\nfor required knowledge based on the original input\n_x_ . (ii) Retrieve: search for related context, _doc_ . (iii)\nRead: comprehend the input along with contexts\n\n[ _doc, x_ ] and predict the output \u02c6 _y_ .\nA straightforward but effective method is to ask\nan LLM to rewrite queries to search for information that is potentially needed. We use a few-shot\nprompt to encourage the LLM to think, and the\n\n- utput can be none, one or more queries to search.\n\n\n**3.2** **Trainable Scheme**\n\n\nBesides, total reliance on a frozen LLM has shown\nsome drawbacks. Reasoning errors or invalid\nsearch hinders the performance (Yao et al., 2023;\nBehnamGhader et al., 2022). On the other hand,\nretrieved knowledge may sometimes mislead and\ncompromise the language model (Mallen et al.,\n2022). To better align to the frozen modules, it is\nfeasible to add a trainable model and adapt it by\ntaking the LLM reader feedback as a reward.\nBased on our framework, we further propose to\nutilize a trainable small language model to take\n\n- ver the rewriting step, as is shown in the right\npart of Figure 1. The trainable model is initialized with the pre-trained T5-large (770M) (Raffel\net al., 2020), denoted as _trainable"," testing. This ability to maintain consistent performance despite variations in test-time document numbers further validates the robustness of\n\n- ur approach, RAFT . This finding underscores the importance of a well-calibrated training\nenvironment to prepare the model for a range of scenarios it may encounter in real-world.\n\n\n**6** **Related Works**\n\n\n**Retrieval-Augmented Language Models** Retrieval-Augmented Language Models (RALMs)\nenhance LLMs by integrating a retrieval module that sources relevant information from\nexternal knowledge bases, significantly improving performance across various NLP tasks,\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,\n2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;\nWang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,\n2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,\ntreating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the\ndecoder-only architecture to include retrieved texts and conducts pre-training from scratch.\nkNN-LM (Khandelwal et al., 2019) interpolates between the LM\u2019s next token distribution\nand distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram\net al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or\nfine-tuned retriever.\n\n\n**Memorization** A key question around large neural language models is whether they truly\n\u201cunderstand\u201d text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern\nmemorization (Carlini et al., 2019; T\u00e4nzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;\n2022) develop methodologies to quantify the extent of memorization in neural models.\n(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored"," GenRead (Yu et al., 2023)\nprompts an LLM to generate context instead of\ndeploying a retriever, showing that LLMs can retrieve internal knowledge by prompting. ReAct\n\n\n\n(Yao et al., 2023) and Self-Ask (Press et al., 2022)\ncombines the Chain-of-Thought (CoT) (Wei et al.,\n2022; Wang et al., 2022) and inter-actions with web\nAPIs. Only relying on prompt construction, ReAct provides novel baselines for interactive tasks.\nDemonstrate\u2013Search\u2013Predict (DSP) (Khattab et al.,\n2022) defines a sophisticated pipeline between an\nLLM and a retriever. Unlike ReAct, DSP integrates\nprompts for demonstration bootstrap besides multihop breakdown and retrieval.\nDespite the promising performance in the zero or\nfew-shot setting, the behavior of LLMs sometimes\nneeds adjustments. A feasible approach is to append trainable small models in front of or after the\nLLM. The small models, as a part of the parameters\n\n - f the system, can be fine-tuned for optimization.\nRePlug (Shi et al., 2023) is proposed to fine-tune a\ndense retriever for the frozen LLM in the _retrieve-_\n\n_then-read_ pipeline. The retriever is trained under\nthe LLM\u2019s supervision to retrieve documents that\nare suitable for the LLM. With the same purpose,\nDirectional Stimulus Prompting (Li et al., 2023)\ndeploys a small model to provide the LLM with\nstimulus (e.g., keywords for summarization, or dialogue actions for response generation), which is\nupdated according to the LLM reward.\nDifferent from the inspiring work mentioned\nabove, our proposed pipeline contains a query\nrewriting step in front of the _retrieve-then-read_\nmodule. We further propose a trainable scheme\nwith a small rewriting model, which is a novel\nenhancement for retrieval-augmented LLM by re\n\nconstructing the search query.\n\n\n**3** **Methodology**\n\n\nWe present _Rewrite-Retrieve-Read_, a pipeline that\nimproves the retrieval-augmented LLM from the\nperspective of query rewriting. Figure 1 shows an\n\n- verview. This section first introduces the pipeline\nframework in section 3.1, then the trainable scheme\n\n"],"24":["une and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks,\n\n     - utperforming parametric seq2seq models and task-specific retrieve-and-extract\narchitectures. For language generation tasks, we find that RAG models generate\nmore specific, diverse and factual language than a state-of-the-art parametric-only\nseq2seq baseline.\n\n\n**1** **Introduction**\n\n\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\u2019t straightforwardly provide insight into\ntheir predictions, and may produce \u201challucinations\u201d [38]. Hybrid models that combine parametric\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\ninspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\n\n\n|Col1|Col2|Col3|z4<br>z3<br>z2|Col5|Col6|\n|---|---|---|---|---|---|\n||||**z3**<br>**z2**|**z3**<br>**z2**|**z3**<br>**z2**|\n||||**z2**|**z2**|**z2**|\n|||**z**|**z**|||\n|||**1**|**1**|||\n|||||||\n\n\n\nFigure 1: Overview of our approach. We combine a pre-trained retriever ( _Query Encoder_ + _Document_\n_Index_ ) with a pre-trained seq2seq model ( _Generator_ ) and fine-tune end-to-end. For query _x_, we use\nMaximum Inner Product Search (MIPS) to find the top-K documents _zi_ . For final prediction _y_, we\ntreat _z_ as a latent variable and marginalize over seq2seq predictions given different",". These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\ncould represent promising future work.\n\n\n**6** **Discussion**\n\n\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\nfound that people prefer RAG\u2019s generation over purely parametric BART, finding RAG more factual\nand specific. We conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\nanother objective. Our work opens up new research directions on how parametric and non-parametric\nmemories interact and how to most effectively combine them, showing promise in being applied to a\nwide variety of NLP tasks.\n\n\n9\n\n\n**Broader Impact**\n\n\nThis work offers several positive societal benefits over previous work: the fact that it is more\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less\nwith generations that are more factual, and offers more control and interpretability. RAG could be\nemployed in a wide variety of scenarios with direct benefit to society, for example by endowing it\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\neffective at their jobs.\n\n\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or","x_, we use\nMaximum Inner Product Search (MIPS) to find the top-K documents _zi_ . For final prediction _y_, we\ntreat _z_ as a latent variable and marginalize over seq2seq predictions given different documents.\n\n\nbut have only explored open-domain extractive question answering. Here, we bring hybrid parametric\nand non-parametric memory to the \u201cworkhorse of NLP,\u201d i.e. sequence-to-sequence (seq2seq) models.\n\n\nWe endow pre-trained, parametric-memory generation models with a non-parametric memory through\na general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG).\nWe build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the\nnon-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural\nretriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together with\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\neither on a per-output basis (assuming the same document is responsible for all tokens) or a per-token\nbasis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG\ncan be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.\n\n\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\nmemory which are trained from scratch for specific tasks, e.g. memory networks [64, 55], stackaugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both\nparametric and non-parametric memory components are pre-trained and pre-loaded with extensive\nknowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is\npresent without additional training.\n\n\nOur results highlight the benefits of combining parametric and non-parametric memory with generation for _knowledge-intensive tasks_ - tasks that humans could not reasonably be expected to perform\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\n\n- n open Natural Questions ["," the retrieval and generation\nstages, achieving a 20\u201340% end-to-end performance gain over traditional textbased RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing\ntraining data and demonstrates strong generalization capability, positioning it as a\npromising solution for RAG on multi-modality documents. Our code and data are\n[available at https:\/\/github.com\/openbmb\/visrag.](https:\/\/github.com\/openbmb\/visrag)\n\n\n1 INTRODUCTION\n\n\nTrained on massive data, large language models (LLMs) have shown strong abilities in common\nNLP tasks using their parametric knowledge (Wei et al., 2022; Zhao et al., 2023; Achiam et al.,\n2023). However, the issue of hallucination (Ji et al., 2023; Bang et al., 2023) and the challenge of\nupdating the parametric knowledge limit their real-world application in specific domains. Retrievalaugmented generation (RAG) alleviates this problem by supplying the LLM with information retrieved from a custom outer knowledge base (Guu et al., 2020; Lewis et al., 2020; Yu et al., 2023).\nOpen-source RAG frameworks like llamaindex (Liu, 2022) have been developed to facilitate the\nresearch and deployment of RAG.\n\n\nTypical retrieval-augmented generation (RAG) pipelines are _text-based_, operating on segmented\ntexts as retrieval units (Yu et al., 2023; Asai et al., 2024; Yan et al., 2024), which we refer to as\nTextRAG. In real-world scenarios, knowledge is often presented in multi-modality documents such\nas textbooks and manuals, which may have texts and figures intersected together. To acquire texts\nfrom such data sources, a _parsing_ stage is required, which typically involves a cascade of processes,\nincluding layout recognition, optical character recognition (OCR), and post-processing steps like\ntext joining (Zhang et al., 2024; Liu, 2022). While effective in most scenarios, the parsing process\ninevitably introduces errors, which can negatively impact the retrieval and generation phases. More\n- ver, TextRAG utilizes only textual information, overlooking potential information present in other\nmodalities like images. Although research has been conducted on image retrieval and multi-modal\n\n\n_\ufffd"," and non-parametric memory with generation for _knowledge-intensive tasks_ - tasks that humans could not reasonably be expected to perform\nwithout access to an external knowledge source. Our RAG models achieve state-of-the-art results\n\n- n open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\nextractive tasks, we find that unconstrained generation outperforms previous extractive approaches.\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\ngeneration, and we find that our models generate responses that are more factual, specific, and\ndiverse than a BART baseline. For FEVER [56] fact verification, we achieve results within 4.3% of\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\nthe non-parametric memory can be replaced to update the models\u2019 knowledge as the world changes. [1]\n\n\n**2** **Methods**\n\n\nWe explore RAG models, which use the input sequence _x_ to retrieve text documents _z_ and use them\nas additional context when generating the target sequence _y_ . As shown in Figure 1, our models\nleverage two components: (i) a retriever _p\u03b7_ ( _z|x_ ) with parameters _\u03b7_ that returns (top-K truncated)\ndistributions over text passages given a query _x_ and (ii) a generator _p\u03b8_ ( _yi|x, z, y_ 1: _i\u2212_ 1) parametrized\n\n\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at `[https:\/\/github.com\/huggingface\/transformers\/blob\/master\/](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/rag\/)`\n`[examples\/rag\/](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/examples\/rag\/)` . An interactive demo of RAG models can be found at `[https:\/\/huggingface.co\/rag\/](https:\/\/huggingface.co\/rag\/)`\n\n\n2\n\n\nby _\u03b8_ that generates a current token based on a context of the previous _i \u2212_ 1 tokens _y_ 1: _i"],"25":["## **A Survey on Retrieval-Augmented Text Generation**\n\n**Huayang Li** _[\u2665][,][\u2217]_ **Yixuan Su** _[\u2660][,][\u2217]_ **Deng Cai** _[\u2666][,][\u2217]_ **Yan Wang** _[\u2663][,][\u2217]_ **Lemao Liu** _[\u2663][,][\u2217]_\n\n_\u2665_ Nara Institute of Science and Technology _\u2660_ University of Cambridge\n_\u2666_ The Chinese University of Hong Kong _\u2663_ Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\n\n\n\n**Abstract**\n\n\nRecently, retrieval-augmented text generation\nattracted increasing attention of the computational linguistics community. Compared\nwith conventional generation models, retrievalaugmented text generation has remarkable advantages and particularly has achieved state-ofthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrievalaugmented text generation. It firstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks including\ndialogue response generation, machine translation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval technology, has achieved state-of-the-art (SOTA) performance in many NLP tasks and attracted the attention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counterpart, this new paradigm has some remarkable advantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation"," from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (Weston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n\n\n_\u2217_ All authors contributed equally.\n\n\n\nfirstly present the generic paradigm of retrievalaugmented generation as well as three key components under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are organized with respect to different tasks. Specifically,\n\n- n the dialogue response generation task, exemplar\/template retrieval as an intermediate step has\nbeen shown beneficial to informative response generation (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation exploring different forms of knowledge such as knowledge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summarize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we intensively highlight several popular methods to integrating translation memory to NMT models (Gu\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\nHe et al., 2021). We also review the applications\n\n- f retrieval-augmented generation in other generation tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b)."," for example,\ndata compression for the retrieval memory.\n\n\n**Local vs. Global Optimization** Theoretically, it\nseems promising to jointly learn retrieval metrics\nand generation models. However, in practice, there\nis an essential gap about the retrieval metric between the training and inference phrases. In the\ntraining phase, the loss is locally back-propagated\nto only a few retrieved examples while in the inference phase the metric is globally conducted among\nall examples in the memory. It would be interesting\nto narrow such a gap when learning a better metric\nfor generation tasks.\n\n\n**Multi-Modalities** With recent advancement in\n\nimage-text retrieval, directly associating images\nwith relevant text becomes possible. This urges\nresearchers to investigate the possibility of retrievalbased text generation in tasks that involve data from\ndifferent modalities. One typical task is image\ncaptioning. Beyond images, other tasks like speechto-text transcription could potentially benefit from\nretrieval-based generation methods as well.\n\n\n**Diverse & Controllable Retrieval** Most of the\n\nexisting approaches adopt a universal metric for\nretrieval, such as lexical similarities of sentences.\nFuture work should explore how to use customized\nmetrics for retrieval. This can be beneficial for\nmore controlled text generation. For example, instances with emotions and styles may be more desirable in the personalized dialogue generation, parallel data that contains specific terminologies is\nmore helpful in machine translation, and so on. On\nthe other hand, using a universal metric for retrieval\nmay lead to the lack of diversity of the retrieval results. Collecting a diverse set of retrieval results\ncan improve the coverage of useful information.\nThus, considering multiple different metrics for retrieval may lead to generation with higher quality\nin the future.\n\n\n**7** **Conclusion**\n\n\nIn this paper, we surveyed recent approaches for\nretrieval-augmented text generation. We reviewed\nand summarized the development of different components of retrieval-augmented text generation including retrieval metrics, retrieval sources, and integration paradigms. We gave in-depth discussions\nwhen retrieval-augmented text generation comes to\ndifferent applications including dialogue response\ngeneration, machine translation, and other generation tasks. We also pointed out some future directions for retrieval-augmented text generation.\n\n\n**References**\n\n\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly\nlearning to align and translate. _"," generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n\n\n**2** **Retrieval-Augmented Paradigm**\n\n\nIn this section, we first give a general formulation\n\n- f retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrievalaugmented generation paradigm, including the re\n\nOutput\n\n\n\n\n\nFigure 1: The overview of this survey.\n\n\n\ntrieval source, retrieval metric and integration meth\n- ds.\n\n\n**2.1** **Formulation**\n\n\nMost text generation tasks can be formulated as a\nmapping from input sequence _**x**_ to output sequence\n_**y**_ : _**y**_ = _f_ ( _**x**_ ). For instance, _**x**_ and _**y**_ could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\n\n\n_**y**_ = _f_ ( _**x**_ _,_ _**z**_ ) (1)\n\n\nwhere _**z**_ = _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that _**y**_ _[r]_\n\nmay benefit the response generation, if _**x**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r"," candidate texts,\nand extract a set of more fine-grained prototypes\nfrom the candidates. Lastly, a generation model\ntakes the prototypes as input to produce the text\nthat describes the given structured data.\n\nWhile retrieval-augmented generation has been\nwidely explored in the NLP community, we suggest that future research could extend this approach\nto tasks that involve data from multiple modalities. For instance, with recent advancements in\nimage-text retrieval (Jia et al., 2021; Radford et al.,\n2021), the structural gap between images and texts\nis largely bridged. Some early studies (Zhang et al.,\n2020) have shown that information retrieved from\nimages could improve the performance of neural\nmachine translation model. Naturally, such meth\n- ds could be extended to other multi-modal tasks,\nsuch as image captioning (Karpathy and Li, 2015).\nA similar idea could also be applied to tasks beyond images, such as speech-to-text transcription\n(Gales and Young, 2007).\n\n\n**6** **Future Directions**\n\n\nDespite the current success of retrieval augmented\ntext generation, there is still a long way to go as\ndiscussed in previous sections. We highlight some\ndirections to facilitate the future research as fol\nlows:\n\n\n**Retrieval Sensitivity** The performance of retrieval augmented text generation is very sensitive\nto the retrieval quality, i.e., the similarity between\nthe query and the retrieved examples. Currently, retrieval augmented text generation models perform\nwell when the retrieved examples are very similar to the query. However, they are even worse\nthan the generation models without retrieval when\nthe retrieval examples are less similar. Therefore,\nit would be important to exploit new methods to\naddress such an issue on similarity.\n\n\n\n**Retrieval Efficiency** Generally, if one enlarges\nthe retrieval memory to some extent, it would be\npossible to retrieve an example which is very similar to the query.Unfortunately, the downside is that\nthe overall inference for the retrieval augmented\ngeneration models is less efficient due the considerable retrieval overhead. In this sense, it is urgent\nto consider some methods to trade off the retrieval\n\nmemory size and retrieval efficiency, for example,\ndata compression for the retrieval memory.\n\n\n**Local vs. Global Optimization** Theoretically, it\nseems promising to jointly learn retrieval metrics\nand generation models. However, in practice, there\nis an essential gap about the retrieval"],"26":["fect. Further analysis reveals that VisRAG possesses\nbetter training data efficiency and generalization ability than baseline models, and demonstrates robustness across both text-centric and vision-centric documents. VisRAG shows great promise in\nreplacing TextRAG as the next-generation standard for RAG pipelines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: TextRAG vs. VisRAG on final generation accuracy. In TextRAG, parsed text\nserves as the basis for both retrieval and generation processes. In contrast, VisRAG leverages the original document image directly by\nusing a VLM-based retriever and generator.\nDetails can be found in Sec. 5.1.\n\n\n\n2 RELATED WORK\n\n\n**Retrieval-augmented Generation (RAG).** RAG enhances large language models (LLMs) by\nincorporating retrieved information from external knowledge bases, which assists in addressing\nknowledge-intensive tasks (Guu et al., 2020), reducing hallucinations (Semnani et al., 2023), and\n\n\n2\n\n\nPublished as a conference paper at ICLR 2025\n\n\nacquiring new knowledge (Vu et al., 2023). An RAG pipeline typically comprises a text-based\nretriever that fetches relevant information from the knowledge base given the user query, and an\nLLM-based generator that reads the query along with the retrieved information to generate an answer (Shi et al., 2024b; Yu et al., 2023). Prior research on RAG primarily focuses on: a) improving\nthe retriever, which is typically a text encoder producing text embeddings, through generator feedback (Yu et al., 2023; Shi et al., 2024b); b) enhancing the generator via supervised fine-tuning (Lin\net al., 2024; Xu et al., 2024a), in-context pre-training (Shi et al., 2024a), or advanced prompting (Xu\net al., 2024c); and c) developing advanced RAG pipelines to handle long-form or multi-hop question answering (Jiang et al., 2023; Asai et al., 2024). However, research on RAG has predominantly\ntargeted cleaned text corpora like Wikipedia from an academic standpoint. Building effective RAG\npipelines for real-world, multi-modal documents remains a challenge.\n\n\n**Vision-language Models.** Recent advancements in vision-language models (VLMs"," pri\n- ritize textual information over visual content. For each group, we calculate and plot the average\nperformance differences between VisRAG and TextRAG, as well as between VisRAG (SigLIP) and\nTextRAG, to compare how each model performs relative to TextRAG. We observe that, in general,\nthe relative performance of VisRAG and VisRAG (SigLIP) improves as the length of the relevant\ndocument decreases. This suggests that models with vision encoders can better understand documents that emphasize visual information. However, VisRAG (SigLIP) consistently underperforms\nVisRAG across all data subsets and, in some cases, even performs worse than TextRAG. In contrast,\nVisRAG outperforms TextRAG on most subsets, indicating that the underlying language model in\nVisRAG is crucial for better understanding the semantics conveyed through visual cues.\n\n\n6 CONCLUSION\n\n\nIn this paper, we propose VisRAG, a novel retrieval-augmented generation (RAG) paradigm that utilizes vision-language models (VLMs) to facilitate retrieval and generation within an RAG pipeline,\nthereby eliminating the parsing stage required in traditional text-based RAG. Our empirical results demonstrate that VisRAG consistently outperforms text-based RAG on retrieval and generation\nwhile maintaining a simpler pipeline. We hope that VisRAG will inspire future RAG development\nto incorporate VLMs for handling multi-modal documents.\n\n\n10\n\n\nPublished as a conference paper at ICLR 2025\n\n\nACKNOWLEDGMENTS\n\n\nThis work is supported by the Institute Guo Qiang at Tsinghua University. It is also partially supported by the National Natural Science Foundation of China under Grant No. 62206042.\n\n\nREFERENCES\n\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical\nreport. _arXiv preprint arXiv:2303.08774_, 2023.\n\n\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Fl","LM\nfine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth,\nyet it has not been accompanied by a systematic synthesis that\ncould clarify its broader trajectory. This survey endeavors to\nfill this gap by mapping out the RAG process and charting\nits evolution and anticipated future paths, with a focus on the\nintegration of RAG within LLMs. This paper considers both\ntechnical paradigms and research methods, summarizing three\nmain research paradigms from over 100 RAG studies, and\nanalyzing key technologies in the core stages of \u201cRetrieval,\u201d\n\u201cGeneration,\u201d and \u201cAugmentation.\u201d On the other hand, current\nresearch tends to focus more on methods, lacking analysis and\nsummarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks,\nand evaluation methods applicable to RAG. Overall, this\npaper sets out to meticulously compile and categorize the\nfoundational technical concepts, historical progression, and\nthe spectrum of RAG methodologies and applications that\nhave emerged post-LLMs. It is designed to equip readers and\nprofessionals with a detailed and structured understanding of\nboth large models and RAG. It aims to illuminate the evolution\n\n- f retrieval augmentation techniques, assess the strengths and\nweaknesses of various approaches in their respective contexts,\nand speculate on upcoming trends and innovations.\nOur contributions are as follows:\n\n\n_\u2022_ In this survey, we present a thorough and systematic\nreview of the state-of-the-art RAG methods, delineating\nits evolution through paradigms including naive RAG,\n\n\n2\n\n\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\n\n\n\nadvanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRet",", which can negatively impact the retrieval and generation phases. More\n- ver, TextRAG utilizes only textual information, overlooking potential information present in other\nmodalities like images. Although research has been conducted on image retrieval and multi-modal\n\n\n_\u2217_ Equal contribution.\n\n_\u2020_ Corresponding authors.\n\n\n1\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\nRAG, these approaches primarily focus on predefined scenarios wherein images and descriptive\ntexts are properly extracted and paired (Wei et al., 2023; Sharifymoghaddam et al., 2024; Zhou\net al., 2024), differing from real-world scenarios where texts and images (including figures) are\n\n- ften interleaved within a single document page.\n\n\nThe recent development of vision-language models (VLMs) has introduced a promising approach\nto understanding complex visual cues in images and documents (OpenBMB, 2024b; Wang et al.,\n2024). By integrating a language model with a vision encoder, VLMs demonstrate superior abilities in applications such as describing pictures (Alayrac et al., 2022), explaining figures (Bavishi\net al., 2023), and transcribing (printed and handwritten) text from document images (Laurenc\u00b8on\net al., 2024). Given the robust capabilities of VLMs in capturing multi-modal information present in\nimages, an intriguing question arises: can the basic language model in the retrieval and generation\ncomponents of TextRAG be substituted with a VLM, thus the parsing stage is bypassed and all the\ninformation of the document is preserved?\n\n\nIn this paper, we present **Vis** ion-based **R** etrieval- **a** ugmented **G** eneration (VisRAG), to study the feasibility of building a pure-vision RAG pipeline using VLMs. VisRAG is built with a VLM-based\nretriever VisRAG-Ret and generator VisRAG-Gen. Inherited the bi-encoder of text-based dense retriever (Karpukhin et al., 2020), VisRAG-Ret maps the query and the document into an embedding\nspace, but utilizing the document\u2019s image directly instead of relying on extracted textual content.\nThe embedding is obtained by applying weighted mean pooling on the final hidden states of the input text or vision tokens. After retrieving top- _k_","multi-modal documents, requiring an additional parsing step to obtain text. In this paper, we propose\nto use the _page_ as the fundamental unit for retrieval and generation, which is directly processed\nby vision language models (VLMs) as an image without further processing during retrieval and\ngeneration. In subsequent sections, we use the terms \u201cpage\u201d and \u201cdocument\u201d interchangeably.\n\n\n3.2 VISRAG: VISION-BASED RETRIEVAL-AUGMENTED GENERATION\n\n\nIn this section, we present **Vis** ion-based **R** etrieval- **a** ugmented **G** eneration (VisRAG), as shown in\nFigure 2 (right). In contrast to traditional RAG frameworks which use text segments for both retrieval and generation, VisRAG leverages the image of the document to preserve all information.\n\n\n3.2.1 RETRIEVAL\n\n\nThe first stage of VisRAG, VisRAG-Ret, aims to retrieve a set of pages from the corpus _D_ given\n_q_ . We follow the dual-encoder paradigm in text-based dense retrieval models (Karpukhin et al.,\n2020) but employ a VLM rather than an LLM to encode the query and page. Specifically, the query\nand page are encoded separately as text and image in the VLM, producing in a sequence of hidden\nstates. To derive the final embedding, and given that we use generative VLMs with causual attention,\nwe adopt the position-weighted mean pooling over the last-layer VLM hidden states (Muennighoff,\n2022), giving higher weights to later tokens:\n\n\n\n**v** =\n\n\n\n_S_\n\n- _wi_ **h** _i,_ (1)\n\n\n_i_ =1\n\n\n\nwhere **h** _i_ is the _i_ - th hidden state, _S_ is the sequence length, _wi_ = ~~\ufffd~~ _Sji_ =1 _[j]_ [is the] _[ i]_ [-th weight, and] **[ v]** [ is]\n\nthe query or page embedding. The similarity score is calculated by the cosine similarity of the query\n\n\n4\n\n\nPublished as a conference paper at ICLR 2025\n\n\nand page embedding. VisRAG-Ret is optimized using the InfoNCE loss:\n\n\nexp( _s_ ( _q, d_ [+] ) _\/\u03c4"],"27":[" mechanism (roughly sum for\nSparTerm [1], max for EPIC [16] and SPARTA [29]), that allows to\ncompute an importance weight for each term of the vocabulary, for\nthe full document or query. However, EPIC and SPARTA (document)\nrepresentations are not sparse enough by construction \u2013 unless\nresorting on top- _\ud835\udc58_ pooling \u2013 contrary to SparTerm, for which fast\nretrieval is thus possible. Furthermore, the latter does not include\n(like SNRM) an _explicit_ sparsity regularization, which hinders its\nperformance. Our SPLADE model relies on such regularization, as\nwell as other key changes, that boost both the efficiency and the\neffectiveness of this type of models.\n\n\n**3** **SPARSE LEXICAL REPRESENTATIONS FOR**\n\n**FIRST-STAGE RANKING**\n\n\nIn this section, we first describe in details the SparTerm model [1],\nbefore presenting our model named SPLADE.\n\n\n**3.1** **SparTerm**\n\n\nSparTerm predicts term importance \u2013 in BERT WordPiece vocabulary (| _\ud835\udc49_ | = 30522) \u2013 based on the logits of the Masked Language Model (MLM) layer. More precisely, let us consider an input\nquery or document sequence (after WordPiece tokenization) _\ud835\udc61_ =\n( _\ud835\udc61_ 1 _,\ud835\udc61_ 2 _, ...,\ud835\udc61\ud835\udc41_ ), and its corresponding BERT embeddings ( _\u210e_ 1 _,\u210e_ 2 _, ...,\u210e\ud835\udc41_ ).\nWe consider the importance _\ud835\udc64\ud835\udc56\ud835\udc57_ - f the token _\ud835\udc57_ (vocabulary) for a\ntoken _\ud835\udc56_ (of the input sequence):\n\n\n_\ud835\udc64\ud835\udc56\ud835\udc57_ = transform( _\u210e\ud835\udc56_ ) _[\ud835\udc47]_ _\ud835\udc38_ _\ud835\udc57_ + _\ud835\udc4f_ _\ud835\udc57_ _\ud835\udc57_ \u2208{1 _, ...,_ | _\ud835\udc49_ |} (1)\n\n\n\nwhere _\ud835\udc38_ _\ud835\udc57_ denotes the BERT input embedding for token _\ud835\udc57_, _\ud835\udc4f_ _\ud835\udc57_ is a\ntoken-level bias, and transform("," with approximate nearest neighbors search\nhas shown impressive results [8, 15, 26], but is still combined with\nBOW models because of its inability to explicitly model term matching. Hence, there has recently been a growing interest in learning\n_sparse representations_ for queries and documents [1, 4, 19, 28, 29].\nBy doing so, models can inherit from the desirable properties of\nBOW models like exact-match of (possibly latent) terms, efficiency\n\n- f inverted indexes and interpretability. Additionally, by modeling\nimplicit or explicit (latent, contextualized) _expansion_ mechanisms \u2013\nsimilarly to standard expansion models in IR \u2013 these models can\nreduce the vocabulary mismatch.\nThe contributions of this paper are threefold: (1) we build upon\nSparTerm [1], and show that a mild tuning of hyperparameters\nbrings improvements that largely outperform the results reported\nin the original paper; (2) we propose the SParse Lexical AnD Expansion (SPLADE) model, based on a logarithmic activation and\nsparse regularization. SPLADE performs an efficient document expansion [1, 16], with competitive results with respect to complex\ntraining pipelines for dense models like ANCE [26]; (3) finally, we\nshow how the sparsity regularization can be controlled to influence the trade-off between efficiency (in terms of the number of\nfloating-point operations) and effectiveness.\n\n\n**2** **RELATED WORKS**\n\n\nDense retrieval based on BERT Siamese models [22] has become\nthe standard approach for candidate generation in Question Answering and IR [8, 10, 12, 15, 25]. While the backbone of these models remains the same, recent works highlight the critical aspects\n\n- f the training strategy to obtain state-of-the-art results, ranging\nfrom improved negative sampling [8, 25] to distillation [11, 15].\nColBERT [13] pushes things further: the postponed token-level\ninteractions allow to efficiently apply the model for first-stage retrieval, benefiting of the effectiveness of modeling fine-grained\ninteractions, at the cost of storing embeddings for each (sub)term\n\n- raising concerns about the actual scalability of the approach for\nlarge collections. To the best of our knowledge, very few studies\nhave discussed the impact of using _approximate_ nearest neighbors\n\n\n(ANN) search on IR metrics [","ing by emphasizing on _important_ terms and discarding\nmost of the terms without information content. Expansion allows\nto enrich documents, either by implicitly adding stemming effects\n(legs \u2192 leg) or by adding relevant topic words (e.g. treatment).\n\n\n**5** **CONCLUSION**\n\n\nRecently, dense retrieval based on BERT has demonstrated its superiority for first-stage retrieval, questioning the competitiveness of\ntraditional sparse models. In this work, we have proposed SPLADE,\na sparse model revisiting query\/document expansion. Our approach\nrelies on in-batch negatives, logarithmic activation and FLOPS regularization to learn effective and efficient sparse representations.\nSPLADE is an appealing candidate for initial retrieval: it rivals the\nlatest state-of-the-art dense retrieval models, its training procedure\nis straightforward, its sparsity\/FLOPS can be controlled explicitly\nthrough the regularization, and it can operate on inverted indexes.\nIn reason of its simplicity, SPLADE is a solid basis for further improvements in this line of research.\n\n\n\n33\n\n\n32\n\n\n31\n\n\n30\n\n\n29\n\n\n28\n\n\n27\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||a<br>a|a<br>a|vg q len <br>vg d len|_ \u2243_15<br>_ \u2243_58||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||y-T5<br>|y-T5<br>|\n||||||||||||doc2quer<br>|y-T5<br>|y-T5<br>|\n||||~~a~~<br>a|~~a~~<br>a|~~vg q len~~ <br>vg d len|~~_ \u2243_6~~<br>_ \u2243_18||<br> <br>|||~~SparTerm~~<br>ST exp-_\u2113F_<br>SPLADE-|~~ lexical~~<br>_LOPS_<br>_\u2113_~~1~~||\n||||~~a~~<br>a|~~a~~<br>a||||||||||\n||||||||||||SPLADE-|_\u2113FLOPS_|_\u2113FLO","_the bone alignment issue than you may be able to correct through bow legs_\n\n_correction exercises._\n\n\n**expansion terms**\n\n\n(leg, 1.62) (arrow, 0.7) (exercise, 0.64) (bones, 0.63) (problem, 0.41) (treatment,\n0.35) (happen, 0.29) (create, 0.22) (can, 0.14) (worse, 0.14) (effect, 0.08) (teeth,\n0.06) (remove, 0.03)\n\n\nSPLADE in terms of efficiency. In the meantime, SPLADE reaches efficiency levels equivalent to sparse BOW models, while outperforming doc2query-T5. Interestingly, strongly regularized models still\nshow competitive performance (e.g. FLOPS=0 _._ 05 _,_ MRR@10=0 _._ 296).\nFinally, the regularization effect brought by _\u2113_ FLOPS compared to _\u2113_ 1\nis clear: for the same level of efficiency, performance of the latter is\nalways lower.\n\n\n**The role of expansion.** Experiments show that the expansion\nbrings improvements w.r.t. to the purely lexical approach by increasing recall. Additionally, representations obtained from expansionregularized models are sparser: the models learn how to balance\nexpansion and compression, by both turning-off irrelevant dimensions and activating useful ones. On a set of 10k documents, the\nSPLADE- _\u2113_ FLOPS from Table 1 drops in average 20 terms per document, while adding 32 expansion terms. For one of our most efficient\nmodel (FLOPS=0 _._ 05), 34 terms are dropped in average, for only 5\nnew expansion terms. In this case, representations are extremely\nsparse: documents and queries contain in average 18 and 6 non-zero\nvalues respectively, and we need less that 1 _._ 4 GB to store the index\n\n- n disk. Table 2 shows an example where the model performs term\nre-weighting by emphasizing on _important_ terms and discarding\nmost of the terms without information content. Expansion allows\nto enrich documents, either by implicitly adding stemming effects\n(legs \u2192 leg) or by adding relevant topic words (e.g. treatment","671 0.813 2.83\nSPLADE- _\u2113_ 1 0.322 0.954 0.667 0.792 0.88\n\nSPLADE- _\u2113_ FLOPS 0.322 0.955 0.665 0.813 0.73\n\n\n- f the FLOPS regularization over _\u2113_ 1 in order to decrease the computing cost. Note that in contrast to SparTerm, SPLADE is trained\nend-to end in a single step. It is also remarkably simple, compared\nto dense state-of-the-art baselines such as ANCE [25], and avoids\nresorting to approximate neighbors search, whose impact on IR\nmetrics has not been fully evaluated yet.\n\n\n**Effectiveness-efficiency trade-off.** Figure 1 illustrates the trade\n- ff between effectiveness (MRR@10) and efficiency (FLOPS), when\nwe vary _\ud835\udf06\ud835\udc5e_ and _\ud835\udf06\ud835\udc51_ (varying both implies that plots are not smooth).\nWe observe that ST exp- _\u2113_ FLOPS falls far behind BOW models and\n\n\n**Figure 1: Performance vs FLOPS for SPLADE models trained**\n**with different regularization strength** _\ud835\udf06_ **on MS MARCO**\n\n\n\n**Table 2: Document and expansion terms: between parenthesis**\n**is the weight associated with the term \u2013 omitted for the second oc-**\n**currence of the term in the document, and strike-through for zeros**\n\n\n**original document (doc ID: 7131647)**\n\n\nif (1.2) bow (2.56) legs (1.18) ~~is~~ caused (1.29) by (0.47) ~~the~~ bone (1.2) alignment\n(1.88) issue (0.87) ~~than you may be~~ able (0.29) ~~to~~ correct (1.37) through (0.43)\n_bow legs_ correction (1.05) ~~exercises. read more here..~~ _if bow legs is caused by_\n_the bone alignment issue than you may be able to correct through bow legs_\n\n_correction exercises._\n\n\n**expansion terms**\n\n\n(leg, 1.62) (arrow, 0.7) (exercise, 0.64) ("],"28":["## **ARES: An Automated Evaluation Framework for Retrieval-Augmented** **Generation Systems**\n\n\n\n**Jon Saad-Falcon**\nStanford University _[\u2217]_\n\njonsaadfalcon@stanford.edu\n\n\n**Christopher Potts**\nStanford University\ncgpotts@stanford.edu\n\n\n**Abstract**\n\n\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an _Automated RAG Evaluation_\n_System_, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By creating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and\/or documents used in the evaluated RAG systems. We\nmake our code and datasets publicly available\n\n[on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to generate a response. This formulation admits a multitude\n\n- f choices: what retrieval model to use, how to di\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab"," evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges). We compare against RAGAS (version 0.0.18) and a baseline few-shot prompted GPT3.5 judge ( _gpt-3.5-turbo-16k_ ). For the few-shot\nGPT-3.5 judge, we provide few-shot examples for\nguiding predictions; the prompts are included in\nAppendices A.2, A.3, and A.4. For both ARES\nand the GPT-3.5 judge baseline, we augment the\nLLM with PPI, using a 300-datapoint human preference validation set to rectify the ML predictions\nand produce confidence intervals.\n\n\n\nAcross almost all settings across the datasets\nfrom KILT and SuperGLUE, ARES provides a\nmore accurate ranking of RAG systems than RAGAS. ARES averages a Kendall\u2019s _\u03c4 0.065 higher_\n_for context relevance_ and _0.132 higher for answer_\n_relevance than RAGAS_ . Additionally, the LLMjudge is substantially more accurate than RAGAS\nat predicting context relevance and answer relevance of a query-passage-answer triple. For context relevance, ARES with a fine-tuned LLM-judge\nis _59.9 percentage points higher than RAGAS_ while\nfor answer relevance, our system is _14.4 percent-_\n_age points higher than RAGAS_ . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompting and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall\u2019s _\u03c4_ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall\u2019s _\u03c4_ for ARES is\n0.08 higher"," 5: **ARES Ranking on Real-World RAG Systems** : For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall\u2019s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\n\n\n**ARES Cross-Domain Ranking of Pseudo RAG Systems**\n\n\nNQ to FEVER to NQ to MultiRC to NQ to ReCoRD to\nFEVER NQ MultiRC NQ ReCoRD NQ\n\n\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\n\n\nKendall\u2019s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\n\n\nKendall\u2019s Tau of\n0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nIn-Domain LLM Judge\n\n\nAverage PPI Range 8.7% 7.2%"," (is the response generated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\n\n\nRAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\n\nMany existing RAG evaluation frameworks require substantial human annotations for scoring.\nARES significantly improves data efficiency during evaluation by only requiring three inputs: an indomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data generation.\n\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question\u2013answer\npairs, derived from the passages in the corpus. Sec\n- nd, it defines three separate judge models to perform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores\nthe different RAG systems being assessed using\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals\nfor RAG scoring. PPI utilizes a small set of human\nannotated datapoints for computing its confidence\nintervals; we designate this annotated set as our _hu-_\n_man preference validation set_, which is composed\n\n- f approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an\nswer relevance.\n\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating existing automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occurrences in the AIS attribution dataset (Rashkin et al.,\n2022), predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach.","LM judges\n\n- n synthetically generated queries and answers.\nARES can evaluate each component of a RAG system separately to help improve system understanding and create targeted solutions, and it requires\n\n- nly minimal human annotations. For the eight different datasets in KILT, SuperGLUE, and AIS requiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\n\n- n context relevance, answer faithfulness, and answer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\n\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing\n\n\nlogits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n\n\n**7** **Limitations**\n\n\nARES relies on a small set of annotations in the\n\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\n\n- ften require an annotator familiar with the RAG\nsystem\u2019s domain application. While these annotations can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our evaluation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\n\n- ther languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\n\n\n**References**\n\n\nMubashara Ak"],"29":["## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [","## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar","Published as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto finetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved significant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\n\n- f questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model\u2019s hidden states. However, their method requires an initial round\n\n- f TF-IDF\/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\n","## **Leveraging Passage Retrieval with Generative Models** **for Open Domain Question Answering**\n\n**Gautier Izacard** [1] _[,]_ [2] _[,]_ [3] **Edouard Grave** [1]\n\n1 Facebook AI Research, Paris\n2 ENS, PSL University, Paris\n3 Inria, Paris\ngizacard|egrave@fb.com\n\n\n**Abstract**\n\n\n\nGenerative models for open domain question\nanswering have proven to be competitive, with\n  - ut resorting to external knowledge. While\npromising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we\ninvestigate how much these models can benefit from retrieving text passages, potentially\ncontaining evidence. We obtain state-of-theart results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that\nsequence-to-sequence models offers a flexible\nframework to efficiently aggregate and combine evidence from multiple passages.\n\n\n**1** **Introduction**\n\n\n\n\n\n\n\n\n\n\n\nRecently, several works have shown that factual\ninformation can be extracted from large scale\nlanguage models trained on vast quantities of\ndata (Radford et al., 2019; Petroni et al., 2019;\nJiang et al., 2019; Talmor et al., 2019). Building\n\n- n that observation and the advances in pretraining of natural language processing models, Roberts\net al. (2020) introduced a generative model for open\ndomain question answering. Without relying on\nexternal knowledge, this method obtained competitive results on several benchmarks. However, it\nrequires models containing billions of parameters,\nsince all the information needs to be stored in the\n\nweights. This makes models expensive to query\nand train. In this paper, we investigate how much\nthis method could benefit from having access to an\nexternal source of knowledge, such as Wikipedia.\nRetrieval based approaches were previously considered in the context of open domain question\nanswering with extractive models (Chen et al.,\n2017). In that case, systems start by retrieving\n\n\n\nFigure 1: A simple approach to open domain question\nanswering. First, it retrieves support text passages from\nan external source of knowledge such as Wikipedia.\nThen, a generative"," shown effective in passage\n\n- r dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates\nthe feasibility of full dense retrieval in IR tasks.\nInstead of employing the dual-encoder framework,\nthey introduced a late-interaction operator on top\n\n- f the BERT encoders.\n\n\nDense retrieval for open-domain QA has been\nexplored by Das et al. (2019), who propose to retrieve relevant passages iteratively using reformulated question vectors. As an alternative approach\nthat skips passage retrieval, Seo et al. (2019) propose to encode candidate answer phrases as vectors\nand directly retrieve the answers to the input questions efficiently. Using additional pretraining with\nthe objective that matches surrogates of questions\nand relevant passages, Lee et al. (2019) jointly train\nthe question encoder and reader. Their approach\n\n- utperforms the BM25 plus reader paradigm on\nmultiple open-domain QA datasets in QA accuracy,\nand is further extended by REALM (Guu et al.,\n2020), which includes tuning the passage encoder\nasynchronously by re-indexing the passages during training. The pretraining objective has also\nrecently been improved by Xiong et al. (2020b).\nIn contrast, our model provides a simple and yet\n\n\n\neffective solution that shows stronger empirical performance, without relying on additional pretraining\n\n- r complex joint training schemes.\nDPR has also been used as an important module in very recent work. For instance, extending\nthe idea of leveraging hard negatives, Xiong et al.\n(2020a) use the retrieval model trained in the previous iteration to discover new negatives and construct a different set of examples in each training\niteration. Starting from our trained DPR model,\nthey show that the retrieval performance can be\nfurther improved. Recent work (Izacard and Grave,\n2020; Lewis et al., 2020b) have also shown that\nDPR can be combined with generation models\nsuch as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2019), achieving good performance on\n\n- pen-domain QA and other knowledge-intensive\ntasks.\n\n\n**8** **Conclusion**\n\n\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the"],"30":[" we expect the encoder\u2019s dense bottleneck\nto serve a lossy compressor, where the extra\n(hallucinated) details are filtered out from the\nembedding. We use this vector to search against\nthe corpus embeddings. The most similar _real_\ndocuments are retrieved and returned. The retrieval\n\nleverages document-document similarity encoded\nin the inner-product during contrastive training.\nNote that, interestingly, with HyDE factorization,\nthe query-document similarity score is no longer\nexplicitly modeled nor computed. Instead, the\nretrieval task is cast into two NLU and NLG tasks.\n\nHyDE appears unsupervised. **No** model is trained\nin HyDE: both the generative model and the contrastive encoder remain intact. Supervision signals\nwere only involved in instruction learning of our\nbackbone LLM.\n\nIn our experiments, we show HyDE using InstructGPT (Ouyang et al., 2022) and Contriever (Izacard\net al., 2021) as backbone models significantly outperforms the previous state-of-the-art Contriever\n- nly zero-shot no-relevance system on 11 queries\n\n\n\nsets, covering tasks like Web Search, Question\nAnswering, Fact Verification and languages like\nSwahili, Korean, Japanese.\n\n\n**2** **Related Works**\n\n\n**Dense Retrieval** (Lee et al., 2019; Karpukhin\net al., 2020) has been extensively studied after the\nemergence of pre-trained Transformer language\nmodels (Devlin et al., 2019). Researchers studied the metric learning problems, such as training\nloss (Karpukhin et al., 2020) and negative sampling (Xiong et al., 2021; Qu et al., 2021), and also\nintroduced distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofst\u00e4tter et al., 2021). Later works studied\nthe second stage pre-training of language model\nspecifically for retrieval (Izacard et al., 2021; Gao\nand Callan, 2021; Lu et al., 2021; Gao and Callan,\n2022; Liu and Shao, 2022).\nThe popularity of dense retrieval can be partially\nattributed to the rich and successful research in very\nefficient minimum inner","triever [FT]\n\nw\/ Flan-T5 (11b) 60.2 62.1\nw\/ Cohere (52b) 61.4 63.1\nw\/ GPT (175b) **67.4** **63.5**\n\n\nTable 4: NDCG@10 on TREC DL19\/20. Effect\n\n- f changing different instruction LMs and using finetuned encoder. Best w\/o relevance and overall models\n\nare marked **bold** .\n\n\nmodels bring improvement to the unsupervised\nContriever, with larger models bringing larger\nimprovements. At the time when this paper is\nwritten, the Cohere model is still experimental\nwithout much detail disclosed. We can only\ntentatively hypothesize that training techniques\nmay have also played some role in the performance\ndifference.\n\n\n**5.2** **HyDE with Fine-tuned Encoder**\n\n\nTo begin with, HyDE with fine-tuned encoder is\n_not_ the intended usage: HyDE is more powerful\nand irreplaceable when few relevance labels are\npresent. Here we are interested to find out if\nand how HyDE embedding can affect fine-tuned encoders. In Table 4, we see that less powerful instruction LMs can negatively impact the overall performance of the fine-tuned retriever. (To remind our\nreaders, Contriever [FT] is in-domain supervisedly\nfine-tuned for TREC DL19\/20). The performance\ndegradations remain small. On the other hand, we\nalso observe the InstructGPT model able to fur\nther bring up the performance, especially on DL19.\nThis suggests that there may still exist certain factors not captured by the fine-tuned encoder but only\nby the generative model.\n\n\n**6** **Conclusion**\n\n\nAt the end of the paper, we encourage the readers\nto take a moment and reflect on the HyDE model.\nCompare it to some of the other recently seen retrievers or re-ranker. These other models probably\ndiffer in their architecture, training method, and\/or\ntask, but probably all of them involve modeling\nrelevance scores between a pair of query and docu\n\nment. Dense retrievers consider vector similarities\n\nwhile self-attentive re-rankers regression scores. In\ncomparison, the concept of relevance in HyDE is\ncaptured by an","d) _\u27e9_ = _\u27e8_ **vq** _,_ **vd** _\u27e9_ (1)\n\n\nFor zero-shot retrieval, we consider _L_ query sets\n_Q_ 1 _, Q_ 2 _, ..., QL_ and their corresponding search corpus, document sets _D_ 1 _, D_ 2 _, ..., DL_ . Denote the\n_j_ - th query from _i_ - th set query set _Qi_ as q _ij_ . We\nneed to fully define mapping _functions_ enc _q_ and\nenc _d_ without access to any query set _Qi_, document\nset _Di_, or any relevance judgment _rij_ .\nThe difficulty of zero-shot dense retrieval lies\nprecisely in Equation 1: it requires learning of two\nembedding functions (for query and document respectively) into the _same_ embedding space where\ninner product captures _relevance_ . Without relevance judgments\/scores to fit, learning becomes\nintractable.\n\n\n**3.2** **HyDE**\n\n\nHyDE circumvents the aforementioned learning\nproblem by performing search in document\n- nly embedding space that captures documentdocument similarity. This can be easily learned\nusing unsupervised contrastive learning (Izacard\net al., 2021; Gao et al., 2021; Gao and Callan,\n2022). We set document encoder enc _d_ directly as a\ncontrastive encoder enccon.\n\n\n_f_ = enc _d_ = enccon (2)\n\n\nThis function is also denoted as _f_ for simplicity. This unsupervised contrastive encoder will\nbe shared by all incoming document corpus.\n\n\n**vd** = _f_ ( _d_ ) _\u2200d \u2208_ _D_ 1 _\u222a_ _D_ 2 _\u222a_ _... \u222a_ _DL_ (3)\n\n\nTo build the query vector, we consider in addition\nan instruction following LM, InstructLM. It takes a\nquery _q_ and a textual instruction INST and follows\nthem to perform the task specified by INST. For\nsimplicity, denote,\n\n\n_g_ ( _q,_ INST) = InstructLM( _q,_ INST) (4)\n\n\nNow we can use _g_ to map queries to \"hypothetical\" documents by sampling from _g_, setting INST\n\n\nto be \u201cwrite a","## **Precise Zero-Shot Dense Retrieval without Relevance Labels**\n\n**Luyu Gao** _[\u2217]_ _[\u2020]_ **Xueguang Ma** _[\u2217\u2021]_ **Jimmy Lin** _[\u2021]_ **Jamie Callan** _[\u2020]_\n\n_\u2020_ Language Technologies Institute, Carnegie Mellon University\n\n_\u2021_ David R. Cheriton School of Computer Science, University of Waterloo\n{luyug, callan}@cs.cmu.edu, {x93ma, jimmylin}@uwaterloo.ca\n\n\n\n**Abstract**\n\n\nWhile dense retrieval has been shown effec\ntive and efficient across tasks and languages,\nit remains difficult to create effective fully\nzero-shot dense retrieval systems when no relevance label is available. In this paper, we\nrecognize the difficulty of zero-shot learning\nand encoding relevance. Instead, we propose to pivot through Hypothetical Document\nEmbeddings (HyDE). Given a query, HyDE first\nzero-shot instructs an instruction-following\nlanguage model (e.g. InstructGPT) to generate a _hypothetical_ document. The document captures relevance patterns but is unreal\nand may contain false details. Then, an unsupervised contrastively learned encoder (e.g.\nContriever) encodes the document into an\nembedding vector. This vector identifies a\nneighborhood in the corpus embedding space,\nwhere similar _real_ documents are retrieved\n\nbased on vector similarity. This second step\nground the generated document to the actual\ncorpus, with the encoder\u2019s dense bottleneck\nfiltering out the incorrect details. Our experiments show that HyDE significantly outperforms the state-of-the-art unsupervised dense\nretriever Contriever and shows strong performance comparable to fine-tuned retrievers,\nacross various tasks (e.g. web search, QA, fact\nverification) and languages (e.g. sw, ko, ja). [1]\n\n\n**1** **Introduction**\n\n\nDense retrieval (Lee et al., 2019; Karpukhin et al.,\n2020), the method of retrieving documents using\nsemantic embedding similarities, has been shown\nsuccessful across tasks like web search, question\nanswering, and fact verification. A variety of meth\n- ds such as negative mining (Xiong et al., 2021; Qu\net al., 2021"," been shown\nsuccessful across tasks like web search, question\nanswering, and fact verification. A variety of meth\n- ds such as negative mining (Xiong et al., 2021; Qu\net al., 2021), distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofst\u00e4tter et al., 2021) and task-specific\n\n\n_\u2217_ Equal contribution.\n1No models were trained or fine-tuned in making this pre[print. Our open source code is available at https:\/\/github.](https:\/\/github.com\/texttron\/hyde)\n[com\/texttron\/hyde.](https:\/\/github.com\/texttron\/hyde)\n\n\n\npre-training (Izacard et al., 2021; Gao and Callan,\n2021; Lu et al., 2021; Gao and Callan, 2022; Liu\nand Shao, 2022) have been proposed to improve the\neffectiveness of supervised dense retrieval models.\n\n\nOn the other hand, zero-shot dense retrieval still\nremains difficult. Many recent works consider the\nalternative transfer learning setup, where the dense\nretrievers are trained on a high-resource dataset and\nthen evaluated on queries from new tasks. The MSMARCO collection (Bajaj et al., 2016), a massive\njudged dataset with a large number of judged querydocument pairs, is arguably the most commonly\nused. As argued by Izacard et al. (2021), in practice, however, the existence of such a large dataset\ncannot always be assumed. Even MS-MARCO restricts commercial use and cannot be adopted in a\nvariety of real-world search scenarios.\n\nIn this paper, we aim to build effective fully\nzero-shot dense retrieval systems that require **no**\n**relevance** supervision, work out-of-box and generalize across tasks. As supervision is not available,\nwe start by examining self-supervised representation learning methods. Modern deep learning enables two distinct learning algorithms. At the token\nlevel, generative large language models (LLM) pretrained on large corpus have demonstrated strong\nnatural language understanding (NLU) and generation (NLG) capabilities (Brown et al., 2020;\nChen et al., 2021; Rae et al., 2021; Hoffmann\net al., 2022; Thoppilan et al.,"],"31":[" question, LLMs often fail to provide accurate\nanswer. Finally, we found that even when the LLMs contain\nthe required knowledge and are given warnings about potential risks in the retrieved information through instruction,\nthey still tend to trust and prioritize the retrieved information\n\n- ver their own existing knowledge. The experimental results\nmentioned above highlight the need for further resolution of\nimportant issues in the existing RAG method. Therefore, it\nis crucial to exercise caution and carefully design its usage.\nGenerally speaking, the contributions of this paper are [1] :\n\n\n - We proposed to evaluate four capabilities for retrievalaugmented generation  - f LLMs and created the\nRetrieval-Augmented Generation Benchmark in both English and Chinese. To best of our knowledge, it is the first\nbenchmark designed to assess these four capabilities for\nretrieval-augmented generation of LLMs.\n\n - We evaluated the existing LLMs using RGB and found\nthe limitations of them in the four different abilities.\n\n - We analyzed the responses of LLMs in RGB and identified their current shortcomings as well as suggested directions for improvement.\n\n\n**Related work**\n\n**Retrieval-augmented models** The knowledge stored in\nlarge language models is commonly out-of-date (He, Zhang,\nand Roth 2022) and they also sometimes generate hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023) i.e., they may generate irrelevant or factually incorrect contents. By using external\nknowledge as guidance, retrieval-augmented models can\ngenerate more accurate and reliable responses (Guu et al.\n2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\net al. 2022; Shi et al. 2023; Ren et al. 2023). Retrievalaugmented models have achieved remarkable results in various tasks such as open-domain QA (Izacard and Grave\n2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\n\n\n1Our code&data: https:\/\/github.com\/chen700564\/RGB.\n\n\n|Retrieve using<br>search engine|Col2|\n|---|---|\n|**Retrieve using**<br>**search engine**|{\"link\": \"https","## **Benchmarking Large Language Models in Retrieval-Augmented Generation**\n\n**Jiawei Chen** [1,3] **, Hongyu Lin** [1,*] **, Xianpei Han** [1,2,*] **, Le Sun** [1,2]\n\n1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n_{_ jiawei2020,hongyu,xianpei,sunle _}_ @iscas.ac.cn\n\n\n\n**Abstract**\n\n\nRetrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language\nmodels (LLMs). However, existing research lacks rigorous\nevaluation of the impact of retrieval-augmented generation\n\n  - n different large language models, which make it challenging to identify the potential bottlenecks in the capabilities\n\n  - f RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance\n\n  - f different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case.\nThen we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\n\n  - f noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n\n\n**Introduction**\n\n\nRecently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao",". These challenges result in LLMs being unable to\nconsistently generate reliable and accurate responses. Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how\ncould each model survives from these drawbacks and improvement their performance via information retrieval. As a\nresult, there is a pressing need for a comprehensive evaluation of LLMs on their ability to effectively utilize retrieved\ninformation, as well as their ability to withstand the various\ndrawbacks present in information retrieval.\nTo this end, this paper conducts a comprehensive evaluation of RAG for current LLMs. Specifically, we create a new\nRetrieval-Augmented Generation Benchmark, namely RGB,\nin both English and Chinese. In order to ensure that the internal knowledge of LLMs does not introduce bias into the\nevaluation results, RGB chooses to aggregate the latest news\ninformation and constructs queries based on the news information. Then, based on these queries, we use Search API to\nfetch relevant documents and select most relevant snippets\nfrom the content as external retrieved documents. Finally,\nbased on different compositions of query and document-set\npairs, we expand the corpus and divided it into 4 testbeds to\nevaluate the following basic abilities of LLMs according to\nthe common challenges in RAG, as shown in Figure 1:\n\n\n - **Noise Robustness**, which means a LLM can extract useful information from noisy documents. In this paper, we\ndefine noisy documents as those that are relevant to the\nquestion but do not contain any information of the answer. For the instance in Figure 1, the noisy documents\nrelated to the question \u201cWho was awarded the 2022 Nobel Prize in Literature\u201d include reports about the 2021\nNobel Prize in Literature. To this end, the testbed for\nnoise robustness contains instances whose external documents contain a certain number of noisy documents\nbased on the desired noise ratio.\n\n - **Negative Rejection**, which means that a LLM should reject to answer the question when the required knowledge\nis not present in any retrieved document. The testbed for\nnegative rejection contains instances whose external documents are only with noisy documents. LLMs are expected to indicate \u201cinsufficient information\u201d or other rejection signals.\n\n - **Information Integration**, which evaluates whether\nLLMs can answer complex questions that require integrating information from multiple documents. For the instance in Figure 1, for the question \u201cWhen were the ChatGPT app"," et al. 2023)\nfocuses on human exam and ToolBench (Qin et al. 2023)\nevaluates how well LLMs use external tools. Recently, Adlakha et al. (2023) evaluate the RAG of LLMs in exist QA\ndataset. Different from their work, we focus on 4 required\nabilities of RAG and create Retrieval-Augmented Generation Benchmark to evaluate the LLMs.\n\n\n**Retrieval-Augmented Generation Benchmark**\n\n\nIn this section, we first introduce the specific retrievalaugmented generation abilities we aim to evaluate. Next, we\n\n- utline the process of constructing the RAG benchmark for\nevaluation. Lastly, we present the evaluation metrics.\n\n\n**Required abilities of RAG**\n\n\nExternal knowledge is the key to resolving the problems\n\n- f LLMs such as hallucination and outdated knowledge,\nwhich can make LLMs generate more accurate and reliable\nresponses through retrieval-augmented generation (RAG).\nHowever, LLMs cannot always response as expected with\nRAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating\nthese external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the\nuseful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the\nretrieval-augmented generation of LLMs, and we concern\nabout 4 specific abilities:\n**Noise Robustness** is the robustness of LLMs in noisy\ndocuments. As retrievers are not perfect, the external knowledge they retrieve often contains a significant amount of\nnoise, i.e., documents which are relevant to the question but\ndo not contain any information about the answer. To effectively answer user questions, LLMs must be able to extract\nthe necessary information from documents despite there are\nnoisy documents.\n**Negative Rejection** is a measure of whether LLMs can\ndecline to answer a question when none of the contexts provide useful information. In real-world situations, the search\nengine often fails to retrieve documents containing the answers. In these cases, it is important for the"," the future development of RAG in LLMs.\n\n\n**Conclusion**\n\nIn this paper, we evaluated four abilities of retrievalaugmented generation in LLMs: noise robustness, negative rejection, information integration, and counterfactual\nrobustness. To conduct the evaluation, we built RetrievalAugmented Generation Benchmark (RGB). The instances of\nRGB are generated from latest news articles and the external\ndocuments obtained from search engines. The experimental\nresults suggest that current LLMs have limitations in the 4\nabilities. This indicates that there is still a significant amount\n\n- f work needed to effectively apply RAG to LLMs. To ensure accurate and reliable responses from LLMs, it is crucial\nto exercise caution and carefully design for RAG.\n\n\n\n\n\n\n\n\n\n\n|Question|Answer|Response|Errors|\n|---|---|---|---|\n|What groupings are France and<br>Germany in World Cup 2022?|Group D<br>Group E|France and Germany are in<br>Group D of the 2022 FIFA<br>World Cup.|**Merging**<br>**Error**|\n|Who were the MVP of Super<br>Bowl 2022 and 2023?|Cooper Kupp<br>Patrick<br>Mahomes|According to the provided<br>document, the MVP of Super<br>Bowl LVI was Cooper Kupp<br>of the Los Angeles Rams.|**Ignoring**<br>**Error**|\n|What flms won the 2022 and<br>2023 Academy Awards for<br>Best Picture?|CODA<br>Everything<br>Everywhere<br>All at Once|The flm CODA won the<br>award for Best Picture at the<br>95th Academy Awards<br>ceremony held on 2023.|**Misalignment**<br>**Error**|\n\n\n\nTable 6: Error cases of information integration, the responses are generated by ChatGLM2-6B. The blue and red\ntexts represent the answers to two sub-questions.\n\n\n**(1) Merging Error (28% of the total).** The model sometimes merges the answers of the two sub-questions, resulting\nin an error. It mistakenly uses the answer from one question\nto address both two questions. At this point, the model will\ndisregard any documents related to one sub-question. For\nexample, in Table 6, it incorrectly states that Group"],"32":[" in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f"," question via\ncontrolled experiments with a variety of state-ofthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\nAnthropic\u2019s Claude-1.3) language models in settings that require accessing and using information\nwithin an input context. In particular, our experiments make controlled changes to the input context\nsize and the position of the relevant information\nwithin the input context and study their effects on\nlanguage model performance. If language models\ncan robustly use information within long input contexts, then their performance should be _minimally_\n_affected_ by the position of the relevant information\nin the input context.\n\n\nWe first experiment with multi-document question answering, which requires models to reason\n\n- ver provided documents to find relevant information and use it to answer a given question; this task\nmimics the retrieval-augmented generation setup\nunderlying many commercial generative search and\nquestion answering applications (e.g., Bing Chat).\nIn this setting, we control (i) the input context\nlength by changing the number of documents in\nthe input context (akin to retrieving more or less\ndocuments in retrieval-augmented generation), and\n(ii) control the position of the relevant information\nwithin the input context by changing the order of\nthe documents to place the relevant document at\nthe beginning, middle or end of the context.\n\n\nWe find that changing the position of relevant\ninformation in the input context can substantially\naffect model performance, indicating that current\nlanguage models do not robustly access and use\ninformation in long input contexts. Furthermore,\nwe observe a distinctive U-shaped performance\ncurve (Figure 1); language model performance is\nhighest when relevant information occurs at the\nvery beginning (primacy bias) or end of its input context (recency bias), and performance significantly degrades when models must access and\nuse information in the middle of their input context (\u00a72.3). For example, when relevant information is placed in the middle of its input context, GPT-3.5-Turbo\u2019s performance on the multi\n\n\ndocument question task is lower than its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily","## **Lost in the Middle: How Language Models Use Long Contexts**\n\n**Nelson F. Liu** [1] _[\u2217]_ **Kevin Lin** [2] **John Hewitt** [1] **Ashwin Paranjape** [3]\n\n**Michele Bevilacqua** [3] **Fabio Petroni** [3] **Percy Liang** [1]\n\n1Stanford University 2University of California, Berkeley 3Samaya AI\n[nfliu@cs.stanford.edu](mailto:nfliu@cs.stanford.edu)\n\n\n\n**Abstract**\n\n\nWhile recent language models have the ability to take long contexts as input, relatively\nlittle is known about how well they _use_\nlonger context. We analyze the performance\n\n   - f language models on two tasks that require\nidentifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that\nperformance can degrade significantly when\nchanging the position of relevant information, indicating that current language models\ndo not robustly make use of information in\nlong input contexts. In particular, we observe\nthat performance is often highest when relevant information occurs at the beginning or\nend of the input context, and significantly\ndegrades when models must access relevant\ninformation in the middle of long contexts,\neven for explicitly long-context models. Our\nanalysis provides a better understanding of\nhow language models use their input context\nand provides new evaluation protocols for\nfuture long-context language models.\n\n\n**1** **Introduction**\n\n\nLanguage models have become an important and\nflexible building block in a variety of user-facing\nlanguage technologies, including conversational\ninterfaces, search and summarization, and collaborative writing (Shuster et al., 2022; Thoppilan et al.,\n2022; Lee et al., 2022, _inter alia_ ). These models\nperform downstream tasks primarily via prompting:\nall relevant task specification and data to process is\nformatted as a textual input context, and the model\nreturns a generated text completion. These input\ncontexts can contain thousands of tokens, especially when language models are used to process\nlong documents (e.g., legal or scientific documents,\nconversation histories, etc.) or when language models are augmented with external information (e.g.,\n\n\n*Work partially completed as an intern at Samaya AI.\n\n\n\n\n|Total|Col2|Ret|rievedDocuments( ~4"," its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily\nbetter at using their input context (\u00a72.3).\nGiven that language models struggle to retrieve\nand use relevant information in the multi-document\n\nquestion answering task, to what extent can language models even _retrieve_ from their input contexts? We study this question with a synthetic keyvalue retrieval task, which is designed to be a minimal testbed for the basic ability to retrieve matching\ntokens from the input context. In this task, models\nare given a collection of JSON-formatted key-value\npairs and must return the value associated with a\nspecific key. Similar to the multi-document QA\ntask, the key-value retrieval task admits controlled\nchanges to the input context length (adding more\nkey-value pairs) and the position of relevant information. Although some models perform the\nsynthetic key-value retrieval task perfectly, other\nmodels struggle to simply retrieve matching tokens\nthat occur in the middle of their input context and\ncontinue to exhibit a U-shaped performance curve.\nTo better understand why language models struggle to robustly access and use information in their\ninput contexts, we study the role of model architecture (decoder-only vs. encoder-decoder), queryaware contextualization, and instruction fine-tuning\n(\u00a74). We find that:\n\n\n  - Encoder-decoder models are relatively robust\nto changes in the position of relevant information within their input context, but only when\nevaluated on sequences within its trainingtime sequence length. When evaluated on\nsequences longer than those seen during training, we observe a U-shaped performance\ncurve (\u00a74.1).\n\n\n  - Query-aware contextualization (placing the\nquery before _and_ after the documents or keyvalue pairs) enables near-perfect performance\n\n   - n the synthetic key-value task, but minimally\nchanges trends in multi-document QA (\u00a74.2).\n\n\n  - Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of\nrelevant information in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that","2\u201310% for Gemini,\nGPT, and Gemma. Key findings and the prompts used in our autorater analysis are\n[available on our github.](https:\/\/github.com\/hljoren\/sufficientcontext)\n\n\n1 INTRODUCTION\n\n\nProviding Large Language Models (LLMs) with additional context, such as in Retrieval Augmented\nGeneration (RAG) systems, has led to major improvements in LLM factuality and verifiability when\nadapting to new domains (Lewis et al., 2020). In the case of open-domain question answering, a\nretrieval model provides context at inference time in the form of snippets or long-form text (Zhu\net al., 2021). Then, the model synthesizes the query along with this added context to generate the\nanswer. Unfortunately, current RAG-based LLMs exhibit many undesirable traits, such as confidently\npredicting the incorrect answer with retrieved evidence (Mishra et al., 2024; Niu et al., 2024; Ru\net al., 2024), being distracted by unrelated information (Cuconasu et al., 2024; Yoran et al., 2024),\nand failing to properly extract answers from long text snippets (Hsieh et al., 2024; Liu et al., 2024).\n\n\nThe ideal outcome is for the LLM to output the correct answer if the provided context contains\nenough information to answer the question when combined with the model\u2019s parametric knowledge.\nOtherwise, the model should abstain from answering and\/or ask for more information. One core\nchallenge in achieving this ideal outcome is building models that can use the provided context only\nwhen it helps answer the question correctly. Several works have investigated this issue by evaluating\n\n\n_\u2217_ Work done during an internship at Google.\n\n_\u2020_ Work done during an internship at Google.\n\n\n1\n\n\nPublished as a conference paper at ICLR 2025\n\n\nmodels in the presence of irrelevant information in the context (discussed in Section 2). However,\n\u201crelevant information\u201d can range from directly containing the answer to simply being topically related\nto the question. Even \u201cgolden\u201d or oracle documents in datasets vary in how much information they\nprovide about the query, and whether they directly inform the ground truth answer or not. In other\nwords, while the goal seems to be to understand how LLMs behave when they do or do"],"33":[" in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f","learn to use these long contexts from similarlyformatted data that may occur in Internet text seen\nduring pre-training, e.g., StackOverflow questions\n\n\n\nand answers.\n\nTo better understand the effect of additional finetuning and model scale, we also experimented\nwith Llama-2 models of varying sizes (7B, 13B,\nand 70B) with and without additional supervised\nfine-tuning and reinforcement learning from human feedback (Appendix E). We find that the Ushaped performance curve only appears in sufficiently large language models (with or without additional fine-tuning)\u2014the 7B Llama-2 models are\nsolely recency biased, while the 13B and 70B models exhibit a U-shaped performance curve. In addition, we see that the Llama-2 supervised fine-tuning\nand reinforcement learning from human feedback\nprocedure slightly mitigates the positional bias in\nsmaller models (13B, akin to trends shown when\ncomparing MPT-30B and MPT-30B-Instruct), but\nminimally affects trends on larger models (70B).\n\n\n**5** **Is More Context Is Always Better?**\n**A Case Study With Open-Domain QA**\n\n\nOur results indicate that prompting language models with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing\naccuracy. Even if a language model can take in\n16K tokens, is it actually beneficial to provide 16K\ntokens of context? The answer to this question\nis ultimately downstream task-specific since it depends on the marginal value of the added context\nand the model\u2019s ability to effectively use long input\ncontexts, but we perform a case study with opendomain question answering on NaturalQuestionsOpen to better understand this trade-off in existing\nlanguage models.\nWe use language models in a standard retrieverreader setup. A retrieval system (Contriever, finetuned on MS-MARCO) takes an input query from\nNaturalQuestions-Open and returns the _k_ documents from Wikipedia with the highest relevance\nscore. To condition language models on these retrieved documents, we simply include them in the\nprompt. We evaluate retriever recall and reader\naccuracy (whether any of the annotated answers\nappear in the predicted output) as a function of the\nnumber of","5-Turbo 56.1% 88.3%\n\nGPT-3.5-Turbo (16K) 56.0% 88.6%\n\nClaude-1.3 48.3% 76.1%\n\nClaude-1.3 (100K) 48.2% 76.4%\n\n\nTable 1: Closed-book and oracle accuracy of language\nmodels on the multi-document question answering task.\n\n\nshaped performance curve\u2014models are often much\nbetter at using relevant information that occurs at\nthe very beginning (primacy bias) and very end of\ncontexts (recency bias), and suffer degraded performance when forced to use information within the\n\nmiddle of its input context. For example, GPT-3.5Turbo\u2019s multi-document QA performance can drop\nby more than 20%\u2014in the worst case, performance\nin 20- and 30-document settings is lower than performance without _any_ input documents (i.e., closedbook performance; 56.1%). These results indicate\nthat current models cannot effectively reason over\ntheir entire context window when prompted for\ndownstream tasks.\n\n\n**Extended-context models are not necessarily bet-**\n**ter at using input context.** When the input context fits in the context window of both a model\nand its extended-context counterpart, we see that\nperformance between them is nearly identical. For\nexample, the 10- and 20-document settings both\nfit in the context window of GPT-3.5-Turbo and\nGPT-3.5-Turbo (16K), and we observe that their\nperformance as a function of position of relative\ninformation is nearly superimposed (solid purple\nand dashed brown series in Figure 5). These results\n\n\nInput Context\n\n\nExtract the value corresponding to the specified key in the JSON object below.\n\n\nJSON data:\n\n{\"2a8d601d-1d69-4e64-9f90-8ad825a74195\": \"bb3ba2a5-7de8-434b-a86e-a88bb9fa7289\",\n\n\"a54e2eed-e625-4570-9f74-3624e77d6684\": \"d1ff29be-4e2a-4208-a182-0cea716be3d4\",\n\n\" **9f4a92b9"," context\nlength of 4K tokens, and GPT-3.5-Turbo (16K) is a\nversion with an extended maximum context length\n\n- f 16K tokens. We evaluate Claude-1.3 and Claude\n1.3 (100K) with the Anthropic API; Claude-1.3\nhas a maximum context length of 8K tokens, and\nClaude-1.3 (100K) has an extended context length\n\n- f 100K tokens. [6]\n\n\n**2.3** **Results and Discussion**\n\n\nWe experiment with input contexts containing 10,\n20, and 30 total documents. Figure 5 presents multidocument question answering performance when\nvarying the position of relevant information within\nthe input context. To contextualize model performance, we also evaluate on the closed-book and\n\n- racle settings (Table 1). In the closed-book setting,\nmodels are not given any documents in their input\ncontext, and must rely on their parametric memory\nto generate the correct answer. On the other hand,\nin the oracle setting, language models are given the\nsingle document that contains the answer and must\nuse it to answer the question.\n\n\n**Model performance is highest when relevant in-**\n**formation occurs at the beginning or end of its**\n**input context.** As illustrated in Figure 5, changing the position of relevant information in the input context leads to substantial decreases in model\nperformance. In particular, we see a distinctive U\n\n5We use the 0613 OpenAI model versions.\n6We also evaluate GPT-4 (8K) on a subset of multidocument QA experiments, finding similar results and trends\nas other models (though GPT-4 has higher absolute performance). Evaluating GPT-4 on the full multi-document QA\nand key-value retrieval experiments would cost upwards of\n$6000. See Appendix D for GPT-4 results and discussion.\n\n\n\nModel Closed-Book Oracle\n\n\nLongChat-13B (16K) 35.0% 83.4%\nMPT-30B-Instruct 31.5% 81.9%\n\nGPT-3.5-Turbo 56.1% 88.3%\n\nGPT-3.5-Turbo (16K) 56.0% 88.6%\n\nClaude-1.3 48.3% 76"," the position of relevant information. To better understand why, we perform some preliminary investigations into the role\n\n- f model architecture (decoder-only vs. encoderdecoder), query-aware contextualization, and instruction fine-tuning.\n\n\n**4.1** **Effect of Model Architecture**\n\n\nThe open models we evaluated are all decoder-only\nmodels\u2014at each timestep, they may only attend\nto prior tokens. To better understand the potential effects of model architecture on how language\nmodel use context, we compare decoder-only and\nencoder-decoder language models.\nWe experiment with Flan-T5-XXL (Raffel et al.,\n2020; Chung et al., 2022) and Flan-UL2 (Tay et al.,\n2023). Flan-T5-XXL is trained with a sequences\n\n- f 512 tokens (encoder and decoder). Flan-UL2 is\ninitially trained with sequences of 512 tokens (encoder and decoder), but is then pre-trained for an\nextra 100K steps with 1024 tokens (encoder and decoder) before instruction fine-tuning on sequences\nwith 2048 tokens in the encoder and 512 tokens\n\nin the decoder. However, since these models use\n\n\n\nrelative positional embeddings, they can (in principle) extrapolate beyond these maximum context\nlengths; Shaham et al. (2023) find that both models can perform well with sequences of up to 8K\ntokens.\n\nFigure 8 compares the performance of decoder\n- nly and encoder-decoder models. When Flan-UL2\nis evaluated on sequences within its 2048-token\ntraining-time context window (Figure 8; left subplot), its performance is relatively robust to changes\nin the position of relevant information within the\ninput context (1.9% absolute difference between\nbest- and worst-case performance). When evaluated on settings with sequences longer than 2048\ntokens (Figure 8; center and right), Flan-UL2 performance begins to degrade when relevant information is placed in the middle. Flan-T5-XXL shows\na similar trend, where longer input contexts result\nin a greater performance degradation when placing\nrelevant information in the middle of the input context. We hypothesize that encoder-decoder models\nmay make better use of their context windows because their bidirectional encoder allows processing\neach document in the context of future documents,\npotentially improving relative importance estimation between documents.\n\n\n"],"34":[" predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach. We also find that ARES consistently\ndistinguishes competitive RAG systems that are\n\n- nly a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop\n\n\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\n[available on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**2** **Related Work**\n\n\nRAG (Guu et al., 2020; Lewis et al., 2020; Khattab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combining them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual information (Shuster et al., 2021; Huo et al., 2023), and\n\n- ffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essential for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\n\n- ut of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. However, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min"," (is the response generated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\n\n\nRAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\n\nMany existing RAG evaluation frameworks require substantial human annotations for scoring.\nARES significantly improves data efficiency during evaluation by only requiring three inputs: an indomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data generation.\n\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question\u2013answer\npairs, derived from the passages in the corpus. Sec\n- nd, it defines three separate judge models to perform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores\nthe different RAG systems being assessed using\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals\nfor RAG scoring. PPI utilizes a small set of human\nannotated datapoints for computing its confidence\nintervals; we designate this annotated set as our _hu-_\n_man preference validation set_, which is composed\n\n- f approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an\nswer relevance.\n\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating existing automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occurrences in the AIS attribution dataset (Rashkin et al.,\n2022), predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach."," 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RAGAS (James and Es, 2023). To evaluate RAG systems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several associated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\n\n- ffer little adaptability to new RAG evaluation set\n\ntings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n\n\n**3** **ARES**\n\n\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\n\n- r more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query\u2013passage\u2013answer\ntriples to train LLM judges. Subsequently, we apply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to estimate a confidence interval for the quality of each\nRAG system.\n\n\n**3.1** **LLM Generation of Synthetic Dataset**\n\n\nWe generate synthetic queries and answers from\nthe corpus"," confidence set of the ML\nmodel\u2019s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence\ninterval for the performance of the evaluated RAG\nsystem (e.g. its context relevance, answer faithfulness, or answer relevance accuracy individually)\ncompared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the human preference validation set with the much larger\nset of datapoints with ML predictions, PPI can develop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\n\n\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confidence bounds for the success and failure rates of the\n\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (probability) for our confidence interval.\n\n\nWith the accuracy confidence interval for each\ncomponent of the RAG, we find the midpoint of\neach confidence interval and use the midpoints to\nrank the RAG systems. With our ranking, we can\ncompare different RAG systems, as well as different configurations of the same RAG system, to find\nthe best-performing approach for a given domain.\n\n\n**4** **Experiments**\n\n\n**4.1** **Models**\n\n\nFor our fine-tuned judges, ARES relies on generating cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use OpenAI\u2019s _gpt-3.5-turbo-16k_, version 10\/23, (Brown\net al., 2020) in a zero\/few-shot setting. For similarity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand Open"," Gemma has\nmuch more hallucinations (higher **red bars** ) than the other models, except for HotPotQA, where we\nattribute the higher accuracy to the smaller retrieved contexts.\n\n\n4.3 QUALITATIVELY ANALYZING RESPONSES WITH INSUFFICIENT CONTEXT\n\n\nOne curious observation in our analysis is the ability of models to sometimes provide correct answers\neven when presented with insufficient context. For example, from Figure 3, all three models are able\nto correctly answer upwards of 35% of instances with insufficient context on HotpotQA. A natural\nassumption is that the models already know the answer from pre-training, and they can generate a\ncorrect response from parametric memory. However, this only explains part of the story.\n\n\nLooking deeper, we provide a qualitative categorization in Table 2 of instance types where our\nautorater labels an instance as insufficient context, while the LLM evaluator marks the model answer\n\n\n7\n\n\nPublished as a conference paper at ICLR 2025\n\n\n**Instance type** **Why model may be correct** **Example**\n\n\nYes\/No question 50% chance of correct **Q:** Is there a total eclipse in the United States this year?\n\n\nLimited choice Some chance of correct **Q:** Which band has more members, Chvrches or\nGoodbye Mr. Mackenzie?\n\n\n\nMulti-hop: fragment Use parametric inference\n\n\nMulti-hop: partial Use parametric knowledge\n\n\nToo many hops Execute complex reasoning\n\n\nAmbiguous query Guess right interpretation\n\n\n\n**Q:** Who did the original voice for the character\nwhose series Mickey\u2019s Safari in Letterland is from?\n_Context says Mickey\u2019s Safari is a video game_\n_and Walt Disney voices Mickey Mouse in cartoons._\n_Must infer the game is in the Mickey Mouse series._\n\n\n**Q:** Claudine\u2019s Return starred the actress who played\nwhich role on \u201cMarried...with Children\u201d?\n\n_Context lists actresses but not their roles in_\n_\u201cMarried...with Children\u201d. Must know extra facts._\n\n\n**Q:** How many cyclists have won all three of women\u2019s\ncycling Grand Tours equivalents in the same year?\n_Context requires cross-referencing lists of events_\n_and lists of winners while tracking winners by year._\n\n\n**Q:** Who is the spouse of a cast member from King\n\n- f the Mountain?\n\n_Context has many cast members and query\/context do_\n_not specify which spouse to answer about._\n\n\n\nRater error Mislabel insuff. or correct"],"35":[" evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges). We compare against RAGAS (version 0.0.18) and a baseline few-shot prompted GPT3.5 judge ( _gpt-3.5-turbo-16k_ ). For the few-shot\nGPT-3.5 judge, we provide few-shot examples for\nguiding predictions; the prompts are included in\nAppendices A.2, A.3, and A.4. For both ARES\nand the GPT-3.5 judge baseline, we augment the\nLLM with PPI, using a 300-datapoint human preference validation set to rectify the ML predictions\nand produce confidence intervals.\n\n\n\nAcross almost all settings across the datasets\nfrom KILT and SuperGLUE, ARES provides a\nmore accurate ranking of RAG systems than RAGAS. ARES averages a Kendall\u2019s _\u03c4 0.065 higher_\n_for context relevance_ and _0.132 higher for answer_\n_relevance than RAGAS_ . Additionally, the LLMjudge is substantially more accurate than RAGAS\nat predicting context relevance and answer relevance of a query-passage-answer triple. For context relevance, ARES with a fine-tuned LLM-judge\nis _59.9 percentage points higher than RAGAS_ while\nfor answer relevance, our system is _14.4 percent-_\n_age points higher than RAGAS_ . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompting and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall\u2019s _\u03c4_ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall\u2019s _\u03c4_ for ARES is\n0.08 higher"," 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RAGAS (James and Es, 2023). To evaluate RAG systems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several associated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\n\n- ffer little adaptability to new RAG evaluation set\n\ntings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n\n\n**3** **ARES**\n\n\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\n\n- r more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query\u2013passage\u2013answer\ntriples to train LLM judges. Subsequently, we apply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to estimate a confidence interval for the quality of each\nRAG system.\n\n\n**3.1** **LLM Generation of Synthetic Dataset**\n\n\nWe generate synthetic queries and answers from\nthe corpus"," 5: **ARES Ranking on Real-World RAG Systems** : For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall\u2019s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\n\n\n**ARES Cross-Domain Ranking of Pseudo RAG Systems**\n\n\nNQ to FEVER to NQ to MultiRC to NQ to ReCoRD to\nFEVER NQ MultiRC NQ ReCoRD NQ\n\n\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\n\n\nKendall\u2019s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\n\n\nKendall\u2019s Tau of\n0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nIn-Domain LLM Judge\n\n\nAverage PPI Range 8.7% 7.2%","\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-tosequence model (Lewis et al., 2019). During retrieval, each RAG system only retrieves one passage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applications, averaging a Kendall\u2019s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com\npared to RAGAS, ARES is 0.16 higher for context\nrelevance and 0.15 higher for answer relevance, on\naverage. ARES also provided accurate confidence\nbounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on average, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n\n\n**5.4** **Strengths and Limits of Cross-Domain**\n**Applications**\n\n\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are difficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in _query type_ from\ntraining to test (e.g. NQ to FEVER), change in\n_document type_ from training to test (e.g. NQ to\nMultiRC), and change in both _query and document_\n_type_ (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in crossdomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generalizability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Furthermore, we found that even when the LLM judge\u2019s accuracy suffered in cross-domain applications, PPI\nhelped"," LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge** :\nFor scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES\nwith our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.\nFor our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels\nto score the system. RAGAS also uses GPT-3.5 as its judge but it uses few-shot prompts that are not targeted for\neach evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and\nGPT-3.5 across all the explored datasets. The Kendall\u2019s tau for ARES was _0.065 higher on average for scoring_\n_context relevance_ and _0.132 higher on average for scoring answer relevance_ than RAGAS. Additionally, we include\nthe Kendall\u2019s taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of\nthe judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run.\nFor PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation\nset. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\n\n\n\nused ARES with human annotation sets ranging\nin size from 25 to 400 and found that 150 is the\n\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).\n\n\n**5.2** **ARES Performance on AIS**\n\n\n**WoW** **CNN \/ DM**\n\n\nARES Split Prediction 0.478 0.835\nCorrect Positive\/Negative Split 0.458 0.859\nARES Judge Accuracy 62.5% 84.0%\nEvaluation Set Size 707 510\n\nHuman Preference Data Size 200 200\n\n\nTable 2: ARES Results on the AIS benchmark"],"36":[" (DisambigF1), ROUGE (Lin, 2004), and an overall score combining Disambig-F1 and ROUGE (DR).\n\n\n**Open-domain summarization** The goal of opendomain summarization is to generate a comprehensive summary about a topic by gathering information from open web (Giorgi et al., 2022). We use\nWikiAsp (Hayashi et al., 2021) which aims to generate aspect-based summaries about entities from\n20 domains in Wikipedia, e.g., \u201cGenerate a summary about Echo School (Oregon) including the\n\n\nNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp\n\n\nFigure 4: Comparision between FLARE and baselines across all tasks\/datasets. We report the primary metric for\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\n\n\n\nfollowing aspects: academics, history.\u201d Experimental setting details are included in Appendix B.\nMetrics include ROUGE, named entity-based F1,\nand UniEval (Zhong et al., 2022) which measures\nfactual consistency.\n\n\n**6** **Experimental Results**\n\n\nWe first report overall results across 4 tasks\/datasets\nand compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\n\nrun ablation experiments to study the efficacy of\nvarious design choices of our method.\n\n**6.1** **Comparison with Baselines**\n\n**Overall results.** The overall performance of\nFLARE and baseline across all tasks\/datasets are\n\nreported in Figure 4. FLARE outperforms all baseline on all tasks\/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks, multihop QA shows the\nmost significant improvement. This is largely due\nto the task\u2019s clear definition and specific objective\n\n- f producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to\ngenerate on-topic output. In contrast, ASQA and\nWikiAsp are more open-ended, which increases the\ndifficulty of both generation and evaluation. The\n","2022).\n\n\nA.2 TIMEQA ERROR ANALYSIS\n\n\nWe conduct error analysis to understand where STEP-BACK PROMPTING fixes the errors the baseline\nmodels make. Figure 6 shows that compared to the predictions of baseline PaLM-2L, STEP-BACK\nPROMPTING can fix 39 _._ 9% of the predictions where the baseline prediction is wrong, while causing\n5 _._ 6% errors.Furthermore, Step-Back + RAG fixes 21 _._ 6% errors coming from RAG. The % of errors\nintroduced by STEP-BACK PROMPTING to RAG is still relatively low (6 _._ 3%). Together, this shows\nthat the STEP-BACK PROMPTING is helpful most of the time, signifying the need and effectiveness\n\n- f doing Abstraction before directly addressing the original question.\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Error Analysis of Step-Back Prompting on TimeQA. _Left_ : Step-Back + RAG vs Baseline\npredictions. _Right_ : Step-Back RAG vs RAG predictions. Step-Back + RAG can fix 39 _._ 9% of\nthe predictions where the baseline prediction is wrong while causing 5 _._ 6% errors. Furthermore,\nStep-Back + RAG fixes 21 _._ 6% errors coming from RAG. The % of errors introduced by STEP-BACK\nPROMPTING to RAG is still relatively low (6 _._ 3%).\n\n\n13\n\n\nStep-Back Prompting Enables Reasoning Via Abstraction in Large Language Models\n\n\nTable 5: Stats of the evaluation datasets used in this paper.\n\n\nDomain Dataset Split Number of Examples\n\n\nSTEM MMLU high-school Physics Test 151\nMMLU high-school Chemistry Test 203\nGSM8K Test 1319\n\n\nKnowledge QA TimeQA Test 5226\nTimeQA Easy Test 2613\nTimeQA Hard Test 2613\nSituatedQA Test 2901\n\n\nMulti-hop Reasoning MuSiQue Dev 2417\nStrategyQA Dev 229\n\n\nA.3 STRATEGYQA ERROR ANALYSIS\n\n\nFigure 7 shows the error analysis of StrategyQA on the predictions of Step-Back + RAG against the\nbaseline model and the raw retrieval augmentation variant of PaLM-2L. Compared to the baseline,\nStep-Back + RAG can turn 15 _._ 4%",".0 39.9 27.9 **34.3** 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\n\nFLARE (ours) **77.3** **41.3 28.2 34.3 31.1** **46.2 36.7 37.7 37.2** **53.4** **18.9 27.6**\n\n\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is\nDisambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\n\n\n\n**2WikiMultihopQA** **ASQA-hint**\n**EM** **F** 1 **Prec. Rec.** **EM D-F** 1 **R-L DR**\n\n\nPrevious 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\n\nNext 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\n\n\nTable 3: A head-to-head comparison between using the\nprevious sentence and the next sentence for retrieval.\n\n\n**#Tokens** **EM** **F** 1 **Prec.** **Rec.**\n\n\n16 43.2 52.3 51.7 54.5\n\n32 43.6 52.4 52.0 55.0\n\n48 40.0 49.3 49.0 52.0\n\nAll 39.0 48.5 48.2 51.1\n\n\nTable 4: Previous-window approaches using different\nnumbers of tokens as queries.\n\n\n- us window underperforms single-time retrieval\n\n- n ASQA, which we hypothesize is because the\nprevious window does not accurately reflect future\nintent. Since we focus on evaluating factuality, metrics with an emphasis on factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n\n\n**6.2** **Ablation Study**\n\n\n**Importance of forward-looking"," factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n\n\n**6.2** **Ablation Study**\n\n\n**Importance of forward-looking retrieval.** We\nfirst validate that forward-looking retrieval is more\neffective than past-context-based retrieval. We run\nablation experiments on 2WikiMultihopQA and\nASQA-hint comparing retrieval using the previ\n- us versus the next sentence. Specifically, both\nmethods retrieve every sentence and directly use\nthe complete previous\/next sentence as queries. As\nshown in Table 3, using the next sentence to retrieve\nis clearly better than using the previous sentence,\nconfirming our hypothesis.\nWe also run previous-window approaches using\ndifferent numbers of past tokens as queries. As\nshown in Table 4, using too many tokens ( _>_ 32) in\n\n\n\nFigure 5: Performance (EM) of FLARE with respect\nto the percentage of steps\/sentences with retrieval on\n2WikiMultihopQA and StrategyQA.\n\n\nthe past hurts the performance, further confirming\n\n- ur hypothesis that previous context might not be\nrelevant to intent of future generations.\n\n\n**Importance of active retrieval.** Next, we investigate how active retrieval threshold _\u03b8_ affects performance. To alter our method from not retrieving\nto retrieving every sentence, we adjust the confidence threshold _\u03b8_ that determines when to trigger\nretrieval from 0 to 1. We then calculate the proportion of steps\/sentences where retrieval is activated, and present the performance based on it. As\nshown in Figure 5, on 2WikiMultihopQA, the performance plateaus when the retrieval percentage\nexceeds 60%, indicating that retrieval when LMs\nare confident is not necessary. On StrategyQA, the\nperformance drops when the retrieval percentage\nexceeds 50%, indicating that unnecessary retrieval\ncan introduce noise and impede the original generation process. We found triggering retrieval for\n40%-80% of sentences usually leads to a good performance across tasks\/datasets.\n\n\n**Effectiveness of different query formulation**\n**methods** We study implicit query formation by\nmasking and explicit query formulation through\nquestion generation. In Table 5, we compare the\nperformance of FLARE with different masking\n\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n","6.1** **Limitations of evaluation approach**\n\n\nOur evaluation to date has focused on sensemaking questions specific to two corpora each containing\napproximately 1 million tokens. More work is needed to understand how performance generalizes to\ndatasets from various domains with different use cases. Comparison of fabrication rates, e.g., using\napproaches like SelfCheckGPT (Manakul et al., 2023), would also strengthen the current analysis.\n\n\n11\n\n\nTable 4: Average number of clusters across different distance thresholds, reported by condition and\ndataset type. Bolded values represent the highest score in each row.\n\n\n**Average Number of Clusters**\n**Dataset** **Distance Threshold**\n\n**C0** **C1** **C2** **C3** **TS** **SS**\n\n\n\nNews Articles\n\n\nPodcast Transcripts\n\n\n**6.2** **Future work**\n\n\n\n0.5 **23.42** 21.85 21.90 22.13 21.80 17.92\n\n0.6 **21.65** 20.38 20.30 20.52 20.13 16.78\n\n0.7 **20.19** 19.06 19.03 19.13 18.62 15.80\n\n0.8 **18.86** 17.78 17.82 17.79 17.30 14.80\n\n\n0.5 **23.16** 22.62 22.52 21.93 21.14 18.55\n\n0.6 **21.65** 21.33 21.21 20.62 19.70 17.39\n\n0.7 **20.41** 20.04 19.79 19.22 18.08 16.28\n\n0.8 **19.26** 18.77 18.46 17.89 16.66 15.07\n\n\n\nThe graph index, rich text annotations, and hierarchical community structure supporting the current\nGraphRAG approach offer many possibilities for refinement and adaptation. This includes RAG\napproaches that operate in a more local manner, via embedding-based matching of user queries and\ngraph annotations. In particular, we see potential in hybrid RAG schemes that combine embeddingbased matching with just-in-time community report generation before employing our map-reduce\nsummar"],"37":["2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao et al., 2023; Khattab et al., 2022; Shao et al., 2023; Su et al., 2020; Trivedi et al., 2022;\nWang et al., 2024). In particular, GraphRAG is similar to other approaches that use hierarchical\nindexing to create summaries (similar to Kim et al. 2023; Sarthi et al. 2024). GraphRAG contrasts\nwith these approaches by generating a graph index from the source data, then applying graph-based\ncommunity detection to create a thematic partitioning of the data.\n\n\n**2.2** **Using Knowledge Graphs with LLMs and RAG**\n\n\nApproaches to knowledge graph extraction from natural language text corpora include rulematching, statistical pattern recognition, clustering, and embeddings (Etzioni et al., 2004; Kim et al.,\n2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of\nresearch that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022;\nOpenAI, 2023; Tan et al., 2017; Trajanoska et al., 2023; Yao et al., 2023; Yates et al., 2007; Zhang\net al., 2024a). It also adds to a growing body of RAG approaches that use a knowledge graph as\nan index (Gao et al., 2023). Some techniques use subgraphs, elements of the graph, or properties\n\n- f the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023)\n\n- r as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n","hi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n(e.g., passages, tables) and edges encoding lexical and semantical similarity or structural relationships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of\ngraphs in this context: their inherent _modularity_ (Newman, 2006) and the ability to partition graphs\ninto nested modular communities of closely related nodes (e.g., Louvain, Blondel et al. 2008; Leiden, Traag et al. 2019). Specifically, GraphRAG recursively creates increasingly global summaries\nby using the LLM to create summaries spanning this community hierarchy.\n\n\n**2.3** **Adaptive benchmarking for RAG Evaluation**\n\n\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthese benchmarks are oriented towards vector RAG performance, i.e., they evaluate performance\n\n- n explicit fact retrieval. In this work, we propose an approach for generating a set of questions\nfor evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM\nmethods that use a corpus to generate questions whose answers would be summaries of the corpus,\nsuch as in Xu and Lapata (2021). However, in order to produce a fair evaluation, our method avoids\ngenerating the questions directly from the corpus itself (as an alternative implementation, one can\nuse a subset of the corpus held out from subsequent graph extraction and answer evaluation steps).\n\n\n_Adaptive benchmarking_ refers to the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases. Recent work has used LLMs for adaptive benchmarking\nto ensure relevance, diversity, and alignment with the target application or task (Yuan et al., 2024;\nZhang et al., 2024b). In this work, we propose an adaptive benchmarking approach to generating\nglobal sensemaking queries for the LLM. Our approach builds on prior work in LLM-based persona\ngeneration, where the LLM is used to generate diverse and authentic sets of personas (","\u223c_\n\n- verlaps between chunks ( 1 million tokens).\n\n\n**News articles** . A benchmark dataset comprised of news articles published from September 2013\nto December 2023 in a range of categories, including entertainment, business, sports, technology,\nhealth, and science (Tang and Yang, 2024). The corpus is divided into 3197 _\u00d7_ 600-token text\n\n_\u223c_\nchunks, with 100-token overlaps between chunks ( 1.7 million tokens).\n\n\n**4.1.2** **Conditions**\n\n\nWe compared six conditions including GraphRAG at four different graph community levels ( **C0**,\n**C1**, **C2**, **C3** ), a text summarization method that applies our map-reduce approach directly to source\ntexts ( **TS** ), and a vector RAG \u201csemantic search\u201d approach ( **SS** ):\n\n\n    - **CO** . Uses root-level community summaries (fewest in number) to answer user queries.\n\n\n    - **C1** . Uses high-level community summaries to answer queries. These are sub-communities\n\n     - f C0, if present, otherwise C0 communities projected downwards.\n\n\n    - **C2** . Uses intermediate-level community summaries to answer queries. These are subcommunities of C1, if present, otherwise C1 communities projected downwards.\n\n\n    - **C3** . Uses low-level community summaries (greatest in number) to answer queries. These\nare sub-communities of C2, if present, otherwise C2 communities projected downwards.\n\n\n    - **TS** . The same method as in Section 3.1.6, except source texts (rather than community\nsummaries) are shuffled and chunked for the map-reduce summarization stages.\n\n\n    - **SS** . An implementation of vector RAG in which text chunks are retrieved and added to the\navailable context window until the specified token limit is reached.\n\n\nThe size of the context window and the prompts used for answer generation are the same across\nall six conditions (except for minor modifications to reference styles to match the types of context\ninformation used). Conditions only differ in how the contents of the context window are created.\n\n\nThe graph index supporting conditions **C0** - **C3** was created using our generic prompts for entity and\nrelationship extraction, with entity types and few-shot examples tailored to the domain of the data.\n\n\n8\n\n\n**4","Microsoft, 2023)\nand intelligence analysis (Ranade and Joshi, 2023). Given a sensemaking query and a text with an\nimplicit and interconnected set of concepts, an LLM can generate a summary that answers the query.\nThe challenge, however, arises when the volume of data requires a RAG approach, since vector RAG\napproaches are unable to support sensemaking over an entire corpus.\n\n\nIn this paper, we present **GraphRAG** - a graph-based RAG approach that enables sensemaking over\nthe entirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph,\nwhere nodes correspond to key entities in the corpus and edges represent relationships between those\nentities. Next, it partitions the graph into a hierarchy of communities of closely related entities,\nbefore using an LLM to generate community-level summaries. These summaries are generated in\na bottom-up manner following the hierarchical structure of extracted communities, with summaries\nat higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these\ncommunity summaries provide global descriptions and insights over the corpus. Finally, GraphRAG\nanswers queries through map-reduce processing of community summaries; in the map step, the\nsummaries are used to provide partial answers to the query independently and in parallel, then in the\nreduce step, the partial answers are combined and used to generate a final global answer.\n\n\nThe GraphRAG method and its ability to perform global sensemaking over an entire corpus form\nthe main contribution of this work. To demonstrate this ability, we developed a novel application\n\n- f the LLM-as-a-judge technique (Zheng et al., 2024) suitable for questions targeting broad issues\nand themes where there is no ground-truth answer. This approach first uses one LLM to generate\na diverse set of global sensemaking questions based on corpus-specific use cases, before using a\nsecond LLM to judge the answers of two different RAG systems using predefined criteria (defined\nin Section 3.3). We use this approach to compare GraphRAG to vector RAG on two representative\nreal-world text datasets. Results show GraphRAG strongly outperforms vector RAG when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https:\/\/github _._ [com\/microsoft\/graphrag. In ad-](https:\/\/github.com\/microsoft\/graphrag)\ndition, versions"," when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https:\/\/github _._ [com\/microsoft\/graphrag. In ad-](https:\/\/github.com\/microsoft\/graphrag)\ndition, versions of the GraphRAG approach are also available as extensions to multiple opensource libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), NebulaGraph (NebulaGraph, 2024), and Neo4J (Neo4J, 2024).\n\n\n**2** **Background**\n\n\n**2.1** **RAG Approaches and Systems**\n\n\nRAG generally refers to any system where a user query is used to retrieve relevant information from\nexternal data sources, whereupon this information is incorporated into the generation of a response\nto the query by an LLM (or other generative AI model, such as a multi-media model). The query and\nretrieved records populate a prompt template, which is then passed to the LLM (Ram et al., 2023).\nRAG is ideal when the total number of records in a data source is too large to include in a single\nprompt to the LLM, i.e. the amount of text in the data source exceeds the LLM\u2019s context window.\n\n\nIn canonical RAG approaches, the retrieval process returns a set number of records that are semantically similar to the query and the generated answer uses only the information in those retrieved\nrecords. A common approach to conventional RAG is to use text embeddings, retrieving records\nclosest to the query in vector space where closeness corresponds to semantic similarity (Gao et al.,\n2023). While some RAG approaches may use alternative retrieval mechanisms, we collectively refer\nto the family of conventional approaches as _vector RAG_ . GraphRAG contrasts with vector RAG in\nits ability to answer queries that require global sensemaking over the entire data corpus.\n\n\n2\n\n\nGraphRAG builds upon prior work on advanced RAG strategies. GraphRAG leverages summaries\n\n- ver large sections of the data source as a form of \u201dself-memory\u201d (described in Cheng et al. 2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao"],"38":[" [51, 52] propose a single, pre-trained encoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nand generative tasks. Our work aims to expand the space of possible tasks with a single, unified\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\n\n\n**Learned Retrieval** There is significant work on learning to retrieve documents in information\nretrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\nwork optimizes the retrieval module to aid in a specific, downstream task such as question answering,\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\ncan be fine-tuned for strong performance on a variety of tasks.\n\n\n**Memory-based Architectures** Our document index can be seen as a large external memory for\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\nwork. Other work improves the ability of dialog models to generate factual text by attending over\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\u2019s\nmemory by editing the document index. This approach has also been used in knowledge-intensive\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\nrather than end-to-end learnt retrieval [9].\n\n\n**Retrieve-and-Edit approaches** Our method shares some similarities with retrieve-and-edit style\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\nto provide a final output. These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content","downstream models, our system can match the best published result while being 10x faster.\n\n\n2 METHOD\n\n\n2.1 PROBLEM DEFINITION\n\n\nThe retrieval task considered in this work can be described as follows (see also Figure 1). Given a\nmulti-hop question _q_ and a large text corpus _C_, the retrieval module needs to retrieve a sequence of\npassages _Pseq_ : _{p_ 1 _, p_ 2 _, ..., pn}_ that provide _sufficient_ information for answering _q_ . Practically, the\nretriever returns the _k_ best-scoring sequence candidates, _{Pseq_ [1] _[,][ P]_ _seq_ [2] _[, ...,][ P]_ _seq_ _[k]_ _[}]_ [ (] _[k][ \u226a|C|]_ [), with the]\nhope that at least one of them has the desired qualities. _k_ should be small enough for downstream\nmodules to process in a reasonable time while maintaining adequate recall. In general, retrieval also\nneeds to be efficient enough to handle real-world corpora containing millions of documents.\n\n\n2.2 MULTI-HOP DENSE RETRIEVAL\n\n\n**Model** Based on the sequential nature of the multi-hop retrieval problem, our system solves it in an\niterative fashion. We model the probability of selecting a certain passage sequence as follows:\n\n\n\n_P_ ( _Pseq|q_ ) =\n\n\n\n_n_\n\n- _P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) _,_\n\n\n_t_ =1\n\n\n2\n\n\nPublished as a conference paper at ICLR 2021\n\n\nwhere for _t_ = 1, we only condition on the original question for retrieval. At each retrieval step, we\nconstruct a new query representation based on previous results and the retrieval is implemented as\nmaximum inner product search over the dense representations of the whole corpus:\n\n\nexp ( _\u27e8_ _**p**_ _t,_ _**q**_ _t\u27e9_ )\n_P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) =\n\n~~\ufffd~~ _p\u2208C_ [exp (] _[\u27e8]_ _**[p]**_ _[,]_ _**[ q]**_ _[t][\u27e9]_ [)] _[,]_"," Patterns:_ Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration\nto address specific challenges. This goes beyond the fixed\nstructures of Naive and Advanced RAG, characterized by a\nsimple \u201cRetrieve\u201d and \u201cRead\u201d mechanism. Moreover, Modular\nRAG expands this flexibility by integrating new modules or\nadjusting interaction flow among existing ones, enhancing its\napplicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read [7]model\nleverage the LLM\u2019s capabilities to refine retrieval queries\nthrough a rewriting module and a LM-feedback mechanism\nto update rewriting model., improving task performance.\nSimilarly, approaches like Generate-Read [13] replace traditional retrieval with LLM-generated content, while ReciteRead [22] emphasizes retrieval from model weights, enhancing the model\u2019s ability to handle knowledge-intensive tasks.\nHybrid retrieval strategies integrate keyword, semantic, and\nvector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings\n(HyDE) [11] seeks to improve retrieval relevance by focusing\n\n- n embedding similarities between generated answers and real\ndocuments.\n\nAdjustments in module arrangement and interaction, such\nas the Demonstrate-Search-Predict (DSP) [23] framework\nand the iterative Retrieve-Read-Retrieve-Read flow of ITERRETGEN [14], showcase the dynamic use of module outputs to bolster another module\u2019s functionality, illustrating a\nsophisticated understanding of enhancing module synergy.\nThe flexible orchestration of Modular RAG Flow showcases\nthe benefits of adaptive retrieval through techniques such as\nFLARE [24] and Self-RAG [25]. This approach transcends\nthe fixed RAG retrieval process by evaluating the necessity\n\n- f retrieval based on different scenarios. Another benefit of\na flexible architecture is that the RAG system can more\neasily integrate with other technologies (such as fine-tuning\n\n- r reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\n\n\n_D. RAG vs Fine-tuning_\n\nThe augmentation of LLMs has attracted considerable attention due to their growing prevalence. Among the optimization\n\n\n\n5\n\n\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt engineering. Each method has distinct characteristics"," a\nstraightforward way of expressing information\nneeds for retrieval is to generate \u201c[Search(query)]\u201d\nwhen additional information is needed (Schick\net al., 2023), e.g., \u201cThe colors on the flag of\nGhana have the following meanings. Red is for\n\n[Search(Ghana flag red meaning)] the blood of martyrs, ...\u201d When working with GPT-3.5 models that\n\n\nSearch results:  !!\n\n[2]: \u2026\n\n\n\nretrieval instructions might not be reliable. Therefore, we propose a more direct way of forwardlooking active retrieval that uses the next sentence\nto decide when and what to retrieve.\n\n\n**3.2.1** **Confidence-based Active Retrieval**\n\n\n\n\n\n_Retriever_\n\n\n\n\n\n\n\n$\n\n\n&#\n\n\n%$\n\n\n&$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%\n\n\n&%\n\n\n\n\n\n\n\nFigure 2: An illustration of forward-looking active retrieval augmented generation with retrieval instructions\n(FLAREinstruct). It iteratively generates search queries\n(shown in _gray italic_ ) to retrieve relevant information to\naid future generations.\n\n\n- ffer only API access, we elicit such behavior by\nfew-shot prompting (Brown et al., 2020).\nSpecifically, for a downstream task, we place\nthe search-related instruction and exemplars at the\nbeginning as skill 1, followed by the instruction and\nexemplars of the downstream task as skill 2. Given\na test case, we ask LMs to combine skills 1 and 2 to\ngenerate search queries while performing the task.\nThe structure of the prompt is shown in Prompt 3.1,\nand full details can be found in Prompt D.3.\n\n\n\nAs shown in Figure 1, at step _t_, we first generate a\ntemporary next sentence \u02c6 _**s**_ _t_ = LM([ _**x**_ _,_ _**y**_ _<t_ ]) with\n- ut conditioning on retrieved documents. Then we\ndecide whether to trigger retrieval and formulate\nqueries based on \u02c6 _**s**_ _t_ . If the LM is confident about \u02c6 _**s**_ _t_,\nwe accept it without retrieving additional information; if not, we use \u02c6 _**s**_ _t_ to formulate search queries\n_**q**_ _t_ to retrieve relevant documents, and then regenerate the next sentence _**s**_","ERT-based en\ncoders. The retrieval score are computed via inner\nproducts between vectors.\n\n\n**Task-specific** **Retrieval** Similarity-based retrieval is based on a simple heuristic. That is, the\nmore _**x**_ _[r]_ resembles with _**x**_, the more likely _**x**_ _[r]_\n\nand _**y**_ _[r]_ will help the generation. However, the\nmost similar one by universal textual similarity\ndoes not necessarily serve the best for downstream\nmodels. Ideally, the retrieval metric would be\nlearned from the data in a task-dependent way: we\nwish to consider a memory only if it can indeed\nboost the quality of final generation. To this end,\nCai et al. (2021) propose to unify the memory\nretriever and its downstream generation model\ninto a learnable whole. Such memory retrieval is\nend-to-end optimized for _task-specific_ - bjectives.\n\n\n**2.4** **Integration**\n\n\n**Data Augmentation** There are several ways to\nintegrate the retrieved external memory in generation. One straightforward way is _data augmen-_\n_tation_, which constructs some augmented inputs\nby concatenating spans from _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ with the\n\n- riginal input _**x**_ . By training on the augmented\ninputs, a generation model implicitly leans how\nto integrate the retrieved information. Despite the\nsimplicity, this kind of methods works efficiently\nin lots of tasks (Song et al., 2016; Weston et al.,\n2018; Bulte and Tezcan, 2019).\n\n\n\n**Attention** **Mechanisms** Another integration\nmethod is based - n _attention_ _mechanisms_\n\n(Bahdanau et al., 2014). The main idea of this\nfashion is adopting additional encoders (in various\narchitectures) to encode retrieved target sentences,\nand integrate them through attention (Cao and\nXiong, 2018; Gu et al., 2018; Bapna and Firat,\n2019). Since the attention mechanism is becoming\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\nkey module in lots of"],"39":[".1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7 **81.3**\n\n\nDPR **79.4** 78.8 **75.0** **89.1** 51.6 **86.0** 84.7 **82.9** 93.9 67.6\nMulti\nBM25 + DPR 78.0 **79.9** 74.7 88.5 66.2 83.9 84.4 82.3 **94.1** 78.6\n\n\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20\/100 retrieved\npassages that contain the answer. _Single_ and _Multi_ denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.\n\n\n\ntraditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10 _[\u2212]_ [5] using Adam, linear\nscheduling with warm-up and dropout rate 0 _._ 1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be desirable to obtain a single retriever that works well\nacross the board. To this end, we train a _multi_ dataset encoder by combining training data from\nall datasets excluding SQuAD. [8] In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages","**\n\n\nBoth datasets assume 2 hops, so we fix _n_ = 2 for all experiments. Since HotpotQA does not provide\nthe order of the passage sequences, as a heuristic, we consider the passage that includes the answer\nspan as the final passage. [2] In \u00a73.1.3, we show that the order of the passages is important for effective\nretriever training. The hyperparameters can be found in Appendix B.1.\n\n\n3.1 EXPERIMENTS: RETRIEVAL\n\n\nWe evaluate our multi-hop dense retriever (MDR) in two different use cases: _direct_ and _reranking_,\nwhere the former outputs the top- _k_ results directly using the retriever scores and the latter applies a\ntask-specific reranking model to the initial results from MDR.\n\n\n3.1.1 DIRECT\n\n\nWe first compare MDR with several efficient retrieval methods that can directly find the top- _k_ passage\nsequences from a large corpus, including TF-IDF, TF-IDF + Linked, DrKIT and Entity Linking. **TF-**\n**IDF** is the standard term-matching baseline, while **TF-IDF + Linked** is a straightforward extension\nthat also extracts the hyperlinked passages from TF-IDF passages, and then reranks both TF-IDF\nand hyperlinked passages with BM25 [3] scores. **DrKIT** (Dhingra et al., 2020) is a recently proposed\ndense retrieval approach, which builds a entity-level (mentions of entities) dense index for retrieval.\nIt relies on hyperlinks to extract entity mentions and prunes the search space with a binary mask\nthat restricts the next hop to using hyperlinked entities. On FEVER, we additionally consider an\nentity linking baseline (Hanselowski et al., 2018) that is commonly used in existing fact verification\npipelines. This baseline first uses a constituency parser to extract potential entity mentions in the fact\nclaim and then uses the MediaWiki API to search documents with titles that match the mentions.\n\n\nTable 1 shows the performance of different retrieval methods. On HotpotQA the metric is recall at the\ntop _k_ paragraphs [4], while on FEVER the metrics are precision, recall and F1 in order to be consistent\nwith previous results. On both datasets, MDR substantially outperforms all baselines.\n\n\n3.1.","## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar","models (PaLM-540B), and require fine-tuning to\n\n- utperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be\neffective for smaller models without any training.\nWhile a direct comparison with these approaches is\nnot straightforward (difference in knowledge corpus, LLMs, examples), we find that our ODQA\nperformance is much higher than all their reported\nnumbers where available (\u00a75).\n\n\n**Supervised** **Multi-Step** **Open-Domain** **QA.**\nPrior work has explored iterative retrieval for\n\n- pen-domain QA in a fully supervised setting. Das\net al. (2019) proposes an iterative retrieval model\nthat retrieves using a neural query representation\nand then updates it based on a reading comprehension model\u2019s output. Feldman and El-Yaniv\n(2019) apply similar neural query reformulation\nidea for multihop open-domain QA. Xiong et al.\n(2021) extends the widely-used Dense Passage\nRetrieval (DPR) (Karpukhin et al., 2020) to\nmultihop setting, which has since been improved\nby Khattab et al. (2021). Asai et al. (2020)\nleverages the graph structure induced by the entity\nlinks present in Wikipedia paragraphs to perform\niterative multi-step retrieval. GoldEn (Gold Entity)\nretriever (Qi et al., 2019) iteratively generates\ntext queries based on paragraphs retrieved from\nan off-the-shelf retriever but requires training\ndata for this next query generator. Nakano et al.\n(2021) used GPT3 to answer long-form questions\nby interacting with the browser but relied on\nhuman annotations of these interactions. All of\n\nthese methods rely on supervised training on a\nlarge-scale dataset and can not be easily extended\nto a few-shot setting.\n\n\n\n**3** **Chain-of-Thought-Guided Retrieval**\n**and Open-Domain QA**\n\n\nOur goal is to answer a knowledge-intensive multistep reasoning question _Q_ in a few-shot setting\nby using a knowledge source containing a large\nnumber of documents. To do this we follow a\n\nretrieve-and-read paradigm (Zhu et al., 2021),\nwhere the retriever first retrieves documents from\nthe knowledge source and the QA model reads the\nretrieved documents and the question to","score and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is twice as large as the extractive\nmodels and has 770M parameters. We reuse the hyperparameters as described in Izacard & Grave\n(2020). The original FiD uses the top 100 passages for NaturalQuestions. In our case, we use the top\n50 retrieved passage sequences and concatenate the passages in each sequence before feeding into T5.\nIn order to fit this model into GPU, we make use of PyTorch checkpoint [11] for training.\n\n\nB.2.3 MULTI-HOP RAG\n\n\nThe RAG model aims to generate answer _y_ given question _x_ and the retrieved documents _z_ . Similarly,\nthe goal of multi-hop RAG can be expressed as: generate answer _y_ given question _x_ and retrieved\ndocuments in hop one _z_ 1 and hop two _z_ 2 (Limiting to two hops for HotpotQA). The model has three\ncomponents:\n\n\n   - Hop-one retriever _p\u03b7_ 1( _z_ 1 _|x_ ) with parameter _\u03b7_ 1 to represent the retrieved top-k passage\ndistribution (top-k truncated distribution) given the input question _x_ .\n\n\n    - Hop-two retriever _p\u03b7_ 2( _z_ 2 _|x, z_ 1) with parameter _\u03b7_ 2 to represent the hop-two retrieved top-k\npassage distribution given not only the question _x_ but also the retrieved document _z_ 1 from\nhop-one.\n\n\n   - A generator _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _,, y_ 1: _i\u2212_ 1) to represent the next token distribution given input\nquestion _x_, hop-one retrieved document _z_ 1, hop-two retrieved document _z_ 2 and previous\npredicted token _y_ 1: _i\u2212_ 1 parametrized by _\u03b8_\n\n\n**Multi-Hop RAG Sequence Model** As the RAG Sequence model, this model generates the answer\nsequence given the fixed set of documents from hop-one retriever and hop-two retriever. In order"],"40":[" in improving\nfactuality and citation accuracy for long-form generations relative to these models. [1]\n\n\n1 INTRODUCTION\n\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces **Self-Reflective Retrieval-augmented Generation (SELF-RAG)** to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., _reflection tokens_ ). Reflection tokens are categorized into _retrieval_ and _critique_ tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a **retrieval** token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and then **generating** corresponding task outputs (Step\n2). It then generates critique tokens to **criticize** its own output and choose best one (Step 3) in terms\n\n- f factuality and overall quality. This process differs from conventional RAG ("," As assessed in Sun et al. (2024a), smaller\n\n- pen-source LLMs generally fall short of their proprietary\ncounterparts in trustworthiness metrics. Therefore, considering trustworthiness alongside the distillation of capabilities into student models is crucial. It is imperative that\nfuture research on KD not only enhances the capabilities\n\n- f student models but also ensures that broader aspects of\ntrustworthiness are meticulously addressed.\n\n\n_**Weak-to-strong Distillation.**_ The concept of \u201cweak-tostrong generalization\u201d in LLMs (Burns et al., 2023) emphasizes the potential to leverage weak supervision to elicit\nthe advanced capabilities of more powerful models. This\napproach challenges the traditional distillation paradigm by\nsuggesting that even with limited or imperfect supervision,\nit is possible to enhance the performance of LLMs significantly. This necessitates exploring innovative strategies\nthat enable weaker models to guide the learning process\n\n- f stronger ones effectively, highlighting the importance\n\n- f developing methods that can bridge the gap between\nthese models. Such research could unlock new avenues\nfor improving LLMs\u2019 efficiency and effectiveness, making\nthe pursuit of \u201cweak-to-strong distillation\u201d a crucial area\nfor future investigations in this LLM era. Initially, Burns\net al. (2023) investigates whether weak model supervision\ncan unlock the full capabilities of much stronger models.\nThrough experiments with pre-trained language models in\nthe GPT-4 family across NLP, chess, and reward modeling\ntasks, it finds that finetuning strong models on weak labels\nleads to better performance than their weak supervisors,\ndemonstrating weak-to-strong generalization. Then, Li et al.\n\n\n(2024g) introduce Superfiltering, a method that employs\nsmaller, weaker models like GPT-2 to select high-quality\ndata for fine-tuning larger, more capable models such as\nLLaMA2. This approach is rooted in discovering a strong\nconsistency in evaluating instruction tuning data difficulty\nacross models of varying sizes. More recently, Ji et al. (2024)\nintroduce Aligner, a novel approach for aligning LLMs with\nhuman values and intentions by utilizing weak supervisory\nsignals from smaller models to improve the performance\n\n- f larger models. However, Burns et al. (2023) find that\nachieving the full capabilities of strong models requires\nmore than naive fin","_4.4.2_ _Natural Language Generation_\n\n\nNatural Language Generation (NLG) is a key aspect of evaluating the capabilities of LLMs, encompassing tasks such as\nsummarization, machine translation, and other open-ended\ntext generation tasks. Known for their potent generative\nabilities and creativity, LLMs excel in these areas, making\nthem prime sources for distilling knowledge into student\nmodels tailored for NLG tasks (Xu et al., 2023c, 2024b;\nRamnath et al., 2023; Agarwal et al., 2024). Additionally,\nthe knowledge distilled from LLMs can be effectively used\nfor NLG task-specific data augmentation (Jung et al., 2023;\nWang et al., 2021b; Guo et al., 2023a; Yang and Nicolai,\n2023; Wang et al., 2023h; Yang et al., 2023d). While the\nprevious sections have focused on the works about openended generation and multi-turn dialogue, this part will\n\n\nspecifically highlight the distillation techniques relevant to\n\n- ther NLG tasks.\nAlthough automatic metrics often favor smaller, finetuned models in summarization tasks, human evaluators\ntend to prefer the summaries generated by LLMs. Addressing this discrepancy, Xu et al. (2023c) develop a student summarization model by distilling a GPTSUMM dataset, which\ncomprises over 4 million paragraph-summary pairs generated by querying GPT-3.5. In a different approach, Jung et al.\n(2023) introduce \u2018Impossible Distillation,\u2019 a method that\ncreates high-quality summarization-specific dataset from\nweak teacher LLMs. This method involves training a student model on the generated dataset and enhancing its\ncapabilities through Self-Knowledge. Turning to the task of\nmachine translation, where creating parallel corpora is traditionally expensive and time-consuming, Yang and Nicolai\n(2023) propose a three-step distillation process. This process\ninvolves generating seeds of verbs and nouns, forming sentences, and then translating these sentences. Their findings\nsuggest that while the distilled dataset may lack diversity,\nit effectively improves the translation signal for training\nstudent translation models. To distill high-quality contentgrounded data automatically, Genie (Yehudai et al., 2024)\nproposes a"," still retains\nits fundamental role in compressing LLMs, making them\nmore efficient without significant loss in performance. (Gu\net al., 2024; Agarwal et al., 2024). More recently, the strategy\n\n- f employing open-source LLMs as teachers for their own\nself-improvement has emerged as a promising approach,\nenhancing their capabilities significantly (Yuan et al., 2024a;\nChen et al., 2024a). Figure 1 provides an illustration of these\nthree key roles played by KD in the context of LLMs.\nA key aspect of the knowledge distillation is the enhancement of skills such as advanced context following\n\n\n\n2\n\n\n\u2462\n\n\nSelf-Improvement\n\n\n\u2460 \u2461\n\nAdvance Compress\n\n\nClosed-Source LLMs Open-Source LLMs Smaller LMs\n\n\nDirection of KD\n\n\nFig. 1: KD plays three key roles in LLMs: 1) Primarily\nenhancing capabilities, 2) offering traditional compression\nfor efficiency, and 3) an emerging trend of self-improvement\nvia self-generated knowledge.\n\n\n(e.g., in-context learning (Huang et al., 2022a) and instruction following (Taori et al., 2023)), improved alignment with user intents (e.g., human values\/principles (Cui\net al., 2023a), and thinking patterns like chain-of-thought\n(CoT) (Mukherjee et al., 2023)), and NLP task specialization\n(e.g., semantic understanding (Ding et al., 2023a), and code\ngeneration (Chaudhary, 2023)). These skills are crucial for\nthe wide array of applications that LLMs are expected\nto perform, ranging from casual conversations to complex problem-solving in specialized domains. For instance,\nin vertical domains like healthcare (Wang et al., 2023a),\nlaw (LAW, 2023), or science (Zhang et al., 2024), where\naccuracy and context-specific knowledge are paramount,\nknowledge distillation allows open-source models to significantly improve their performance by learning from the\nproprietary models that have been extensively trained and\nfine-tuned in these areas.\nThe benefits of knowledge distillation in the era of\nLLMs are multifaceted and transformative (Gu et al., 2024).\nThrough"," question answering tasks\nand to generate with tree search, guided by LM-generated value scores. While their value function\nsimply indicates an overall score of each generation, SELF-RAG trains to an arbitrary LM to learn to\ngenerate fine-grained self-reflection and customizable inference.\n\n\n**Training and generating with critics.** Training LLMs with reinforcement learning (e.g., Proximal\nPolicy Optimization or PPO; Schulman et al. 2017) from human feedback (RLHF) has proven\neffective in aligning LLMs with human preferences (Ouyang et al., 2022). Wu et al. (2023) introduce\nfine-grained RLHF with multiple reward models. Though our work also studies fine-grained critique\n\n- n retrieval and generation, we train our target LM on task examples augmented with reflection\ntokens from a critic model offline, with a far lower training cost compared to RLHF. In addition,\nreflection tokens in SELF-RAG enable controllable generation at inference, while RLHF focuses on\nhuman preference alignment during training. Other works use general control tokens to guide LM\ngeneration (Lu et al., 2022; Korbak et al., 2023), while SELF-RAG uses reflection tokens to decide the\nneed for retrieval and to self-evaluate generation quality. Xie et al. (2023) propose a self-evaluationguided decoding framework, but they focus only on reasoning tasks with one evaluation dimension\n(reasoning path consistency) and without retrieval. Recent work on LLM refinement (Dhuliawala\net al., 2023; Madaan et al., 2023; Paul et al., 2023) prompts a model to generate task output, natural\nlanguage feedback and refined task output iteratively, but at the cost of inference efficiency.\n\n\n3 SELF-RAG: LEARNING TO RETRIEVE, GENERATE AND CRITIQUE\n\n\nWe introduce Self-Reflective Retrieval-Augmented Generation (SELF-RAG), shown in Figure 1.\nSELF-RAG is a framework that enhances the quality and factuality of an LLM through retrieval and\nself-reflection, without sacrificing LLM\u2019s original creativity and versatility. Our end-to-end training\nlets an LM _M_ **generate** text informed by **retrieved** passages, if needed, and **criticize** the output by\nlearning to generate special tokens. These _reflection tokens"],"41":[" with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does not know, it can still refer to the context.\n\n\n4.1 Results\n\n\nUsing the above datasets and baselines, we evaluate our model RAFT and demonstrate\nthe effectiveness of RAFT in Tab. 1. We see that RAFT consistently and significantly\n\n- utperforms the baselines. Compared with the base Llama-2 instruction-tuned model,\nRAFT with RAG does much better in terms of extracting information as well as being\nrobust towards distractors. The gain can be as big as 35.25% on Hotpot QA and 76.35% on\nTorch Hub evaluation. Compared with DSF on the specific dataset, our model does better at\nrelying on the provided context to solve the problem. RAFT does much better on the tasks\nlike Hotpot and HuggingFace datasets (30.87% on Hotpot and 31.41% on HuggingFace).\nNote that for PubMed QA, since it is a binary yes\/no question, we don\u2019t observe significant\ngains when we compare our model with DSF + RAG. Even compared with a much larger\nand better model GPT-3.5, RAFT demonstrates significant advantages.\n\n\nOverall, the LLaMA-7B model, both with and without the RAG, performs poorly due to its\nanswering style not aligning with the ground truth. By applying domain-specific tuning,\nwe significantly enhance its performance. This process enables the model to learn and adopt\nthe appropriate style of answering. However, introducing RAG to a domain-specifically\nfine-tuned (DSF) model doesn\u2019t invariably lead to better outcomes. This might indicate that\nthe model lacks training in context processing and extracting useful information from it. By\nincorporating our method, RAFT, we train the model not only to match its answering style\nwith that required but also to improve its document processing capabilities. Consequently,\n\n- ur approach outperforms all others.\n\n\n4.2 Effect of CoT\n\n\nWe also conduct an analysis to evaluate the effectiveness of the Chain-of-Thought approach\nin enhancing the model\u2019s performance. As indicated in Table 2, simply providing the answer\nto a question"," **RE** trieval augmented generation\n( **FLARE** ), as illustrated in Figure 1. FLARE iteratively generates _a temporary next sentence_, use\nit as the query to retrieve relevant documents _if it_\n_contains low-probability tokens_ and regenerate the\nnext sentence until reaches the end.\n\n\nFLARE is applicable to any existing LMs at\ninference time without additional training. Con\n\nsidering the impressive performance achieved by\nGPT-3.5 (Ouyang et al., 2022) on a variety of\ntasks, we examine the effectiveness of our meth\n- ds on text-davinci-003. We evaluate FLARE\n\n- n 4 diverse tasks\/datasets involving generating\nlong outputs, including multihop QA (2WikiMultihopQA), commonsense reasoning (StrategyQA),\nlong-form QA (ASQA), and open-domain summarization (WikiAsp) (Ho et al., 2020; Geva et al.,\n2021; Stelmakh et al., 2022; Hayashi et al., 2021).\nOver all tasks, FLARE achieves superior or competitive performance compared to single-time and\nmulti-time retrieval baselines, demonstrating the\neffectiveness and generalizability of our method.\n\n\n**2** **Retrieval Augmented Generation**\n\n\nWe formally define single-time retrieval augmented\ngeneration and propose the framework of active\nretrieval augmented generation.\n\n\n**2.1** **Notations and Definitions**\n\n\nGiven a user input _**x**_ and a document corpus _D_ =\n_{_ _**d**_ _i}_ _[|D|]_ _i_ =1 [(such as all Wikipedia articles), the goal of]\nretrieval augmented LMs is to generate the answer\n_**y**_ = [ _**s**_ 1 _,_ _**s**_ 2 _, ...,_ _**s**_ _m_ ] = [ _w_ 1 _, w_ 2 _, ..., wn_ ] containing\n_m_ sentences or _n_ tokens leveraging information\nretrieved from the corpus.\nIn retrieval augmented LM, the LM typically\npairs with a retriever that can retrieve a list of\ndocuments _D_ _**q**_ = ret( _**q**_ ) for a query _**q**_ ; the LM\nconditions on both the user input"," factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n\n\n**6.2** **Ablation Study**\n\n\n**Importance of forward-looking retrieval.** We\nfirst validate that forward-looking retrieval is more\neffective than past-context-based retrieval. We run\nablation experiments on 2WikiMultihopQA and\nASQA-hint comparing retrieval using the previ\n- us versus the next sentence. Specifically, both\nmethods retrieve every sentence and directly use\nthe complete previous\/next sentence as queries. As\nshown in Table 3, using the next sentence to retrieve\nis clearly better than using the previous sentence,\nconfirming our hypothesis.\nWe also run previous-window approaches using\ndifferent numbers of past tokens as queries. As\nshown in Table 4, using too many tokens ( _>_ 32) in\n\n\n\nFigure 5: Performance (EM) of FLARE with respect\nto the percentage of steps\/sentences with retrieval on\n2WikiMultihopQA and StrategyQA.\n\n\nthe past hurts the performance, further confirming\n\n- ur hypothesis that previous context might not be\nrelevant to intent of future generations.\n\n\n**Importance of active retrieval.** Next, we investigate how active retrieval threshold _\u03b8_ affects performance. To alter our method from not retrieving\nto retrieving every sentence, we adjust the confidence threshold _\u03b8_ that determines when to trigger\nretrieval from 0 to 1. We then calculate the proportion of steps\/sentences where retrieval is activated, and present the performance based on it. As\nshown in Figure 5, on 2WikiMultihopQA, the performance plateaus when the retrieval percentage\nexceeds 60%, indicating that retrieval when LMs\nare confident is not necessary. On StrategyQA, the\nperformance drops when the retrieval percentage\nexceeds 50%, indicating that unnecessary retrieval\ncan introduce noise and impede the original generation process. We found triggering retrieval for\n40%-80% of sentences usually leads to a good performance across tasks\/datasets.\n\n\n**Effectiveness of different query formulation**\n**methods** We study implicit query formation by\nmasking and explicit query formulation through\nquestion generation. In Table 5, we compare the\nperformance of FLARE with different masking\n\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n"," study implicit query formation by\nmasking and explicit query formulation through\nquestion generation. In Table 5, we compare the\nperformance of FLARE with different masking\n\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n\n\n2WikiMultihopQA StrategyQA\n\n\n0.0 25.0 50.0 75.0 100.0\n\n\n%steps\/sentences with retrieval\n\n\n_\u03b2_ **EM** **F** 1 **Prec.** **Rec.**\n\n\n0.0 0.488 0.576 0.571 0.605\n\n0.2 0.498 0.588 0.582 0.616\n\n0.4 0.510 0.597 0.591 0.627\n\n0.6 0.506 0.593 0.586 0.622\n\n\nTable 5: Performance of FLARE with respect to the\nmasking threshold _\u03b2_ - n 2WikiMultihopQA.\n\n\n**ASQA-hint** **WikiAsp**\n**EM D-F** 1 **R-L DR** **UniEval E-F** 1 **R-L**\n\n\nImplicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7\nExplicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6\n\n\nTable 6: A comparison between implicit and explicit\nquery formulation methods in FLARE.\n\n\nthresholds _\u03b2_ . Retrieving directly with the complete\nsentence ( _\u03b2_ = 0) is worse than masking tokens\nwith low probabilities, confirming our hypothesis\nthat low-confidence erroneous tokens can distract\nretrievers. We compare implicit and explicit query\nformulation methods in Table 6. Performances of\n\nboth methods are similar, indicating that both meth\n- ds can effectively reflect information needs.\n\n\n**7** **Related Work**\n\n\nWe refer to subsection 2.2 and section 4 for ex\ntensively discussion on single-time and multi-time\nretrieval augmented LMs, which is the most relevant area to this paper.\n\n\n**Iterative and adaptive retrieval** Iterative retrieval and refinement has been studied in both\ntext and code generation tasks (Peng et al., 202",".0 39.9 27.9 **34.3** 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\n\nFLARE (ours) **77.3** **41.3 28.2 34.3 31.1** **46.2 36.7 37.7 37.2** **53.4** **18.9 27.6**\n\n\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is\nDisambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\n\n\n\n**2WikiMultihopQA** **ASQA-hint**\n**EM** **F** 1 **Prec. Rec.** **EM D-F** 1 **R-L DR**\n\n\nPrevious 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\n\nNext 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\n\n\nTable 3: A head-to-head comparison between using the\nprevious sentence and the next sentence for retrieval.\n\n\n**#Tokens** **EM** **F** 1 **Prec.** **Rec.**\n\n\n16 43.2 52.3 51.7 54.5\n\n32 43.6 52.4 52.0 55.0\n\n48 40.0 49.3 49.0 52.0\n\nAll 39.0 48.5 48.2 51.1\n\n\nTable 4: Previous-window approaches using different\nnumbers of tokens as queries.\n\n\n- us window underperforms single-time retrieval\n\n- n ASQA, which we hypothesize is because the\nprevious window does not accurately reflect future\nintent. Since we focus on evaluating factuality, metrics with an emphasis on factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n\n\n**6.2** **Ablation Study**\n\n\n**Importance of forward-looking"]},"response":{"0":"Retrieval-Augmented Generation (RAG) is a paradigm in text generation that integrates information retrieval techniques into the generation process. This approach allows models to access external memory during text generation, enhancing their ability to produce more informed outputs. The fundamental idea behind RAG is that by retrieving relevant instances from a dataset, models can improve the quality of the generated text.\n\nIn RAG, the generation process can be formulated as:\n\n\\[ y = f(x, z) \\]\n\nwhere \\( y \\) is the generated output, \\( x \\) is the input, and \\( z \\) is a set of relevant retrieved instances. This allows the model to leverage additional data, potentially leading to more accurate and contextually appropriate responses (Li et al., 2022) <2202.01110_rag_survey_li:chunk_900>.\n\nRAG addresses certain challenges faced by large language models, such as hallucinations and factual inaccuracies, by incorporating external knowledge which can improve response reliability (Chen et al., 2023) <2309.01431_rgb_benchmark_chen:chunk_0>. This paradigm has shown remarkable performance across various natural language processing tasks (Li et al., 2022) <2202.01110_rag_survey_li:chunk_0>.","1":"The FiD (Fusion-in-Decoder) architecture was proposed by the authors of the paper titled \"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval,\" specifically by Wenhan Xiong and others in 2020 <document id=\"2009.12756_multihop_qi:chunk_15750\">.","2":"RAFT stands for \"Retrieval-Augmented Fine-Tuning.\" This method is designed to adapt language models to be more effective in domain-specific retrieval-augmented generation scenarios, enhancing their performance by incorporating context from relevant documents <document id=\"2403.10131_raft_zhang:chunk_3600\">.","3":"Hybrid retrieval (combining dense and sparse retrieval) outperforms either approach alone because they exploit very different signals. The overlap in results between dense and sparse retrieval methods is quite small, indicating that they are effective in complementary ways. This complementary nature is what makes hybrid retrieval effective, as it allows for leveraging the strengths of both techniques. In particular, hybrid retrieval techniques have been shown to yield statistically significant gains in accuracy over dense retrieval alone, demonstrating their enhanced effectiveness in retrieval tasks <document id=\"2104.05740_hybrid_retrieval_ma:chunk_5400\">.","4":"ColBERT differs from traditional dense retrieval by implementing a \"late interaction\" paradigm. In traditional dense retrieval, models typically require feeding the query and each document through a neural network, such as BERT, which incurs a high computational cost due to the quadratic attention mechanism in relation to the length of the sequences. This approach becomes inefficient as the number of documents increases.\n\nIn contrast, ColBERT only feeds BERT a single, much shorter sequence of the query, drastically reducing computation. It decouples the query and document processing, allowing for efficient similarity searches using pre-computed document embeddings and vector-similarity indexes. This method retains high effectiveness while enabling scalability to large document collections, thus improving recall without the exhaustive evaluation of all candidates as seen in traditional methods <document id=\"2004.12832_colbert_khattab:chunk_6300\">.","5":"The trade-offs between long-context LLMs and RAG systems can be summarized as follows:\n\n1. **Inference Speed vs. Operational Efficiency**: Long-context LLMs can manage contexts exceeding 200,000 tokens, allowing them to incorporate entire documents directly into prompts. However, providing extensive context at once can significantly slow down inference speed. In contrast, RAG systems use chunked retrieval and on-demand input, which can enhance operational efficiency during processing (<document id=\"2312.10997_rag_survey_gao:chunk_13950\">).\n\n2. **Transparency and Verification**: RAG systems allow for the retrieval process to be observable, meaning users can easily verify the original sources of generated answers. This is crucial for trust and accountability in outputs. Long-context LLMs, while capable of managing large amounts of information, may produce results in a more opaque manner (<document id=\"2312.10997_rag_survey_gao:chunk_13950\">).\n\n3. **Role in Complex Problems**: The capability of long-context LLMs to process a vast amount of information enables them to address more complex questions and integrative tasks. This provides an opportunity for evolving RAG methods that can leverage longer contexts to solve similar complex issues (<document id=\"2312.10997_rag_survey_gao:chunk_13950\">).\n\n4. **Robustness to Noise**: RAG systems can be adversely affected by noise or contradictory information in retrieved documents, which may lead to misinformation. On the other hand, long-context LLMs may avoid this issue since they process information in a more unified context, though they might struggle with identifying relevant information when it's not effectively segmented (<document id=\"2312.10997_rag_survey_gao:chunk_13950\">).\n\nOverall, while long-context LLMs offer advantages in handling extensive information and addressing complex problems, RAG systems provide operational benefits, greater transparency, and mechanisms for verification, especially in scenarios where sourcing and accuracy are critical.","6":"GraphRAG and FiD (Fusion-in-Decoder) differ significantly in their approaches to multi-document reasoning. \n\n**GraphRAG** focuses on leveraging the modularity of graphs to create summaries from documents. It builds a hierarchy of community summaries that allows for recursive summarization, leading to global understanding of the dataset. This approach enables GraphRAG to handle sensemaking queries that require reasoning over interconnected concepts across multiple documents. The core advantage lies in its ability to generate comprehensive and diverse answers by traversing a community hierarchy, which supports answering complex queries about relationships and overall trends within the data corpus <document id=\"2404.16130_graphrag_edge:chunk_2250\">.\n\nOn the other hand, **FiD** typically merges information from retrieved documents during the decoding process itself. While Fusion-in-Decoder also aims to combine multiple document sources, it does this by integrating relevant information directly into the response generation phase without the hierarchical community structure employed by GraphRAG. This can limit its effectiveness in queries that require complex multi-document reasoning over broader datasets <document id=\"2404.16130_graphrag_edge:chunk_450\">.\n\nIn summary, GraphRAG's graph-based approach allows for more comprehensive reasoning across multiple documents, which is essential for global sensemaking tasks, while FiD primarily focuses on integrating information during the decoding phase, which might not fully address the intricate relationships required for deeper understanding.","7":"The recommended evaluation metrics for both retrieval and generation in Retrieval-Augmented Generation (RAG) systems are as follows:\n\n### Retrieval Quality Metrics:\n1. **Hit Rate**\n2. **Mean Reciprocal Rank (MRR)**\n3. **Normalized Discounted Cumulative Gain (NDCG)**\n\n### Generation Quality Metrics:\n1. **Exact Match (EM)**\n2. **F1 Score**\n3. **Accuracy**\n4. **BLEU**\n5. **ROUGE (including ROUGE-L)**\n6. **BertScore**\n7. **R-Rate (Reappearance Rate)**\n\nAdditionally, evaluation aspects may include:\n- **Context Relevance**\n- **Answer Faithfulness**\n- **Answer Relevance**\n\nFor retrieval, it's essential to assess noise robustness and negative rejection, while for generation, focus is placed on counterfactual robustness and information integration <document id=\"2312.10997_rag_survey_gao:chunk_11700\">.","8":"I don't have enough information in the provided materials to answer this question.","9":"I don't have enough information in the provided materials to answer this question.","10":"The authors of the paper \"Dense Passage Retrieval for Open-Domain Question Answering\" are Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih <document id=\"2004.04906_dpr_karpukhin:chunk_0\">.","11":"BM25 is used in open-domain question answering (QA) as a traditional retrieval method to select candidate passages from a large collection of documents. It is part of the retrieval component that aims to reduce the number of passages a reader needs to consider for finding an answer efficiently. The importance of BM25 lies in its ability to process and retrieve relevant contexts quickly, despite being outperformed by newer dense retrieval methods, such as Dense Passage Retrieval (DPR) in accuracy on various datasets (<document:2004.04906_dpr_karpukhin:chunk_0>).","12":"The purpose of using Wikipedia in open-domain question answering is primarily to provide a rich source of information that models can retrieve and utilize to answer questions. Specifically, Wikipedia serves as a comprehensive corpus for training and evaluating models, facilitating the retrieval of relevant passages to support answers. The effectiveness of this approach relies on the ability of models to search for and integrate information from Wikipedia efficiently, especially in scenarios where multiple supporting documents may need to be analyzed to find the correct answer (<document id=\"2212.10509_cot_retrieval_trivedi:chunk_13500\">).\n\nAdditionally, using Wikipedia allows for a standardized dataset that can be manipulated and repurposed for various questions, enhancing the models' knowledge and retrieval capabilities in an open-domain context (<document id=\"2004.04906_dpr_karpukhin:chunk_10350\">).","13":"Roberts et al. use the dot product of their vector representations to measure the similarity between questions and passages. Specifically, they map the question and passage to \\(d\\)-dimensional vectors using encoders and define the similarity as:\n\n\\[\n\\text{sim}(q, p) = E_Q(q)^\\top E_P(p)\n\\]\n\nThis method allows them to retrieve passages whose vectors are closest to the question vector <document id=\"2004.04906_dpr_karpukhin:chunk_2250\" metadata>.","14":"The purpose of passage embeddings in machine learning, particularly in the context of open-domain question answering, is to effectively represent passages in a way that enables efficient retrieval of relevant information. In this framework, dense representations (embeddings) are learned from a limited set of questions and passages using a dual-encoder system. This allows the retrieval of candidate passages where some may contain the answers to the posed questions. The embeddings facilitate the process of finding passages that are semantically close to a given question, thereby improving retrieval accuracy compared to traditional sparse vector space models like TF-IDF or BM25. This approach leads to significant improvements in performance for end-to-end QA systems across various benchmarks, making it an essential component of modern retrieval strategies (<document 2004.04906_dpr_karpukhin:chunk_0>, <document 2004.04906_dpr_karpukhin:chunk_2700>).","15":"BM25 is used as a traditional retrieval method for open-domain question answering (QA). It serves as a baseline for comparing the effectiveness of newer retrieval techniques, such as Dense Passage Retrieval (DPR). In experiments, BM25 acts as a ranking function that selects top passages based on relevance to a question, and its performance is evaluated against more advanced methods. While BM25 has historically been effective, recent advancements have shown that dense retrieval methods can outperform it in accuracy on various QA datasets (<document id=\"2004.04906_dpr_karpukhin:chunk_900\">).","16":"TREC (Text REtrieval Conference) is used in the context of passage retrieval experiments primarily as a benchmark dataset that provides questions and correct answers. In the experiments, TREC dataset helps in evaluating the performance of retrieval models by allowing researchers to assess how well the models can identify passages that contain the correct answers to the given questions. The specific methodology involves using the highest-ranked passage from a retrieval system, such as BM25, as a positive passage if it contains the answer, which is crucial for model training and evaluation (<document id=\"2004.04906_dpr_karpukhin:chunk_4500\">).","17":"According to Lee et al., the lower performance of SQuAD is attributed to two main reasons. First, the annotators constructed questions after reviewing the corresponding passages, leading to a high lexical overlap between the passages and the questions, which gives a clear advantage to traditional methods like BM25 over dense passage retrieval methods. Second, the dataset draws from only about 500 Wikipedia articles, introducing significant bias in the distribution of training examples, which impacts retrieval performance negatively <document id=\"2004.04906_dpr_karpukhin:chunk_5850\">.<\/document>","18":"The CPU model used to profile the passage retrieval speed for Dense Passage Retrieval (DPR) was the Intel Xeon CPU E5-2698 v4 @ 2.20GHz <document id=\"2004.04906_dpr_karpukhin:chunk_7200\">.","19":"The passage selection score in an end-to-end QA system serves as a mechanism to re-rank passages retrieved from a larger corpus. This score is generated by a neural reader, which evaluates the top retrieved passages and assigns a score to each based on its relevance to the question. The passage with the highest selection score is used to determine the final answer span. This process enhances the precision of the answer extraction by filtering through potential candidates, ensuring that only the most relevant passages are considered for generating answers <document id=\"2004.04906_dpr_karpukhin:chunk_7200\">.","20":"REALMWiki is a model developed for dense passage retrieval, specifically pretrained on Wikipedia. It is part of a family of models, including REALMNews, which is pretrained on CC-News. The model aims to enhance question answering by effectively retrieving relevant passages from the training data <document id=\"2004.04906_dpr_karpukhin:chunk_8550\" metadata>.\n\n","21":"I don't have enough information in the provided materials to answer this question.","22":"The sufficient context autorater helps reduce hallucinations in Retrieval-Augmented Generation (RAG) systems by providing a mechanism for the model to assess whether the input context contains enough information to answer a query accurately. If the context is determined to be insufficient, the autorater can prompt the model to abstain from providing an answer instead of generating potentially incorrect responses, thus minimizing hallucinations.\n\nThis method combines the outputs of the sufficient context autorater with the model\u2019s self-rated confidence scores to fine-tune a selective trade-off between accuracy and coverage. By defining a threshold for when the model should abstain or generate an answer based on the context's sufficiency, the autorater helps to maintain higher accuracy rates while enabling flexible operational settings depending on specific applications, such as medical advice or creative tasks <document id=\"2411.06037_sufficient_context_joren:chunk_14850\">.<\/document>.\n\nAdditionally, the implementation of this autorater reveals that many state-of-the-art large language models (LLMs) commonly output hallucinations when provided with insufficient context. By employing the sufficient context autorater, RAG systems can systematically reduce these instances of hallucination while still striving to answer as many queries correctly as possible <document id=\"2411.06037_sufficient_context_joren:chunk_13500\">.<\/document>","23":"Query rewriting enhances the performance of retrieval-augmented language models (RALMs) by clarifying the retrieval need from the input text. The approach utilizes a framework called **Rewrite-Retrieve-Read**, where an initial query is generated and then refined to efficiently search for relevant contexts needed for answering questions or fulfilling knowledge-intensive tasks. This process adapts the original input to align better with the frozen retriever and the language model reader, effectively bridging the gap between them and improving overall performance <document id=\"2305.14283_query_rewriting_ma:chunk_0\">.\n\nMore specifically, a small trainable language model is employed as a rewriter, trained through reinforcement learning using feedback from the larger frozen LLM, which allows it to learn how to construct queries that yield better retrieval results for tasks like open-domain question answering and multiple-choice QA <document id=\"2305.14283_query_rewriting_ma:chunk_1350\">. Evaluations have shown that this rewriting process consistently leads to improved performance across various tasks, making the retrieval process more precise and effective <document id=\"2305.14283_query_rewriting_ma:chunk_2700\">.","24":"The combination of parametric memory (pre-trained models) with non-parametric memory (retrieval-based mechanisms) in Retrieval-Augmented Generation (RAG) models significantly enhances performance on knowledge-intensive NLP tasks in several ways:\n\n1. **Knowledge Expansion and Revision**: Parametric models have limitations in that they cannot easily expand or revise their memory, which can lead to outdated or incorrect information. By integrating non-parametric memory, RAG models can directly access updated knowledge from external sources, allowing for more relevant and current information to be utilized in responses <document id=\"2005.11401_rag_lewis:chunk_450\">.\n\n2. **Improved Output Quality**: The hybrid approach leads to more specific, diverse, and factual language generation compared to parametric-only models. RAG models have been shown to outperform traditional seq2seq models and achieve state-of-the-art results on various question answering tasks <document id=\"2005.11401_rag_lewis:chunk_900\">.\n\n3. **Enhanced Interpretability**: Non-parametric memory allows for easier inspection and interpretation of the information being used, addressing the \"hallucination\" problem where models generate plausible but incorrect information. This feature increases the reliability of the outputs <document id=\"2005.11401_rag_lewis:chunk_900\">.\n\n4. **Dynamic Knowledge Updates**: The ability to replace the non-parametric memory source (e.g., by switching the retrieval index) enables the model to adapt to new information without needing extensive retraining, making it more flexible and applicable to changing knowledge domains <document id=\"2005.11401_rag_lewis:chunk_1350\">.\n\nIn summary, the integration of parametric and non-parametric memories in RAG models not only enhances their knowledge breadth and accuracy but also facilitates dynamic updates and improves the interpretability of generated responses.","25":"Notable approaches for retrieval-augmented text generation tasks leverage deep learning technology to improve performance across various natural language processing (NLP) tasks. Here are some key methodologies:\n\n1. **Dialogue Response Generation**: Techniques such as exemplar\/template retrieval serve as intermediate steps for generating informative responses. This involves retrieving relevant examples to aid in producing contextually appropriate dialogue (Weston et al., 2018; Wu et al., 2019) <document id=\"2202.01110_rag_survey_li:chunk_450\">.\n\n2. **Machine Translation**: Retrieving sentences from a translation memory has been integrated into neural machine translation (NMT) models to enhance translation quality. This method allows models to capitalize on previously translated segments, improving fluency and accuracy (Gu et al., 2018; Zhang et al., 2018) <document id=\"2202.01110_rag_survey_li:chunk_450\">.\n\n3. **Knowledge-Grounded Generation**: Merging various forms of external knowledge, such as knowledge bases and documents, allows models to generate responses that are not only relevant but also factually correct and informative (Dinan et al., 2018) <document id=\"2202.01110_rag_survey_li:chunk_450\">.\n\n4. **Multi-Modal Retrieval**: Recent advancements in image-text retrieval prompt exploration into tasks that combine different data modalities, such as image captioning and speech-to-text tasks. This approach effectively utilizes the relationships between images and corresponding textual information to enhance generation tasks (Zhang et al., 2020) <document id=\"2202.01110_rag_survey_li:chunk_8100\">.\n\n5. **Diverse and Controllable Retrieval**: Future work aims to develop customized metrics for retrieval that can improve the diversity and control of generated texts. This involves retrieving instances with specific characteristics like emotional tone or stylistic choices, which can be particularly beneficial in personalized dialogue settings (Li et al., 2019) <document id=\"2202.01110_rag_survey_li:chunk_8550\">.\n\nEach of these approaches demonstrates the integration of deep learning techniques with traditional retrieval methods, allowing for more scalable and effective text generation.","26":"VisRAG enhances the retrieval and generation stages of retrieval-augmented generation (RAG) by directly employing vision-language models (VLMs), which allows the system to utilize the original document images instead of relying purely on extracted textual content. This shift eliminates the need for a parsing stage common in traditional text-based RAG frameworks like TextRAG, facilitating a more streamlined process.\n\nIn the retrieval stage, VisRAG utilizes a VLM-based retriever (VisRAG-Ret) that encodes both the query and document image to produce embeddings. By employing weighted mean pooling over the hidden states of the VLM, it effectively retrieves relevant pages based on their visual content, thereby preserving all information presented in the document images (Yu et al., 2024, <document id=\"2410.10594_vision_rag_yu:chunk_3600\">).\n\nFor the generation stage, VisRAG employs a VLM-based generator (VisRAG-Gen) that similarly utilizes the encoded document images, allowing it to pull meaningful context from the visual aspects of the content. The integration of VLMs facilitates a richer understanding of multi-modal documents, improving accuracy in both retrieval and generation processes (Yu et al., 2024, <document id=\"2410.10594_vision_rag_yu:chunk_1800\">).\n\nOverall, VisRAG offers a more efficient and effective RAG paradigm by leveraging visual information directly, surpassing traditional methods in handling multi-modal documents (Yu et al., 2024, <document id=\"2410.10594_vision_rag_yu:chunk_900\">).","27":"The SPLADE model addresses the limitations of previous sparse lexical representation models by incorporating several key improvements in terms of efficiency and effectiveness. Firstly, SPLADE utilizes a logarithmic activation function and an explicit sparsity regularization mechanism. This allows the model to selectively emphasize important terms while eliminating terms that have little informational content, leading to more efficient representations.\n\nMoreover, SPLADE's approach to document expansion enhances its effectiveness by enriching documents with relevant terms while maintaining a sparse representation. For instance, SPLADE can drop many irrelevant terms while introducing a small number of expansion terms, balancing the trade-off between expansion and compression effectively (<document id=\"2107.05720_splade_formal:chunk_6300\">).\n\nAdditionally, SPLADE is designed to be end-to-end trained in a single step, simplifying its training compared to other dense models and avoiding the complications associated with approximate nearest neighbor searches. This results in performance that rivals state-of-the-art dense retrieval models while retaining the efficiency characteristics typical of sparse Bag-of-Words (BOW) models (<document id=\"2107.05720_splade_formal:chunk_6750\">).\n\nIn summary, the SPLADE model improves upon past models by ensuring higher sparsity through regularization, enhancing term importance through controlled expansion, and maintaining a straightforward training process, ultimately achieving a better effectiveness-efficiency balance (<document id=\"2107.05720_splade_formal:chunk_900\">).","28":"ARES utilizes Prediction-Powered Inference (PPI) to enhance model-based evaluation accuracy by leveraging a small set of human-annotated datapoints. Specifically, approximately 150 annotated datapoints compose the human preference validation set, which is used to compute confidence intervals for the evaluations of context relevance, answer faithfulness, and answer relevance. This allows ARES to rectify machine learning predictions and improve the overall accuracy of its evaluations across various RAG systems. The use of PPI ensures that the predictions from the lightweight model judges are not only more accurate but also supported by statistical confidence intervals, reinforcing the reliability of the evaluation process <document id=\"2311.09476_ares_saad_falcon:chunk_0\">.","29":"The introduction of dense passage retrieval (DPR) has significantly improved open-domain question answering systems by allowing the retrieval process to utilize dense representations of passages and questions. This method performs better than traditional sparse vector space models, such as TF-IDF or BM25, which typically rely on simple n-gram overlaps for matching.\n\nKey improvements include:\n\n1. **Enhanced Semantic Matching**: Dense retrieval techniques capture deeper semantic relationships than sparse methods, enabling more accurate matching of questions to relevant passages. This is achieved through embeddings learned from question-passage pairs, leading to better comprehension of context and meaning (<document id=\"2004.04906_dpr_karpukhin:chunk_0\">).\n\n2. **Higher Passage Retrieval Accuracy**: Empirical evaluations show that the dense retriever outperforms conventional systems like BM25 by 9% to 19% in top-20 passage retrieval accuracy. This accuracy boost translates to improved performance in end-to-end question answering systems across multiple benchmarks (<document id=\"2004.04906_dpr_karpukhin:chunk_0\">).\n\n3. **Integration with Generative Models**: The dense passage retrieval can also augment generative models, leading to improved results in knowledge-intensive tasks. By increasing the number of retrieved passages, these models can better aggregate and utilize evidence from multiple sources (<document id=\"2007.01282_fid_izacard:chunk_0\">).\n\nIn summary, dense passage retrieval enhances the efficiency and accuracy of information extraction within open-domain question answering systems, contributing to state-of-the-art performance in this area (<document id=\"2104.05740_hybrid_retrieval_ma:chunk_0\">; <document id=\"2004.04906_dpr_karpukhin:chunk_0\">).","30":"The core idea behind HyDE (Hypothetical Document Embeddings) is to enable effective zero-shot dense retrieval without needing any relevance labels. HyDE involves two main components: \n\n1. **Generating Hypothetical Documents**: For each query, it uses an instruction-following language model (like InstructGPT) to create a hypothetical document that reflects potential relevance patterns, even though it may be factually incorrect.\n\n2. **Retrieving Real Documents**: This generated document is then encoded using an unsupervised contrastive learning approach, facilitating retrieval of real documents from a corpus based on similarity in the embedding space. The process eschews explicit modeling of query-document similarity; retrieval is instead seen as a task of evaluating document-document similarities, leveraging inner-product similarities from contrastive training.\n\nThis methodology allows HyDE to efficiently filter out \"hallucinated\" details, focusing on real document retrieval, and it has been shown to significantly outperform prior state-of-the-art techniques in zero-shot settings across multiple tasks and languages (<document id=\"2212.10496_hyde_gao:chunk_1350\">).","31":"The purpose of constructing a Retrieval-Augmented Generation Benchmark (RGB) is to evaluate the capabilities of retrieval-augmented generation in large language models (LLMs). Specifically, RGB assesses four fundamental abilities essential for RAG: \n\n1. **Noise Robustness**: The ability of LLMs to extract useful information from noisy documents that may not contain relevant answers.\n2. **Negative Rejection**: The capacity to reject a question when none of the retrieved documents contain useful information.\n3. **Information Integration**: The skill to answer complex questions that require synthesizing information from multiple documents.\n4. **Counterfactual Robustness**: The ability to handle questions that involve counterfactual scenarios.\n\nThis benchmark is designed to identify the limitations of existing LLMs in these areas and provide insights for their improvement (<document id=\"2309.01431_rgb_benchmark_chen:chunk_0\">, <document id=\"2309.01431_rgb_benchmark_chen:chunk_3150\">).","32":"Language models face several challenges when retrieving information from their input contexts in open-domain question answering. One key issue is that while providing longer input contexts can improve model performance, it also increases the complexity of the content that models must reason over, which may lead to decreased accuracy. Specifically, in open-domain QA settings, models often struggle to effectively utilize additional retrieved documents; for example, using 50 documents instead of 20 results in only marginal performance improvements (approximately 1% for GPT-3.5-Turbo) despite higher retriever recall.\n\nAnother significant challenge is the dependency on the position of relevant information within the input context. Models perform best when relevant information is found at the beginning or at the end of the context, demonstrating a U-shaped performance curve. Performance significantly decreases when relevant information is located in the middle, indicating that these models do not robustly access and utilize information spread throughout longer contexts. For instance, models might perform worse when relevant content is in the middle than in a closed-book setting without any documents (Liu et al. 2023).\n\nAdditionally, certain models exhibit difficulty retrieving matching tokens from the middle of their input contexts, which suggests that they struggle with both access and utilization of relevant information. Adjusting the input context length and position can have substantial effects on model performance, revealing that current language models are not optimally designed to handle the retrieval demands of open-domain QA (Liu et al. 2023; Joren et al. 2024).\n\nIn summary, the challenges include:\n1. Complexity and saturation of context leading to decreased accuracy.\n2. Performance dependence on the position of relevant information.\n3. Difficulty in accessing information located in the middle of long contexts.","33":"The trade-off between providing more context to language models in open-domain question answering (QA) involves the balance between potentially enhancing performance with additional information and the risk of decreased accuracy due to the increase in content that the model must process. Specifically, while longer contexts can help models perform better, they also increase the complexity of reasoning, which might lead to accuracy issues. \n\nResearch indicates that model performance tends to plateau well before the maximum number of retrieved documents is reached, showing limited effectiveness in utilizing additional context. For instance, using 50 documents instead of 20 only marginally improved performance by approximately 1.5% for GPT-3.5-Turbo and about 1% for Claude-1.3 (<document id=\"2307.03172_lost_in_middle_liu:chunk_1800\">).\n\nFurthermore, language models exhibit positional biases, performing significantly better when relevant information appears at the beginning or end of the context, rather than the middle, leading to substantial performance drops under certain conditions. This suggests that adding more context does not necessarily equate to better results if the model struggles to reason over longer input contexts effectively (<document id=\"2307.03172_lost_in_middle_liu:chunk_4500\">).","34":"ARES used OpenAI\u2019s _gpt-3.5-turbo-16k_, version 10\/23, as its in-context learning baseline in a zero\/few-shot setting <document id=\"2311.09476_ares_saad_falcon:chunk_1800\">.","35":"I don't have enough information in the provided materials to answer this question.","36":"The overall performance of FLARE and baseline methods across all tasks\/datasets is shown in Figure 4 <document id=\"2305.06983_active_rag_jiang:chunk_6300\">.","37":"GraphRAG is a graph-based Retrieval-Augmented Generation (RAG) approach designed to enable global sensemaking over large text corpora. It starts by constructing a knowledge graph using a Large Language Model (LLM), where nodes represent key entities and edges depict their relationships. GraphRAG then partitions this graph into a hierarchy of communities of closely related entities. The LLM generates community-level summaries in a bottom-up manner, recursively combining these summaries to provide global insights about the corpus. Queries are answered through a map-reduce process that synthesizes these community summaries into comprehensive answers <document id=\"2404.16130_graphrag_edge:chunk_900\">.\n\nIn contrast, vector RAG focuses on retrieving documents or text chunks that are semantically similar to a query. It typically employs text embeddings to find records that are closest in vector space, making it suitable for explicit fact retrieval. However, vector RAG approaches struggle with providing coherent understanding or summaries over entire corpora, as they limit their responses to the information gathered from a set number of retrieved semantically relevant texts <document id=\"2404.16130_graphrag_edge:chunk_1350\">. Therefore, while vector RAG excels in specific retrieval tasks, GraphRAG is tailored for broader, more integrative sensemaking tasks <document id=\"2404.16130_graphrag_edge:chunk_900\">.","38":"To optimize the retrieval module for a specific downstream task, such as question answering, several strategies can be employed:\n\n1. **Learned Retrieval Techniques**: Fine-tuning the retrieval module with task-specific optimization techniques, such as reinforcement learning or latent variable approaches, can significantly enhance its effectiveness in retrieving relevant documents for tasks like question answering <2005.11401_rag_lewis:chunk_8550>.\n\n2. **Task-Specific Retrieval Metrics**: Instead of relying on generic similarity measures, retrieval metrics can be learned in a task-dependent manner. This allows the model to focus only on memory that truly enhances the quality of the final output, thereby improving the performance on the specific task <2202.01110_rag_survey_li:chunk_2250>.\n\n3. **Active Retrieval Strategies**: Implementing confidence-based active retrieval, where the model generates queries based on its confidence in the generated content, can ensure that relevant documents are retrieved only when necessary, optimizing the retrieval process for subsequent generations <2305.06983_active_rag_jiang:chunk_3150>.\n\n4. **Modular RAG Approaches**: Utilizing a modular retrieval-augmented generation (RAG) framework allows for flexible adjustments in the retrieval process and enables the integration of new modules tailored for specific tasks, enhancing overall performance <2312.10997_rag_survey_gao:chunk_3600>.\n\n5. **Integration of Attention Mechanisms**: Attention mechanisms can be employed to integrate retrieved information effectively, which helps in focusing on the most relevant parts of the retrieved documents during generation <2202.01110_rag_survey_li:chunk_2250>.\n\nThese methods collectively contribute to tailoring the retrieval module to better serve the specific nuances and requirements of tasks such as question answering.","39":"The researchers used the in-batch negative training methodology for their Dense Passage Retriever (DPR) model. Specifically, they trained the question and passage encoders with a batch size of 128 and included one additional BM25 negative passage per question. The training involved up to 40 epochs for large datasets and 100 epochs for smaller datasets, using a learning rate of \\(10^{-5}\\) with Adam optimization, linear scheduling with warm-up, and a dropout rate of 0.1. They even combined datasets (excluding SQuAD) to create a multi-dataset encoder for improved performance across various datasets <document id=\"2004.04906_dpr_karpukhin:chunk_4950\" \/>.","40":"Gao et al. introduced the **Self-Reflective Retrieval-augmented Generation (SELF-RAG)** method to improve the generation quality of large language models (LLMs) without sacrificing their versatility. This method involves on-demand retrieval and self-reflection, where the model generates both task outputs and special reflection tokens. These tokens guide the model in determining when to retrieve relevant information and allow it to critique its own outputs for better factuality and overall quality. In particular, the approach enables the model to decide if augmenting its generation with external retrieved passages is beneficial, enhancing factual accuracy while maintaining creativity and versatility <document id=\"2310.11511_self_rag_asai:chunk_450\">.","41":"The effectiveness of FLARE in multihop QA tasks, such as 2WikiMultihopQA, is demonstrated to be superior or competitive compared to single-time and multi-time retrieval baselines. Specifically, FLARE achieved a significant improvement in certain evaluations, boasting an EM score of 48.8, which is a marked increase compared to other baseline approaches. In the context of multihop QA and other tasks, FLARE leverages a unique iterative retrieval mechanism, allowing it to refine its answers based on the content retrieved relevant to each generated sentence, which contributes to its improved performance.<document id=\"2305.06983_active_rag_jiang:chunk_7650\">\n\nSeveral factors contribute to FLARE's performance:\n1. **Next-Sentence Retrieval**: FLARE employs next-sentence retrieval instead of previous-context-based retrieval, which has shown to be more effective for upcoming generations mentioned in the evaluations. This approach allows the model to focus on the most relevant information to the current generation step.<document id=\"2305.06983_active_rag_jiang:chunk_8100\">\n   \n2. **Active Retrieval**: The ability to dynamically adjust when retrieval occurs based on confidence thresholds helps the model avoid unnecessary noise from irrelevant information during generation. Results indicated that optimal performance is maintained when retrieval is activated for 40%-80% of sentences, highlighting the significance of active retrieval in the overall process.<document id=\"2305.06983_active_rag_jiang:chunk_8100\"> \n\n3. **Query Formulation**: The study of both implicit query formation (masking) and explicit query formulation methods indicates that they can effectively reflect information needs, enhancing the model\u2019s reliability in generating accurate responses.<document id=\"2305.06983_active_rag_jiang:chunk_8550\">\n\nOverall, FLARE's design enables it to maintain high performance levels across various tasks in a way that other models might not, directly impacting its results in multihop QA tasks.<document id=\"2305.06983_active_rag_jiang:chunk_7650\">"},"reference":{"0":"Retrieval-Augmented Generation (RAG) is a method that combines parametric memory (a pre-trained language model) with non-parametric memory (a dense vector index of documents) for knowledge-intensive NLP tasks. RAG retrieves relevant documents from an external corpus and conditions the language model on both the input query and retrieved documents to generate responses. This approach allows models to access up-to-date knowledge without retraining and reduces hallucinations by grounding generations in retrieved evidence. The method was introduced by Lewis et al. in 2020.","1":"The FiD (Fusion-in-Decoder) architecture was proposed by Izacard and Grave in 2021. FiD processes multiple retrieved passages independently in the encoder and then fuses them in the decoder, allowing the model to effectively leverage information from many passages simultaneously while maintaining computational efficiency.","2":"RAFT stands for Retrieval-Augmented Fine-Tuning. It is a training method proposed by Zhang et al. in 2024 that fine-tunes language models to better utilize retrieved documents by training them to distinguish relevant from irrelevant retrieved passages and to extract correct information from the relevant ones.","3":"Hybrid retrieval combines dense (embedding-based) and sparse (keyword-based like BM25) retrieval methods, leveraging their complementary strengths. Sparse retrieval excels at exact keyword matching and is effective for queries with specific terminology, proper nouns, or rare terms. Dense retrieval captures semantic similarity and handles paraphrasing, synonyms, and conceptual queries better. By combining both approaches (typically using rank fusion methods like RRF), hybrid retrieval achieves higher recall by retrieving documents that either method would miss alone, leading to more comprehensive and robust retrieval performance across diverse query types.","4":"ColBERT (Contextualized Late Interaction over BERT) differs from traditional dense retrieval by using late interaction instead of single-vector representations. While traditional dense retrieval encodes queries and documents into single dense vectors and computes similarity via dot product, ColBERT preserves token-level representations and computes interactions between all query and document token embeddings. This approach captures fine-grained token-level matching signals while maintaining efficiency through maximum similarity (MaxSim) operations and precomputation of document embeddings. The late interaction enables better matching of specific terms and phrases compared to single-vector approaches.","5":"Long-context LLMs and RAG systems have different trade-offs. Long-context LLMs can process entire documents directly, avoiding retrieval errors and maintaining full context, but they face challenges including higher computational costs (quadratic attention complexity), difficulty focusing on relevant information in very long contexts (lost-in-the-middle problem), inability to update knowledge without retraining, and higher inference latency. RAG systems offer advantages including efficient access to large knowledge bases, easy knowledge updates by modifying the retrieval index, lower inference costs by processing only relevant chunks, and explicit source attribution. However, RAG systems depend on retrieval quality, may miss relevant information if retrieval fails, and require careful engineering of chunking and retrieval strategies. The choice depends on the use case: RAG is often preferred for knowledge-intensive tasks requiring up-to-date information, while long-context LLMs suit applications requiring deep understanding of specific documents.","6":"GraphRAG and FiD take fundamentally different approaches to multi-document reasoning. FiD (Fusion-in-Decoder) uses a sequence-to-sequence architecture where retrieved passages are processed independently in parallel by the encoder, and then their representations are concatenated and fused in the decoder during generation. This allows the model to implicitly learn to combine information from multiple passages through attention mechanisms, but relationships between passages are only captured during decoding. GraphRAG, on the other hand, explicitly constructs a knowledge graph from retrieved documents, creating structured representations of entities and their relationships across documents. It then uses graph-based reasoning methods to traverse and aggregate information from the graph structure. GraphRAG's explicit graph structure makes multi-hop reasoning paths more interpretable and can better capture complex relationships, while FiD's approach is more flexible and doesn't require structured knowledge extraction but is less interpretable.","7":"RAG systems should be evaluated on both retrieval and generation quality. For retrieval, recommended metrics include: Recall@K (what fraction of relevant documents are in top-K results), Mean Reciprocal Rank (MRR, measures ranking quality), and NDCG (Normalized Discounted Cumulative Gain, considers both relevance and ranking). For generation quality, recommended metrics include: answer correctness (semantic similarity to reference answer using LLM-based evaluation), faithfulness (whether the answer is grounded in retrieved context), answer relevancy (whether the answer addresses the question), and context precision\/recall (whether retrieved chunks are relevant and sufficient). Framework like RAGAS provides LLM-based metrics that avoid issues with incomplete ground truth. End-to-end metrics like exact match and F1 score on question answering benchmarks are also commonly used to measure overall RAG system performance.","8":"I cannot answer this question as it is outside the scope of the available RAG research papers. The corpus contains information about Retrieval-Augmented Generation systems and related NLP techniques, not culinary topics.","9":"While neural network training is a broad machine learning topic, the RAG research corpus focuses specifically on retrieval-augmented generation systems. For general neural network training information (backpropagation, gradient descent, loss functions, etc.), you would need to consult general deep learning resources. The corpus may contain some references to neural network training in the context of training retrieval models or fine-tuning language models for RAG, but does not provide comprehensive coverage of training neural networks from scratch.","10":"The authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering' are Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.","11":"BM25 is used for retrieval in open-domain QA, usually implemented using TF-IDF or BM25, which matches keywords efficiently with an inverted index and can be seen as representing the question and context in high-dimensional, sparse vectors (with weighting).","12":"Wikipedia is a large corpus of diversified topics that can be used to answer factoid questions in an extractive QA setting.","13":"Roberts et al. define the similarity between a question and a passage using the dot product of their vectors: sim(q, p) = EQ(q)\u22baEP (p).","14":"Passage embeddings are used to represent text passages as dense vectors, which can then be compared using similarity scores. This allows for efficient training on large batches of question-passage pairs by reusing computation and creating multiple positive and negative examples within each batch.","15":"BM25 is used to select positive passages by ranking them based on their relevance to the questions. The highest-ranked passage containing the answer is selected as the positive passage.","16":"TREC is used as one of the QA datasets to evaluate the retrieval performance of the Dense Passage Retriever (DPR) model.","17":"The lower performance on SQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage, resulting in high lexical overlap between passages and questions, which gives BM25 an advantage. Second, the data was collected from only 500+ Wikipedia articles, leading to a biased distribution of training examples.","18":"The Intel Xeon CPU E5-2698 v4 @ 2.20GHz was used to profile the passage retrieval speed for DPR.","19":"In an end-to-end QA system, a passage selection score assigns a value to each retrieved passage based on its relevance to the question. This score helps determine which passage is most likely to contain the correct answer.","20":"REALMWiki is a model pretrained on Wikipedia.","21":"Lee et al. made a significant contribution by jointly training the question encoder and reader, which outperformed the BM25 plus reader paradigm on multiple open-domain QA datasets in terms of QA accuracy. Their approach was further extended by REALM (Guu et al., 2020), which included tuning the passage encoder asynchronously by re-indexing the passages during training.","22":"The sufficient context autorater helps in reducing hallucinations in RAG systems by providing a binary label for whether the context is sufficient to answer the query. This label is then combined with model self-rated confidence scores to tune a selective accuracy-coverage trade-off, where 'coverage' denotes the portion of inputs on which the model does not abstain. By using these signals to train a simple logistic regression model, the approach leads to a better selective accuracy-coverage trade-off compared to using model confidence alone.","23":"Query rewriting improves the performance of retrieval-augmented language models by adapting the retrieval query to better match the input text, thus enhancing the reader's ability to retrieve relevant and accurate information. This leads to consistent improvements in the model's performance on downstream tasks such as open-domain QA and multiple-choice QA.","24":"The combination of parametric memory with non-parametric memory in RAG models improves their performance on knowledge-intensive NLP tasks by allowing for the expansion and revision of knowledge, as well as direct access to and inspection of accessed knowledge. This hybrid approach addresses some limitations of pre-trained neural language models, which cannot easily expand or revise their memory and may produce 'hallucinations'. RAG models achieve state-of-the-art results on open-domain extractive question answering tasks such as Natural Questions, WebQuestions, and CuratedTrec.","25":"Notable approaches for retrieval-augmented text generation tasks include exemplar\/template retrieval for dialogue response generation, integrating translation memory into NMT models for machine translation, and applying retrieval-augmented generation in other tasks such as abstractive summarization, code generation, paraphrase, and knowledge-intensive generation. These approaches leverage deep learning technology to enhance the scalability and effectiveness of text generation.","26":"VisRAG enhances the retrieval and generation stages of RAG by employing a vision-language model (VLM)-based retriever and generator. Instead of parsing documents to obtain text, VisRAG directly embeds the document page as an image using a VLM and retrieves it to enhance the generation process. This approach maximizes the retention and utilization of data information in the original documents, eliminating information loss introduced during the parsing process.","27":"The SPLADE model addresses the limitations of previous sparse lexical representation models by incorporating sparsity regularization, which boosts both the efficiency (in terms of the number of floating-point operations) and effectiveness of these types of models. Additionally, it relies on a logarithmic activation function and efficient document expansion, achieving competitive results with respect to complex training pipelines for dense models like ANCE.","28":"ARES utilizes prediction-powered inference (PPI) to improve model-based evaluation accuracy by leveraging a small set of human-annotated datapoints for computing its confidence intervals. This approach allows ARES to provide statistical confidence intervals for RAG scoring, enhancing the precision and reliability of its evaluations.","29":"Dense Passage Retrieval for Open-Domain Question Answering introduced a method that uses dense representations alone to perform efficient passage retrieval, outperforming traditional sparse vector space models like TF-IDF or BM25. This was achieved by learning embeddings from a small number of questions and passages using a simple dual-encoder framework. When evaluated on various open-domain QA datasets, the dense retriever demonstrated significant improvements in terms of top-20 passage retrieval accuracy compared to strong Lucene-BM25 systems.","30":"The core idea behind Hypothetical Document Embeddings (HyDE) is to pivot through generating a hypothetical document that captures relevance patterns but may contain false details. This document is then encoded into an embedding vector by an unsupervised contrastively learned encoder, which identifies a neighborhood in the corpus embedding space where similar real documents are retrieved based on vector similarity. HyDE improves zero-shot dense retrieval by grounding the generated document to the actual corpus and filtering out incorrect details with the encoder's dense bottleneck.","31":"The purpose of constructing a Retrieval-Augmented Generation Benchmark (RGB) is to evaluate the retrieval-augmented generation of LLMs. It evaluates four specific abilities: Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness.","32":"Language models struggle to retrieve and use relevant information in the multi-document question answering task, as evidenced by their performance on a synthetic key-value retrieval task. This challenge is particularly pronounced for models that rely on decoder-only architectures or do not employ query-aware contextualization techniques.","33":"The trade-off between providing more context and its impact on language model performance in open-domain QA is that while longer input contexts may help improve downstream task performance, they also increase the amount of content the model must reason over, potentially decreasing accuracy. The study found that reader model performance saturates long before retriever performance saturates, indicating that readers are not effectively using the extra context. Using more than 20 retrieved documents only marginally improves reader performance (\u223c1.5% for GPT-3.5-Turbo and \u223c1% for Claude-1.3), while significantly increasing the input context length (and thus latency and cost).","34":"ARES used OpenAI\u2019s gpt-3.5-turbo-16k, version 10\/23, (Brown et al., 2020) as its in-context learning baseline.","35":"The use of FLAN-T5-XXL in the ARES evaluation framework has several limitations. It requires substantial GPU-based hardware with about 32GB of memory to run, taking several hours for fine-tuning and generation. Additionally, all of the datasets used in the evaluation are in English, which may limit its applicability to other languages.","36":"Figure 4 shows the overall performance of FLARE and baseline methods across all tasks\/datasets, demonstrating that FLARE outperforms all baselines on all tasks\/datasets.","37":"GraphRAG is a method for knowledge graph extraction that uses advanced machine learning models, specifically leveraging summaries over large sections of the data source as a form of 'self-memory'. It contrasts with vector RAG by generating a graph index from the source data and applying graph-based community detection to create a thematic partitioning of the data. This approach allows GraphRAG to answer queries that require global sensemaking over the entire data corpus, making it more effective for complex knowledge extraction tasks.","38":"Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work.","39":"The researchers used the in-batch negative setting for training their Dense Passage Retriever model, with a batch size of 128 and one additional BM25 negative passage per question.","40":"Gao et al. introduced Self-Reflective Retrieval-augmented Generation (SELF-RAG) to improve an LLM's generation quality, including its factual accuracy without hurting its versatility, via on-demand retrieval and self-reflection.","41":"FLARE outperforms all baselines on multihop QA tasks by a large margin. This is largely due to the task's clear definition and specific objective of producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to generate on-topic output. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs stay on topic."},"answer_correctness":{"0":0.6286892542,"1":0.2317620713,"2":0.7858682872,"3":0.9839586689,"4":0.7992948361,"5":0.7204351889,"6":0.5223668589,"7":0.6502281824,"8":0.4233486398,"9":0.1858465953,"10":0.9976729155,"11":0.8191710766,"12":0.5258644408,"13":0.7991154692,"14":0.6604290204,"15":0.452326893,"16":0.7284019173,"17":0.9916288287,"18":0.9907011839,"19":0.8149407558,"20":0.5336343598,"21":0.1843189687,"22":0.8805707714,"23":0.4590706281,"24":0.9920691487,"25":0.6163769349,"26":0.9935030706,"27":0.6362933212,"28":0.5165889611,"29":0.9844129008,"30":0.9642780289,"31":0.7425108266,"32":0.3425374093,"33":0.7403641226,"34":0.5394802357,"35":0.1907606764,"36":0.7375145989,"37":0.4663567101,"38":0.8683854744,"39":0.5811741253,"40":0.6175117159,"41":0.8207069843},"answer_relevancy":{"0":0.9284954933,"1":0.9238993459,"2":0.9056144139,"3":0.9651469636,"4":0.9952359044,"5":0.9999988371,"6":0.9995150497,"7":0.9698016181,"8":0.0,"9":0.0,"10":1.0,"11":0.9545173743,"12":1.0,"13":0.9907355054,"14":0.9595362753,"15":0.9428369908,"16":0.9523044057,"17":0.9966229525,"18":0.9826562374,"19":0.9842610323,"20":0.9651798473,"21":0.0,"22":0.9534968863,"23":0.9703119619,"24":0.9773587495,"25":0.9711727442,"26":0.9235368259,"27":0.9643994662,"28":0.9231941658,"29":0.9731766499,"30":0.9010715017,"31":0.9774165321,"32":0.9745583038,"33":0.9657774255,"34":0.9406745841,"35":0.0,"36":0.9347029897,"37":0.9405522615,"38":0.9577470382,"39":0.9583598256,"40":0.812076837,"41":0.9509035997},"faithfulness":{"0":1.0,"1":0.0,"2":0.25,"3":1.0,"4":1.0,"5":0.7,"6":0.5294117647,"7":1.0,"8":1.0,"9":0.0,"10":1.0,"11":1.0,"12":0.8888888889,"13":1.0,"14":1.0,"15":0.875,"16":0.8888888889,"17":1.0,"18":1.0,"19":0.625,"20":1.0,"21":0.0,"22":0.7692307692,"23":0.9473684211,"24":1.0,"25":0.8666666667,"26":1.0,"27":0.8,"28":1.0,"29":1.0,"30":1.0,"31":1.0,"32":0.95,"33":0.9230769231,"34":1.0,"35":0.0,"36":1.0,"37":1.0,"38":1.0,"39":1.0,"40":0.9,"41":0.9473684211},"context_precision":{"0":1.0,"1":0.0,"2":0.0,"3":1.0,"4":0.8041666666,"5":0.95,"6":0.5,"7":1.0,"8":0.0,"9":0.0,"10":0.45,"11":1.0,"12":0.8875,"13":0.8333333333,"14":0.8333333333,"15":1.0,"16":0.9166666666,"17":1.0,"18":0.5,"19":0.8875,"20":0.9999999999,"21":0.5833333333,"22":1.0,"23":1.0,"24":1.0,"25":1.0,"26":0.8875,"27":1.0,"28":1.0,"29":1.0,"30":1.0,"31":1.0,"32":1.0,"33":1.0,"34":0.25,"35":0.0,"36":0.8333333333,"37":1.0,"38":1.0,"39":0.9999999999,"40":0.7,"41":1.0},"context_recall":{"0":1.0,"1":0.5,"2":0.0,"3":0.5,"4":1.0,"5":0.5,"6":0.1666666667,"7":1.0,"8":1.0,"9":0.3333333333,"10":1.0,"11":1.0,"12":1.0,"13":1.0,"14":1.0,"15":1.0,"16":1.0,"17":1.0,"18":1.0,"19":1.0,"20":1.0,"21":0.0,"22":0.6666666667,"23":1.0,"24":1.0,"25":1.0,"26":0.6666666667,"27":1.0,"28":1.0,"29":1.0,"30":1.0,"31":1.0,"32":1.0,"33":1.0,"34":1.0,"35":0.3333333333,"36":1.0,"37":0.6666666667,"38":1.0,"39":1.0,"40":1.0,"41":0.6666666667}}