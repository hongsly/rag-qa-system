{"user_input":"What is Retrieval-Augmented Generation?","response":"Retrieval-Augmented Generation (RAG) is a technique that enhances large language models (LLMs) by allowing them to utilize external knowledge sources during the generation process. This approach aims to improve the accuracy and reliability of the generated content by incorporating information retrieved from external databases or datasets. \n\nIn a typical formulation, RAG can be represented as _**y**_ = _f_(_**x**, _**z**_), where _**x**_ is the input and _**z**_ is a set of relevant instances retrieved that can aid the generation of _**y**_ (the output) <document id=\"2202.01110_rag_survey_li:chunk_900\">. The primary goal of RAG is to mitigate common issues faced by LLMs, such as factual hallucinations and knowledge outdating, by providing access to real-time, relevant information through techniques like information retrieval (Guu et al., 2020; Lewis et al., 2020) <document id=\"2309.01431_rgb_benchmark_chen:chunk_0\">. \n\nOverall, RAG presents a promising framework for improving LLM outputs by enabling them to leverage a broader range of knowledge during text generation <document id=\"2410.10594_vision_rag_yu:chunk_0\">.","retrieved_contexts":[" generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n\n\n**2** **Retrieval-Augmented Paradigm**\n\n\nIn this section, we first give a general formulation\n\n- f retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrievalaugmented generation paradigm, including the re\n\nOutput\n\n\n\n\n\nFigure 1: The overview of this survey.\n\n\n\ntrieval source, retrieval metric and integration meth\n- ds.\n\n\n**2.1** **Formulation**\n\n\nMost text generation tasks can be formulated as a\nmapping from input sequence _**x**_ to output sequence\n_**y**_ : _**y**_ = _f_ ( _**x**_ ). For instance, _**x**_ and _**y**_ could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\n\n\n_**y**_ = _f_ ( _**x**_ _,_ _**z**_ ) (1)\n\n\nwhere _**z**_ = _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that _**y**_ _[r]_\n\nmay benefit the response generation, if _**x**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r","Published as a conference paper at ICLR 2025\n\n## - - VISRAG: VISION BASED RETRIEVAL AUGMENTED - GENERATION ON MULTI MODALITY DOCUMENTS\n\n\n**Shi Yu** [1] _[\u2217]_ **, Chaoyue Tang** [2] _[\u2217]_ **, Bokai Xu** [2] _[\u2217]_ **, Junbo Cui** [2] _[\u2217]_ **, Junhao Ran** [3] **, Yukun Yan** [1] _[\u2020]_ **,**\n**Zhenghao Liu** [4] **, Shuo Wang** [1] **, Xu Han** [1] **, Zhiyuan Liu** [1] _[\u2020]_ **, Maosong Sun** [1]\n\n1Department of Computer Science and Technology, Tsinghua University\n2ModelBest Inc. 3Rice University 4Northeastern University\nyus21@mails.tsinghua.edu.cn\n\n\nABSTRACT\n\n\nRetrieval-augmented generation (RAG) is an effective technique that enables large\nlanguage models (LLMs) to utilize external knowledge sources for generation.\nHowever, current RAG systems are solely based on text, rendering it impossible\nto utilize vision information like layout and images that play crucial roles in realworld multi-modality documents. In this paper, we introduce VisRAG, which\ntackles this issue by establishing a vision-language model (VLM)-based RAG\npipeline. In this pipeline, instead of first parsing the document to obtain text,\nthe document is directly embedded using a VLM as an image and then retrieved\nto enhance the generation of a VLM. Compared to traditional text-based RAG,\nVisRAG maximizes the retention and utilization of the data information in the\n\n     - riginal documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in\nVisRAG and explore a variety of generation methods. Experiments demonstrate\nthat VisRAG outperforms traditional RAG in both the retrieval and generation\nstages, achieving a 20\u201340% end-to-end performance gain over traditional textbased RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing\ntraining data and demonstrates strong generalization capability, positioning"," (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao et al. 2020; Raunak, Menezes,\nand Junczys-Dowmunt 2021; Ji et al. 2023), knowledge outdating (He, Zhang, and Roth 2022), and the lack of domainspecific expertise (Li et al. 2023c; Shen et al. 2023).\nIncorporating external knowledge via information retrieval, i.e., Retrieval-Augmented Generation (RAG), has\nbeen regarded as a promising way to resolve the above challenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.\n\n\n  - Corresponding authors.\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n\n\n\n2022; Izacard et al. 2022). With the help of external knowledge, LLMs can generate more accurate and reliable responses. The most common method is to use a search engine\nas a retriever such as New Bing. Due to the vast amount of\ninformation available on the Internet, using a search engine\ncan provide more real-time information.\n\nHowever, Retrieval-Augmented Generation brings not\n\n- nly positive effects to LLMs (Liu, Zhang, and Liang 2023;\nMaynez et al. 2020). On one hand, there is a significant\namount of noise information even fake news in the content\navailable on the Internet, which poses challenges for search\nengines in accurately retrieving desirable knowledge. On the\n\n- ther hand, LLMs suffer from unreliable generation challenge. LLMs can be misled by incorrect information contained in the context (Bian et al. 2023) and also suffer from\nhallucination during the generation (Adlakha et al. 2023),\nresulting in generating content that goes beyond external in\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nformation. These challenges result in LLMs being unable to\nconsistently generate reliable and accurate responses. Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how\ncould each model survives from these drawbacks and improvement","## **Benchmarking Large Language Models in Retrieval-Augmented Generation**\n\n**Jiawei Chen** [1,3] **, Hongyu Lin** [1,*] **, Xianpei Han** [1,2,*] **, Le Sun** [1,2]\n\n1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n_{_ jiawei2020,hongyu,xianpei,sunle _}_ @iscas.ac.cn\n\n\n\n**Abstract**\n\n\nRetrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language\nmodels (LLMs). However, existing research lacks rigorous\nevaluation of the impact of retrieval-augmented generation\n\n  - n different large language models, which make it challenging to identify the potential bottlenecks in the capabilities\n\n  - f RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance\n\n  - f different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case.\nThen we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\n\n  - f noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n\n\n**Introduction**\n\n\nRecently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao","## **Ragas: Automated Evaluation of Retrieval Augmented Generation**\n\n**Shahul Es** _[\u2020]_ **, Jithin James** _[\u2020]_ **, Luis Espinosa-Anke** _[\u2217\u2662]_ **, Steven Schockaert** _[\u2217]_\n\n_\u2020_ Exploding Gradients\n_\u2217_ CardiffNLP, Cardiff University, United Kingdom\n_\u2662_ AMPLYFI, United Kingdom\nshahules786@gmail.com,jamesjithin97@gmail.com\n{espinosa-ankel,schockaerts1}@cardiff.ac.uk\n\n\n\n**Abstract**\n\n\nWe introduce **Ragas** ( **R** etrieval **A** ugmented\n**G** eneration **As** sessment), a framework for\nreference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG\nsystems are composed of a retrieval and an\nLLM based generation module, and provide\nLLMs with knowledge from a reference textual\ndatabase, which enables them to act as a natural language layer between a user and textual\ndatabases, reducing the risk of hallucinations.\nEvaluating RAG architectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to\nidentify relevant and focused context passages,\nthe ability of the LLM to exploit such passages\nin a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite\n\n  - f metrics which can be used to evaluate these\n\ndifferent dimensions _without having to rely on_\n_ground truth human annotations_ . We posit that\nsuch a framework can crucially contribute to\nfaster evaluation cycles of RAG architectures,\nwhich is especially important given the fast\nadoption of LLMs.\n\n\n**1** **Introduction**\n\n\nLanguage Models (LMs) capture a vast amount\n\n- f knowledge about the world, which allows them\nto answer questions without accessing any external sources. This idea of LMs as repositories of\nknowledge emerged shortly after the introduction\n\n- f BERT (Devlin et al., 2019) and became more\nfirmly established with the introduction of ever\nlarger LMs (Roberts et al., 2020). While the most\nrecent Large Language Models (LLMs) capture\nenough knowledge to rival human performance\nacross a wide variety"],"reference":"Retrieval-Augmented Generation (RAG) is a method that combines parametric memory (a pre-trained language model) with non-parametric memory (a dense vector index of documents) for knowledge-intensive NLP tasks. RAG retrieves relevant documents from an external corpus and conditions the language model on both the input query and retrieved documents to generate responses. This approach allows models to access up-to-date knowledge without retraining and reduces hallucinations by grounding generations in retrieved evidence. The method was introduced by Lewis et al. in 2020."}
{"user_input":"Who proposed the FiD (Fusion-in-Decoder) architecture?","response":"The FiD (Fusion-in-Decoder) architecture was proposed by Wenhan Xiong, Xiang Lorraine Li, and others in their paper titled \"Answering Complex Open-Domain Questions with Multi-Hop Dense Retrieval\" published in 2020 <document id=\"2009.12756_multihop_qi:chunk_15750\" \/>.","retrieved_contexts":["score and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is twice as large as the extractive\nmodels and has 770M parameters. We reuse the hyperparameters as described in Izacard & Grave\n(2020). The original FiD uses the top 100 passages for NaturalQuestions. In our case, we use the top\n50 retrieved passage sequences and concatenate the passages in each sequence before feeding into T5.\nIn order to fit this model into GPU, we make use of PyTorch checkpoint [11] for training.\n\n\nB.2.3 MULTI-HOP RAG\n\n\nThe RAG model aims to generate answer _y_ given question _x_ and the retrieved documents _z_ . Similarly,\nthe goal of multi-hop RAG can be expressed as: generate answer _y_ given question _x_ and retrieved\ndocuments in hop one _z_ 1 and hop two _z_ 2 (Limiting to two hops for HotpotQA). The model has three\ncomponents:\n\n\n   - Hop-one retriever _p\u03b7_ 1( _z_ 1 _|x_ ) with parameter _\u03b7_ 1 to represent the retrieved top-k passage\ndistribution (top-k truncated distribution) given the input question _x_ .\n\n\n    - Hop-two retriever _p\u03b7_ 2( _z_ 2 _|x, z_ 1) with parameter _\u03b7_ 2 to represent the hop-two retrieved top-k\npassage distribution given not only the question _x_ but also the retrieved document _z_ 1 from\nhop-one.\n\n\n   - A generator _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _,, y_ 1: _i\u2212_ 1) to represent the next token distribution given input\nquestion _x_, hop-one retrieved document _z_ 1, hop-two retrieved document _z_ 2 and previous\npredicted token _y_ 1: _i\u2212_ 1 parametrized by _\u03b8_\n\n\n**Multi-Hop RAG Sequence Model** As the RAG Sequence model, this model generates the answer\nsequence given the fixed set of documents from hop-one retriever and hop-two retriever. In order","The models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of D = 64 and\npooling factor 4. The S4+MLP block has roughly 2 _\ud835\udc37_ [2] + 4 _\ud835\udc37_ [2] parameters (expansion factor 2 in the MLP). The Transformer\nblock has 4 _\ud835\udc37_ [2] + 2 _\ud835\udc37_ [2] parameters (expansion factor 1 in the MLP). The Mamba block has the usual \u2248 6 _\ud835\udc37_ [2] parameters. All\nmodels have roughly 6M total parameters.\n\n\n**E.5** **Efficiency Benchmark**\n\n\n**Scan Operation.** We compare the core operation of selective SSMs, which is the parallel scan (Section 3.3), against\nconvolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost of other operations\n\n- utside of this core operation, such as computing the convolutional kernel in global-convolution models, or computing the\nQKV projections in attention.\n\n\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing the\nparameters _\ud835\udc68, \ud835\udc69, \ud835\udc6a_ in HBM.\n\n\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all the large\nparameters in HBM.\n\n\n35\n\n\nTable 15: ( **Memory benchmark** .) Mamba\u2019s memory footprint is comparable to the most optimized Transformer. Results for 125M\nmodels.\n\n\nBatch size Transformer (w\/ FlashAttention-2) Mamba\n\n\n1 4.6GB 4.8GB\n\n2 5.2GB 5.8GB\n\n4 6.9GB 7.3GB\n\n8 11.5GB 12.3GB\n\n16 20.7GB 23.1GB\n\n32 34.5GB 38.2GB\n\n\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs and the\nfilters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The theoretical complexity\nis _\ud835\udc42_ ( _\ud835\udc3f_ log( _\ufffd","xiv.org\/abs\/1603.09320](https:\/\/arxiv.org\/abs\/1603.09320)` .\n\n\n[38] Gary Marcus. The next decade in ai: four steps towards robust artificial intelligence. _arXiv_\n_preprint arXiv:2002.06177_, 2020. URL `[https:\/\/arxiv.org\/abs\/2002.06177](https:\/\/arxiv.org\/abs\/2002.06177)` .\n\n\n[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\u00e4schel, Vassilis\nPlachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the\nverifiability of generated text. _arXiv preprint arXiv:1911.03587_, 2019. URL `[https:](https:\/\/arxiv.org\/abs\/1911.03587)`\n`[\/\/arxiv.org\/abs\/1911.03587](https:\/\/arxiv.org\/abs\/1911.03587)` .\n\n\n[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In _ICLR_, 2018. URL `[https:\/\/openreview.net\/forum?id=r1gs9JgRZ](https:\/\/openreview.net\/forum?id=r1gs9JgRZ)` .\n\n\n[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploiting background knowledge for building conversation systems. In _Proceedings of the 2018_\n_Conference on Empirical Methods in Natural Language Processing_, pages 2322\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:\n10.18653\/v1\/D18-1255. URL `[https:\/\/www.aclweb.org\/anthology\/D18-1255](https:\/\/www.aclweb.org\/anthology\/D18-1255)` .\n\n\n[42] Preksha Nema and Mitesh M. Khapra. Towards a"," on Learning Representations (ICLR)_ .\n\n2020.\n\n[27] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao\nBai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez,\nAndy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. \u201cA Mathematical Framework for Transformer Circuits\u201d. In: _Transformer Circuits_\n_Thread_ (2021). https:\/\/transformer-circuits.pub\/2021\/framework\/index.html.\n\n[28] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. \u201cBlock-State\nTransformer\u201d. In: _arXiv preprint arXiv:2306.09539_ (2023).\n\n[29] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu,\nYangyang Shi, Ozlem Kalinli, Mike Seltzer, and Mark J. F. Gales. \u201cMulti-Head State Space Model for Speech\nRecognition\u201d. In: _Proc. INTERSPEECH 2023_ [. 2023, pp. 241\u2013245. doi: 10.21437\/Interspeech.2023-1036.](https:\/\/doi.org\/10.21437\/Interspeech.2023-1036)\n\n[30] Karl J Friston, Lee Harrison, and Will Penny. \u201cDynamic Causal Modelling\u201d. In: _Neuroimage_ 19.4 (2003), pp. 1273\u2013\n\n1302.\n\n[31] Daniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher\nR\u00e9. \u201cSimple Hardware-efficient Long Convolutions for Sequence Modeling\u201d. In: _The International Conference on_\n_Machine Learning (ICML)_ (2023).\n\n[32] Ken-ichi Funahashi and Yuichi Nakamura. \u201cApproximation of Dynamical Systems by Continuous Time Rec"," and decoding (Koehn et al., 2003;\nChiang, 2007). As a result, many efforts have been\nmade to make use of translation memory (TM) on\ntop of each component.\n\n\n**Constrained Decoding with TM** Constrained\ndecoding is the most straightforward way to integrating TM into SMT (Smith and Clark, 2009;\nKoehn and Senellart, 2010; Zhechev and Van Genabith, 2010; Ma et al., 2011). Its basic idea is\nto reuse the useful segments in _**y**_ _[r]_ while translate other segments by SMT. Specifically, the approach consists of three steps: 1) identify the unmatched segments in both _**x**_ _[r]_ and _**x**_ through the\nedit-distance algorithm; 2) identify the unmatched\nsegments in _**y**_ _[r]_, each of which is aligned to one\nunmatched segment in _**x**_ _[r]_ by a word alignment\nalgorithm; 3) decode each unmatched segment in\n_**x**_ by SMT and then use the result to replace its\ncorresponding unmatched segment in _**y**_ _[r]_ . Li et al.\n(2016b) further extend this approach from sentence\nlevel to phrase level. The advantage in constrained\ndecoding is that it does not require to change the\ntranslation model (including phrase table and parameters) and can be applied in a plug-and-play\nway. This approach is successful when _**x**_ is highly\nsimilar to _**x**_ _[r]_ ; otherwise its performance is degraded largely, because it explicitly isolates TM\n\n\n\nmatching and SMT decoding and reuses the results\nin _**x**_ _[r]_ - r not in a deterministic way.\n\n\n**Phrase Table Aggregation with TM** There are\nalso notable efforts to augment the phrase table\nfor SMT by extracting translation rules from the\nretrieved bilingual sentences _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ . Then\nthey re-tune the parameters for the SMT model\nwhich makes use of translation knowledge from\n_{\u27e8_ _**x"],"reference":"The FiD (Fusion-in-Decoder) architecture was proposed by Izacard and Grave in 2021. FiD processes multiple retrieved passages independently in the encoder and then fuses them in the decoder, allowing the model to effectively leverage information from many passages simultaneously while maintaining computational efficiency."}
{"user_input":"What does RAFT stand for?","response":"I don't have enough information in the provided materials to answer this question.","retrieved_contexts":["0.150<br>0.175<br>0.200<br>0.225<br>0.250<br>Final Accuracy<br>~~Train D*~~<br>Train D* + 1D<br>~~Train D* + 2D~~<br>Train D* + 3D||||||\n|2<br>4<br>6<br>8<br>1<br># Tt Dt Tk<br>0.125<br>0.150<br>0.175<br>0.200<br>0.225<br>0.250<br>Final Accuracy<br>~~Train D*~~<br>Train D* + 1D<br>~~Train D* + 2D~~<br>Train D* + 3D||||||\n\n\n\nFigure 6: **Test-Time Documents Varying** : To analyze how robust RAFT is to varying number\n\n- f test-time documents, we study three domains \u2013 NQ, Trivia QA and HotPot QA. In NQ,\nwe find that training with 4 documents leads to optimal performance, and this changes to 3\nand 2 for for Trivia QA and HotPot QA respectively. However, we see that training with\n\n- nly _golden_ documents leads to poor performance.\n\n\ntraining with _D_ _[\u2217]_ + 3 _D_ and it is _D_ _[\u2217]_ + 1 _D_ documents with Hotpot QA. This insight has been\nparticularly beneficial for our algorithm, RAFT . In our experiments, we consistently employ\na training setup consisting of one golden document alongside four distractor documents.\n\n\n**Generalization to a variable number of test-time documents.** We extended our research\nto examine the impact of different quantities of test-time documents on the model\u2019s performance. Specifically, our experiments focused on assessing how models, trained with\nvarying numbers of distractor documents, respond to changes in the number of documents\npresented at test time. The results, illustrated in Fig. 6, confirm that the inclusion of distractor documents during training indeed makes the model more resilient to fluctuations in the\nnumber of documents encountered during testing. This ability to maintain consistent performance despite variations in test-time document numbers further validates the robustness of\n\n- ur approach, RAFT . This finding underscores the importance of a well-calibrated training\nenvironment to prepare the model for a range of","begin_quote## The Oberoi family is an\n Indian family that is famous for its involvement in hotels, namely through The\n Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in\n the Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel\n company with its head office in Delhi. ##end_quote## establishes the head\n office of The Oberoi Group. Therefore, the Oberoi family is part of a hotel\n company whose head office is in Delhi. ##Answer: Delhi\n\n```\n\nFigure 3: RAFT prompt to help LLM evaluate its own generated reasoning and answers,\ncontrasting them with the correct reasoning and answers. The LLM is prompted to identify\nerrors in its reasoning and extract key insights for improvement. This figure specifically\nrepresents the \u2018GenerateExplanation\u2018 step in the RAFT algorithm (Section 3).\n\n\nTable 1: **RAFT improves RAG performance for all specialized domains** : Across PubMed,\nHotPot, HuggingFace, Torch Hub, and Tensorflow Hub, we see that Domain-specific Finetuning improves significantly of the performance of the base model, RAFT consistently\n\n- utperforms the existing domain-specific finetuning method with or without RAG. This\nsuggests the need to train the model with context. We compare our model with LLaMA\nfinetuning receipes, and provide GPT-3.5 for reference.\n\n\nPubMed HotPot HuggingFace Torch Hub TensorFlow\n\n\nGPT-3.5 + RAG 71.60 **41.5** 29.08 60.21 65.59\n\n\nLLaMA2-7B 56.5 0.54 0.22 0 0\n\nLLaMA2-7B + RAG 58.8 0.03 26.43 08.60 43.06\n\nDSF 59.7 6.38 61.06 84.94 86.56\n\nDSF + RAG 71.6 4.41 42.59 82.80 60.29\n\n\nRAFT (LLaMA2-7B) **73.30** 35.28 **74.00** **84.95** **86.86**\n\n\n**Datasets** In our experiments, we use the following datasets to evaluate our model and\nall baselines. We"," distractor documents (] _[d][k][\u2212]_ [1][). For][ (][1] _[ \u2212]_ _[P]_ [)][ fraction of the questions]\n( _qi_ ) in the dataset, we include no golden document and only include distractor documents\n( _dk_ ). We then fine-tune the language model using standard supervised training (SFT)\ntechnique, training it to generate answers from the provided documents and question. Fig. 2\nillustrates the high-level design principal for RAFT .\n\n\nWe demonstrate that our RAG approach trains the model to perform better RAG on the set\n\n- f documents it is trained on _i.e., in-domain_ . By removing the golden documents in some\ninstances, we are compelling the model to memorize answers instead of deriving them from\nthe context. The training data for RAFT is as follows, and an example training data can be\nseen in Fig. 3:\n\n\n**P** % of data: **Q** + **D** _[\u2217]_ + **D** 1 + **D** 2 + . . . + **D** _k \u2192_ **A** _\u2217_\n\n\n(1 _\u2212_ **P** ) % of data: **Q** + **D** 1 + **D** 2 + . . . + **D** _k \u2192_ **A** _\u2217_\n\n\nSubsequently, for the test scenario, the model is provided with the Q and top-k documents\nretrieved by the RAG pipeline. Note that RAFT is independent of the retriever used.\n\n\nA key factor in enhancing training quality is the generation of a reasoning process, such\nas Chain-of-Thought, to explain the provided answers. RAFT approach is similar: we\ndemonstrate that creating a full reasoning chain and in-addition, clearly citing sources\nenhances the model\u2019s accuracy in answering questions. In Fig. 3, we illustrate this setup. Generating the training data in this fashion, involves presenting the model with a\nquestion, context, and verified answers, and then requesting it to form a reasoning chain\nthat appropriately references the original context.\n\n\nFor all the datasets in our experiments, we generate the answers using the technique\ndescribed above. Note that the Gorilla APIBench dataset, already includes reasoning\nin the answers. We provide an"," Lake Wales Medical Center is located in the city of Polk County, Florida. The population of Polk County in 2014 was\n\n15,140. So the answer is: 15,140.\n\n\nQ: Who was born first? Jan de Bont or Raoul Walsh?\nA: Jan de Bont was born on 22 October 1943. Raoul Walsh was born on March 11, 1887. Thus, Raoul Walsh was born the\n\nfirst. So the answer is: Raoul Walsh.\n\n\nQ: In what country was Lost Gravity manufactured?\nA: The Lost Gravity (roller coaster) was manufactured by Mack Rides. Mack Rides is a German company. So the answer is:\n\nGermany.\n\n\nQ: Which of the following had a debut album entitled \"We Have an Emergency\": Hot Hot Heat or The Operation M.D.?\nA: The debut album of the band \"Hot Hot Heat\" was \"Make Up the Breakdown\". The debut album of the band \"The Operation\n\nM.D.\" was \"We Have an Emergency\". So the answer is: The Operation M.D..\n\n\nQ: In which country did this Australian who was detained in Guantanamo Bay detention camp and published \"Guantanamo:\n\nMy Journey\" receive para\u2212military training?\nA: The Australian who was detained in Guantanamo Bay detention camp and published \"Guantanamo: My Journey\" is David\n\nHicks. David Hicks received his para\u2212military training in Afghanistan. So the answer is: Afghanistan.\n\n\nQ: Does The Border Surrender or Unsane have more members?\nA: The Border Surrender band has following members: Keith Austin, Simon Shields, Johnny Manning and Mark Austin. That\n\nis, it has 4 members. Unsane is a trio of 3 members. Thus, The Border Surrender has more members. So the answer is:\nThe Border Surrender.\n\n\nQ: Which band formed first, Sponge Cola or Hurricane No. 1?\nA: Sponge Cola band was formed in 1998. Hurricane No. 1 was formed in 1996. Thus, Hurricane No. 1 band formed the first.\n\nSo the answer is: Hurricane No. 1.\n\n\nQ: James Paris Lee is best known for investing the Lee\u2212Metford rifle and another rifle often referred to by what acronymn?\nA: James Paris Lee is best known for investing the Lee\u2212Metford rifle and Lee\u00e2 A\u00b8SEnfield"," Oasis Plaza about the march\n43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March\n\nOutput:\n\n```\n\n22\n\n\n_{{_\n```\n\"title\": \"Verdant Oasis Plaza and Unity March\",\n\"summary\": \"The community revolves around the Verdant Oasis Plaza, which is the location of the Unity\nMarch. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of\nwhich are associated with the march event.\",\n\"rating\": 5.0,\n\"rating e xplanation\": \"The impact severity rating is moderate due to the potential for unrest or conflict\nduring the Unity March.\",\n\"findings\": [\n```\n\n_{{_\n```\n\"summary\": \"Verdant Oasis Plaza as the central location\",\n\"explanation\": \"Verdant Oasis Plaza is the central entity in this community, serving as the location for\nthe Unity March. This plaza is the common link between all other entities, suggesting its significance\nin the community. The plaza\u2019s association with the march could potentially lead to issues such as\npublic disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data:\nEntities (5), Relationships (37, 38, 39, 40, 41,+more)]\"\n```\n\n_}}_ `,`\n_{{_\n```\n\"summary\": \"Harmony Assembly\u2019s role in the community\",\n\"explanation\": \"Harmony Assembly is another key entity in this community, being the organizer of the\nmarch at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of\nthreat, depending on their objectives and the reactions they provoke. The relationship between Harmony\nAssembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6),\nRelationships (38, 43)]\"\n```\n\n_}}_ `,`\n_{{_\n```\n\"summary\": \"Unity March as a significant event\",\n\"explanation\": \"The Unity March is a significant event taking place at Verdant Oasis Plaza. This event\nis a key factor in the community\u2019s dynamics and could be a potential source of threat, depending on the\nnature of the march and the reactions it provokes. The relationship between the march and the plaza is\ncrucial in understanding the dynamics of this community. [Data: Relationships (39)]\"\n```\n\n_}}_ `,"],"reference":"RAFT stands for Retrieval-Augmented Fine-Tuning. It is a training method proposed by Zhang et al. in 2024 that fine-tunes language models to better utilize retrieved documents by training them to distinguish relevant from irrelevant retrieved passages and to extract correct information from the relevant ones."}
{"user_input":"Why does hybrid retrieval (dense + sparse) outperform either approach alone?","response":"Hybrid retrieval (dense + sparse) outperforms either approach alone because dense and sparse retrieval methods exploit very different signals. The experimental results indicate that while dense retrieval generally outperforms sparse retrieval, combining both techniques leads to statistically significant gains in retrieval effectiveness. Specifically, the overlap between the results from dense and sparse retrieval is quite small, suggesting that each method captures distinct aspects of relevance, thereby enhancing the overall retrieval performance when used together. This finding justifies the practice of retrieving and rescoring a union of results from both methods to achieve better outcomes in various retrieval settings <document id=\"2104.05740_hybrid_retrieval_ma:chunk_5400\" \/>.","retrieved_contexts":["6 5.0 4.6\nWQ 5.9 5.9 5.8 5.7\n\nCuratedTrec 6.9 7.2 6.3 5.9\n\nSQuAD 4.5 4.1 4.0 4.0\n\n\nTable 2: The Jaccard overlap between sparse retrieval\nresults and dense retrieval results.\n\n\nhybrid retrieval is less effective than dense retrieval in most settings, which is inconsistent with\n\n- ur experimental results. Instead, we found that\ndense\u2013sparse retrieval consistently beats sparse retrieval across all settings. The gains from both\nhybrid scoring techniques are statistically significant, with the exception of top-20 for CuratedTREC. Our results might be due to better BM25\neffectiveness, but we are unable to further diagnose these differences because, once again, the hybrid retrieval code is not provided in the DPR repo.\nFurther testing also found that the differences between the two hybrid techniques are not significant. Thus, there does not appear to be a strong\nbasis to prefer one hybrid technique over the other.\nIn Table 2, we report overlap when taking different top-k results from dense retrieval and sparse\nretrieval. Overlap is measured in terms of Jaccard overlap, which is computed by the intersection over the union. It is apparent that the overlap between dense and sparse results is quite small,\nwhich suggests that they are effective in very different ways. This provides an explanation of why\nhybrid retrieval is effective, i.e., they are exploiting very different signals. These results also justify the DPR design choice of retrieving k [\u2032] - k\nresults from dense and sparse retrieval and then\nrescoring the union to arrive at the final top-k.\n\n\n4.2 End-to-End QA\n\n\nTable 3 presents results for our end-to-end question answering replication experiments on the NQ\nand TriviaQA datasets in terms of the exact match\nscore. The original results are shown in the\n\u201corig\u201d column. The \u201crepl\u201d column reports our attempt to replicate exactly the span scoring technique described in the original paper, whereas the\n\u201cGAR\u201d column shows results from using the technique proposed by Mao et al. (2020). The version of each technique that incorporates retriever\nscores (see Section 2.3) is denoted with a * symbol, i.e., \u201crepl",", our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar of text retrieval using learned dense representations, which has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using sparse bag-of-words\nrepresentations (Zhan et al., 2020; Xiong et al.,\n2020; Hofst\u00a8atter et al., 2020; Lin et al., 2020).\nOur experiments largely verify the claims of\nKarpukhin et al. (2020) regarding the effectiveness of their proposed techniques. However, we\narrived at two important additional findings, one\n\n- f which is inconsistent with the original work, the\n\n- ther of which presents an enhancement:\n\n\n1. Focusing on retrieval, we found that the effectiveness of the sparse retrieval (BM25)\nbaseline is higher than values reported by\nKarpukhin et al. (2020). Whereas they reported\nthat dense\u2013sparse hybrid results do not meaningfully improve over dense retrieval alone,\nwe arrived at the opposite conclusion, where\nhybrid techniques yield statistically significant\ngains. We are able to achieve on average\na three-point improvement in top-20 accuracy\n\n  - ver the best DPR results across five standard\nQA test collections.\n\n\n2. Focusing on end-to-end QA effectiveness, we\nexplored different techniques for evidence com\n\n[1Artifact Review and Badging](https:\/\/www.acm.org\/publications\/policies\/artifact-review-and-badging-current)\n\n\nbination to extract the final answer span.\nWhereas the original DPR paper only used\nscores from the reader to identify the final\nanswer span, we investigated combining retriever scores and further experimented with\nthe answer span selection technique described\nby Mao et al. (2020). In our best condition,\nwe were able to achieve statistically significant\nimprovements of around three points on exact\nmatch scores over the original DPR implementation, using the same exact models.\n\n\nThe main contribution of this work is the replication of DPR, where our experimental results\nadd a number of important refinements to the\n\n- riginal work. Code associated with our retrieval experiments is packaged in our Pyserini IR\ntoolkit [2] (Lin et al., 2021) and code associated with\n\n-"," with this parlance,\nwe use \u201cdocument\u201d and \u201cpassage\u201d interchangeably throughout this paper. To add to the potential\nconfusion, results of the retriever are also referred\n\nto as \u201ccontexts\u201d that are fed to the reader.\n\nDense retrieval with DPR uses a query encoder\nand a passage encoder, which are both based on\nBERT. Queries and passages are encoded as dense\nrepresentation vectors as follows:\n\n\nq [\u2217] = BERTq(q), Dj [\u2217] [=][ BERT][D][(][D][j][)]\n\n\nwhere q [\u2217] and Dj [\u2217] [are low dimensional vectors]\n(768). The relevance score of a passage to a query\nis computed by dot product:\n\n\nSim(q, Dj) = \u27e8q [\u2217], Dj [\u2217][\u27e9]\n\n\nThus, the top k retrieval problem can be recast\nas a nearest neighbor search problem in vector\nspace. Operationally, this is accomplished via\nFacebook\u2019s Faiss library (Johnson et al., 2017).\n\nKarpukhin et al. (2020) also investigated hybrid\nretrieval, combining results from dense retrieval\n(DPR) and sparse retrieval (BM25) by computing\nthe linear combination of their respective scores to\nrerank the union of the two initial retrieved sets:\n\n\n\u03bb \u00b7 Sim(q, Dj) + BM25(q, Dj),\n\n\nwhere \u03bb = 1.1, an empirical value tuned on the\ndevelopment set. BM25 retrieval was performed\nusing Lucene with parameters b = 0.4 and k1 =\n0.9. However, the DPR repo does not appear to\ncontain code for reproducing the BM25 and hybrid\nfusion results.\n\nWe attempted to replicate the retriever results\nreported in Karpukhin et al. (2020) with Pyserini,\nan IR toolkit that our group has been developing\nsince 2019 (Lin et al., 2021). The toolkit supports\nsparse retrieval (i.e., BM25) via integration with\nanother toolkit called Anserini (Yang et al., 2017),\nwhich is built on Lucene. Like in the original DPR\nwork, Pyserini supports dense retrieval via integration with Facebook\u2019s Faiss library. Combining\ndense and sparse retrieval, our toolkit supports hybrid retrieval as well.\n\nTo be clear, we started with model checkpoint\nreleases in the","ing by emphasizing on _important_ terms and discarding\nmost of the terms without information content. Expansion allows\nto enrich documents, either by implicitly adding stemming effects\n(legs \u2192 leg) or by adding relevant topic words (e.g. treatment).\n\n\n**5** **CONCLUSION**\n\n\nRecently, dense retrieval based on BERT has demonstrated its superiority for first-stage retrieval, questioning the competitiveness of\ntraditional sparse models. In this work, we have proposed SPLADE,\na sparse model revisiting query\/document expansion. Our approach\nrelies on in-batch negatives, logarithmic activation and FLOPS regularization to learn effective and efficient sparse representations.\nSPLADE is an appealing candidate for initial retrieval: it rivals the\nlatest state-of-the-art dense retrieval models, its training procedure\nis straightforward, its sparsity\/FLOPS can be controlled explicitly\nthrough the regularization, and it can operate on inverted indexes.\nIn reason of its simplicity, SPLADE is a solid basis for further improvements in this line of research.\n\n\n\n33\n\n\n32\n\n\n31\n\n\n30\n\n\n29\n\n\n28\n\n\n27\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||a<br>a|a<br>a|vg q len <br>vg d len|_ \u2243_15<br>_ \u2243_58||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||y-T5<br>|y-T5<br>|\n||||||||||||doc2quer<br>|y-T5<br>|y-T5<br>|\n||||~~a~~<br>a|~~a~~<br>a|~~vg q len~~ <br>vg d len|~~_ \u2243_6~~<br>_ \u2243_18||<br> <br>|||~~SparTerm~~<br>ST exp-_\u2113F_<br>SPLADE-|~~ lexical~~<br>_LOPS_<br>_\u2113_~~1~~||\n||||~~a~~<br>a|~~a~~<br>a||||||||||\n||||||||||||SPLADE-|_\u2113FLOPS_|_\u2113FLO","., the weight for the retriever score becomes\n\u03b3 = \u03b2 \u00b7 \u03b4). We started with step size 0.2 for\nboth \u03b2 and \u03b4 and found the optimal pair of values\n\u03b21, \u03b41. We then repeated this process with step\nsize 0.05 and then 0.01 in a smaller range around\nthe optimal \u03b2i and \u03b4i from the previous pass.\nFor final evidence fusion, we tuned the weight\nparameters together with the number of retrieval\nresults (k) up to 500 with a step size of 20. Optimal parameters were selected based on the exact\nhighest match score.\n\n\n4 Results\n\n\n4.1 Retrieval\n\n\nTable 1 reports top-k = {20, 100} retrieval accuracy from our replication attempt, compared to\nfigures copied directly from the original DPR paper; here we focus on results from RetrieverMulti.\nThe hybrid retrieval results reported in the original\nDPR paper is denoted Hybridorig, which is not directly comparable to either of our two techniques:\nHybridnorm (with minimum score normalization)\n\n- r Hybrid (without such normalization). We make\nthe following observations:\nFirst, our dense retrieval results are very close\nto those reported in Karpukhin et al. (2020). We\nconsider this a successful replication attempt and\n\n- ur efforts add veracity to the effectiveness of the\nDPR technique. Yay!\nSecond, our Pyserini BM25 implementation\n\n- utperforms the BM25 results reported in the original paper across all datasets. Furthermore, the gap\nis larger for k = 20. On average, our results represent a nearly seven-point improvement in top-20\naccuracy and a nearly five-point improvement for\ntop-100 accuracy. Since Karpukhin et al. (2020)\nhave not made available their code for generating\nthe BM25 results, we are unable to further diagnose these differences.\n\nNevertheless, the results do support the finding that dense retrieval using DPR is (generally)\nmore effective than sparse retrieval. We confirmed\n\n\n\nTop-20 Top-100\nCondition  - rig repl  - rig repl\n\n\nNQ\n\nDPR 79.4 79.5 86.0 86.1\nBM25 59.1 62.9 [\u2020] 73.7 78.3 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 78.0"],"reference":"Hybrid retrieval combines dense (embedding-based) and sparse (keyword-based like BM25) retrieval methods, leveraging their complementary strengths. Sparse retrieval excels at exact keyword matching and is effective for queries with specific terminology, proper nouns, or rare terms. Dense retrieval captures semantic similarity and handles paraphrasing, synonyms, and conceptual queries better. By combining both approaches (typically using rank fusion methods like RRF), hybrid retrieval achieves higher recall by retrieving documents that either method would miss alone, leading to more comprehensive and robust retrieval performance across diverse query types."}
{"user_input":"How does ColBERT differ from traditional dense retrieval?","response":"ColBERT differs from traditional dense retrieval in several key ways:\n\n1. **Late Interaction Paradigm**: ColBERT employs a \"late interaction\" mechanism, which isolates the computations between queries and documents. This allows for pre-computing document representations offline, enabling more efficient retrieval from large document collections (Khattab & Zaharia, 2020) <document id=\"2004.12832_colbert_khattab:chunk_13050\">.\n\n2. **Efficient Query Processing**: Unlike traditional dense retrieval methods that typically require processing each document for each query, ColBERT only processes the query through BERT once. This results in significantly lower computational costs, allowing for a much faster retrieval process (over 170\u00d7 speedup) and requiring 14,000\u00d7 fewer floating point operations per query compared to existing BERT-based models (Khattab & Zaharia, 2020) <document id=\"2004.12832_colbert_khattab:chunk_2700\">.\n\n3. **Scalability**: ColBERT scales better with the number of top documents it needs to rank. Instead of evaluating all documents exhaustively, it uses vector-similarity search indexes, such as faiss, to efficiently retrieve the top-k documents directly, improving recall relative to term-based methods (Khattab & Zaharia, 2020) <document id=\"2004.12832_colbert_khattab:chunk_6750\">.\n\nIn summary, ColBERT's use of late interaction, optimized query processing, and scalability features distinguish it from traditional dense retrieval methods.","retrieved_contexts":[" of encoding documents offline and amortizing the cost of encoding the query once across\nall ranked documents. Additionally, it enables ColBERT to leverage vector-similarity search indexes (e.g., [1, 15]) to retrieve the\ntop- _k_ results directly from a large document collection, substantially improving _recall_ - ver models that only re-rank the output of\nterm-based retrieval.\n\nAs Figure 1 illustrates, ColBERT can serve queries in tens or\nfew hundreds of milliseconds. For instance, when used for reranking as in \u201cColBERT (re-rank)\u201d, it delivers over 170\u00d7 speedup\n(and requires 14,000\u00d7 fewer FLOPs) relative to existing BERT-based\nmodels, while being more effective than every non-BERT baseline\n(\u00a74.2 & 4.3). ColBERT\u2019s indexing\u2014the only time it needs to feed\ndocuments through BERT\u2014is also practical: it can index the MS\nMARCO collection of 9M passages in about 3 hours using a single\nserver with four GPUs (\u00a74.5), retaining its effectiveness with a space\nfootprint of as litle as few tens of GiBs. Our extensive ablation\nstudy (\u00a74.4) shows that late interaction, its implementation via\nMaxSim operations, and crucial design choices within our BERTbased encoders are all essential to ColBERT\u2019s effectiveness.\nOur main contributions are as follows.\n\n\n(1) We propose _late interaction_ (\u00a73.1) as a paradigm for efficient\nand effective neural ranking.\n(2) We present ColBERT (\u00a73.2 & 3.3), a highly-effective model\nthat employs novel BERT-based query and document encoders within the late interaction paradigm.\n\n\n(3) We show how to leverage ColBERT both for re-ranking on\ntop of a term-based retrieval model (\u00a73.5) and for searching\na full collection using vector similarity indexes (\u00a73.6).\n(4) We evaluate ColBERT on MS MARCO and TREC CAR, two\nrecent passage search collections.\n\n\n**2** **RELATED WORK**\n\n\n**Neural Matching Models.** Over the past few years, IR researchers\nhave introduced numerous neural architectures for ranking. In\nthis work, we compare against KNRM [4, 36], Duet [20, 22], ConvKNRM [4], and fastText+Conv","ation. Finally, we sort the _k_ documents by their total scores.\nRelative to existing neural rankers (especially, but not exclusively, BERT-based ones), this computation is very cheap that, in\nfact, its cost is dominated by the cost of gathering and transferring\nthe pre-computed embeddings. To illustrate, ranking _k_ documents\nvia typical BERT rankers requires feeding BERT _k_ different inputs\neach of length _l_ = | _q_ | + | _di_ | for query _q_ and documents _di_, where\natention has quadratic cost in the length of the sequence. In contrast, ColBERT feeds BERT only a single, much shorter sequence of\nlength _l_ = | _q_ |. Consequently, ColBERT is not only cheaper, it also\nscales much beter with _k_ as we examine in \u00a74.2.\n\n\n**3.6** **End-to-end Top-** _k_ **Retrieval with ColBERT**\n\nAs mentioned before, ColBERT\u2019s late-interaction operator is specifically designed to enable end-to-end retrieval from a large collection,\nlargely to improve recall relative to term-based retrieval approaches.\nTis section is concerned with cases where the number of docu\nments to be ranked is too large for exhaustive evaluation of each\npossible candidate document, particularly when we are only interested in the highest scoring ones. Concretely, we focus here on\nretrieving the top- _k_ results directly from a large document collection with _N_ (e.g., _N_ = 10, 000, 000) documents, where _k_ \u226a _N_ .\nTo do so, we leverage the pruning-friendly nature of the MaxSim\n\n- perations at the backbone of late interaction. Instead of applying MaxSim between one of the query embeddings and all of one\ndocument\u2019s embeddings, we can use fast vector-similarity data\nstructures to efficiently conduct this search between the query\nembedding and _all_ document embeddings across the full collection. For this, we employ an off-the-shelf library for large-scale\nvector-similarity search, namely faiss [15] from Facebook. [4] In particular, at the end of offline indexing (\u00a73.4), we maintain a mapping\nfrom each embedding to its document of origin and then index all\ndocument embeddings into faiss.\nSubsequently"," and another for the document, extracted from\nBERT\u2019s [CLS] contextualized embedding and expanded through a\nlinear layer to dimension 4096 (which equals _Nq_ \u00d7 128 = 32 \u00d7 128).\nRelevance is estimated as the inner product of the query\u2019s and the\ndocument\u2019s embeddings, which we found to perform beter than\ncosine similarity for single-vector re-ranking. As the results show,\nthis model is considerably less effective than ColBERT, reinforcing\nthe importance of late interaction.\nSubsequently, we ask if our MaxSim-based late interaction is better than other simple alternatives. We test a model [B] that replaces\nColBERT\u2019s maximum similarity with _average_ similarity. Te results\nsuggest the importance of individual terms in the query paying\nspecial atention to particular terms in the document. Similarly,\nthe figure emphasizes the importance of our query augmentation\nmechanism: without query augmentation [C], ColBERT has a noticeably lower MRR@10. Lastly, we see the impact of end-to-end\nretrieval not only on recall but also on MRR@10. By retrieving\ndirectly from the full collection, ColBERT is able to retrieve to the\ntop-10 documents missed entirely from BM25\u2019s top-1000.\n\n\n\n|Basic ColBERT Indexing<br>+multi-GPU document processing<br>+per-batch maximum sequence length<br>+length-based bucketing<br>+multi-core pre-processing|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic"," [15] from Facebook. [4] In particular, at the end of offline indexing (\u00a73.4), we maintain a mapping\nfrom each embedding to its document of origin and then index all\ndocument embeddings into faiss.\nSubsequently, when serving queries, we use a two-stage procedure to retrieve the top- _k_ documents from the entire collection.\nBoth stages rely on ColBERT\u2019s scoring: the first is an approximate\nstage aimed at filtering while the second is a refinement stage. For\nthe first stage, we concurrently issue _Nq_ vector-similarity queries\n(corresponding to each of the embeddings in _Eq_ ) onto our faiss index. Tis retrieves the top- _k_ [\u2032] (e.g., _k_ [\u2032] = _k_ \/2) matches for that vector\n\n\n4htps:\/\/github.com\/facebookresearch\/faiss\n\n\n\nColBERT is differentiable end-to-end. We fine-tune the BERT\nencoders and train from scratch the additional parameters (i.e., the\nlinear layer and the [Q] and [D] markers\u2019 embeddings) using the\nAdam [16] optimizer. Notice that our interaction mechanism has\nno trainable parameters. Given a triple \u27e8 _q_, _d_ [+], _d_ [\u2212] \u27e9 with query _q_,\npositive document _d_ [+] and negative document _d_ [\u2212], ColBERT is used\nto produce a score for each document individually and is optimized\nvia pairwise sofmax cross-entropy loss over the computed scores\n\n- f _d_ [+] and _d_ [\u2212] .\n\n\n**3.4** **Offline Indexing: Computing & Storing**\n**Document Embeddings**\n\n\nBy design, ColBERT isolates almost all of the computations between\nqueries and documents, largely to enable pre-computing document\nrepresentations offline. At a high level, our indexing procedure is\nstraight-forward: we proceed over the documents in the collection\nin batches, running our document encoder _fD_ - n each batch and\nstoring the output embeddings per document. Although indexing\na set of documents is an offline process, we incorporate a few\nsimple optimizations for enhancing the throughput of indexing. As\nwe show in \u00a74.5, these optimizations can considerably reduce the\n\n- ffline cost of indexing.\nTo begin with, we exploit multiple GPUs, if available,","27 33.9\n\n\n**Table 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.**\n\n\nTable 4 reports the space footprint of ColBERT under various\nsetings as we reduce the embeddings dimension and\/or the bytes\nper dimension. Interestingly, the most space-efficient seting, that\nis, re-ranking with cosine similarity with 24-dimensional vectors\nstored as 2-byte floats, is only 1% worse in MRR@10 than the most\nspace-consuming one, while the former requires only 27 GiBs to\nrepresent the MS MARCO collection.\n\n\n**5** **CONCLUSIONS**\n\n\nIn this paper, we introduced ColBERT, a novel ranking model that\nemploys _contextualized late interaction_ - ver deep LMs (in particular,\nBERT) for efficient retrieval. By independently encoding queries\nand documents into fine-grained representations that interact via\ncheap and pruning-friendly computations, ColBERT can leverage\nthe expressiveness of deep LMs while greatly speeding up query\nprocessing. In addition, doing so allows using ColBERT for end-toend neural retrieval directly from a large document collection. Our\nresults show that ColBERT is more than 170\u00d7 faster and requires\n14,000\u00d7 fewer FLOPs\/query than existing BERT-based models, all\nwhile only minimally impacting quality and while outperforming\nevery non-BERT baseline.\n**Acknowledgments.** OK was supported by the Eltoukhy Family\nGraduate Fellowship at the Stanford School of Engineering. Tis\nresearch was supported in part by affiliate members and other\nsupporters of the Stanford DAWN project\u2014Ant Financial, Facebook,\nGoogle, Infosys, NEC, and VMware\u2014as well as Cisco, SAP, and the\n\n\nNSF under CAREER grant CNS-1651570. Any opinions, findings,\nand conclusions or recommendations expressed in this material are\nthose of the authors and do not necessarily reflect the views of the\nNational Science Foundation.\n\n\n**REFERENCES**\n\n\n[1] Firas Abuzaid, Geet Sethi, Peter Bailis, and Matei Zaharia. 2019. To Index or Not\nto Index: Optimizing Exact Maximum Inner Product Search. In _2019 IEEE 35th_\n_International Conference on Data Engineering (ICDE)_ . IEEE, 1250\u20131261.\n\n[2] Zhuyun Dai and Jamie Callan. "],"reference":"ColBERT (Contextualized Late Interaction over BERT) differs from traditional dense retrieval by using late interaction instead of single-vector representations. While traditional dense retrieval encodes queries and documents into single dense vectors and computes similarity via dot product, ColBERT preserves token-level representations and computes interactions between all query and document token embeddings. This approach captures fine-grained token-level matching signals while maintaining efficiency through maximum similarity (MaxSim) operations and precomputation of document embeddings. The late interaction enables better matching of specific terms and phrases compared to single-vector approaches."}
{"user_input":"What are the trade-offs between long-context LLMs and RAG systems?","response":"The trade-offs between long-context LLMs and RAG systems are nuanced, with both approaches offering unique benefits and drawbacks.\n\n1. **Contextual Management**: Long-context LLMs can handle contexts exceeding 200,000 tokens, allowing them to incorporate entire documents for tasks like question answering. This capability raises questions about the necessity of RAG systems when LLMs possess such extensive contextual capacity (<document id=\"2312.10997_rag_survey_gao:chunk_13950\">).\n\n2. **Inference Speed vs. Operational Efficiency**: Providing LLMs with vast amounts of context can significantly slow down inference speed. In contrast, RAG systems utilize chunked retrieval and on-demand input, which tend to improve operational efficiency (<document id=\"2312.10997_rag_survey_gao:chunk_13950\">).\n\n3. **Verification of Information**: RAG helps in quickly locating original references, allowing users to verify generated answers. This process remains observable compared to the more opaque nature of long-context generation, where the reasoning is less transparent (<document id=\"2312.10997_rag_survey_gao:chunk_13950\">).\n\n4. **Handling Complexity**: While long-context models have been advancing, RAG systems can still effectively tackle complex problems requiring integration and summarization from large amounts of material. This capability presents opportunities for RAG to evolve and address intricate questions (<document id=\"2312.10997_rag_survey_gao:chunk_13950\">).\n\n5. **Robustness to Misinformation**: RAG can suffer from the presence of noise or contradictory information during retrieval, which can degrade output quality. Hence, mitigating these issues is crucial for RAG, whereas long-context LLMs may rely more on their internalized knowledge, potentially resulting in \"hallucinations\" or inaccurate information (<document id=\"2312.10997_rag_survey_gao:chunk_13950\">).\n\nIn summary, while long-context LLMs offer extensive context and potentially faster integration, RAG systems provide efficiency, verifiable information, and the ability to manage complex queries better.","retrieved_contexts":[" [170]\u2013[172]. Presently, LLMs can\neffortlessly manage contexts exceeding 200,000 tokens [9] . This\ncapability signifies that long-document question answering,\npreviously reliant on RAG, can now incorporate the entire\ndocument directly into the prompt. This has also sparked\ndiscussions on whether RAG is still necessary when LLMs\n\n\n[8https:\/\/www.trulens.org\/trulens eval\/core concepts rag triad\/](https:\/\/www.trulens.org\/trulens_eval\/core_concepts_rag_triad\/)\n[9https:\/\/kimi.moonshot.cn](https:\/\/kimi.moonshot.cn)\n\n\n\nare not constrained by context. In fact, RAG still plays an\nirreplaceable role. On one hand, providing LLMs with a\nlarge amount of context at once will significantly impact its\ninference speed, while chunked retrieval and on-demand input\ncan significantly improve operational efficiency. On the other\nhand, RAG-based generation can quickly locate the original\nreferences for LLMs to help users verify the generated answers. The entire retrieval and reasoning process is observable,\nwhile generation solely relying on long context remains a\nblack box. Conversely, the expansion of context provides new\n\n- pportunities for the development of RAG, enabling it to\naddress more complex problems and integrative or summary\nquestions that require reading a large amount of material to\nanswer [49]. Developing new RAG methods in the context of\nsuper-long contexts is one of the future research trends.\n\n\n_B. RAG Robustness_\n\n\nThe presence of noise or contradictory information during\nretrieval can detrimentally affect RAG\u2019s output quality. This\nsituation is figuratively referred to as \u201cMisinformation can\nbe worse than no information at all\u201d. Improving RAG\u2019s\nresistance to such adversarial or counterfactual inputs is gaining research momentum and has become a key performance\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\ntype of documents should be retrieved, evaluate the relevance\n\n- f the documents to the prompt, their position, and the\nnumber included in the context. The research findings reveal\nthat including irrelevant documents can unexpectedly increase\naccuracy by over 30%, contradicting the initial assumption\n\n- f reduced quality. These results underscore the importance\n\n- f developing specialized strategies to integrate retrieval with\n","LM\nfine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth,\nyet it has not been accompanied by a systematic synthesis that\ncould clarify its broader trajectory. This survey endeavors to\nfill this gap by mapping out the RAG process and charting\nits evolution and anticipated future paths, with a focus on the\nintegration of RAG within LLMs. This paper considers both\ntechnical paradigms and research methods, summarizing three\nmain research paradigms from over 100 RAG studies, and\nanalyzing key technologies in the core stages of \u201cRetrieval,\u201d\n\u201cGeneration,\u201d and \u201cAugmentation.\u201d On the other hand, current\nresearch tends to focus more on methods, lacking analysis and\nsummarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks,\nand evaluation methods applicable to RAG. Overall, this\npaper sets out to meticulously compile and categorize the\nfoundational technical concepts, historical progression, and\nthe spectrum of RAG methodologies and applications that\nhave emerged post-LLMs. It is designed to equip readers and\nprofessionals with a detailed and structured understanding of\nboth large models and RAG. It aims to illuminate the evolution\n\n- f retrieval augmentation techniques, assess the strengths and\nweaknesses of various approaches in their respective contexts,\nand speculate on upcoming trends and innovations.\nOur contributions are as follows:\n\n\n_\u2022_ In this survey, we present a thorough and systematic\nreview of the state-of-the-art RAG methods, delineating\nits evolution through paradigms including naive RAG,\n\n\n2\n\n\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\n\n\n\nadvanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRet",". This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, and\ndelve into their synergies, elucidating how these components intricately collaborate to form a cohesive and\neffective RAG framework.\n\n\n_\u2022_ We have summarized the current assessment methods of\n\nRAG, covering 26 tasks, nearly 50 datasets, outlining\nthe evaluation objectives and metrics, as well as the\ncurrent evaluation benchmarks and tools. Additionally,\nwe anticipate future directions for RAG, emphasizing\npotential enhancements to tackle current challenges.\n\n\nThe paper unfolds as follows: Section II introduces the\nmain concept and current paradigms of RAG. The following\nthree sections explore core components\u2014\u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, respectively. Section III focuses\n\n- n optimization methods in retrieval,including indexing, query\nand embedding optimization. Section IV concentrates on postretrieval process and LLM fine-tuning in generation. Section V\nanalyzes the three augmentation processes. Section VI focuses\n\n- n RAG\u2019s downstream tasks and evaluation system. Section VII mainly discusses the challenges that RAG currently\n\n\n\nfaces and its future development directions. At last, the paper\nconcludes in Section VIII.\n\n\nII. OVERVIEW OF RAG\n\n\nA typical application of RAG is illustrated in Figure 2.\nHere, a user poses a question to ChatGPT about a recent,\nwidely discussed news. Given ChatGPT\u2019s reliance on pretraining data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information\ngap by sourcing and incorporating knowledge from external\ndatabases. In this case, it gathers relevant news articles related\nto the user\u2019s query. These articles, combined with the original\nquestion, form a comprehensive prompt that empowers LLMs\nto generate a well-informed answer.\nThe RAG research paradigm is continuously evolving, and\nwe categorize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\n\n- f the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response"," Wei et al. (2022); Anthropic\n(2023), and in the presence of distractor documents ( _Dk_ ). We explain the methodology in\nSection 3 and analyze the sensitivity to the number of distractor documents ( _k_ ) at train- and\ntest- time in Section 5. RAFT consistently outperforms Supervised-finetuning both withand without- RAG across PubMed Dernoncourt & Lee (2017), HotPot QA Yang et al. (2018),\nand HuggingFace Hub, Torch Hub, and Tensorflow Hub Gorilla datasets Patil et al. (2023),\npresenting a novel, yet simple technique to improve pre-trained LLMs for in-domain RAG.\nOur code is available at `[https:\/\/github.com\/ShishirPatil\/gorilla](https:\/\/github.com\/ShishirPatil\/gorilla)` .\n\n\n**2** **LLMs for Open-Book Exam**\n\n\nTo understand our goal better, we expand on our analogy between training an LLM with\nthe real-world setting of prepararing for an exam.\n\n\n**Closed-Book Exam** A closed book exam often refers to the scenario where the LLMs do\nnot have access to any additional documents or references to answer the questions during\n\n\n2\n\n\nPreprint, Under Review\n\n\nFigure 2: **Overview of our RAFT method.** The top-left figure depicts our approach of\nadapting LLMs to _reading_ solution from a set of positive and distractor documents in\ncontrast to standard RAG setup where models are trained based on the retriever outputs,\nwhich is a mixture of both memorization and reading. At test time, all methods follow the\nstandard RAG setting, provided with a top-k retrieved documents in the context.\n\n\nthe exam. For LLMs, this is equivalent to the scenario, for example, in which the LLM is\nused as a chatbot. In this scenario the LLM draws from the knowledge baked in during\npre-training and supervised-finetuning to respond to the users\u2019 prompt.\n\n\n**Open Book Exam** In contrast, we liken the open-book exam setting to the scenario in\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\nIn such scenarios, typically, the LLM is paired with retriever which retrieves \u2018k\u2019 documents\n(or specific segments of the"," et al., 2024;\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\nrelevant documents, but simply uses \u201cdoes not contain the answer\u201d as a proxy for irrelevant context.\n\n\n**Reducing Hallucinations.** There have also been efforts to improve RAG factuality on open-book QA\ntasks (Asai et al., 2023; Mineiro, 2024; Simhi et al., 2024; Wang et al., 2024b; Zhang et al., 2024b).\nThe main theme is to improve both the generation and retrieval quality, often by fine-tuning one or\nmore components. Also, since RAG leads to very long contexts, another issue that arises is the \u201clost\nin the middle\u201d problem (Hsieh et al., 2024; Liu et al., 2024; Yu et al., 2024). These works start with\nthe premise that the provided query\/context should be precisely answerable by the LLM, and hence,\n\n- nly analyze their findings in the sufficient context scenario. Independent of RAG, many papers\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang\net al., 2024; Kadavath et al., 2022; Yin et al., 2023; Zhang et al., 2024a) and performance across\ndisaggregated subsets of data (Paes et al., 2022; Joren et al., 2023).\n\n\n3 SUFFICIENT CONTEXT\n\n\nAt a high level, our aim is to classify input instances based on whether the context contains enough\ninformation to answer the query. We split possible contexts into two cases: (1) **Sufficient Context.**\nThe context is sufficient to answer the query if it contains all the necessary information to provide a\ndefinitive answer. (2) **Insufficient Context.** Otherwise, a context is insufficient. A context may also\nbe insufficient if the query requires specialized knowledge that is not provided in the context or if\nthe information in the context is incomplete, inconclusive, or contradictory. In this section, we more\nthoroughly discuss sufficient context. Then, we show how to accurately and scalably label instances.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\n3.1 DEFIN"],"reference":"Long-context LLMs and RAG systems have different trade-offs. Long-context LLMs can process entire documents directly, avoiding retrieval errors and maintaining full context, but they face challenges including higher computational costs (quadratic attention complexity), difficulty focusing on relevant information in very long contexts (lost-in-the-middle problem), inability to update knowledge without retraining, and higher inference latency. RAG systems offer advantages including efficient access to large knowledge bases, easy knowledge updates by modifying the retrieval index, lower inference costs by processing only relevant chunks, and explicit source attribution. However, RAG systems depend on retrieval quality, may miss relevant information if retrieval fails, and require careful engineering of chunking and retrieval strategies. The choice depends on the use case: RAG is often preferred for knowledge-intensive tasks requiring up-to-date information, while long-context LLMs suit applications requiring deep understanding of specific documents."}
{"user_input":"How do GraphRAG and FiD differ in their approach to multi-document reasoning?","response":"GraphRAG and FiD (Fusion-in-Decoder) differ in their approaches to multi-document reasoning primarily in their methodologies. \n\n**GraphRAG** utilizes a graph-based approach, focusing on the community structure within the data. It creates a graph index from the source data and applies graph-based community detection to partition the information thematically. This allows for the generation of increasingly global summaries by making use of the LLM to summarize nodes within the graph\u2019s hierarchical community structure. This method enhances global sensemaking by leveraging the modular nature of graphs, thus enabling it to answer queries that necessitate understanding across a broader context of documents rather than being limited to adjacent or closely related passages (<document id=\"2404.16130_graphrag_edge:chunk_2250\">).\n\nOn the other hand, **FiD** operates by aggregating information directly from multiple documents, typically through a sequential encoding and decoding mechanism where relevant information from each document is concatenated and then processed together in the decoder's step. This allows FiD to maintain the individual contributions of each document but may limit its efficiency in handling broader contextual relationships as compared to GraphRAG's modular community-based analysis.\n\nIn summary, GraphRAG emphasizes community structures and modularity to enhance reasoning across documents, while FiD focuses on aggregating content from multiple documents without leveraging the inherent structural relationships present in the data.","retrieved_contexts":["hi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n(e.g., passages, tables) and edges encoding lexical and semantical similarity or structural relationships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of\ngraphs in this context: their inherent _modularity_ (Newman, 2006) and the ability to partition graphs\ninto nested modular communities of closely related nodes (e.g., Louvain, Blondel et al. 2008; Leiden, Traag et al. 2019). Specifically, GraphRAG recursively creates increasingly global summaries\nby using the LLM to create summaries spanning this community hierarchy.\n\n\n**2.3** **Adaptive benchmarking for RAG Evaluation**\n\n\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthese benchmarks are oriented towards vector RAG performance, i.e., they evaluate performance\n\n- n explicit fact retrieval. In this work, we propose an approach for generating a set of questions\nfor evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM\nmethods that use a corpus to generate questions whose answers would be summaries of the corpus,\nsuch as in Xu and Lapata (2021). However, in order to produce a fair evaluation, our method avoids\ngenerating the questions directly from the corpus itself (as an alternative implementation, one can\nuse a subset of the corpus held out from subsequent graph extraction and answer evaluation steps).\n\n\n_Adaptive benchmarking_ refers to the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases. Recent work has used LLMs for adaptive benchmarking\nto ensure relevance, diversity, and alignment with the target application or task (Yuan et al., 2024;\nZhang et al., 2024b). In this work, we propose an adaptive benchmarking approach to generating\nglobal sensemaking queries for the LLM. Our approach builds on prior work in LLM-based persona\ngeneration, where the LLM is used to generate diverse and authentic sets of personas (","2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao et al., 2023; Khattab et al., 2022; Shao et al., 2023; Su et al., 2020; Trivedi et al., 2022;\nWang et al., 2024). In particular, GraphRAG is similar to other approaches that use hierarchical\nindexing to create summaries (similar to Kim et al. 2023; Sarthi et al. 2024). GraphRAG contrasts\nwith these approaches by generating a graph index from the source data, then applying graph-based\ncommunity detection to create a thematic partitioning of the data.\n\n\n**2.2** **Using Knowledge Graphs with LLMs and RAG**\n\n\nApproaches to knowledge graph extraction from natural language text corpora include rulematching, statistical pattern recognition, clustering, and embeddings (Etzioni et al., 2004; Kim et al.,\n2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of\nresearch that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022;\nOpenAI, 2023; Tan et al., 2017; Trajanoska et al., 2023; Yao et al., 2023; Yates et al., 2007; Zhang\net al., 2024a). It also adds to a growing body of RAG approaches that use a knowledge graph as\nan index (Gao et al., 2023). Some techniques use subgraphs, elements of the graph, or properties\n\n- f the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023)\n\n- r as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n","\u201d, \u201cfaithfulness\u201d,\nand \u201canswer relevance\u201d (RAGAS, Es et al. 2023). Lacking a gold standard for evaluation, one can\nquantify relative performance for a given criterion by prompting the LLM to compare generations\nfrom two different competing models (LLM-as-a-judge, (Zheng et al., 2024)). In this work, we design criteria for evaluating RAG-generated answers to global sensemaking questions and evaluate\n\n- ur results using the comparative approach. We also validate results using statistics derived from\nLLM-extracted statements of verifiable facts, or \u201cclaims.\u201d\n\n\n**3** **Methods**\n\n\n**3.1** **GraphRAG Workflow**\n\n\nFigure 1 illustrates the high-level data flow of the GraphRAG approach and pipeline. In this section,\nwe describe the key design parameters, techniques, and implementation details for each step.\n\n\n**3.1.1** **Source Documents** _\u2192_ **Text Chunks**\n\n\nTo start, the documents in the corpus are split into text chunks. The LLM extracts information from\neach chunk for downstream processing. Selecting the size of the chunk is a fundamental design\ndecision; longer text chunks require fewer LLM calls for such extraction (which reduces cost) but\nsuffer from degraded recall of information that appears early in the chunk (Kuratov et al., 2024; Liu\net al., 2023). See Section A.1 for prompts and examples of the recall-precision trade-offs.\n\n\n**3.1.2** **Text Chunks** _\u2192_ **Entities & Relationships**\n\n\nIn this step, the LLM is prompted to extract instances of important _entities_ and the _relationships_\nbetween the entities from a given chunk. Additionally, the LLM generates short descriptions for the\nentities and relationships. To illustrate, suppose a chunk contained the following text:\n\n\n4\n\n\nNeoChip\u2019s (NC) shares surged in their first week of trading on the NewTech Exchange. However, market analysts caution that the chipmaker\u2019s public debut may\nnot reflect trends for other technology IPOs. NeoChip, previously a private entity,\nwas acquired by Quantum Systems in 2016. The innovative semiconductor firm\nspecializes in low-power processors for wearables and IoT devices.\n\n\nThe LLM is prompted such that it extracts the following:\n\n\n    - The entity `NeoChip`, with description \u201cNeoChip is a publicly"," summarization: for low-level community summaries ( **C3** ), GraphRAG required 2633% fewer context tokens, while for root-level community summaries ( **C0** ), it required over 97%\nfewer tokens. For a modest drop in performance compared with other global methods, root-level\nGraphRAG offers a highly efficient method for the iterative question answering that characterizes\nsensemaking activity, while retaining advantages in comprehensiveness (72% win rate) and diversity\n(62% win rate) over vector RAG.\n\n\n10\n\n\nTable 3: Average number of extracted claims, reported by condition and dataset type. Bolded values\nrepresent the highest score in each column.\n\n\n**Average Number of Claims**\n**Condition**\n\n**News Articles** **Podcast Transcripts**\n\n\nC0 **34.18** 32.21\n\nC1 32.50 32.20\n\nC2 31.62 **32.46**\n\nC3 33.14 32.28\n\nTS 32.89 31.39\n\nSS 25.23 26.50\n\n\n**5.2** **Experiment 2**\n\n\nTable 3 shows the results for the average number of extracted claims (i.e., the claim-based measure\n\n- f comprehensiveness) per condition. For both the News and Podcast datasets, all global search\nconditions ( **C0-C3** ) and source text summarization ( **TS** ) had greater comprehensiveness than vector\nRAG ( **SS** ). The differences were statistically significant (p _<_ .05) in all cases. These findings align\nwith the LLM-based win rates from Experiment 1.\n\n\nTable 4 contains the results for the average number of clusters, the claim-based measure of diversity. For the Podcast dataset, all global search conditions had significantly greater diversity than **SS**\nacross all distance thresholds (p _<_ .05), consistent with the win rates observed in Experiment 1. For\nthe News dataset, however, only **C0** significantly outperformed **SS** across all distance thresholds\n(p _<_ .05). While **C1-C3** also achieved higher average cluster counts than **SS**, the differences were\nstatistically significant only at certain distance thresholds. In Experiment 1, all global search conditions significantly outperformed **SS** in the News dataset \u2013 not just **C0** . However, the differences"," when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https:\/\/github _._ [com\/microsoft\/graphrag. In ad-](https:\/\/github.com\/microsoft\/graphrag)\ndition, versions of the GraphRAG approach are also available as extensions to multiple opensource libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), NebulaGraph (NebulaGraph, 2024), and Neo4J (Neo4J, 2024).\n\n\n**2** **Background**\n\n\n**2.1** **RAG Approaches and Systems**\n\n\nRAG generally refers to any system where a user query is used to retrieve relevant information from\nexternal data sources, whereupon this information is incorporated into the generation of a response\nto the query by an LLM (or other generative AI model, such as a multi-media model). The query and\nretrieved records populate a prompt template, which is then passed to the LLM (Ram et al., 2023).\nRAG is ideal when the total number of records in a data source is too large to include in a single\nprompt to the LLM, i.e. the amount of text in the data source exceeds the LLM\u2019s context window.\n\n\nIn canonical RAG approaches, the retrieval process returns a set number of records that are semantically similar to the query and the generated answer uses only the information in those retrieved\nrecords. A common approach to conventional RAG is to use text embeddings, retrieving records\nclosest to the query in vector space where closeness corresponds to semantic similarity (Gao et al.,\n2023). While some RAG approaches may use alternative retrieval mechanisms, we collectively refer\nto the family of conventional approaches as _vector RAG_ . GraphRAG contrasts with vector RAG in\nits ability to answer queries that require global sensemaking over the entire data corpus.\n\n\n2\n\n\nGraphRAG builds upon prior work on advanced RAG strategies. GraphRAG leverages summaries\n\n- ver large sections of the data source as a form of \u201dself-memory\u201d (described in Cheng et al. 2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao"],"reference":"GraphRAG and FiD take fundamentally different approaches to multi-document reasoning. FiD (Fusion-in-Decoder) uses a sequence-to-sequence architecture where retrieved passages are processed independently in parallel by the encoder, and then their representations are concatenated and fused in the decoder during generation. This allows the model to implicitly learn to combine information from multiple passages through attention mechanisms, but relationships between passages are only captured during decoding. GraphRAG, on the other hand, explicitly constructs a knowledge graph from retrieved documents, creating structured representations of entities and their relationships across documents. It then uses graph-based reasoning methods to traverse and aggregate information from the graph structure. GraphRAG's explicit graph structure makes multi-hop reasoning paths more interpretable and can better capture complex relationships, while FiD's approach is more flexible and doesn't require structured knowledge extraction but is less interpretable."}
{"user_input":"What evaluation metrics are recommended for both retrieval and generation in RAG?","response":"The recommended evaluation metrics for both retrieval and generation in Retrieval-Augmented Generation (RAG) systems include:\n\n### Quality Scores:\n1. **Context Relevance**: Evaluates the precision and specificity of the retrieved context.\n2. **Answer Faithfulness**: Ensures that the generated answers are true to the retrieved context.\n3. **Answer Relevance**: Assesses whether the generated answers are pertinent to the questions posed.\n\n### Required Abilities:\n1. **Noise Robustness**: Evaluates the model\u2019s capability to manage irrelevant or misleading documents.\n2. **Negative Rejection**: Assesses the model\u2019s ability to avoid responding when the retrieved documents lack pertinent information.\n3. **Information Integration**: Looks at how well the model synthesizes information from multiple documents.\n4. **Counterfactual Robustness**: Tests the model\u2019s ability to recognize and disregard inaccuracies within documents.\n\n### Specific Metrics:\n- **Retrieval Quality Metrics**: Hit Rate, Mean Reciprocal Rank (MRR), and Normalized Discounted Cumulative Gain (NDCG).\n- **Generation Quality Metrics**: BLEU, ROUGE, and BertScore, along with task-specific metrics like Exact Match (EM) and F1 scores for particular tasks (e.g., question answering).\n\nOverall, both retrieval and generation quality can be assessed through manual or automatic means tailored to the specific contexts of RAG application <document id=\"2312.10997_rag_survey_gao:chunk_12150\">.","retrieved_contexts":["LM\nfine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth,\nyet it has not been accompanied by a systematic synthesis that\ncould clarify its broader trajectory. This survey endeavors to\nfill this gap by mapping out the RAG process and charting\nits evolution and anticipated future paths, with a focus on the\nintegration of RAG within LLMs. This paper considers both\ntechnical paradigms and research methods, summarizing three\nmain research paradigms from over 100 RAG studies, and\nanalyzing key technologies in the core stages of \u201cRetrieval,\u201d\n\u201cGeneration,\u201d and \u201cAugmentation.\u201d On the other hand, current\nresearch tends to focus more on methods, lacking analysis and\nsummarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks,\nand evaluation methods applicable to RAG. Overall, this\npaper sets out to meticulously compile and categorize the\nfoundational technical concepts, historical progression, and\nthe spectrum of RAG methodologies and applications that\nhave emerged post-LLMs. It is designed to equip readers and\nprofessionals with a detailed and structured understanding of\nboth large models and RAG. It aims to illuminate the evolution\n\n- f retrieval augmentation techniques, assess the strengths and\nweaknesses of various approaches in their respective contexts,\nand speculate on upcoming trends and innovations.\nOur contributions are as follows:\n\n\n_\u2022_ In this survey, we present a thorough and systematic\nreview of the state-of-the-art RAG methods, delineating\nits evolution through paradigms including naive RAG,\n\n\n2\n\n\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\n\n\n\nadvanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRet",". This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, and\ndelve into their synergies, elucidating how these components intricately collaborate to form a cohesive and\neffective RAG framework.\n\n\n_\u2022_ We have summarized the current assessment methods of\n\nRAG, covering 26 tasks, nearly 50 datasets, outlining\nthe evaluation objectives and metrics, as well as the\ncurrent evaluation benchmarks and tools. Additionally,\nwe anticipate future directions for RAG, emphasizing\npotential enhancements to tackle current challenges.\n\n\nThe paper unfolds as follows: Section II introduces the\nmain concept and current paradigms of RAG. The following\nthree sections explore core components\u2014\u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, respectively. Section III focuses\n\n- n optimization methods in retrieval,including indexing, query\nand embedding optimization. Section IV concentrates on postretrieval process and LLM fine-tuning in generation. Section V\nanalyzes the three augmentation processes. Section VI focuses\n\n- n RAG\u2019s downstream tasks and evaluation system. Section VII mainly discusses the challenges that RAG currently\n\n\n\nfaces and its future development directions. At last, the paper\nconcludes in Section VIII.\n\n\nII. OVERVIEW OF RAG\n\n\nA typical application of RAG is illustrated in Figure 2.\nHere, a user poses a question to ChatGPT about a recent,\nwidely discussed news. Given ChatGPT\u2019s reliance on pretraining data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information\ngap by sourcing and incorporating knowledge from external\ndatabases. In this case, it gathers relevant news articles related\nto the user\u2019s query. These articles, combined with the original\nquestion, form a comprehensive prompt that empowers LLMs\nto generate a well-informed answer.\nThe RAG research paradigm is continuously evolving, and\nwe categorize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\n\n- f the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response","fulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation methods [29], [161], [163].\n\n\n_C. Evaluation Aspects_\n\nContemporary evaluation practices of RAG models emphasize three primary quality scores and four essential abilities,\nwhich collectively inform the evaluation of the two principal\ntargets of the RAG model: retrieval and generation.\n_1) Quality Scores:_ Quality scores include context relevance, answer faithfulness, and answer relevance. These quality scores evaluate the efficiency of the RAG model from\ndifferent perspectives in the process of information retrieval\nand generation [164]\u2013[166].\n_Context Relevance_ evaluates the precision and specificity\n\n- f the retrieved context, ensuring relevance and minimizing\nprocessing costs associated with extraneous content.\n_Answer Faithfulness_ ensures that the generated answers\nremain true to the retrieved context, maintaining consistency\nand avoiding contradictions.\n_Answer Relevance_ requires that the generated answers are\ndirectly pertinent to the posed questions, effectively addressing\nthe core inquiry.\n_2) Required Abilities:_ RAG evaluation also encompasses\nfour abilities indicative of its adaptability and efficiency:\nnoise robustness, negative rejection, information integration,\nand counterfactual robustness [167], [168]. These abilities are\ncritical for the model\u2019s performance under various challenges\nand complex scenarios, impacting the quality scores.\n_Noise Robustness_ appraises the model\u2019s capability to manage noise documents that are question-related but lack substantive information.\n\n_Negative Rejection_ assesses the model\u2019s discernment in\nrefraining from responding when the retrieved documents do\nnot contain the necessary knowledge to answer a question.\n_Information Integration_ evaluates the model\u2019s proficiency in\nsynthesizing information from multiple documents to address\ncomplex questions.\n_Counterfactual Robustness_ tests the model\u2019s ability to rec\n- gnize and disregard known inaccuracies within documents,\neven when instructed about potential misinformation.\nContext relevance and noise robustness are important for\nevaluating the quality of retrieval, while answer faithfulness,\nanswer relevance, negative rejection, information integration,\nand counterfactual robustness are important for evaluating the\nquality of generation.\n\n\n13\n\n\nTABLE II\n\nDOWNSTREAM TASKS AND DATASETS OF RAG\n\n\nTask Sub Task Dataset Method\n\n\n[26], [30], ["," in the context. The research findings reveal\nthat including irrelevant documents can unexpectedly increase\naccuracy by over 30%, contradicting the initial assumption\n\n- f reduced quality. These results underscore the importance\n\n- f developing specialized strategies to integrate retrieval with\nlanguage generation models, highlighting the need for further\nresearch and exploration into the robustness of RAG.\n\n\n_C. Hybrid Approaches_\n\n\nCombining RAG with fine-tuning is emerging as a leading\nstrategy. Determining the optimal integration of RAG and\nfine-tuning whether sequential, alternating, or through end-toend joint training\u2014and how to harness both parameterized\n\n\n15\n\n\n\nTABLE IV\n\nSUMMARY OF EVALUATION FRAMEWORKS\n\n\n**Evaluation Framework** **Evaluation Targets** **Evaluation Aspects** **Quantitative Metrics**\n\n\n\nAccuracy\nEM\n\nAccuracy\nAccuracy\n\n\n\nRetrieval Quality\nRGB _[\u2020]_\nGeneration Quality\n\n\n\nNoise Robustness\n\nNegative Rejection\nInformation Integration\nCounterfactual Robustness\n\n\n\nRECALL _[\u2020]_ Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\n\n\n\n    \n    \nCosine Similarity\n\n\nAccuracy\nAccuracy\nAccuracy\n\n\n    \n    \n    \n\nBLEU\n\nROUGE-L\n\nBertScore\n\nRAGQuestEval\n\n\n\nRetrieval Quality\nRAGAS _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nARES _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nTruLens _[\u2021]_\nGeneration Quality\n\n\nRetrieval Quality\nCRUD _[\u2020]_\nGeneration Quality\n\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nContext Relevance\n\nFaithfulness\n\nAnswer Relevance\n\n\nCreative Generation\n\nKnowledge-intensive QA\nError Correction\n\nSummarization\n\n\n\n\n_\u2020 represents a benchmark, and \u2021 represents a tool. * denotes customized quantitative metrics, which deviate from traditional_\n_metrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these_\n_metrics, as required._\n\n\n\nand non-parameterized advantages are areas ripe for exploration [27]. Another trend is to introduce SLMs with specific\nfunctionalities into RAG and fine-tuned by the results of RAG\nsystem. For example, CRAG [67] trains a lightweight retrieval\nevaluator to assess the overall quality of the retrieved documents for a query and triggers different knowledge retrieval\nactions based on confidence levels",", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop\/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation"],"reference":"RAG systems should be evaluated on both retrieval and generation quality. For retrieval, recommended metrics include: Recall@K (what fraction of relevant documents are in top-K results), Mean Reciprocal Rank (MRR, measures ranking quality), and NDCG (Normalized Discounted Cumulative Gain, considers both relevance and ranking). For generation quality, recommended metrics include: answer correctness (semantic similarity to reference answer using LLM-based evaluation), faithfulness (whether the answer is grounded in retrieved context), answer relevancy (whether the answer addresses the question), and context precision\/recall (whether retrieved chunks are relevant and sufficient). Framework like RAGAS provides LLM-based metrics that avoid issues with incomplete ground truth. End-to-end metrics like exact match and F1 score on question answering benchmarks are also commonly used to measure overall RAG system performance."}
{"user_input":"How to bake bread?","response":"I don't have enough information in the provided materials to answer this question.","retrieved_contexts":[".2%)<br>15.87 (77.8%)<br>14.72 (72.2%)<br>20.39 (100%)|47.12 (85.6%)<br>51.08 (92.8%)<br>49.28 (89.5%)<br>55.04 (100%)|42.99 (80.3%)<br>46.27 (86.8%)<br>45.91 (85.7%)<br>52.97 (100%)|\n\n\n(b) VisRAG-Gen: _Single-image VLM (MiniCPM-V 2.0)_\n\n|top-1<br>Page Concatenation t to op p- -2<br>3<br>Oracle|5907(980%)<br>57. (95.<br>35 1%)<br>59. (98.<br>19 2%)<br>60. (10.<br>29 0%)<br>.|3492(880%)<br>19. (48.<br>05 0%)<br>. .<br>2222(560%)<br>39. (10.<br>68 0%)<br>.|3942(744%)<br>. .<br>3232(610%)<br>. .<br>2487(470%)<br>52. (10.<br>96 0%)<br>.|2953(865%)<br>. .<br>2214(649%)<br>. .<br>2033(596%)<br>. .<br>3412(100%)<br>.|1784(774%)<br>15. (66.<br>41 8%)<br>16. (73.<br>92 4%)<br>23. (10.<br>06 0%)<br>.|3615(918%)<br>33. (84.<br>45 9%)<br>. .<br>3022(767%)<br>. .<br>3939(100%)<br>.|3616(860%)<br>29. (70.<br>95 1%)<br>28. (68.<br>96 5%)<br>41. (10.<br>58 0"," sinkbasin 1, a\nstoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.\nYour task is to: put a clean lettuce in diningtable.\n\n - think: To solve the task, I need to find and take a lettuce, then clean it with\nsinkbasin, then put it in diningtable.\n\nOK.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - go to fridge 1\nThe fridge 1 is closed.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - open fridge 1\nYou open the fridge 1. The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n\n - think: First I need to find a lettuce.\n\nOK.\n\n - take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n\n - think: I need to clean this lettuce (1) using sinkbasin 1.\n\nOK.\n\n - go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n\n - think: I need to clean this lettuce (1) using sinkbasin 1.\n\nOK.\n\n - clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n\n - think: I need to put this cleaned lettuce (1) in\/on diningtable 1.\n\nOK.\n\n - go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup "," a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n\n1.\n\nYour task is to: put a clean knife in countertop.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nYou pick up the knife 1 from the countertop 2.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\nNothing happens.\n\n - go to countertop 3\nOn the countertop 3, you see a bread 3, a butterknife 2, a cellphone 1, a creditcard\n1, a fork 2, a houseplant 1, a knife 2, a spatula 1, a statue 3, a tomato 3, a tomato\n2, a tomato 1, and a vase 2.\n\n - take knife 2 from countertop 3\nNothing happens.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a plate 2, a\npotato 3, and a statue 1.\n\n - take knife 1 from countertop 2\nNothing happens.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - clean knife 1 with sinkbasin 1\n\n"," 1, a drawer 3, a drawer 2, a drawer 1, a\nfridge 1, a garbagecan 1, a microwave 1, a shelf 3, a shelf 2, a shelf 1, a sinkbasin\n1, a stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster\n\n1.\n\nYour task is to: put a clean knife in countertop.\n\n - think: To solve the task, I need to find and take a knife, then clean it with\nsinkbasin, then put it in countertop.\n\nOK.\n\n - think: First I need to find a knife. A knife is more likely to appear in cabinet\n(1-6), drawer (1-3), countertop (1-3), fridge (1), garbagecan (1), shelf (1-3),\nsinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one, starting with\n\ncabinet 1.\n\nOK.\n\n - go to cabinet 1\nOn the cabinet 1, you see a bowl 1.\n\n - go to cabinet 2\n\nThe cabinet 2 is closed.\n\n - go to cabinet 3\nOn the cabinet 3, you see a glassbottle 1.\n\n - go to cabinet 4\nOn the cabinet 4, you see a mug 1.\n\n - go to cabinet 5\n\nThe cabinet 5 is closed.\n\n - go to cabinet 6\n\nThe cabinet 6 is closed.\n\n - go to drawer 1\n\nThe drawer 1 is closed.\n\n - go to drawer 2\n\nThe drawer 2 is closed.\n\n - go to drawer 3\n\nThe drawer 3 is closed.\n\n - go to countertop 1\nOn the countertop 1, you see a lettuce 2, a mug 2, a peppershaker 1, and a spoon 2.\n\n - go to countertop 2\nOn the countertop 2, you see a cup 1, a dishsponge 1, a glassbottle 3, a knife 1, a\nplate 2, a potato 3, and a statue 1.\n\n - think: Now I find a knife (1). Next, I need to take it.\n\nOK.\n\n - take knife 1 from"," The fridge 1 is open. In it, you see a cup 3, a egg 2, a\npotato 3, and a potato 2.\n_>_ go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a\nfork 2, a knife 2, a knife 1, a ladle 1, a lettuce 1, a mug 2, a mug 1, a pan 2, a\npeppershaker 1, a spatula 3, a tomato 2, and a tomato 1.\n_>_ take lettuce 1 from diningtable 1\nYou pick up the lettuce 1 from the diningtable 1.\n_>_ go to sinkbasin 1\nOn the sinkbasin 1, you see a apple 2, a ladle 2, a spoon 1, and a tomato 3.\n_>_ clean lettuce 1 with sinkbasin 1\n\nYou clean the lettuce 1 using the sinkbasin 1.\n_>_ think: Now I clean a lettuce (1). Next, I need to put it in\/on diningtable 1.\n\nOK.\n\n_>_ go to diningtable 1\nOn the diningtable 1, you see a apple 1, a bread 1, a butterknife 2, a cup 2, a fork\n2, a knife 2, a knife 1, a ladle 1, a mug 2, a mug 1, a pan 2, a peppershaker 1, a\nspatula 3, a tomato 2, and a tomato 1.\n_>_ put lettuce 1 in\/on diningtable 1\nYou put the lettuce 1 in\/on the diningtable 1.\n\n\n23\n\n\nPublished as a conference paper at ICLR 2023\n\n\nTable 8: An ReAct prompt on the ALFWorld clean task.\n\n\nYou are in the middle of a room. Looking quickly around you, you see a cabinet\n13, a cabinet 12, a cabinet 11, a cabinet 10, a cabinet 9, a cabinet 8, a cabinet\n7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet\n"],"reference":"I cannot answer this question as it is outside the scope of the available RAG research papers. The corpus contains information about Retrieval-Augmented Generation systems and related NLP techniques, not culinary topics."}
{"user_input":"How do you train a neural network from scratch?","response":"I don't have enough information in the provided materials to answer this question.","retrieved_contexts":[" a large, complex model (teacher) to a\nsmaller, more efficient model (student) (Gou et al., 2021).\nThis technique is pivotal in mitigating the challenges posed\nby the computational demands and resource constraints of\ndeploying large-scale models in practical applications.\nHistorically, knowledge distillation techniques, prior to\nthe era of LLMs, primarily concentrated on transferring\nknowledge from complex, often cumbersome neural networks to more compact and efficient architectures (Sanh\net al., 2019; Kim and Rush, 2016). This process was largely\ndriven by the need to deploy machine learning models in\nresource-constrained environments, such as mobile devices\n\n- r edge computing platforms, where the computational\npower and memory are limited. The focus was predominantly on ad-hoc neural architecture selection and training\n\n- bjectives tailored for single tasks. These earlier methods\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFig. 3: Taxonomy of Knowledge Distillation of Large Language Models. The detailed taxonomy of Verticalization\nDistillation is shown in Figure 7.\n\n\ninvolved training a smaller student network to mimic the\n\n- utput of a larger teacher network, often through techniques\nlike soft target training, where the student learns from\nthe softened softmax output of the teacher. Please refer to\nthe survey (Gou et al., 2021) for more details on general\nknowledge distillation techniques in AI and DL.\nIn contrast, the advent of LLMs has revolutionized\nthe knowledge distillation landscape. The current era of\nknowledge distillation in LLMs shifts the focus from mere\narchitecture compression to knowledge elicitation and transfer (Taori et al., 2023; Chaudhary, 2023; Tunstall et al., 2023).\nThis paradigm change is largely due to the expansive and\ndeep-seated knowledge that LLMs like GPT-4 and Gemini\npossess. And the inaccessible parameters of LLMs make it\nhard to compress them by using pruning (Han et al., 2016) or\nquantization (Liu et al., 2023a) techniques. Unlike the earlier\nera, where the goal was to replicate the output behavior of\nthe teacher model or reduce the model size, the current focus\nin LLM-based knowledge distillation is to elicit the specific\nknowledge these models have.\nThe key to this modern approach lies in","-specific knowledge distillation in\nachieving superior performance on specialized topics.\n\n\n_**Chemistry and Materials Science.**_ The integration of LLMs\ninto Chemistry and Materials Science has revolutionized\nthe way researchers approach the discovery and development of new compounds and materials. By analyzing vast\ndatasets and scientific literature, LLMs can predict the properties and behaviors of substances, significantly accelerating\nthe innovation cycle.\nGIMLET (Zhao et al., 2023f), Graph Instruction based\nMolecuLe zEro-shoT learning, is a novel approach to\nmolecule property prediction that integrates graph and text\ndata within a single language model framework, aiming\nto improve instruction-based zero-shot learning for molecular tasks. By leveraging a transformer mechanism with\ngeneralized position embedding and decoupled attention,\nGIMLET significantly outperforms traditional molecule-text\nbaselines in zero-shot learning scenarios, demonstrating\nthe model\u2019s effectiveness in generalizing from instructions\nto a broad range of molecule-related tasks without prior\nexplicit task-specific training. LLM-Prop (Rubungo et al.,\n2023), leveraging the T5 model, showcases how LLMs can\n\n- utperform SoTA graph neural networks in predicting the\n\n\n\n25\n\n\nphysical and electronic properties of crystalline solids from\ntext descriptions. This approach underscores the potential of\ntext-based methods in materials science, offering significant\nimprovements in prediction accuracy while also contributing a benchmark dataset, TextEdge, to foster further research in this emerging field. InstructMol (Cao et al., 2023a)\nintegrates multi-modal data, aligning molecular structures\nwith natural language instructions for drug discovery tasks.\nThrough a novel two-stage instruction-tuning approach,\nit significantly enhances performance in molecule-related\ntasks, establishing a reliable molecular assistant that outperforms existing LLMs and reduces the performance gap with\nspecialized models. This demonstrates the value of multimodal integration in developing versatile tools for complex\ndomains like drug discovery.\n\n\n_**Biology.**_ In the field of Biology, particularly in the study\n\n- f proteins, DNA, and RNA, LLMs are revolutionizing our\nunderstanding of the fundamental molecules of life. By analyzing vast datasets of biological sequences and structures,\nLLMs can predict the three-dimensional shapes of proteins,\npotential functions, and interactions at a scale and speed\nbeyond traditional computational methods. This capability\nis critical for unraveling the complexities","Unitary Evolution Recurrent Neural Networks\u201d. In: _The Interna-_\n_tional Conference on Machine Learning (ICML)_ . 2016, pp. 1120\u20131128.\n\n\n17\n\n\n[2] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,\nYannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. \u201cEffective Gene Expression Prediction from\nSequence by Integrating Long-range Interactions\u201d. In: _Nature Methods_ 18.10 (2021), pp. 1196\u20131203.\n\n[3] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. \u201cUsing Fast Weights to Attend\nto the Recent Past\u201d. In: _Advances in Neural Information Processing Systems (NeurIPS)_ 29 (2016).\n\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \u201cLayer Normalization\u201d. In: _arXiv preprint arXiv:1607.06450_\n(2016).\n\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \u201cNeural Machine Translation by Jointly Learning to\nAlign and Translate\u201d. In: _The International Conference on Learning Representations (ICLR)_ . 2015.\n\n[6] David Balduzzi and Muhammad Ghifary. \u201cStrongly-typed Recurrent Neural Networks\u201d. In: _International Conference_\n\n_on Machine Learning_ . PMLR. 2016, pp. 1292\u20131300.\n\n[7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. \u201cPythia: A Suite for Analyzing\nLarge Language Models across Training and Scaling\u201d. In: _The International Conference on Machine Learning (ICML)_ .\nPMLR. 2023, pp. 2397\u20132430.\n\n[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. \u201cPIQA: Reason"," student to better align with\nthe target distribution. Self-Rewarding (Yuan et al., 2024a)\nexplores a novel and promising approach by utilizing the\nlanguage model itself as a reward model. It employs LLMas-a-Judge prompting to autonomously assign rewards for\nthe self-generated responses. The entire process can then\nbe iterated, improving instruction following and reward\nmodeling capabilities.\n\n\n**3.2** **Distillation**\n\n\nThis section focuses on the methodologies for effectively\ntransferring the elicited knowledge from teacher LLMs into\nstudent models. We explore a range of distillation techniques, from the strategies that enhance imitation by _Su-_\n_pervised Fine-Tuning_, _Divergence_ and _Similarity_, to advanced\nmethods like _Reinforcement Learning_ and _Rank Optimization_,\nas shown in Figure 3.\n\n\n_3.2.1_ _Supervised Fine-Tuning_\n\nSupervised Fine-Tuning (SFT), or called Sequence-Level KD\n(SeqKD) (Kim and Rush, 2016), is the simplest and one of\nthe most effective methods for distilling powerful black-box\n\n\n\n11\n\n\n**Divergence Type** _D_ ( _p, q_ ) **Function**\n\nForward KLD ~~\ufffd~~ _p_ ( _t_ ) log _pq_ (( _tt_ ))\n\nReverse KLD ~~\ufffd~~ _q_ ( _t_ ) log _pq_ (( _tt_ ))\n\nJS Divergence 12 ~~\ufffd~~   - _p_ ( _t_ ) log _p_ ( _t_ 2)+ _p_ ( _tq_ )( _t_ ) [+] [\ufffd] _[q]_ [(] _[t]_ [) log] _p_ ( _t_ 2)+ _q_ ( _tq_ )( _t_ ) ~~\ufffd~~\n\n\nTABLE 1: Functional forms of _D_ for various divergence\ntypes. _p_ : reference\n\n\n**Similarity Function** _LF_ **Expression**\n\n\nL2-Norm Distance _\u2225_ \u03a6 _T_ ( _fT_ ( _x, y_ )) _\u2212_ \u03a6 _S_ ( _fS_ ( _x, y_ )) _\u2225_ 2\n\n\nL1-Norm Distance _\u2225_ \u03a6 _T_ ( _"," the corresponding legs of the input horse, more so than\nthe other patches of the horse image. Even at the pixel level, the colors of a zebra\nbody (black and white) can be more strongly associated to the color of a horse\nbody than to the background shades of grass. Thus, we employ a _multilayer,_\n_patch-based_ learning objective.\nSince the encoder _G_ enc is computed to produce the image translation, its\nfeature stack is readily available, and we take advantage. Each layer and spatial\nlocation within this feature stack represents a patch of the input image, with\ndeeper layers corresponding to bigger patches. We select _L_ layers of interest and\npass the feature maps through a small two-layer MLP network _Hl_, as used in\nSimCLR [9], producing a stack of features _{_ _**z**_ _l}L_ = _{Hl_ ( _G_ _[l]_ enc [(] _**[x]**_ [))] _[}][L]_ [, where] _[ G][l]_ enc\nrepresents the output of the _l_ - th chosen layer. We index into layers _l \u2208{_ 1 _,_ 2 _, ..., L}_\nand denote _s \u2208{_ 1 _, ..., Sl}_, where _Sl_ is the number of spatial locations in each\nlayer. We refer to the corresponding feature as _**z**_ _l_ _[s]_ _[\u2208]_ [R] _[C][l]_ [ and the other features]\nas _**z**_ _l_ _[S][\\][s]_ _\u2208_ R [(] _[S][l][\u2212]_ [1)] _[\u00d7][C][l]_, where _Cl_ is the number of channels at each layer. Similarly,\nwe encode the output image \u02c6 _**y**_ into _{_ _**z**_ \u02c6 _l}L_ = _{Hl_ ( _G_ _[l]_ enc [(] _[G]_ [(] _**[x]**_ [)))] _[}][L]_ [.]\nWe aim to match corresponding input-output patches at a specific location.\nWe can leverage the other patches _within_ the input as negatives. For example,\na zebra leg should"],"reference":"While neural network training is a broad machine learning topic, the RAG research corpus focuses specifically on retrieval-augmented generation systems. For general neural network training information (backpropagation, gradient descent, loss functions, etc.), you would need to consult general deep learning resources. The corpus may contain some references to neural network training in the context of training retrieval models or fine-tuning language models for RAG, but does not provide comprehensive coverage of training neural networks from scratch."}
{"user_input":"Who are the authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering'?","response":"The authors of the paper \"Dense Passage Retrieval for Open-Domain Question Answering\" are Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih <document id=\"2004.04906_dpr_karpukhin:chunk_0\">.","retrieved_contexts":["## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice [",":2002.08909 [cs.CL]](https:\/\/arxiv.org\/abs\/2002.08909)\n\n[11] Sebastian Hofst\u00e4tter, Sophia Althammer, Michael Schr\u00f6der, Mete Sertkan, and\nAllan Hanbury. 2020. Improving Efficient Neural Ranking Models with Cross[Architecture Knowledge Distillation. arXiv:2010.02666 [cs.IR]](https:\/\/arxiv.org\/abs\/2010.02666)\n\n[12] Vladimir Karpukhin, Barlas O\u011fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen tau Yih. 2020. Dense Passage Retrieval for Open[Domain Question Answering. arXiv:2004.04906 [cs.CL]](https:\/\/arxiv.org\/abs\/2004.04906)\n\n[13] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage\nSearch via Contextualized Late Interaction over BERT. In _Proceedings of the 43rd_\n_International ACM SIGIR Conference on Research and Development in Information_\n_Retrieval_ (Virtual Event, China) _(SIGIR \u201920)_ . Association for Computing Machinery,\n[New York, NY, USA, 39\u201348. https:\/\/doi.org\/10.1145\/3397271.3401075](https:\/\/doi.org\/10.1145\/3397271.3401075)\n\n[14] Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. 2015. Numba: A llvm-based\npython jit compiler. In _Proceedings of the Second Workshop on the LLVM Compiler_\n_Infrastructure in HPC_ . 1\u20136.\n\n[15] Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin. 2020. Distilling Dense Repre[sentations for Ranking using Tightly-Coupled Teachers. arXiv:2010.11386 [cs.IR]](https:\/\/arxiv.org\/abs\/2010.11386)\n\n[16] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\nGoharian, and Ophir Frieder. 2020. Expansion via Prediction","AAAI)_ .\n\n\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. 2019. Multi-passage BERT:\nA globally normalized bert model for open-domain\nquestion answering. In _Empirical Methods in Natu-_\n_ral Language Processing (EMNLP)_ .\n\n\nTomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question understanding benchmark. _Transactions of the Associa-_\n_tion of Computational Linguistics (TACL)_ .\n\n\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020a. Approximate nearest neighbor\nnegative contrastive learning for dense text retrieval.\n_ArXiv_, abs\/2007.00808.\n\n\nWenhan Xiong, Hankang Wang, and William Yang\nWang. 2020b. Progressively pretrained dense corpus\nindex for open-domain question answering. _ArXiv_,\nabs\/2005.00038.\n\n\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.\nEnd-to-end open-domain question answering with\nbertserini. In _North American Association for Com-_\n_putational Linguistics (NAACL)_, pages 72\u201377.\n\n\nWei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming\nLi, and Jimmy Lin. 2019b. Data augmentation for\nbert fine-tuning in open-domain question answering.\n_ArXiv_, abs\/1904.06652.\n\n\nWen-tau Yih, Kristina Toutanova, John C Platt, and\nChristopher Meek. 2011. Learning discriminative\nprojections for text similarity measures. In _Com-_\n_putational Natural Language Learning (CoNLL)_,\npages 247\u2013256.\n\n\n**A** **Distant Supervision**\n\n\nWhen training our final DPR model using Natural\nQuestions, we use the passages in our collection\nthat best match the gold context as the positive\npassages. As some QA datasets contain only the\nquestion","_\n_preprint arXiv:2002.08909_ .\n\n\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2019. How can we know what language\nmodels know? _arXiv preprint arXiv:1911.12543_ .\n\n\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. In _Proc. ACL_ .\n\n\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Ledell\nWu, Sergey Edunov, Danqi Chen, and Wentau Yih. 2020. Dense passage retrieval for\n\n - pen-domain question answering. _arXiv preprint_\n_arXiv:2004.04906_ .\n\n\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. _arXiv preprint_\n_arXiv:1412.6980_ .\n\n\nTom\u00b4a\u02c7s Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom,\nChris Dyer, Karl Moritz Hermann, G\u00b4abor Melis, and\nEdward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. _TACL_ .\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: a benchmark for question answering\nresearch. _TACL_ .\n\n\nJinhyuk Lee, Seongjun Yun, Hyunjae Kim, Miyoung\nKo, and Jaewoo Kang. 2018. Ranking paragraphs\nfor improving answer recall in open-domain question answering. In _Proc. EMNLP_ .\n\n\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In"],"reference":"The authors of the paper 'Dense Passage Retrieval for Open-Domain Question Answering' are Vladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih."}
{"user_input":"What is BM25 used for in open-domain QA?","response":"BM25 is used in open-domain question answering (QA) as a passage retrieval technique to reduce the candidate passages that a reader needs to consider. This is crucial for efficiently answering user questions in real-time. BM25 operates by scoring passages based on keyword relevance, thus helping in selecting higher quality context passages from which answers can be drawn <document id=\"2004.04906_dpr_karpukhin:chunk_7200\">. It has been observed that performance in end-to-end QA systems can improve with increasing numbers of retrieved passages (denoted as k), as the likelihood of the reader encountering a correct answer increases even if non-relevant materials may also be included <document id=\"2104.05740_hybrid_retrieval_ma:chunk_7200\">.","retrieved_contexts":[" likelihood that the\nanswer appears in the contexts fed to the reader.\nOn the other hand, the reader is asked to con\nsider more contexts, and thus needs to discriminate the correct answer from a larger pool of candidate contexts, some of which might be low quality and thus serve as \u201cdistractors\u201d from the correct answer. How do these factors balance out?\n\nSimilar analyses in previous work with BM25 retrieval have shown that end-to-end QA effectiveness increases with increasing k (Yang et al., 2019;\nXie et al., 2020); that is, the reader does not appear to be \u201cconfused\u201d by the non-relevant material. Indeed, in our BM25 results we also observe\n\nthe same trend.\n\nInterestingly, however, when we switch from\nBM25 results to DPR results, the behavior ap\n\n\npears to change. For TriviaQA, the effectiveness\ncurve behaves as expected, but for NQ, the exact\nmatch score trends up and then decreases after a\npeak. This means that while the likelihood of the\nreader seeing a correct answer in the candidate\ncontexts increases with k, it is more likely to be\nnegatively affected by increasing amounts of nonrelevant contexts as well. This general behavior is\nalso seen for the hybrid scoring techniques: as k\nincreases, so does the exact match score, but only\nup to a certain point. Beyond this point, feeding\nthe reader more candidate contexts leads to slight\ndecreases in end-to-end effectiveness.\n\n\n5 Conclusion\n\n\nThe breakneck pace at which NLP and IR are\nadvancing, we argue, makes reproducibility and\nreplicability critical to advancing science\u2014to ensure that we are building on a firm foundation.\nOur study adds to the veracity of the claims made\nby Karpukhin et al. (2020), and our work does indeed confirm that DPR is an effective dense retrieval technique. However, we arrived at two important additional findings, one of which is inconsistent with the original work, the other of which\npresents an enhancement. Together, they enrich\n\n- ur understanding of DPR.\n\n\n6 Acknowledgments\n\n\nThis research was supported in part by the Canada\nFirst Research Excellence Fund and the Natu\nral Sciences and Engineering Research Council\n(NSERC) of Canada. Computational resources\nwere provided by Compute Ontario and Compute\nCanada.\n\n\nReferences\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine"," like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25\/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer"," = _\u03b2t_ (1 + K _\u03b2et_ ) _,_\n\n\nwhere KL _target_ is set to 0.2, K _\u03b2_ is set to 0.1. _\u03b2_ 0\nis initialized to be 0.001. The generation strategy follows the 4-beam search and returns the one\nsequence. In the implementation of the BM25based retriever, the textboxes from searched URLs\nare parsed from HTML code. We compute BM25\nscores between the paragraph from each textbox\nand the query following the scikit-learn package,\nthen keep those with higher scores until the reserved context reaches a max length. In reinforcement learning, the results of AmbigNQ are with\nthe BM25 method, while others use snippets as\n\ncontext.\n\n\n**C** **Web Search: Tool Use**\n\n\nOur proposed pipeline integrates an externally built\nweb search engine as the retriever module. We\npresent more discussion on the advantages and disadvantages here.\nThe usage of external tools expands the ability boundary of language models, compensating\nfor the parametric knowledge, and grounding the\ncapabilities of language models to interact with environments (Qin et al., 2023; Schick et al., 2023).\nRecent studies show a trend to leverage plug-andplay tools like search engines to enhance language\nagents (Lazaridou et al., 2022; Menick et al., 2022;\nShuster et al., 2022; Shen et al., 2023). Search\nengine APIs are well-developed retrievers, saving\nefforts to build and maintain another retriever, like\n\na Contriever. Accessible to the whole Internet, the\nweb search retrieves from a wide-range, up-to-date\n\n\nknowledge base. The temporal misalignment problem on a fixed candidate database can be alleviated.\nOn the other hand, web search APIs are commercial products requiring subscriptions. Also, the vast\namount of knowledge on the web can be difficult\nto control. The retrieved context from the Internet\n\ncan be occasionally inconsistent, redundant, and\ntoxic, which hinders the LLM reader.\nBeyond retrieval augmentation, in a general\nscope, other tools called by LLMs, like code interpreters, online models, and expert applications,\nare all similar to search engines, without trainable\nparameters to optimize. There could be a gap between the LM and these tools. This paper","forms BM25 and DPR by margins\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\n\n\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\nthe performance observed in Karpukhin et al. (2020), where dpr-multiset-base showed superior results.\n\n\n7\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTable 2: **QuALITY and QASPER Performance With + Without RAPTOR:** Performance comparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outperforms baselines of each respective retrieval method for both datasets.\n\n\n**Model** **Accuracy (QuALITY)** **Answer F1 (QASPER)**\n\n\n**SBERT with RAPTOR** **56.6%** **36.70%**\n\nSBERT without RAPTOR 54.9% 36.23%\n\n**BM25 with RAPTOR** **52.1%** **27.00%**\n\nBM25 without RAPTOR 49.9% 26.47%\n\n**DPR with RAPTOR** **54.7%** **32.23%**\n\nDPR without RAPTOR 53.1% 31.70%\n\n\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different language models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column \u201dTitle +\nAbstract\u201d reflects performance when only the title and abstract of the papers are used for context.\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\nSpecifically, RAPTOR\u2019s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\nhigher than BM25.\n\n\n**Retriever** **GPT-3 F-1 Match** **GPT-4 F-1 Match** **UnifiedQA F-1 Match**\n\n\nTitle + Abstract 25.2 "," 2016. The innovative semiconductor firm\nspecializes in low-power processors for wearables and IoT devices.\n\n\nThe LLM is prompted such that it extracts the following:\n\n\n    - The entity `NeoChip`, with description \u201cNeoChip is a publicly traded company specializing\nin low-power processors for wearables and IoT devices.\u201d\n\n\n    - The entity `Quantum Systems`, with description \u201cQuantum Systems is a firm that previ\n    - usly owned NeoChip.\u201d\n\n\n    - A relationship between `NeoChip` and `Quantum Systems`, with description \u201cQuantum Systems owned NeoChip from 2016 until NeoChip became publicly traded.\u201d\n\n\nThese prompts can be tailored to the domain of the document corpus by choosing domain appropriate\nfew-shot exemplars for in-context learning (Brown et al., 2020). For example, while our default\nprompt extracts the broad class of \u201cnamed entities\u201d like people, places, and organizations and is\ngenerally applicable, domains with specialized knowledge (e.g., science, medicine, law) will benefit\nfrom few-shot exemplars specialized to those domains.\n\n\nThe LLM can also be prompted to extract _claims_ about detected entities. _Claims_ are important\nfactual statements about entities, such as dates, events, and interactions with other entities. As\nwith entities and relationships, in-context learning exemplars can provide domain-specific guidance.\nClaim descriptions extracted from the example tetx chunk are as follows:\n\n\n    - NeoChip\u2019s shares surged during their first week of trading on the NewTech Exchange.\n\n\n   - NeoChip debuted as a publicly listed company on the NewTech Exchange.\n\n\n   - Quantum Systems acquired NeoChip in 2016 and held ownership until NeoChip went public.\n\n\nSee Appendix A for prompts and details on our implementation of entity and claim extraction.\n\n\n**3.1.3** **Entities & Relationships** _\u2192_ **Knowledge Graph**\n\n\nThe use of an LLM to extract entities, relationships, and claims is a form of abstractive summarization \u2013 these are meaningful summaries of concepts that, in the case of relationships and claims, may\nnot be explicitly stated in the text. The entity\/relationship\/claim extraction processes creates multiple instances of a single element because an element is typically detected and extracted multiple\ntimes across documents.\n\n\nIn the final step of the knowledge graph extraction process, these instances of entities and relationships become individual nodes and edges in the graph. Entity descriptions are aggregated and\nsummar"],"reference":"BM25 is used for retrieval in open-domain QA, usually implemented using TF-IDF or BM25, which matches keywords efficiently with an inverted index and can be seen as representing the question and context in high-dimensional, sparse vectors (with weighting)."}
{"user_input":"What is the purpose of using Wikipedia in open-domain question answering?","response":"The purpose of using Wikipedia in open-domain question answering is to provide a rich source of information that can be accessed interactively to assist in reasoning and answering questions. The interaction with Wikipedia is designed to simulate how humans typically search for and retrieve information. The system employs a simple Wikipedia web API that allows for various types of actions, such as searching for an entity and looking up specific strings within Wikipedia pages. This design encourages models to engage in explicit reasoning as they retrieve information, ultimately enhancing their ability to answer questions accurately. Wikipedia serves as both a knowledge base and a tool for developing reasoning strategies within language models <document id=\"2210.03629_react_yao:chunk_4050\">.","retrieved_contexts":[" only receive the question\/claim as input without access to support\nparagraphs, and have to rely on their internal knowledge or retrieve knowledge via interacting with\nan external environment to support reasoning.\n\n\n**Action Space** We design a simple Wikipedia web API with three types of actions to support\ninteractive information retrieval: (1) **search** [entity], which returns the first 5 sentences from\nthe corresponding entity wiki page if it exists, or else suggests top-5 similar entities from the\nWikipedia search engine, (2) **lookup** [string], which would return the next sentence in the page\ncontaining string, simulating Ctrl+F functionality on the browser. (3) **finish** [answer], which\nwould finish the current task with answer. We note that this action space mostly can only retrieve a\nsmall part of a passage based on exact passage name, which is significantly weaker than state-of-theart lexical or neural retrievers. The purpose is to simulate how humans would interact with Wikipedia,\nand force models to retrieve via explicit reasoning in language.\n\n\n3.2 METHODS\n\n\n**ReAct Prompting** For HotpotQA and Fever, we randomly select 6 and 3 cases [2] from the training\nset and manually compose ReAct-format trajectories to use as few-shot exemplars in the prompts.\nSimilar to Figure 1(d), each trajectory consists of multiple thought-action-observation steps (i.e. dense\nthought), where free-form thoughts are used for various purposes. Specifically, we use a combination\n\n- f thoughts that decompose questions (\u201cI need to search x, find y, then find z\u201d), extract information\nfrom Wikipedia observations (\u201cx was started in 1844\u201d, \u201cThe paragraph does not tell x\u201d), perform\ncommonsense (\u201cx is not y, so z must instead be...\u201d) or arithmetic reasoning (\u201c1844 < 1989\u201d), guide\n\n\n2We find more examples do not improve performance.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2023\n\n\n\n**HotpotQA** **Fever**\n**Prompt Method** _[a]_\n(EM) (Acc)\n\n\nStandard 28.7 57.1\nCoT (Wei et al., 2022) 29.4 56.3\nCoT-SC (Wang et al., 2022a) 33.4 60.4\n\n\nAct 25.7 58.9\n\nReAct "," in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f","Raffel et al., 2019), achieving good performance on\n\n- pen-domain QA and other knowledge-intensive\ntasks.\n\n\n**8** **Conclusion**\n\n\nIn this work, we demonstrated that dense retrieval\ncan outperform and potentially replace the traditional sparse retrieval component in open-domain\nquestion answering. While a simple dual-encoder\napproach can be made to work surprisingly well,\nwe showed that there are some critical ingredients\nto training a dense retriever successfully. Moreover,\n\n- ur empirical analysis and ablation studies indicate\nthat more complex model frameworks or similarity\nfunctions do not necessarily provide additional values. As a result of improved retrieval performance,\nwe obtained new state-of-the-art results on multiple\n\n- pen-domain question answering benchmarks.\n\n\n**Acknowledgments**\n\n\nWe thank the anonymous reviewers for their helpful\ncomments and suggestions.\n\n\n**References**\n\n\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning to retrieve reasoning paths over Wikipedia graph\nfor question answering. In _International Conference_\n\n_on Learning Representations (ICLR)_ .\n\n\nPetr Baudi\u02c7s and Jan Sediv`y. 2015. [\u02c7] Modeling of the\nquestion answering task in the yodaqa system. In _In-_\n_ternational Conference of the Cross-Language Eval-_\n_uation Forum for European Languages_, pages 222\u2013\n228. Springer.\n\n\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\n\n\nquestion-answer pairs. In _Empirical Methods in Nat-_\n_ural Language Processing (EMNLP)_ .\n\n\nJane Bromley, Isabelle Guyon, Yann LeCun, Eduard\nS\u00a8ackinger, and Roopak Shah. 1994. Signature verification using a \u201cSiamese\u201d time delay neural network.\nIn _NIPS_, pages 737\u2013744.\n\n\nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,\nMatt Deeds, Nicole Hamilton, and Greg Hullender.\n2005. Learning to rank using gradient descent. In\n_Proceedings of the 22nd international conference on_\n_Machine learning_, pages 89\u201396.\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to",".\nOn PopQA, where question and answer pairs are created based on WikiData in 2022, we found\nthat the 2018 Wikipedia sometimes lacks articles about some entities that have been more recently\nadded to Wikipedia. Therefore, for PopQA, we used the December 2020 preprocessed Wikipedia\ncorpus provided by Izacard et al. (2022b) and generated document embeddings. [8] The issues of\nperformance variance from different Wikipedia dumps have been reported by prior work (Asai et al.,\n2020; Izacard et al., 2022b). Yet, we observe limited effectiveness of such off-the-shelf retrieval\nmodels trained primarily on knowledge-intensive tasks for open-ended generation (e.g., instruction\nfollowing). Recent or concurrent work studies instruction-tuning of retrieval systems (Asai et al.,\n2023b) or joint training of retrieval and LM components (Lin et al., 2023), while we leave exploring\nthe effectivess of such appraoches for future work. For bio generation and open-domain QA tasks,\nwe additionally retrieve five documents using Google Programmable Search [9] and search documents\nfrom English Wikipedia. As this API only provides snippets, we retrieve Wikipedia introductory\nparagraphs for the corresponding entities.\n\n\n**Detailed experimental settings for individual datasets.** For OpenQA datasets, we set the maximum new token number to 100 tokens. For closed-set tasks (PubHealth and ARC-C), we set the\nmaximum new token length to 50 for all baselines. For SELF-RAG inference on PubHealth and\nARC-C, instead of determining the output with the highest score 4 as in other tasks, we aggregate the\nscores for each option and select the answer option with the highest score. We found in zero-shot\nsettings of fact checking, some LLMs can generate capitalized class labels (e.g., True) while our\ngold labels are lower-cased. Therefore, across different LMs, for fact checking, we lowercase the\npredictions. In multiple choice tasks, we found some models generate answers in slightly different\nways (e.g., (A) instead of A). We slightly modify instructions for each LLM to avoid such format\nviolations, and further conduct string matching between each candidate and model predictions if\nformat violations still remain. After that processing, in closed set tasks, model predictions match\n\n- ne of the gold classes in almost all cases. For","** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f the documents contains the answer\n\n\n[1nelsonliu.me\/papers\/lost-in-the-middle](https:\/\/nelsonliu.me\/papers\/lost-in-the-middle)\n\n\n\nto the question and _k \u2212_ 1 \u201cdistractor\u201d documents\ndo not. This task requires the model to access the\ndocument that contains the answer within its input\ncontext and use it to answer the question. Figure 2\npresents an example.\n\nWe instantiate this task with data from\n\nNaturalQuestions-Open (Lee et al., 2019;\nKwiatkowski et al., 2019), which contains\nhistorical queries issued to the Google search\nengine, coupled with human-annotated answers\nextracted from Wikipedia. In particular, we take\nthe 2655 queries where the annotated long answer\nis a paragraph (as opposed to a list or a table). We\nuse passages (chunks of at most 100 tokens) from\nWikipedia as documents within our input contexts.\nFor each of the queries, we need a document\nthat contains the answer and _k \u2212_ 1 distractor\n\ndocuments that do not contain the answer. To\n\n- btain a document that answers the question, we\nuse the Wikipedia paragraph that contains the\nanswer from the NaturalQuestions annotations.\n\nTo collect _k \u2212_ 1 distractor documents that do not\n\ncontain the answer, we use a retrieval system (Contriever, fine-tuned on MS-MARCO; Izacard et al.,\n2021) to retrieve the _k \u2212_ 1 Wikipedia chunks that\nare most relevant to the query and do not contain\nany of the NaturalQuestions-annotated answers. [2][,][3]\n\nIn the input context, the distractor documents are\npresented in order of decreasing relevance. [4]\n\nTo modulate the position of relevant information\nwithin the input context, we adjust the order of the\ndocuments to change the position of the document\nthat contains the answer (Figure 3). To modulate\nthe input context length in this task, we increase or\ndecrease the number of retrieved documents that\n\ndo not contain the answer (Figure 4"],"reference":"Wikipedia is a large corpus of diversified topics that can be used to answer factoid questions in an extractive QA setting."}
{"user_input":"What method does Roberts et al. use to measure the similarity between questions and passages?","response":"Roberts et al. (2020) measure the similarity between questions and passages using the dot product of their vector representations. Specifically, a different encoder maps the input question to a vector, and the similarity is defined as:\n\n\\[ sim(q, p) = E_Q(q) [\\top] E_P(p) \\]\n\nThis method emphasizes the use of decomposable similarity functions, allowing for precomputation of the collection of passages' representations. The inner product is chosen for its efficiency, related to cosine similarity and Euclidean distance, in terms of similarity measurement (Karpukhin et al., 2020) <2004.04906_dpr_karpukhin:chunk_2250>.","retrieved_contexts":[" to a _d_ dimensional real-valued vectors and builds an index\n\nfor all the _M_ passages that we will use for retrieval.\n\n\n3The ideal size and boundary of a text passage are functions of both the retriever and reader. We also experimented\nwith natural paragraphs in our preliminary trials and found that\nusing fixed-length passages performs better in both retrieval\nand final QA accuracy, as observed by Wang et al. (2019).\n4Exceptions include (Seo et al., 2019) and (Roberts et al.,\n2020), which _retrieves_ and _generates_ the answers, respectively.\n\n\nAt run-time, DPR applies a different encoder _EQ_ ( _\u00b7_ )\nthat maps the input question to a _d_ - dimensional\nvector, and retrieves _k_ passages of which vectors\nare the closest to the question vector. We define\nthe similarity between the question and the passage\nusing the dot product of their vectors:\n\n\nsim( _q, p_ ) = _EQ_ ( _q_ ) [\u22ba] _EP_ ( _p_ ) _._ (1)\n\n\nAlthough more expressive model forms for measuring the similarity between a question and a passage\ndo exist, such as networks consisting of multiple\nlayers of cross attentions, the similarity function\nneeds to be decomposable so that the representations of the collection of passages can be precomputed. Most decomposable similarity functions\nare some transformations of Euclidean distance\n\n(L2). For instance, cosine is equivalent to inner\nproduct for unit vectors and the Mahalanobis distance is equivalent to L2 distance in a transformed\nspace. Inner product search has been widely used\nand studied, as well as its connection to cosine\nsimilarity and L2 distance (Mussmann and Ermon,\n2016; Ram and Gray, 2012). As our ablation study\nfinds other similarity functions perform comparably (Section 5.2; Appendix B), we thus choose\nthe simpler inner product function and improve the\ndense passage retriever by learning better encoders.\n\n\n**Encoders** Although in principle the question and\npassage encoders can be implemented by any neural networks, in this work we use two independent\nBERT (Devlin et al., 2019) networks (base, uncased) and take the representation at the [CLS]\ntoken as the output, so _d_ = 768.\n\n\n","clweb.org\/anthology\/D18-1255](https:\/\/www.aclweb.org\/anthology\/D18-1255)` .\n\n\n[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation\nsystems. In _Proceedings of the 2018 Conference on Empirical Methods in Natural Language_\n_Processing_, pages 3950\u20133959, Brussels, Belgium, October-November 2018. Association for\nComputational Linguistics. doi: 10.18653\/v1\/D18-1429. URL `[https:\/\/www.aclweb.org\/](https:\/\/www.aclweb.org\/anthology\/D18-1429)`\n`[anthology\/D18-1429](https:\/\/www.aclweb.org\/anthology\/D18-1429)` .\n\n\n[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,\nand Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In\nTarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne, editors,\n_Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic_\n\n\n13\n\n\n_approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing_\n_Systems (NIPS 2016), Barcelona, Spain, December 9, 2016_, volume 1773 of _CEUR Workshop_\n_Proceedings_ . CEUR-WS.org, 2016. URL `[http:\/\/ceur-ws.org\/Vol-1773\/CoCoNIPS_](http:\/\/ceur-ws.org\/Vol-1773\/CoCoNIPS_2016_paper9.pdf)`\n`[2016_paper9.pdf](http:\/\/ceur-ws.org\/Vol-1773\/CoCoNIPS_2016_paper9.pdf)` .\n\n\n[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. _arXiv preprint_\n_arXiv:1901.04085_, 2019. URL `[https:\/\/arxiv.org\/abs\/1901.04085](https:\/\/arxiv.org\/abs\/1901.04085)` .\n\n\n[45] Myle Ott","; and _comparison_\nquestions where two entities are mentioned simultaneously and compared in some way. In Figure 2,\nwe show the retrieval performance of both question\ntypes. The case of _comparison_ questions proves easier, since both entities needed for retrieval are present\nin the question.\n\n\nThis case appears almost solved, confirming recent\nwork demonstrating that dense retrieval is very effective at entity linking (Wu et al., 2019).\n\n\n\nFigure 2: The retrieval performance gap between comparison and bridge questions. Left:\nrecall of groundtruth passage sequences with\n- ut reranking. Right: Top-1 chain exact\nmatch after reranking.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the case of _bridge_ questions, we manually in\nrecall of groundtruth passage sequences with\nspect 50 randomly sampled erroneous examples after\n\n                          - ut reranking. Right: Top-1 chain exact\n\nreranking. We find that in half of these cases, our re\nmatch after reranking.\n\ntrieval model predicts an alternative passage sequence\nthat is also valid (see Appendix A.1 for examples).\nThis gives an estimated top-1 passage sequence accuracy of about 90%. Other remaining errors are\ndue to the dense method\u2019s inability to capture the exact n-gram match between the question and\npassages. This is a known issue (Lee et al., 2019; Karpukhin et al., 2020) of dense retrieval methods\nwhen dealing with questions that have high lexical overlap with the passages. To this end, a hybrid\nmulti-hop retrieval method with both term and dense index might be used to further improve the\nperformance on _bridge_ questions.\n\n\n\n**Retriever Ablation Study** In Table 3, we examine our model with different variations on HotpotQA\nto show the effectiveness of each proposed component. We see that further training with a memory\nbank results in modest gains, while using a shared encoder is crucial for the best performance.\nRespecting the ordering of passages in two hops is essential - training in an order-agnostic manner\nhardly works at all, and underperforms even the single-hop baseline. Finally, not using hyperlinked\nparagraphs from TF-IDF passages as additional negatives has only a minor impact on performance.\n\n\n**Question Decomposition for Retrieval** As multi-hop questions have more complex structures\nthan simple questions, recent studies (Min et al., 2019; Perez et al., 2020) propose to use"," and comprehensive\ninformation for downstream tasks than DPR. For a detailed discussion and examples, including the\ntext retrieved by both RAPTOR and DPR for specific questions, please refer to the appendix G.\n\n\n4 EXPERIMENTS\n\n\n**Datasets** We measure RAPTOR\u2019s performance across three question-answering datasets: NarrativeQA, QASPER, and QuALITY.\n\n\nNarrativeQA is a dataset that comprises question-answer pairs based on the full texts of books\nand movie transcripts, totaling 1,572 documents (Ko\u02c7cisk`y et al., 2018; Wu et al., 2021). The\nNarrativeQA-Story task requires a comprehensive understanding of the entire narrative in order\nto accurately answer its questions, thus testing the model\u2019s ability to comprehend longer texts in\nthe literary domain. We measure performance on this dataset using the standard BLEU (B-1, B-4),\nROUGE (R-L), and METEOR (M) metrics. Please see appendix H for more details on the NarrativeQA evaluation script used in our experiments.\n\n\nThe QASPER dataset includes 5,049 questions across 1,585 NLP papers, with each question probing\nfor information embedded within the full text (Dasigi et al., 2021). The answer types in QASPER\nare categorized as Answerable\/Unanswerable, Yes\/No, Abstractive, and Extractive. Accuracy is\nmeasured using standard F1.\n\n\nLastly, the QuALITY dataset consists of multiple-choice questions, each accompanied by context\npassages averaging approximately 5,000 tokens in length (Pang et al., 2022). This dataset calls for\nreasoning over the entire document for QA tasks, enabling us to measure the performance of our retrieval system on medium-length documents. The dataset includes a challenging subset, QuALITYHARD, which contains questions that a majority of human annotators answered incorrectly in a\nspeed-setting. We report accuracies for both the entire test set and the HARD subset.\n\n\n**Controlled Baseline Comparisons** We first present controlled comparisons using the UnifiedQA\n3B as the reader, with SBERT (Reimers & Gurevych, 2019), BM25 (Robertson et al., 1995; 2009),\nand DPR (Karpukhin et al., 2020) as the embedding models with and without the R"," only a minor impact on performance.\n\n\n**Question Decomposition for Retrieval** As multi-hop questions have more complex structures\nthan simple questions, recent studies (Min et al., 2019; Perez et al., 2020) propose to use explicit question decomposition to simplify the problem. Wolfson et al. (2020) shows that with\nTF-IDF, using decomposed questions improves the retrieval results. We investigate whether the\n\n\n5\n\n\nPublished as a conference paper at ICLR 2021\n\n\nconclusion still holds with stronger dense retrieval methods. We use the human-annotated question decomposition from the QDMR dataset (Wolfson et al., 2020) for analysis. For a question\nlike Q:Mick Carter is the landlord of a public house located at what\naddress?, QDMR provides two subquestions, SubQ1: What is the public house\nthat Mick Carter is the landlord of? and SubQ2: What is the address\nthat #1 is located at?. We sample 100 bridge questions and replace #1 in SubQ2 with\nthe correct answer (The Queen Victoria) to SubQ1. Note that this gives advantages to the decomposed\nmethod as we ignore any intermediate errors. We estimate the performance of potential decomposed\nmethods with the state-of-the-art single-hop dense retrieval model (Karpukhin et al., 2020).\n\n\nAs shown in Table 4, we did not observe\nany strong improvements from explicit ques\nTable 4: Comparison with decomposed dense retrieval\n\ntion decomposition, which is contrary to the\n\nwhich uses oracle question decomposition (test on\n\nfindings by Wolfson et al. (2020) when using\n\n100 bridge questions). See text for details about the\n\nterm-based IR methods. Moreover, as shown decomposed settings.\nin the third row of the table, when the 1st hop\n\nMethod R@2 R@10 R@20\n\n- f the decomposed retrieval (i.e., SubQ1)\nis replaced with the original question, no MDR 54.9 63.7 70.6\nperformance degradation is observed. This Decomp (SubQ1;SubQ2) 50.0 64.7 67.6\nsuggests that strong pretrained encoders can Decomp (Q;SubQ2) 51.0 64.7 68.6\neffectively learn to select necessary"],"reference":"Roberts et al. define the similarity between a question and a passage using the dot product of their vectors: sim(q, p) = EQ(q)\u22baEP (p)."}
{"user_input":"What is the purpose of passage embeddings in machine learning?","response":"I don't have enough information in the provided materials to answer this question.","retrieved_contexts":[" Special Publication Sp_ [, 109:109, 1995. URL https:\/\/www.](https:\/\/www.microsoft.com\/en-us\/research\/publication\/okapi-at-trec-3\/)\n[microsoft.com\/en-us\/research\/publication\/okapi-at-trec-3\/.](https:\/\/www.microsoft.com\/en-us\/research\/publication\/okapi-at-trec-3\/)\n\n\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\nZaheer. Questions are all you need to train a dense passage retriever. _Transactions of the As-_\n_sociation for Computational Linguistics_, 11:600\u2013616, 2023. doi: 10.1162\/tacl ~~a 0~~ 0564. URL\n[https:\/\/aclanthology.org\/2023.tacl-1.35.](https:\/\/aclanthology.org\/2023.tacl-1.35)\n\n\nGideon Schwarz. Estimating the Dimension of a Model. _The annals of statistics_, pp. 461\u2013464,\n[1978. URL https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/](https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/aos\/1176344136.full)\n[volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/](https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/aos\/1176344136.full)\n[aos\/1176344136.full.](https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/aos\/1176344136.full)\n\n\nKaren Sp\u00a8arck Jones. A Statistical Interpretation of Term Specificity and its Application in Retrieval. _Journal of documentation_ [, 28(1):11\u201321, 1972. URL https:\/\/doi.org\/10.1108\/](https:\/\/doi.org\/10.1108\/eb026526)\n[eb026526","aclanthology.org\/2020.findings-emnlp.171)\n[171.](https:\/\/aclanthology.org\/2020.findings-emnlp.171)\n\n\nOmar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via contextualized late interaction over bert. In _Proceedings of the 43rd International ACM SIGIR_\n_conference on research and development in Information Retrieval_, pp. 39\u201348, 2020. URL\n[https:\/\/arxiv.org\/abs\/2004.12832.](https:\/\/arxiv.org\/abs\/2004.12832)\n\n\nTom\u00b4a\u02c7s Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00b4abor Melis,\nand Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. _Transactions_\n\n_of the Association for Computational Linguistics_ [, 6:317\u2013328, 2018. URL https:\/\/arxiv.](https:\/\/arxiv.org\/abs\/1712.07040)\n\n[org\/abs\/1712.07040.](https:\/\/arxiv.org\/abs\/1712.07040)\n\n\n12\n\n\nPublished as a conference paper at ICLR 2024\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K\u00a8uttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00a8aschel, et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. _Advances in Neural Information Processing Systems_,\n[33:9459\u20139474, 2020. URL https:\/\/doi.org\/10.48550\/arXiv.2005.11401.](https:\/\/doi.org\/10.48550\/arXiv.2005.11401)\n\n\n[Jerry Liu. LlamaIndex, 2022. URL https:\/\/github.com\/jerryjliu\/llama_index.](https:\/\/github.com\/jerryjliu\/llama_index)\n\n\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\nPercy Liang. Lost in the middle: How language models use long contexts. _arXiv pre",".\n\n\nWe also go one step further and construct a realistic but simple dataset based on these theoretical limitations (called LIMIT). Despite the simplicity of the task (e.g., `who likes Apples?` and\n`Jon likes Apples, ...` ), we find it is very difficult for even state-of-the-art embedding models [Lee et al., 2025, Zhang et al., 2025] on MTEB [Enevoldsen et al., 2025] due to the theoretical\nunderpinnings, and impossible [1] for models with small embedding dimensions.\n\n\nOverall, our work contributes: (1) a theoretical basis for the fundamental limitations of embedding\nmodels, (2) a best-case empirical analysis showing that this proof holds for any dataset instantiation\n(by free embedding optimization), and (3) a simple real-world natural language instantiation called\nLIMIT that even state-of-the-art embedding models cannot solve.\n\n\nThese results imply interesting findings for the community: on one hand we see neural embedding\nmodels becoming immensely successful. However, academic benchmarks test only a small amount of\nthe queries that could be issued (and these queries are often overfitted to), hiding these limitations.\nOur work shows that as the tasks given to embedding models require returning ever-increasing\ncombinations of top- _\ud835\udc58_ relevant documents (e.g., through instructions connecting previously unrelated\n\n\n1At least with current optimization techniques for retrieval.\n\n\n2\n\n\nOn the Theoretical Limitations of Embedding-Based Retrieval\n\n\ndocuments with logical operators), we will reach a limit of combinations they cannot represent.\n\n\nThus, the community should be aware of these limitations, both when designing evaluations (as\nLIMIT shows) and by choosing alternative retrieval approaches \u2013 such as cross-encoders or multi-vector\nmodels \u2013 when attempting to create models that can handle the full range of instruction-based queries,\ni.e. _any query and relevance definition_ .\n\n##### **2. Related Work**\n\n\n**2.1. Neural Embedding Models**\n\n\nThere has been immense progress on embedding models in recent years [Lee et al., 2019, Craswell\net al., 2020, BehnamGhader et al., 2024], moving from simple web search (text-only) to advanced\ninstruction-following and multi-modal representations. These models generally followed advances in\nlanguage models, such as pre-trained LMs [Hoffmann et al., 2022], multi"," extreme, we could allow the\nencoder full access to the entire corpus, through some form of cross attention. The latter approach\nhas been explored on a small scale in methods like neural processes (Garnelo et al., 2018); however,\nit would be difficult to scale to larger datasets.\n\n\nWe opt for a middleground that allows the model to learn corpus statistics, but is also relatively\nefficient to compute, shown in Figure 1. Specifically, we note that document embeddings retain\na surprising amount of lexical information even after embedding (Morris et al., 2023). Therefore,\nif we pre-embed a subset of the corpus, we believe we can still dynamically calculate key dataset\ninformation during encoding.\n\n\n5\n\n\n\n\n\n\n\n\n - _m_ (( _d, q_ ) _, m_ _[b]_ ) (3)\n\n( _d,q_ ) _\u2208B_ _[b]_\n\n\n\n\n\n\n\n_b_\n\n\nWe produce contextualized embeddings via a two-stage process:\n\n\n_**First stage:**_ _Gather and embed context._ Given context documents _d_ [1] _, ..., d_ _[J]_ _\u2208D_, we embed each\nusing a unique embedding model and concatenate embeddings into a sequence _M_ 1( _d_ [1] ) _...M_ 1( _d_ _[J]_ ).\n\n\n_**Second stage:**_ _Embed document with additional context tokens._ To compute _\u03d5_ for document _d_ _[\u2032]_ we\nintegrate contextual embedding sequence at the input of second-stage embedding model _M_ 2:\n\n\n_\u03d5_ ( _d_ _[\u2032]_ ; _D_ ) = _M_ 2( _M_ 1( _d_ [1] ) _, . . ., M_ 1( _d_ _[J]_ ) _, E_ ( _d_ _[\u2032]_ 1 [)] _[, . . ., E]_ [(] _[d][\u2032]_ _T_ [))] (5)\n\n\nHere _M_ 1 is the first-stage encoder model, _M_ 2 is a second-stage encoder model, and _E_ is the token\nembedding matrix of _M_ 2 applied to each token in _d_ _[\u2032]_ . In practice, we parameterize both _M_ 1"," versatility. Our end-to-end training\nlets an LM _M_ **generate** text informed by **retrieved** passages, if needed, and **criticize** the output by\nlearning to generate special tokens. These _reflection tokens_ (Table 1) signal the need for retrieval\n\n- r confirm the output\u2019s relevance, support, or completeness. In contrast, common RAG approaches\nretrieve passages indiscriminately, without ensuring complete support from cited sources.\n\n\n3.1 PROBLEM FORMALIZATION AND OVERVIEW\n\n\nFormally, given input _x_, we train _M_ to sequentially generate textual outputs _y_ consisting of multiple\nsegments _y_ = [ _y_ 1 _, . . ., yT_ ], where _yt_ indicates a sequence of tokens for the _t_ - th segment. [3] Generated\ntokens in _yt_ include text from the original vocabulary as well as the reflection tokens (Table 1).\n\n\n2All work is arXived within a week of this preprint.\n3In this paper, we treat one sentence as a segment in our experiments, but our framework is applicable to any\nsegment unit (i.e., sub-sentence).\n\n\n3\n\n\nPreprint.\n\n\nType Input Output Definitions\n\n\n**Retrieve** _x_ \/ _x, y_ _{_ yes, no, continue _}_ Decides when to retrieve with _R_\n**ISREL** _x, d_ _{_ **relevant**, irrelevant _}_ _d_ provides useful information to solve _x_ .\n**ISSUP** _x, d, y_ _{_ **fully supported**, partially All of the verification-worthy statement in _y_\nsupported, no support _}_ is supported by _d_ .\n\n|Retrieve|Col2|\n|---|---|\n|**ISREL**||\n|**ISSUP**|**ISSUP**|\n\n**ISUSE** _x, y_ _{_ **5**, 4, 3, 2, 1 _}_ _y_ is a useful response to _x_ .\n\n\nTable 1: Four types of reflection tokens used in SELF-RAG. Each type uses several tokens to represent\nits output values. The bottom three rows are three types of **Critique** tokens, and **the bold text** indicates\nthe most desirable critique tokens. _x, y, d_ indicate input, output, and a relevant passage, respectively.\n\n\n"],"reference":"Passage embeddings are used to represent text passages as dense vectors, which can then be compared using similarity scores. This allows for efficient training on large batches of question-passage pairs by reusing computation and creating multiple positive and negative examples within each batch."}
{"user_input":"What is BM25 used for in the context of question answering datasets?","response":"BM25 is used in the context of question answering datasets to determine the likelihood that an answer appears in the contexts fed to the reader. It helps in ranking candidate contexts, thus enabling more effective discrimination of the correct answer from a pool of candidates, some of which may be distractors. Research indicates that end-to-end QA effectiveness increases with the number of candidate contexts (k) presented to the reader, suggesting that the inclusion of more context can enhance performance, although there is a point after which adding more irrelevant contexts may negatively affect effectiveness (<document id=\"2104.05740_hybrid_retrieval_ma:chunk_7200\">).","retrieved_contexts":[" graph comprehension\nand question-answering capabilities through soft prompting\n\n- f the LLM, and employs the Prize-Collecting Steiner Tree\n(PCST) optimization problem for targeted graph retrieval. On\nthe contrary, it requires additional effort to build, validate,\nand maintain structured databases. On the contrary, it requires\nadditional effort to build, validate, and maintain structured\n\ndatabases.\n\n_LLMs-Generated Content._ Addressing the limitations of\nexternal auxiliary information in RAG, some research has\nfocused on exploiting LLMs\u2019 internal knowledge. SKR [58]\nclassifies questions as known or unknown, applying retrieval\nenhancement selectively. GenRead [13] replaces the retriever\nwith an LLM generator, finding that LLM-generated contexts\n\n- ften contain more accurate answers due to better alignment\nwith the pre-training objectives of causal language modeling.\nSelfmem [17] iteratively creates an unbounded memory pool\nwith a retrieval-enhanced generator, using a memory selector to choose outputs that serve as dual problems to the\n\n- riginal question, thus self-enhancing the generative model.\nThese methodologies underscore the breadth of innovative\ndata source utilization in RAG, striving to improve model\nperformance and task effectiveness.\n_2) Retrieval Granularity:_ Another important factor besides\nthe data format of the retrieval source is the granularity of\nthe retrieved data. Coarse-grained retrieval units theoretically\ncan provide more relevant information for the problem, but\nthey may also contain redundant content, which could distract\nthe retriever and language models in downstream tasks [50],\n\n[87]. On the other hand, fine-grained retrieval unit granularity\nincreases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge. Choosing\n\n\nthe appropriate retrieval granularity during inference can be\na simple and effective strategy to improve the retrieval and\ndownstream task performance of dense retrievers.\nIn text, retrieval granularity ranges from fine to coarse,\nincluding Token, Phrase, Sentence, Proposition, Chunks, Document. Among them, DenseX [30]proposed the concept of\nusing propositions as retrieval units. Propositions are defined\nas atomic expressions in the text, each encapsulating a unique\nfactual segment and presented in a concise, self-contained natural language format. This approach aims to enhance retrieval\nprecision and relevance. On the Knowledge Graph (KG),\nretrieval granularity includes Entity, Triplet, and sub-Graph"," likelihood that the\nanswer appears in the contexts fed to the reader.\nOn the other hand, the reader is asked to con\nsider more contexts, and thus needs to discriminate the correct answer from a larger pool of candidate contexts, some of which might be low quality and thus serve as \u201cdistractors\u201d from the correct answer. How do these factors balance out?\n\nSimilar analyses in previous work with BM25 retrieval have shown that end-to-end QA effectiveness increases with increasing k (Yang et al., 2019;\nXie et al., 2020); that is, the reader does not appear to be \u201cconfused\u201d by the non-relevant material. Indeed, in our BM25 results we also observe\n\nthe same trend.\n\nInterestingly, however, when we switch from\nBM25 results to DPR results, the behavior ap\n\n\npears to change. For TriviaQA, the effectiveness\ncurve behaves as expected, but for NQ, the exact\nmatch score trends up and then decreases after a\npeak. This means that while the likelihood of the\nreader seeing a correct answer in the candidate\ncontexts increases with k, it is more likely to be\nnegatively affected by increasing amounts of nonrelevant contexts as well. This general behavior is\nalso seen for the hybrid scoring techniques: as k\nincreases, so does the exact match score, but only\nup to a certain point. Beyond this point, feeding\nthe reader more candidate contexts leads to slight\ndecreases in end-to-end effectiveness.\n\n\n5 Conclusion\n\n\nThe breakneck pace at which NLP and IR are\nadvancing, we argue, makes reproducibility and\nreplicability critical to advancing science\u2014to ensure that we are building on a firm foundation.\nOur study adds to the veracity of the claims made\nby Karpukhin et al. (2020), and our work does indeed confirm that DPR is an effective dense retrieval technique. However, we arrived at two important additional findings, one of which is inconsistent with the original work, the other of which\npresents an enhancement. Together, they enrich\n\n- ur understanding of DPR.\n\n\n6 Acknowledgments\n\n\nThis research was supported in part by the Canada\nFirst Research Excellence Fund and the Natu\nral Sciences and Engineering Research Council\n(NSERC) of Canada. Computational resources\nwere provided by Compute Ontario and Compute\nCanada.\n\n\nReferences\n\n\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine",") **73.30** 35.28 **74.00** **84.95** **86.86**\n\n\n**Datasets** In our experiments, we use the following datasets to evaluate our model and\nall baselines. We selected these datasets to represent both popular and diverse domains\nincluding Wikipedia, Coding\/API documents, and question-answering on medical documents. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and\nHotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,\nmainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,\nand TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.\nThese benchmarks measure how to generate the correct, functional, and executable API\ncalls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering\ndataset tailored only for biomedical-research question-answering. It mainly focuses on\nanswering medical and biology questions based on a given set of documents. We would\n\n\n5\n\n\nPreprint, Under Review\n\n\nlike to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas\nthe latter two domains are on domain-specific documents.\n\n\n**Baselines** We consider the following baselines for our experiments:\n\n\n   - LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used\ninstruction-finetuned model for QA tasks, where we provide clearly written instructions, but no reference documentation.\n\n   - LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,\nexcept here we include reference documents. This is a popular technique when\ndealing with domain-specific QA tasks.\n\n   - Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervisedfinetuning, without documents in context. We find that its mostly useful to align\nthe answering style of the model as well as get familiar with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does"," documents directly from MS\nMARCO\u2019s 8.8M documents per query. In addition to MRR@10 and\nlatency in milliseconds, the table reports Recall@50, Recall@200,\nand Recall@1000, important metrics for a full-retrieval model that\nessentially filters down a large collection on a per-query basis.\nWe compare against BM25, in particular MS MARCO\u2019s official\nBM25 ranking as well as a well-tuned baseline based on the Anserini\ntoolkit. [7] While many other traditional models exist, we are not\naware of any that substantially outperform Anserini\u2019s BM25 implementation (e.g., see RM3 in [28], LMDir in [2], or Microsof\u2019s\nproprietary feature-based RankSVM on the leaderboard).\nWe also compare against doc2query, DeepCT, and docTTTTTquery. All three rely on a traditional bag-of-words model (primarily BM25) for retrieval. Crucially, however, they re-weigh the\nfrequency of terms per document and\/or expand the set of terms\nin each document before building the BM25 index. In particular,\ndoc2query expands each document with a pre-defined number\n\n- f synthetic queries generated by a seq2seq transformer model\n(which docTTTTquery replaced with a pre-trained language model,\nT5 [31]). In contrast, DeepCT uses BERT to produce the term frequency component of BM25 in a context-aware manner.\nFor the latency of Anserini\u2019s BM25, doc2query, and docTTTTquery, we use the authors\u2019 [26, 28] Anserini-based implementation.\nWhile this implementation supports multi-threading, it only utilizes\nparallelism across different queries. We thus report single-threaded\nlatency for these models, noting that simply parallelizing their\ncomputation over _shards_ - f the index can substantially decrease\ntheir already-low latency. For DeepCT, we only estimate its latency\nusing that of BM25 (as denoted by _(est.)_ in the table), since DeepCT\nre-weighs BM25\u2019s term frequency without modifying the index\n\n- therwise. [8] As discussed in \u00a74.1, we use ColBERTL2 for end-toend retrieval, which employs negative squared L2 distance as its\nvector-similarity function. For its latency,"," question and contexts. Dense retrieval methods have thus never\n\nbe shown to outperform TF-IDF\/BM25 for opendomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\n\n - bjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then finetuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain\n\n\nQA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot fine-tuned using pairs of questions and answers,\nthe corresponding representations could be suboptimal.\n\nIn this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), _with-_\n\n_out_ additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\n\n- f question and passage pairs. Through a series\n\n- f careful ablation studies, our final solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective comparing all pairs of questions and passages in a batch.\nOur _Dense Passage Retriever_ (DPR) is exceptionally strong. It not only outperforms BM25 by a\nlarge margin (65.2% vs. 42.9% in Top-5 accuracy),\nbut also results in a substantial improvement on\nthe end-to-end QA accuracy compared to ORQA\n(41.5% vs. 33.3%) in the open Natural Questions\nsetting (Lee et al., 2019; Kwiatkowski et al., 2019).\n\nOur contributions are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso"],"reference":"BM25 is used to select positive passages by ranking them based on their relevance to the questions. The highest-ranked passage containing the answer is selected as the positive passage."}
{"user_input":"What is TREC used for in the context of passage retrieval experiments?","response":"TREC, in the context of passage retrieval experiments, serves as a benchmark for evaluating retrieval systems. Specifically, it uses a setting where the number of retrieved candidates (k) is often set to 1000, a default in many TREC evaluations. This benchmark allows researchers to assess the performance of different retrieval methods by comparing their ability to select relevant passages for a given query, which is especially crucial in studies like Dense Passage Retrieval (DPR) <document id=\"2104.05740_hybrid_retrieval_ma:chunk_2250\">.","retrieved_contexts":[" = 1000 in our experiments since it is a\nmore common setting in information retrieval experiments (for example, k = 1000 is the default in\nmost TREC evaluations).\n\n\n2.2 Reader\n\n\nAs is standard in a retriever\u2013reader design, the retriever in Karpukhin et al. (2020) returns k candidate passages (i.e., splits from Wikipedia) for each\nquery q. The reader extracts the final answer span\nfrom these candidate contexts, where each context\nCi is comprised of the Wikipedia article title Ci [title]\nand the content text Ci [text] .\nThe reader in DPR uses BERT-base and takes\n\nas input each candidate context Ci concatenated to\nthe question q. Answer extraction is treated as a labeling task, and the reader identifies the answer by\npredicting the start and end tokens of the answer\n\n\n\nspan in the contexts. To do so, the DPR reader\nadds a linear layer on top of BERT to predict the\nstart logit and end logit for each token from the final hidden layer representations. The score of an\nanswer span is calculated by adding the start logit\n\n- f the first token and the end logit of the last token. The reader returns the m highest scoring answer spans. In addition, the reader uses the learned\nrepresentation of [CLS] to predict the overall relevance of the context to the question.\n\nIn more detail, the reader operates as follows:\n\n\nri, S = Reader([CLS] q [SEP] Ci [title] [SEP] Ci [text] )\n\n\nwhere ri is the overall relevance score for context\nCi, and S comprises m potential (answer span,\nspan score) pairs extracted from context Ci:\n\n\n{(Si,1, si,1), (Si,2, si,2), . . . (Si,m, si,m)}.\n\n\nIn the original paper, the final answer span is the\ncandidate with the maximum span score from the\ncontext with the highest relevance score.\n\nWe attempted to replicate exactly the DPR implementation of answer extraction using our opensource PyGaggle neural reranking library, which\nholds the code to many of our other search-related\nprojects. Once again, we began with reader checkpoints released in the DPR repo, but otherwise our\nimplementation is completely independent (other\nthan, again, the evaluation code).\n\nIn addition to the answer extraction algorithm above, we also implemented the normalized answer span scoring technique described\n",". Murdock Jr. 1962. The serial position\neffect of free recall. _Journal of experimental_\n_psychology_, 64(5):482.\n\n\nJoe O\u2019Connor and Jacob Andreas. 2021. What con\ntext features can Transformer language models\nuse? In _Proc. of ACL_ .\n\n\nDimitris Papailiopoulos, Kangwook Lee, and Jyyong Sohn. 2023. A little retrieval test for\n[large language models. https:\/\/github.com\/](https:\/\/github.com\/anadim\/the-little-retrieval-test)\n\n[anadim\/the-little-retrieval-test.](https:\/\/github.com\/anadim\/the-little-retrieval-test)\n\n\n[Bo Peng. 2023. RWKV-LM. https:\/\/github.](https:\/\/github.com\/BlinkDL\/RWKV-LM)\n\n[com\/BlinkDL\/RWKV-LM.](https:\/\/github.com\/BlinkDL\/RWKV-LM)\n\n\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong.\n2021. Random feature attention. In _Proc. of_\n\n_ICLR_ .\n\n\nFabio Petroni, Patrick Lewis, Aleksandra Piktus,\nTim Rockt\u00e4schel, Yuxiang Wu, Alexander H\nMiller, and Sebastian Riedel. 2020. How context\naffects language models\u2019 factual predictions. In\n_Proc. of AKBC_ .\n\n\nMichael Poli, Stefano Massaroli, Eric Nguyen,\nDaniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua\nBengio, Stefano Ermon, and Christopher R\u00e9.\n2023. Hyena hierarchy: Towards larger convolutional language models. In _Proc. of ICML_ .\n\n\nOfir Press, Noah A. Smith, and Mike Lewis. 2021.\nShortformer: Better language modeling using\nshorter inputs. In _Proc. of ACL_ .\n\n\nOfir Press, Noah A. Smith, and Mike Lewis. 2022.\nTrain short, test long: Attention with linear biases enables input length extrapolation. In _Proc._\n\n_of ICLR_ .\n\n\nGuanghui Qin, Yukun Feng, and Benjamin\nVan Durme. 2023. The NLP task effectiveness\n\n - f long-range transformers. In _","bo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models.\n\n\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\nZhiruo Wang, Jamie Callan, and Graham Neubig.\n2022. Retrieval as attention: End-to-end learning of\nretrieval and reading within a single transformer. In\n_Conference on Empirical Methods in Natural Lan-_\n_guage Processing (EMNLP)_, Abu Dhabi, UAE.\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n[Wen-tau Yih. 2020. Dense passage retrieval for open-](https:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.550)\n[domain question answering. In](https:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.550) _Proceedings of the_\n_2020 Conference on Empirical Methods in Natural_\n_Language Processing (EMNLP)_, pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\n\n\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP. _arXiv preprint_\n_arXiv:2212.14024_ .\n\n\n\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\n\n[Internet-augmented dialogue generation. In](https:\/\/doi.org\/10.18653\/v1\/2022.acl-long.579) _Proceed-_\n_ings of the 60th Annual Meeting of the Association_\n_for Computational Linguistics (Volume 1: Long Pa-_\n_pers)_, pages 8460\u20138478, Dublin, Ireland. Association\nfor Computational Linguistics.\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee","]_ are learnable vectors.\nWe compute a span score of the _s_ - th to _t_ - th words\nfrom the _i_ - th passage as _P_ start _,i_ ( _s_ ) _\u00d7 P_ end _,i_ ( _t_ ), and\na passage selection score of the _i_ - th passage as\n_P_ selected( _i_ ).\nDuring training, we sample one positive and\n\n\u02dc\n_m_ _\u2212_ 1 negative passages from the top 100 passages\nreturned by the retrieval system (BM25 or DPR)\nfor each question. \u02dc _m_ is a hyper-parameter and we\nuse \u02dc _m_ = 24 in all the experiments. The training objective is to maximize the marginal log-likelihood\n\n- f all the correct answer spans in the positive passage (the answer string may appear multiple times\nin one passage), combined with the log-likelihood\n\n- f the positive passage being selected. We use the\nbatch size of 16 for large (NQ, TriviaQA, SQuAD)\nand 4 for small (TREC, WQ) datasets, and tune _k_\n\n- n the development set. For experiments on small\ndatasets under the _Multi_ setting, in which using\n\n- ther datasets is allowed, we fine-tune the reader\ntrained on Natural Questions to the target dataset.\nAll experiments were done on eight 32GB GPUs.\n\n\n**6.2** **Results**\n\n\nTable 4 summarizes our final end-to-end QA results, measured by _exact match_ with the reference\nanswer after minor normalization as in (Chen et al.,\n2017; Lee et al., 2019). From the table, we can\n\n\n**Training** **Model** **NQ** **TriviaQA** **WQ** **TREC** **SQuAD**\n\n\nSingle BM25+BERT (Lee et al., 2019) 26.5 47.1 17.7 21.3 33.2\nSingle ORQA (Lee et al., 2019) 33.3 45.0 36.4 30.1 20.2\nSingle HardEM (Min et al., 2019a) 28.1 50.9    -    -    Single GraphRetriever (Min et al., 201","relevant documents from the data source. There are several\n\nkey issues involved, such as the retrieval source, retrieval\ngranularity, pre-processing of the retrieval, and selection of\nthe corresponding embedding model.\n\n\n_A. Retrieval Source_\n\n\nRAG relies on external knowledge to enhance LLMs, while\nthe type of retrieval source and the granularity of retrieval\nunits both affect the final generation results.\n_1) Data Structure:_ Initially, text is s the mainstream source\n\n- f retrieval. Subsequently, the retrieval source expanded to include semi-structured data (PDF) and structured data (Knowledge Graph, KG) for enhancement. In addition to retrieving\nfrom original external sources, there is also a growing trend in\nrecent researches towards utilizing content generated by LLMs\nthemselves for retrieval and enhancement purposes.\n\n\n6\n\n\n\nTABLE I\n\nSUMMARY OF RAG METHODS\n\n\nRetrieval Retrieval Augmentation Retrieval\nMethod Retrieval Source\nData Type Granularity Stage process\n\n\nCoG [29] Wikipedia Text Phrase Pre-training Iterative\nDenseX [30] FactoidWiki Text Proposition Inference Once\nEAR [31] Dataset-base Text Sentence Tuning Once\nUPRISE [20] Dataset-base Text Sentence Tuning Once\nRAST [32] Dataset-base Text Sentence Tuning Once\nSelf-Mem [17] Dataset-base Text Sentence Tuning Iterative\nFLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive\nPGRA [33] Wikipedia Text Sentence Inference Once\nFILCO [34] Wikipedia Text Sentence Inference Once\nRADA [35] Dataset-base Text Sentence Inference Once\nFilter-rerank [36] Synthesized dataset Text Sentence Inference Once\nR-GQA [37] Dataset-base Text Sentence Pair Tuning Once\nLLM-R [38] Dataset-base Text Sentence Pair Inference Iterative\nTIGER [39] Dataset-base Text Item-base Pre-training Once\nLM-Indexer [40] Dataset-base Text Item-base Tuning Once\nBEQUE [9] Dataset-base Text Item-base Tuning Once\nCT-RAG [41] Synthesized dataset Text Item-base Tuning Once\nAtlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative\nRAVEN [43] Wikipedia Text Chunk Pre-training Once\nRETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative\nINSTRUCTRETRO [45] Pre-training corpus Text Chunk"],"reference":"TREC is used as one of the QA datasets to evaluate the retrieval performance of the Dense Passage Retriever (DPR) model."}
{"user_input":"What is the reason for the lower performance of SQuAD according to Lee et al.?","response":"According to Lee et al. (referenced in the document by Karpukhin et al. 2020), the lower performance of SQuAD can be attributed to two main reasons. First, the annotators who created the questions did so after reviewing the passages, leading to a high lexical overlap between the questions and the texts, which adversely affects retrieval effectiveness. Second, SQuAD is drawn from a very small set of Wikipedia articles, introducing an unwanted bias that impacts performance outcomes (<document 2004.04906_dpr_karpukhin:chunk_5400>).","retrieved_contexts":[".05)  - 90.1  - 95.0 [\u2021]\n\nHybrid (\u03b1 = 0.7)  - 89.6  - 94.6 [\u2021]\n\n\nSQuAD\n\nDPR 51.6 52.0 67.6 67.7\nBM25 68.8 71.1 [\u2020] 80.0 81.8 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 66.2  - 78.6  Hybridnorm (\u03b1 = 2.00)  - 75.1 [\u2021]  - 84.4 [\u2021]\n\nHybrid (\u03b1 = 28)  - 75.0 [\u2021]  - 84.0 [\u2021]\n\n\nTable 1: Retrieval effectiveness comparing results from\nthe original DPR paper (\u201corig\u201d) and our replication attempt (\u201crepl\u201d). The symbol [\u2020] - n a BM25 result indicates effectiveness that is significantly different from\nDPR. The symbol [\u2021] indicates that the hybrid technique\nis significantly better than BM25 (for SQuAD) or DPR\n(for all remaining collections).\n\n\nthat the effectiveness differences between DPR\n\nand BM25 in our replication results are statistically significant. In all datasets except for SQuAD,\nDPR outperforms BM25; this is consistent with\nthe original paper. We further confirmed that for\nSQuAD, DPR is significantly worse than BM25.\nAs Karpukhin et al. (2020) noted, RetrieverMulti\nwas trained by combining training data from all\ndatasets but excluding SQuAD; these poor results\nare expected, since SQuAD draws from a very\nsmall set of Wikipedia articles.\nThird, the effectiveness of hybrid dense\u2013sparse\nfusion appears to be understated in the original\nDPR paper. Karpukhin et al. (2020) found that\n\n\nCondition k = 20 100 500 1000\n\n\nNQ 6.1 5.2 4.4 4.2\nTriviaQA 9.2 6.6 5.0 4.6\nWQ 5.9 5.9 5.8 5.7\n\nCuratedTrec 6.9 7.2 6.3 5.9\n\n"," is only outperformed by BM25 on\n\n- ne dataset, TREC-Covid but with a tiny 0.2 margin; in comparison, the underlying Contriever\nunderperforms by more than 50%.\nWe also observe HyDE demonstrates strong\nperformance compared to fine-tuned models.\nHyDE generally shows better performance than\nANCE and DPR, even though the two are\nfine-tuned on MS-MARCO and ANCE also involves some sophisticated hard negative techniques.\nContriever [FT] shows performance advantages on\nFiQA and DBPedia. These involve retrieval of financial posts or entities respectively. We believe\nthe performance difference can be attributed to the\n\n\nSwahili Korean Japanese Bengali\n\n\n_w\/o relevance judgement_\nBM25 38.9 28.5 21.2 **41.8**\n\nmContriever 38.3 22.3 19.5 35.3\nHyDE **41.7** **30.6** **30.7** 41.3\n\n\n_w\/ relevance judgement_\nmDPR 7.3 21.9 18.1 25.8\n\nmBERT 37.4 28.1 27.1 35.1\n\nXLM-R 35.1 32.2 24.8 41.7\nmContriever [FT] **51.2** **34.2** **32.4** **42.3**\n\n\nTable 3: MRR@100 on Mr.Tydi. Best performing w\/o\nrelevance and overall system(s) are marked **bold** .\n\n\nunder-specification of the instruction; more elaborative instructions may help.\n\n\n**4.4** **Multilingual Retrieval**\n\n\nMultilingual setup poses several additional challenges to HyDE. The small-sized contrastive encoder gets saturated as the number of languages\nscales (Conneau et al., 2020; Izacard et al., 2021).\nMeanwhile, our generative LLM faces an opposite\nissue: with languages of not as high resource as\nEnglish or French, the high capacity LLM can get\nunder-trained (Hoffmann et al., 2022).\nNevertheless, in Table 3, we still find HyDE\nable to improve the mContriever model. It can\n\n- utperform non-Contriever models fine-tuned on\nand"," BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages based on BM25 and DPR,\nrespectively, and rerank the union of them using\nBM25( _q_, _p_ ) + _\u03bb \u00b7_ sim( _q, p_ ) as the ranking function.\nWe used _\u03bb_ = 1 _._ 1 based on the retrieval accuracy in\nthe development set.\n\n\n**5.1** **Main Results**\n\n\nTable 2 compares different passage retrieval systems on five QA datasets, using the top- _k_ accuracy\n( _k \u2208{_ 20 _,_ 100 _}_ ). With the exception of SQuAD,\nDPR performs consistently better than BM25 on\nall datasets. The gap is especially large when _k_ is\nsmall (e.g., 78.4% vs. 59.1% for top-20 accuracy\n\n- n Natural Questions). When training with mul\n\n8SQuAD is limited to a small set of Wikipedia documents\nand thus introduces unwanted bias. We will discuss this issue\n\nmore in Section 5.1.\n\n9\n[Lucene implementation. BM25 parameters](https:\/\/lucene.apache.org\/) _b_ = 0 _._ 4 (document length normalization) and _k_ 1 = 0 _._ 9 (term frequency\nscaling) are tuned using development sets.\n\n\n\nFigure 1: Retriever top- _k_ accuracy with different numbers of training examples used in our dense passage retriever vs BM25. The results are measured on the de\nvelopment set of Natural Questions. Our DPR trained\nusing 1,000 examples already outperforms BM25.\n\n\ntiple datasets, TREC, the smallest dataset of the\nfive, benefits greatly from more training examples.\nIn contrast, Natural Questions and WebQuestions\nimprove modestly and TriviaQA degrades slightly.\nResults can be improved further in some cases by\ncombining DPR with BM25 in both single- and\nmulti-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and","26.0 77.5 16.3 20.5 24.7 2.3 3.6\nAlpaca13B 46.1 66.9 51.1 57.6 77.7 **34.8** 36.7 56.6 2.0 3.8\n**Our** SELF-RAG 7B 54.9 66.4 72.4 67.3 **81.2** 30.0 35.7 **74.3** 66.9 67.8\n**Our** SELF-RAG 13B **55.8** **69.3** **74.5** **73.1** 80.2 31.7 **37.0** 71.6 **70.3** **71.3**\n\n\n5 RESULTS AND ANALYSIS\n\n\n5.1 MAIN RESULTS\n\n\n**Comparison against baselines without retrieval.** Table 2 (top) presents the baselines without\nretrieval. Our SELF-RAG (bottom two rows) demonstrates a substantial performance advantage\n\n- ver supervised fine-tuned LLMs in all tasks and even outperforms ChatGPT in PubHealth, PopQA,\nbiography generations, and ASQA (Rouge and MAUVE). Our approach also significantly outperforms\na concurrent method that employs sophisticated prompt engineering; specifically, on the bio generation\ntask, our 7B and 13B models outperform the concurrent CoVE (Dhuliawala et al., 2023), which\niteratively prompts Llama265B to refine output.\n\n\n**Comparison against baselines with retrieval.** As shown in Tables 2 (bottom), our SELF-RAG also\n\n- utperforms existing RAG in many tasks, obtaining the best performance among non-proprietary\nLM-based models on all tasks. While our method outperforms other baselines, on PopQA or Bio,\npowerful instruction-tuned LMs with retrieval (e.g., LLama2-chat, Alpaca) show large gains from\ntheir non-retrieval baselines. However, we found that these baselines provide limited solutions for\ntasks where we cannot simply copy or extract sub-strings of retrieved passages. On PubHealth\nand ARC-Challenge, baselines with retrieval do not improve performance notably from their noretrie","3), BELLE-7B-2M (Yunjie Ji 2023).\n\n\n**Results on Noise Robustness**\n\nWe evaluated the accuracy based on the different noise ratios\nin external documents, and the results are shown in Table 1.\nWe can see that:\n**(1) RAG can effect improve the responses of LLMs.**\nLLMs have shown strong performance even in the presence\n\n- f noise, indicating that RAG is a promising way for LLMs\nto generate accurate and reliable responses.\n**(2) The increasing noise rate poses a challenge for**\n**RAG in LLMs.** Specifically, when the noise ratio exceeds\n80%, the accuracy decreases significantly at a significance\nlevel of 0.05. For example, the performance of ChatGPT has\ndecreased from 96.33% to 76.00%, while the performance\n\n- f ChatGLM2-6B has decreased from 91.33% to 57.33%.\n\n\n**Error Analysis.** To better comprehend the negative impact of noise on model generation, we examined the incorrect answers and found that these errors typically originate\nfrom three reasons, as shown in Table 2.\n**(1) Long-distance information.** LLMs often face difficulty in identifying the correct answer from external documents when the information related to the question is distant\nfrom the information related to the answer. This scenario\nis quite common as longer texts are frequently encountered\n\n\n3We use gpt-3.5-turbo api in the experiments.\n\n\n- n the internet. In such cases, it is typical for the question\u2019s\ninformation to be initially presented at the start of the document and subsequently referred to using pronouns. In Table 2, the question information (\u201cQatar Open 2022\u201d) is only\nmentioned once at the beginning and is far from where the\nanswer text \u201cAnett Kontaveit\u201d appears. This situation may\ncause LLMs to depend on information from other documents and create false impressions, i.e., hallucination.\n**(2) Evidence uncertainty.** Before highly anticipated\nevents, like the release of new Apple products or the announcement of the Oscars, there is often a significant\namount of speculative information circulating on the internet. Although the relevant documents explicitly state that\nit is uncertain or speculative content, they can still impact\n\n- n the retrieval-augmented generation of LLMs. In Table"],"reference":"The lower performance on SQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage, resulting in high lexical overlap between passages and questions, which gives BM25 an advantage. Second, the data was collected from only 500+ Wikipedia articles, leading to a biased distribution of training examples."}
{"user_input":"What CPU model was used to profile the passage retrieval speed for DPR?","response":"The CPU model used to profile the passage retrieval speed for Dense Passage Retrieval (DPR) is the Intel Xeon CPU E5-2698 v4 @ 2.20GHz <document id=\"2004.04906_dpr_karpukhin:chunk_7200\">.","retrieved_contexts":[".1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7 **81.3**\n\n\nDPR **79.4** 78.8 **75.0** **89.1** 51.6 **86.0** 84.7 **82.9** 93.9 67.6\nMulti\nBM25 + DPR 78.0 **79.9** 74.7 88.5 66.2 83.9 84.4 82.3 **94.1** 78.6\n\n\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20\/100 retrieved\npassages that contain the answer. _Single_ and _Multi_ denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.\n\n\n\ntraditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10 _[\u2212]_ [5] using Adam, linear\nscheduling with warm-up and dropout rate 0 _._ 1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be desirable to obtain a single retriever that works well\nacross the board. To this end, we train a _multi_ dataset encoder by combining training data from\nall datasets excluding SQuAD. [8] In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages","., the weight for the retriever score becomes\n\u03b3 = \u03b2 \u00b7 \u03b4). We started with step size 0.2 for\nboth \u03b2 and \u03b4 and found the optimal pair of values\n\u03b21, \u03b41. We then repeated this process with step\nsize 0.05 and then 0.01 in a smaller range around\nthe optimal \u03b2i and \u03b4i from the previous pass.\nFor final evidence fusion, we tuned the weight\nparameters together with the number of retrieval\nresults (k) up to 500 with a step size of 20. Optimal parameters were selected based on the exact\nhighest match score.\n\n\n4 Results\n\n\n4.1 Retrieval\n\n\nTable 1 reports top-k = {20, 100} retrieval accuracy from our replication attempt, compared to\nfigures copied directly from the original DPR paper; here we focus on results from RetrieverMulti.\nThe hybrid retrieval results reported in the original\nDPR paper is denoted Hybridorig, which is not directly comparable to either of our two techniques:\nHybridnorm (with minimum score normalization)\n\n- r Hybrid (without such normalization). We make\nthe following observations:\nFirst, our dense retrieval results are very close\nto those reported in Karpukhin et al. (2020). We\nconsider this a successful replication attempt and\n\n- ur efforts add veracity to the effectiveness of the\nDPR technique. Yay!\nSecond, our Pyserini BM25 implementation\n\n- utperforms the BM25 results reported in the original paper across all datasets. Furthermore, the gap\nis larger for k = 20. On average, our results represent a nearly seven-point improvement in top-20\naccuracy and a nearly five-point improvement for\ntop-100 accuracy. Since Karpukhin et al. (2020)\nhave not made available their code for generating\nthe BM25 results, we are unable to further diagnose these differences.\n\nNevertheless, the results do support the finding that dense retrieval using DPR is (generally)\nmore effective than sparse retrieval. We confirmed\n\n\n\nTop-20 Top-100\nCondition  - rig repl  - rig repl\n\n\nNQ\n\nDPR 79.4 79.5 86.0 86.1\nBM25 59.1 62.9 [\u2020] 73.7 78.3 [\u2020]\n\n\nHybridorig (\u03bb = 1.1) 78.0"," like\nBM25 are sensitive to highly selective keywords\nand phrases, while DPR captures lexical variations\n\n- r semantic relationships better. See Appendix C\nfor examples and more discussion.\n\n\n**5.4** **Run-time Efficiency**\n\n\nThe main reason that we require a retrieval component for open-domain QA is to reduce the number\n\n- f candidate passages that the reader needs to consider, which is crucial for answering user\u2019s questions in real-time. We profiled the passage retrieval\nspeed on a server with Intel Xeon CPU E5-2698 v4\n@ 2.20GHz and 512GB memory. With the help of\nFAISS in-memory index for real-valued vectors [10],\nDPR can be made incredibly efficient, processing\n995.0 questions per second, returning top 100 passages per question. In contrast, BM25\/Lucene (implemented in Java, using file index) processes 23.7\nquestions per second per CPU thread.\nOn the other hand, the time required for building\nan index for dense vectors is much longer. Computing dense embeddings on 21-million passages\nis resource intensive, but can be easily parallelized,\ntaking roughly 8.8 hours on 8 GPUs. However,\nbuilding the FAISS index on 21-million vectors\n\n- n a single server takes 8.5 hours. In comparison,\nbuilding an inverted index using Lucene is much\ncheaper and takes only about 30 minutes in total.\n\n\n**6** **Experiments: Question Answering**\n\n\nIn this section, we experiment with how different\npassage retrievers affect the final QA accuracy.\n\n\n**6.1** **End-to-end QA System**\n\n\nWe implement an end-to-end question answering\nsystem in which we can plug different retriever\nsystems directly. Besides the retriever, our QA system consists of a neural _reader_ that outputs the\nanswer to the question. Given the top _k_ retrieved\npassages (up to 100 in our experiments), the reader\nassigns a passage selection score to each passage.\nIn addition, it extracts an answer span from each\npassage and assigns a span score. The best span\nfrom the passage with the highest passage selection\n\n\n10FAISS configuration: we used HNSW index type on CPU,\nneighbors to store per node = 512, construction time search\ndepth = 200, search depth = 128.\n\n\n\nscore is chosen as the final answer"," pages and extracting relevant information from noisy\nretrieved data. Results for Qwen2-VL demonstrate stronger document understanding capabilities,\n\n- utperforming MiniCPM-V 2.6 in these tasks.\n\n\nH RETRIEVAL EFFICIENCY\n\n\nIn this experiment, we evaluate the retrieval efficiency of VisRAG-Ret and MiniCPM (OCR) by\nmeasuring two key components: offline document parsing and encoding latency, and online query\nencoding and search latency. Query and document encoding are conducted on an NVIDIA A100\n40G GPU with a batch size of 1, while document parsing is performed on a single core of an Intel\nXeon Platinum 8350C CPU. The reported latencies are averaged over the queries and documents\nfrom the PlotQA dataset. The results are summarized in Table 12.\n\n\nAs shown in the table, although VisRAG-Ret, a VLM-based model, requires more time for document\nencoding compared to MiniCPM (OCR), it bypasses the time-consuming parsing stage required by\n\n\n24\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 12: Retrieval efficiency (ms). We report offline latencies per document, including document\nparsing and encoding latencies, as well as online latencies per query, including query encoding and\nsearch latencies.\n\n|Col1|OfflineLatencyperDocument<br>Parsing Encoding Total|OnlineLatencyperQuery<br>Encoding Search Total|\n|---|---|---|\n|MiniCPM (OCR)<br>VisRAG-Ret|284<br>28<br>312<br>\u2013<br>121<br>121|28<br>26<br>54<br>28<br>26<br>54|\n\n\n\nMiniCPM (OCR). This leads to a 58% reduction in total document processing time for VisRAG-Ret.\nFor online query processing, the latencies of VisRAG-Ret and MiniCPM (OCR) are nearly identical,\nas the queries consist solely of textual inputs.\n\n\nI RETRIEVAL PERFORMANCE ON TEXT RETRIEVAL BENCHMARKS\n\n\nTable 13: Retrieval performance on subsets of the text retrieval benchmark BEIR (Thakur et al.,\n2021) in NDCG@10. VisRAG-Ret performs retrieval on rendered document screenshots.\n\n\n**Model** **SciFact** **NFCorpus** **Scidocs**\n\n\nMiniCP","forms BM25 and DPR by margins\nranging from 1.7 to 5.8 and 0.7 to 2.1 points, respectively.\n\n\n2For the DPR experiments in Tables 1 and 2, we used the dpr-multiset-base model as opposed to\ndpr-single-nq-base which was used in rest of the experiments done earlier. This decision was based on\nthe performance observed in Karpukhin et al. (2020), where dpr-multiset-base showed superior results.\n\n\n7\n\n\nPublished as a conference paper at ICLR 2024\n\n\nTable 2: **QuALITY and QASPER Performance With + Without RAPTOR:** Performance comparison across the QuALITY and QASPER datasets of various retrieval methods (SBERT, BM25,\nDPR) with and without RAPTOR. UnifiedQA-3B is used as the language model. RAPTOR outperforms baselines of each respective retrieval method for both datasets.\n\n\n**Model** **Accuracy (QuALITY)** **Answer F1 (QASPER)**\n\n\n**SBERT with RAPTOR** **56.6%** **36.70%**\n\nSBERT without RAPTOR 54.9% 36.23%\n\n**BM25 with RAPTOR** **52.1%** **27.00%**\n\nBM25 without RAPTOR 49.9% 26.47%\n\n**DPR with RAPTOR** **54.7%** **32.23%**\n\nDPR without RAPTOR 53.1% 31.70%\n\n\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different language models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column \u201dTitle +\nAbstract\u201d reflects performance when only the title and abstract of the papers are used for context.\nRAPTOR outperforms the established baselines BM25 and DPR across all tested language models.\nSpecifically, RAPTOR\u2019s F-1 scores are at least 1.8% points higher than DPR and at least 5.3% points\nhigher than BM25.\n\n\n**Retriever** **GPT-3 F-1 Match** **GPT-4 F-1 Match** **UnifiedQA F-1 Match**\n\n\nTitle + Abstract 25.2 "],"reference":"The Intel Xeon CPU E5-2698 v4 @ 2.20GHz was used to profile the passage retrieval speed for DPR."}
{"user_input":"What is the purpose of a passage selection score in an end-to-end QA system?","response":"The purpose of a passage selection score in an end-to-end QA system is to evaluate and select the most relevant passages for a given question. This score helps determine which passages are most likely to contain the correct answer, thereby enhancing the efficiency and accuracy of the question-answering process. It plays a crucial role in improving retrieval quality and ensures that the context sourced by the retriever component is effective for generating coherent and relevant answers from the retrieved passages <document id=\"2312.10997_rag_survey_gao:chunk_11700\">. Additionally, sophisticated models may use reranking strategies to refine the selection based on relevance, thereby achieving better performance in retrieving the most pertinent information for answering complex queries <document id=\"2009.12756_multihop_qi:chunk_3600\">.","retrieved_contexts":[", datasets, and how to evaluate\nRAG systems.\n\n\n_A. Downstream Task_\n\n\nThe core task of RAG remains Question Answering (QA),\nincluding traditional single-hop\/multi-hop QA, multiplechoice, domain-specific QA as well as long-form scenarios\nsuitable for RAG. In addition to QA, RAG is continuously\nbeing expanded into multiple downstream tasks, such as Information Extraction (IE), dialogue generation, code search, etc.\nThe main downstream tasks of RAG and their corresponding\ndatasets are summarized in Table II.\n\n\n_B. Evaluation Target_\n\n\nHistorically, RAG models assessments have centered on\ntheir execution in specific downstream tasks. These evaluations\nemploy established metrics suitable to the tasks at hand. For\ninstance, question answering evaluations might rely on EM\nand F1 scores [7], [45], [59], [72], whereas fact-checking\ntasks often hinge on Accuracy as the primary metric [4],\n\n[14], [42]. BLEU and ROUGE metrics are also commonly\nused to evaluate answer quality [26], [32], [52], [78]. Tools\nlike RALLE, designed for the automatic evaluation of RAG\napplications, similarly base their assessments on these taskspecific metrics [160]. Despite this, there is a notable paucity\n\n- f research dedicated to evaluating the distinct characteristics\n\n- f RAG models.The main evaluation objectives include:\n_Retrieval Quality_ . Evaluating the retrieval quality is crucial\nfor determining the effectiveness of the context sourced by\nthe retriever component. Standard metrics from the domains\n\n\n\n12\n\n\n- f search engines, recommendation systems, and information\nretrieval systems are employed to measure the performance of\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\nNDCG are commonly utilized for this purpose [161], [162].\n_Generation Quality_ . The assessment of generation quality\ncenters on the generator\u2019s capacity to synthesize coherent and\nrelevant answers from the retrieved context. This evaluation\n\ncan be categorized based on the content\u2019s objectives: unlabeled\nand labeled content. For unlabeled content, the evaluation\nencompasses the faithfulness, relevance, and non-harmfulness\n\n- f the generated answers. In contrast, for labeled content,\nthe focus is on the accuracy of the information produced by\nthe model [161]. Additionally, both retrieval and generation\nquality assessments can be conducted through manual or\nautomatic evaluation"," the given text. Although SQuAD\nhas been used previously for open-domain QA research, it is not ideal because many questions lack\ncontext in absence of the provided paragraph. We\nstill include it in our experiments for providing\na fair comparison to previous work and we will\ndiscuss more in Section 5.1.\n\n\n**Selection of positive passages** Because only\npairs of questions and answers are provided in\nTREC, WebQuestions and TriviaQA [6], we use the\nhighest-ranked passage from BM25 that contains\nthe answer as the positive passage. If none of the\ntop 100 retrieved passages has the answer, the question will be discarded. For SQuAD and Natural\nQuestions, since the original passages have been\nsplit and processed differently than our pool of\ncandidate passages, we match and replace each\ngold passage with the corresponding passage in the\ncandidate pool. [7] We discard the questions when\nthe matching is failed due to different Wikipedia\nversions or pre-processing. Table 1 shows the number of questions in training\/dev\/test sets for all the\ndatasets and the actual questions used for training\nthe retriever.\n\n\n**5** **Experiments: Passage Retrieval**\n\n\nIn this section, we evaluate the retrieval performance of our Dense Passage Retriever (DPR),\nalong with analysis on how its output differs from\n\n\n6We use the unfiltered TriviaQA version and discard the\nnoisy evidence documents mined from Bing.\n7The improvement of using gold contexts over passages\nthat contain answers is small. See Section 5.2 and Appendix A.\n\n\n**Training** **Retriever** **Top-20** **Top-100**\nNQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD\n\n\nNone BM25 59.1 66.9 55.0 70.9 68.8 73.7 76.7 71.1 84.1 80.0\n\n\nDPR 78.4 79.4 73.2 79.8 63.2 85.4 **85.0** 81.4 89.1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7","\ntop _k_ paragraphs [4], while on FEVER the metrics are precision, recall and F1 in order to be consistent\nwith previous results. On both datasets, MDR substantially outperforms all baselines.\n\n\n3.1.2 RERANKING\n\n\n_Reranking_ documents returned by efficient retrieval methods with a more sophisticated model is a\ncommon strategy for improving retrieval quality. For instance, state-of-the-art multi-hop QA systems\nusually augment traditional IR techniques with large pretrained language models to select a more\ncompact but precise passage set. On HotpotQA, we test the effectiveness of MDR after a simple\ncross-attention reranking: each of the top _k_ passage sequences from MDR is first prepended with the\n\n- riginal question and then fed into a pretrained Transformer encoder, i.e., ELECTRA-large (Clark\net al., 2020), that predicts relevant scores. We train this reranking model with a binary crossentropy loss, with the target being whether the passage sequence cover both groundtruth passages.\nWe empirically compare our approach with two other existing reranking-based retrieval methods:\n**Semantic Retrieval** (Nie et al., 2019) uses BERT at both passage-level and sentence-level to select\ncontext from the initial TF-IDF and hyperlinked passages; **Graph Recurrent Retriever** (Asai et al.,\n2020) learns to recursively select the best passage sequence on top of a hyperlinked passage graph,\nwhere each passage node is encoded with BERT.\n\n\nTable 2 shows the reranking results. Following Asai et al. (2020), we use _Answer Recall_ and\n_Support Passage Exact Match (SP EM)_ [5] as the evaluation metrics. Even without reranking, MDR\nis already better than Semantic Retrieval, which requires around 50 BERT encoding (where each\n\n\n2If the answer span is in both, the one that has its title mentioned in the other passage is treated as the second.\n3https:\/\/pypi.org\/project\/rank-bm25\n4As the sequence length is 2 for HotpotQA, we pick the top _k_ \/2 sequences predicted by MDR.\n5Whether the final predicted sequence covers both gold passages.\n\n\n4\n\n\nPublished as a conference paper at ICLR 2021\n\n\nTable 2: HotpotQA reranked retrieval results\n(input passages for final answer prediction).\n\n\nMethod SP"," end, we use a prompt that describes the\nmeaning of the quality metric and then asks to\nscore the given answer\/context in line with that\ndefinition. For instance, for evaluating faithfulness,\nwe used the following prompt:\n\n\n_Faithfulness measures the information_\n_consistency of the answer against the_\n_given context. Any claims that are made_\n\n_in the answer that cannot be deduced_\n\n_from context should be penalized._\n_Given an answer and context, assign a_\n_score for faithfulness in the range 0-10._\n_context_ : [context]\n\n_answer_ : [answer]\n\n\nTies, where the same score is assigned by the LLM\nto both answer candidates, were broken randomly.\nThe second baseline, shown as _GPT Ranking_, instead asks ChatGPT to select the preferred answer\/\n\n\ncontext. In this case, the prompt again includes\na definition of the considered quality metric. For\ninstance, for evaluating answer relevance, we used\nthe following prompt:\n\n\n_Answer Relevancy measures the degree_\n_to which a response directly addresses_\n_and is appropriate for a given question._\n_It penalizes the present of redundant in-_\n_formation or incomplete answers given a_\n_question. Given an question and answer,_\n_rank each answer based on Answer Rele-_\n\n_vancy._\n_question_ : [question]\n_answer 1_ : [answer 1]\n\n_answer 2_ : [answer 2]\n\n\nThe results in Table 1 show that our proposed\nmetrics are much closer aligned with the human\njudgements than the predictions from the two baselines. For faithfulness, the Ragas prediction are in\ngeneral highly accurate. For answer relevance, the\nagreement is lower, but this is largely due to the\nfact that the differences between the two candidate\n\nanswers are often very subtle. We found context\nrelevance to be the hardest quality dimension to\nevaluate. In particular, we observed that ChatGPT\n\n- ften struggles with the task of selecting the sentences from the context that are crucial, especially\nfor longer contexts.\n\n\n**6** **Conclusions**\n\n\nWe have highlighted the need for automated\nreference-free evaluation of RAG systems. In particular, we have argued the need for an evaluation\nframework that can assess faithfulness (i.e. is the\nanswer grounded in the retrieved context), answer\nrelevance (i.e. does the answer address the question) and context relevance (i.e.","score and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is twice as large as the extractive\nmodels and has 770M parameters. We reuse the hyperparameters as described in Izacard & Grave\n(2020). The original FiD uses the top 100 passages for NaturalQuestions. In our case, we use the top\n50 retrieved passage sequences and concatenate the passages in each sequence before feeding into T5.\nIn order to fit this model into GPU, we make use of PyTorch checkpoint [11] for training.\n\n\nB.2.3 MULTI-HOP RAG\n\n\nThe RAG model aims to generate answer _y_ given question _x_ and the retrieved documents _z_ . Similarly,\nthe goal of multi-hop RAG can be expressed as: generate answer _y_ given question _x_ and retrieved\ndocuments in hop one _z_ 1 and hop two _z_ 2 (Limiting to two hops for HotpotQA). The model has three\ncomponents:\n\n\n   - Hop-one retriever _p\u03b7_ 1( _z_ 1 _|x_ ) with parameter _\u03b7_ 1 to represent the retrieved top-k passage\ndistribution (top-k truncated distribution) given the input question _x_ .\n\n\n    - Hop-two retriever _p\u03b7_ 2( _z_ 2 _|x, z_ 1) with parameter _\u03b7_ 2 to represent the hop-two retrieved top-k\npassage distribution given not only the question _x_ but also the retrieved document _z_ 1 from\nhop-one.\n\n\n   - A generator _p\u03b8_ ( _yi|x, z_ 1 _, z_ 2 _,, y_ 1: _i\u2212_ 1) to represent the next token distribution given input\nquestion _x_, hop-one retrieved document _z_ 1, hop-two retrieved document _z_ 2 and previous\npredicted token _y_ 1: _i\u2212_ 1 parametrized by _\u03b8_\n\n\n**Multi-Hop RAG Sequence Model** As the RAG Sequence model, this model generates the answer\nsequence given the fixed set of documents from hop-one retriever and hop-two retriever. In order"],"reference":"In an end-to-end QA system, a passage selection score assigns a value to each retrieved passage based on its relevance to the question. This score helps determine which passage is most likely to contain the correct answer."}
{"user_input":"What is REALMWiki?","response":"I don't have enough information in the provided materials to answer this question.","retrieved_contexts":[") **73.30** 35.28 **74.00** **84.95** **86.86**\n\n\n**Datasets** In our experiments, we use the following datasets to evaluate our model and\nall baselines. We selected these datasets to represent both popular and diverse domains\nincluding Wikipedia, Coding\/API documents, and question-answering on medical documents. Natural Questions (NQ) Kwiatkowski et al. (2019), Trivia QA Joshi et al. (2017) and\nHotpotQA Yang et al. (2018) are the open-domain question-answers based on Wikipedia,\nmainly focused on common knowledge (e.g., movies, sports, etc). HuggingFace, Torch Hub,\nand TensorFlow Hub are from the APIBench Patil et al. (2023) proposed in the Gorilla paper.\nThese benchmarks measure how to generate the correct, functional, and executable API\ncalls based on the documentation. PubMed QA Jin et al. (2019) is a question-answering\ndataset tailored only for biomedical-research question-answering. It mainly focuses on\nanswering medical and biology questions based on a given set of documents. We would\n\n\n5\n\n\nPreprint, Under Review\n\n\nlike to highlight that (NQ, Trivia QA, and HotpotQA) are relatively general domain whereas\nthe latter two domains are on domain-specific documents.\n\n\n**Baselines** We consider the following baselines for our experiments:\n\n\n   - LlaMA2-7B-chat model with 0-shot prompting: this is the commonly used\ninstruction-finetuned model for QA tasks, where we provide clearly written instructions, but no reference documentation.\n\n   - LlaMA2-7B-chat model with RAG (Llama2 + RAG): similar to the previous setting,\nexcept here we include reference documents. This is a popular technique when\ndealing with domain-specific QA tasks.\n\n   - Domain-Specific Finetuning with 0-shot prompting (DSF): Standard supervisedfinetuning, without documents in context. We find that its mostly useful to align\nthe answering style of the model as well as get familiar with the domain context.\n\n   - Domain-Specific Finetuning with RAG (DSF + RAG): Equip a domain-specific\nfinetuned-model with external knowledge using RAG. So, for the \u201cknowledge\u201d the\nmodel does"," and\nmethod of leveraging meta-information. UltraChat (Ding\net al., 2023b) effectively demonstrates the process of curating\nboth high-quality and diverse data by distilled knowledge.\nThey collect extensive meta-information across three domains: _Questions about the World, Creation and Generation_,\nand _Assistance on Existing Materials_ . For example, under\n_Questions about the World_, they explore 30 meta-topics like\n\u201dTechnology\u201d and \u201dFood and Drink.\u201d the teacher LLMs\nthen use this meta-information to distill a broad array\n\n- f instructions and conversations, achieving a substantial\nscale of 1.5 million instances. UltraChat stands out with its\nlexical and topical diversity. The UltraLLaMA model, finetuned on this data, consistently surpasses other open-source\nmodels. Another notable series, **phi** (Gunasekar et al., 2023;\nLi et al., 2023a; Mar, 2023), focuses on distilling smaller,\nhigh-quality datasets akin to \u201dtextbooks.\u201d **Phi-1** (Gunasekar\net al., 2023) experiments with synthesizing \u201dtextbook quality\u201d data in the coding domain. Their approach involves\ndistilling clear, self-contained, instructive, and balanced content from LLMs, guided by random topics or function names\nto enhance diversity. The distilled data is a synthesis of 1\nbillion tokens of Python textbooks, complete with natural\nlanguage explanations and code snippets, as well as 180 million tokens of Python exercises with solutions. Remarkably,\nthe **phi-1** model, despite its smaller size, outperforms nearly\nall open-source models on coding benchmarks like HumanEval and MBPP while being 10 times smaller in model\nsize and 100 times smaller in dataset size. MFTCoder (Liu\net al., 2023d) utilizes hundreds of Python knowledge points\nas meta-information to create a CodeExercise Dataset. In\ncontrast, Magicoder (Wei et al., 2023) and WaveCoder (Yu\net al., 2024) get raw code collections from open-source\ncode datasets, using this as meta-information for generating\ninstructional data. In the context of NLU tasks, certain\nstudies (Ye et al., 2022; Gao et al., 2023a; Wang et al., 2021a)\n"," (DisambigF1), ROUGE (Lin, 2004), and an overall score combining Disambig-F1 and ROUGE (DR).\n\n\n**Open-domain summarization** The goal of opendomain summarization is to generate a comprehensive summary about a topic by gathering information from open web (Giorgi et al., 2022). We use\nWikiAsp (Hayashi et al., 2021) which aims to generate aspect-based summaries about entities from\n20 domains in Wikipedia, e.g., \u201cGenerate a summary about Echo School (Oregon) including the\n\n\nNo ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp\n\n\nFigure 4: Comparision between FLARE and baselines across all tasks\/datasets. We report the primary metric for\neach dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.\n\n\n\nfollowing aspects: academics, history.\u201d Experimental setting details are included in Appendix B.\nMetrics include ROUGE, named entity-based F1,\nand UniEval (Zhong et al., 2022) which measures\nfactual consistency.\n\n\n**6** **Experimental Results**\n\n\nWe first report overall results across 4 tasks\/datasets\nand compare the performance of FLARE with all\nthe baselines introduced in section 4. We then\n\nrun ablation experiments to study the efficacy of\nvarious design choices of our method.\n\n**6.1** **Comparison with Baselines**\n\n**Overall results.** The overall performance of\nFLARE and baseline across all tasks\/datasets are\n\nreported in Figure 4. FLARE outperforms all baseline on all tasks\/datasets, indicating that FLARE\nis a generic method that can effectively retrieve\nadditional information throughout the generation.\nAmong various tasks, multihop QA shows the\nmost significant improvement. This is largely due\nto the task\u2019s clear definition and specific objective\n\n- f producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to\ngenerate on-topic output. In contrast, ASQA and\nWikiAsp are more open-ended, which increases the\ndifficulty of both generation and evaluation. The\n"," in\nwhich the LLM can refer to external sources of information (e.g., a website or a book chapter).\nIn such scenarios, typically, the LLM is paired with retriever which retrieves \u2018k\u2019 documents\n(or specific segments of the document) which are appended to the users\u2019 prompt. It is\n\n- nly through these documents retrieved that the LLM gains access to \u201cdomain-specific\ninformation\u201d. As a result, we argue that the LLM\u2019s performance in these settings, where it\nis trained as a general-purpose LLM is largely dependent on the quality of the retriever and\nhow accurately the retriever can identify the most relevant piece of information.\n\n\n**Domain-Specific Open-Book Exam** In this paper, we focus on the narrower but increasingly popular domain than the general open book exam, which we call the domain-specific\n\n- pen-book exam. Here, we know apriori the domain in which the LLM will be tested. The\nLLM can respond to the users\u2019 prompt using use any and all information from this specific\ndomain, which it has been fine-tuned on. Examples of domain specific examples include\nenterprise documents, code repositories belonging to an organization, etc. In all these\nscenarios, the LLM will be used to respond to the questions, whose answers can be found\nwithin a collection of documents. The retrieval technique itself has little to no-impact on the\nmechanism (though it may impact the accuracy). This paper studies the domain-specific\n\n- pen-book setting and how to adapt a pretrained LLM to this specific domain, including\nhow to make it more robust to a varying number of retrieved documents and distractors.\n\n\n**3** **RAFT**\n\n\nIn this section, we present RAFT, a novel way of training LLMs for domain-specific openbook exams. We first introduce the classical technique of supervised fine-tuning, followed\nwith the key takeaways from our experiments. Then, we introduce RAFT, a modified\nversion of general instruction tuning. Lastly, we provide an overview of the experiments to\nexpect in the later sections.\n\n\n**Supervised Finetuning**\n\n\nConsider the supervised fine-tuning (SFT) setting for a Question-Answer dataset. The\nformulation consists of the Dataset ( _D_ ) from which a set of Question ( _Q_ ) and corresponding\nanswer ( _A_ ) pairs are derived or already available. In the","QA already comes with the associated\nWikipedia corpus for the open-domain setting,\nso we use it directly. 2WikiMultihopQA and\nMuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20\nparagraphs respectively, 2-4 of which are supporting and others are non-supporting. To turn these\ndatasets into an open-domain setting, we make two\ncorpora, one for each dataset, by combining all\nsupporting and non-supporting paragraphs for all\nits questions in the train, development, and test\nsets. IIRC is originally a mix between reading\ncomprehension and an open-domain setting. Each\nquestion is grounded in one main paragraph, which\ncontains links to multiple Wikipedia pages with\nseveral paragraphs each. We create a corpus out\n\n- f all the paragraphs from all the Wikipedia pages\npresent in the dataset. [18] We do assume the availability of the main passage which doesn\u2019t need\nto be retrieved and is always present. We don\u2019t\nassume the availability of Wikipedia links in the\nmain passage, however, to keep the retrieval problem challenging. [19]\n\n\n**B** **Special Handling of Models for IIRC**\n\n\nIIRC is slightly different from the other datasets,\nin that the question is grounded in the main passage and other supporting paragraphs come from\nthe Wikipedia pages of entities mentioned in this\npassage. We modify the retrievers and readers to\naccount for this difference: (i) We always keep the\nmain passage as part of the input to the model regardless of the retrieval strategy used. (ii) For all\nthe retrieval methods, we first prompt the model to\ngenerate a list of Wikipedia page titles using the\nmain passage and the question. We map these generated titles to the nearest Wikipedia page titles in\nthe corpus (found using BM25), and then the rest\n\n- f the paragraph retrieval queries are scoped within\n\n- nly those Wikipedia pages.\nTo prompt the model to generate Wikipedia page\ntitles using the main passage and the question for\n\n\n18Following are the corpus sizes for the datasets: HotpotQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue\n(139,416), and IIRC (1,882,415)\n19IIRC corpus has a positional bias, i.e"],"reference":"REALMWiki is a model pretrained on Wikipedia."}
{"user_input":"What significant contributions did Lee et al. make to the field of information retrieval, particularly in the context of dense retrieval models?","response":"I don't have enough information in the provided materials to answer this question.","retrieved_contexts":["eng, X. A. Lin, L. He, and\nD. Jiang, \u201cHyper: Multitask hyper-prompted training enables large-scale retrieval generalization,\u201d in _The Eleventh_\n\n\n_International Conference on Learning Representations_, 2022.\nC. Liu, C. Tao, X. Geng, T. Shen, D. Zhao, C. Xu, B. Jiao,\nand D. Jiang, \u201cAdam: Dense retrieval distillation with\nadaptive dark examples,\u201d _arXiv preprint arXiv:2212.10192_,\n2022.\nJ. Feng, C. Tao, X. Geng, T. Shen, C. Xu, G. Long, D. Zhao,\nand D. Jiang, \u201cKnowledge refinement via interaction between search engines and large language models,\u201d _arXiv_\n_preprint arXiv:2305.07402_, 2023.\nT. Shen, G. Long, X. Geng, C. Tao, T. Zhou, and D. Jiang,\n\u201cLarge language models are strong zero-shot retriever,\u201d\n_arXiv preprint arXiv:2304.14233_, 2023.\nX. Ma, X. Zhang, R. Pradeep, and J. Lin, \u201cZero-shot listwise\ndocument reranking with a large language model,\u201d 2023.\nZ. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, J. Shen,\nT. Liu, J. Liu, D. Metzler, X. Wang, and M. Bendersky,\n\u201cLarge language models are effective text rankers with\npairwise ranking prompting,\u201d 2023.\nX. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, \u201cQuery\nrewriting in retrieval-augmented large language models,\u201d\nin _Proceedings of the 2023 Conference on Empirical Methods_\n_in Natural Language Processing_, H. Bouamor, J. Pino, and\nK. Bali, Eds. Singapore: Association for Computational\nLinguistics, Dec. 2023, pp. 5303\u20135315. [Online]. Available:\n[https:\/\/aclanthology.org\/2023.emnlp-main.322](https:\/\/aclanthology.org\/2023.emnlp-main.322)\nD.","11386)\n\n[16] Sean MacAvaney, Franco Maria Nardini, Raffaele Perego, Nicola Tonellotto, Nazli\nGoharian, and Ophir Frieder. 2020. Expansion via Prediction of Importance with\n\n\n\nContextualization. _Proceedings of the 43rd International ACM SIGIR Conference on_\n_Research and Development in Information Retrieval_ [(Jul 2020). https:\/\/doi.org\/10.](https:\/\/doi.org\/10.1145\/3397271.3401262)\n[1145\/3397271.3401262](https:\/\/doi.org\/10.1145\/3397271.3401262)\n\n[17] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT.\n[arXiv:1901.04085 [cs.IR]](https:\/\/arxiv.org\/abs\/1901.04085)\n\n[18] Rodrigo Nogueira and Jimmy Lin. 2019. From doc2query to docTTTTTquery.\n\n[19] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019. Document\n[Expansion by Query Prediction. arXiv:1904.08375 [cs.IR]](https:\/\/arxiv.org\/abs\/1904.08375)\n\n[20] Biswajit Paria, Chih-Kuan Yeh, Ian E. H. Yen, Ning Xu, Pradeep Ravikumar, and\nBarnab\u00e1s P\u00f3czos. 2020. Minimizing FLOPs to Learn Efficient Sparse Representa[tions. arXiv:2004.05665 [cs.LG]](https:\/\/arxiv.org\/abs\/2004.05665)\n\n[21] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.\nPyTorch: An Imperative Style, High-Performance Deep Learning Library.. In\n_NeurIPS_ .\n\n[22] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In _Proceed","Published as a conference paper at ICLR 2021\n\n## ANSWERING COMPLEX OPEN-DOMAIN QUESTIONS\n### WITH MULTI-HOP DENSE RETRIEVAL\n\n\n**Wenhan Xiong** [1] _[\u2217]_ **Xiang Lorraine Li** [2] _[\u2217]_ **Srinivasan Iyer** _[\u2021]_ **Jingfei Du** _[\u2021]_\n\n\n**Patrick Lewis** _[\u2021\u2020]_ **William Wang** [1] **Yashar Mehdad** _[\u2021]_ **Wen-tau Yih** _[\u2021]_\n\n\n**Sebastian Riedel** _[\u2021\u2020]_ **Douwe Kiela** _[\u2021]_ **Barlas O\u02d8guz** _[\u2021]_\n\n\n_\u2021_ Facebook AI\n1University of California, Santa Barbara\n2University of Massachusetts Amherst\n\n_\u2020_ University College London\n_{_ xwhan, william _}_ @cs.ucsb.edu, xiangl@cs.umass.edu,\n_{_ sviyer, jingfeidu, plewis, mehdad, scottyih, sriedel, dkiela, barlaso _}_ @fb.com\n\n\nABSTRACT\n\n\nWe propose a simple and efficient multi-hop dense retrieval approach for answering\ncomplex open-domain questions, which achieves state-of-the-art performance on\ntwo multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previ\n     - us work, our method does not require access to any corpus-specific information,\nsuch as inter-document hyperlinks or human-annotated entity markers, and can\nbe applied to any unstructured text corpus. Our system also yields a much better\nefficiency-accuracy trade-off, matching the best published accuracy on HotpotQA\nwhile being 10 times faster at inference time. [1]\n\n\n1 INTRODUCTION\n\n\n_Open domain question answering_ is a challenging task where the answer to a given question needs to\nbe extracted from a large pool of documents. The prevailing approach (Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set"," models for information retrieval.ater and Allan Hanbury. 2019. Let\u2019s measure run time! Extending\u00a8 _arXiv preprint arXiv:1903.06902_ (2019).\nthe IR replicability infrastructure to include performance aspects. _arXiv preprint_\n\n[10] Sebastian Hofst _arXiv:1907.04614_ ater, Navid Rekabsaz, Carsten Eickhoff, and Allan Hanbury.\u00a8 (2019).\n2019. On the effect of low-frequency terms on neural-IR models. In _Proceedings_\n\n_of the 42nd International ACM SIGIR Conference on Research and Development in_\n\n[11] Sebastian Hofst _Information Retrieval_ ater, Markus Zlabinger, and Allan Hanbury. 2019. TU Wien@\u00a8 . 1137\u20131140.\nTREC Deep Learning\u201919\u2013Simple Contextualization for Re-ranking. _arXiv preprint_\n_arXiv:1912.01385_ (2019).\n\n[12] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry\nHeck. 2013. Learning deep structured semantic models for web search using\nclickthrough data. In _Proceedings of the 22nd ACM international conference on_\n_Information & Knowledge Management_ . 2333\u20132338.\n\n[13] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural\nRanking with Locality Sensitive Hashing. In _Te World Wide Web Conference_ .\nACM, 2858\u20132864.\n\n[14] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,\nand Qn Liu. 2019. Tinybert: Distilling bert for natural language understanding.\n\n[15] Jeff Johnson, Mathijs Douze, and Herv _arXiv preprint arXiv:1909.10351_ (2019). e J\u00b4 egou. 2017. Billion-scale similarity\u00b4\nsearch with GPUs. _arXiv preprint arXiv:1702.08734_ (2017).\n\n[16] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization."," and another for the document, extracted from\nBERT\u2019s [CLS] contextualized embedding and expanded through a\nlinear layer to dimension 4096 (which equals _Nq_ \u00d7 128 = 32 \u00d7 128).\nRelevance is estimated as the inner product of the query\u2019s and the\ndocument\u2019s embeddings, which we found to perform beter than\ncosine similarity for single-vector re-ranking. As the results show,\nthis model is considerably less effective than ColBERT, reinforcing\nthe importance of late interaction.\nSubsequently, we ask if our MaxSim-based late interaction is better than other simple alternatives. We test a model [B] that replaces\nColBERT\u2019s maximum similarity with _average_ similarity. Te results\nsuggest the importance of individual terms in the query paying\nspecial atention to particular terms in the document. Similarly,\nthe figure emphasizes the importance of our query augmentation\nmechanism: without query augmentation [C], ColBERT has a noticeably lower MRR@10. Lastly, we see the impact of end-to-end\nretrieval not only on recall but also on MRR@10. By retrieving\ndirectly from the full collection, ColBERT is able to retrieve to the\ntop-10 documents missed entirely from BM25\u2019s top-1000.\n\n\n\n|Basic ColBERT Indexing<br>+multi-GPU document processing<br>+per-batch maximum sequence length<br>+length-based bucketing<br>+multi-core pre-processing|Col2|Col3|Col4|Col5|Col6|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic ColBERT Indexing**<br>**+multi-GPU document processing**<br>**+per-batch maximum sequence length**<br>**+length-based bucketing**<br>**+multi-core pre-processing**||||||||\n|**Basic"],"reference":"Lee et al. made a significant contribution by jointly training the question encoder and reader, which outperformed the BM25 plus reader paradigm on multiple open-domain QA datasets in terms of QA accuracy. Their approach was further extended by REALM (Guu et al., 2020), which included tuning the passage encoder asynchronously by re-indexing the passages during training."}
{"user_input":"How does the sufficient context autorater help in reducing hallucinations in RAG systems?","response":"The sufficient context autorater helps reduce hallucinations in retrieval-augmented generation (RAG) systems by providing a mechanism to assess whether the context accompanying a query is sufficient to yield a definitive answer. When context is deemed insufficient, the autorater can trigger abstention from providing an answer, thereby reducing the occurrence of hallucinated responses.\n\nInstead of using a heavy-handed approach that would have all models abstain from answering under insufficient context\u2014which might lower performance overall\u2014the sufficient context autorater operates alongside model self-rated confidence scores. This dual-signal system facilitates a balanced trade-off between accuracy and coverage, enabling the model to abstain when it lacks sufficient context while still allowing correct answers when context is adequate.\n\nThis approach is distinct because it functions independently of the generation process, minimizing unintended negative impacts on performance while allowing for different operational settings tailored to specific application needs, such as higher accuracy in critical domains like medicine or greater coverage in creative contexts (<document id=\"2411.06037_sufficient_context_joren:chunk_14850\">).","retrieved_contexts":[" have seen that models may hallucinate rather than abstain and that\nthis happens more with RAG than in a closed-book setting. A natural next question is whether we\ncan prompt or fine-tune a model to perform closer to the ideal case. Can we steer the model to either\n\n- utput the correct answer or abstain, while hallucinating an incorrect answer as little as possible?\n\n\n5.1 SELECTIVE RAG USING SUFFICIENT CONTEXT SIGNAL\n\n\nOne simple solution to improving RAG performance would be to use the sufficient context autorater\nto abstain given insufficient context. However, this heavy-handed approach can lower overall\nperformance, since all models answer some questions correctly even with insufficient context, as\ndescribed in Table 2 and demonstrated in Figure 3. Instead, we propose a method for combining\n\n\n8\n\n\nPublished as a conference paper at ICLR 2025\n\n\nthe sufficient context autorater outputs with model self-rated confidence scores to tune a selective\naccuracy-coverage trade-off, where \u201ccoverage\u201d denotes the portion of inputs on which the model does\nnot abstain. Specifically, we use these signals to train a simple linear model to predict hallucinations,\nand then use it to set coverage-accuracy trade-off thresholds.\n\n\nThis mechanism differs from other strategies for improving abstention in two key ways. First, because\nit operates independently from generation, it mitigates unintended downstream effects, whereas\nstrategies like fine-tuning to improve abstention can inadvertently worsen performance on certain\ninputs (see Section 5.2). Second, it offers a _controllable_ mechanism for tuning abstention, which\nallows for different operating settings in differing applications, such as strict accuracy compliance in\nmedical domains or maximal coverage on creative generation tasks.\n\n\n**Abstention Signals** We utilize two main signals for abstention: the self-rated probabilities as\nin Li et al. (2024); Kadavath et al. (2022) and the sufficient context autorater. For the self-rated\nprobabilities, we use two strategies: P(True) and P(Correct). P(True) requires sampling answers\nfrom the model multiple times, and then prompting the model multiple times to label each model as\ncorrect or incorrect, resulting in a final probability of correctness associated with each question as in\nKadavath et al. (2022). For proprietary models, where extensive querying is prohibitively expensive,\nwe use P(Correct) instead."," et al., 2024;\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\nrelevant documents, but simply uses \u201cdoes not contain the answer\u201d as a proxy for irrelevant context.\n\n\n**Reducing Hallucinations.** There have also been efforts to improve RAG factuality on open-book QA\ntasks (Asai et al., 2023; Mineiro, 2024; Simhi et al., 2024; Wang et al., 2024b; Zhang et al., 2024b).\nThe main theme is to improve both the generation and retrieval quality, often by fine-tuning one or\nmore components. Also, since RAG leads to very long contexts, another issue that arises is the \u201clost\nin the middle\u201d problem (Hsieh et al., 2024; Liu et al., 2024; Yu et al., 2024). These works start with\nthe premise that the provided query\/context should be precisely answerable by the LLM, and hence,\n\n- nly analyze their findings in the sufficient context scenario. Independent of RAG, many papers\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang\net al., 2024; Kadavath et al., 2022; Yin et al., 2023; Zhang et al., 2024a) and performance across\ndisaggregated subsets of data (Paes et al., 2022; Joren et al., 2023).\n\n\n3 SUFFICIENT CONTEXT\n\n\nAt a high level, our aim is to classify input instances based on whether the context contains enough\ninformation to answer the query. We split possible contexts into two cases: (1) **Sufficient Context.**\nThe context is sufficient to answer the query if it contains all the necessary information to provide a\ndefinitive answer. (2) **Insufficient Context.** Otherwise, a context is insufficient. A context may also\nbe insufficient if the query requires specialized knowledge that is not provided in the context or if\nthe information in the context is incomplete, inconclusive, or contradictory. In this section, we more\nthoroughly discuss sufficient context. Then, we show how to accurately and scalably label instances.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\n3.1 DEFIN","Ab|rrect<br>Ab|sta|in<br>Hal|in<br>Hal|rect<br>Abst|rect<br>Abst|ain<br>Hall|\n\n\n\nFigure 3: **Model Performance on Datasets Stratified by Sufficient Context.** Given sufficient\ncontext, models have a higher correct percentage on these challenging datasets. Performance drops,\nbut the models are still able to answer a large portion of questions correct without sufficient context.\nOne prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient\ncontext. The smallest model Gemma 27B struggles to avoid hallucinations given insufficient context.\n\n\nappropriate. Without RAG, Claude 3.5 Sonnet abstains on 84.1% questions, while with RAG, the\nfraction of abstentions drops to 52%. Similarly, GPT 4o\u2019s abstention fraction moves from 34.4%\nto 31.2% and Gemini 1.5 Pro\u2019s drops from 100% to 18.6%. This phenomenon may arise from the\nmodel\u2019s increased confidence in the presence of any contextual information, leading to a higher\npropensity for hallucination rather than abstention.\n\n\n**Models Hallucinate with Both Sufficient and Insufficient Context.** Considering Figure 3, models\ngenerally achieve higher accuracy with sufficient context (higher **green bars**, top row) than without\nsufficient context (lower **green bars**, bottom row). However, looking at each row separately, we\ndiscover several findings. First, in the sufficient context case (top row), we see that models hallucinate\nmore than they abstain ( **red bars** are higher than **blue bars**, usually). The trend holds across all\nthree datasets. Moving to insufficient context (bottom row), we find a different distribution of model\nresponses, with more abstentions and hallucinations. This tendency varies notably across different\nmodels. For instance, Claude abstains more (higher **blue bars** ) with insufficient context, but answers\nfewer questions correctly (lower **green bars** ) than Gemini and GPT. These differences underscore\nthe potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\nmuch more hallucinations (higher **red bars** ) than the other models, except for HotPotQA, where we\nattribute the higher accuracy to the smaller retrieved contexts.\n\n\n4.3 QUALITATIVELY ANALYZING","\npossible and to otherwise abstain. RAG seeks to move models towards this desired behavior, such\nthat the provided context shifts hallucinations to correct answers, or to abstentions if needed. We\nanalyze several cases to assess how far we are from this ideal trade-off.\n\n\n**Experimental Set-up and LLMEval.** We employed a basic chain of thought (CoT) prompting\napproach, with the prompt structure and further information detailed in Appendix C.4. We then\nprocessed the outputted answers to identify matches between the response and any of the ground truth\nanswers. Responses where a clear correct match could not be determined were processed through\nthe LLMEval pipeline using a zero-shot approach, with the prompt based on Krishna et al. (2024)\n(see Appendix C.3). Then, for each example, we can rate it as \u201ccorrect\u201d or \u201cabstain\u201d or \u201challucinate\u201d\ndepending on the LLMEval output. We use an LLM for evaluation instead of checking for an exact\nmatch because it is more robust to syntactic variations. See Appendix B.3 for details and examples.\n\n\n**Models Abstain Less with RAG.** While overall performance improves with RAG, the introduction\n\n- f additional context paradoxically reduces the model\u2019s ability to abstain from answering when\n\n\n6\n\n\nPublished as a conference paper at ICLR 2025\n\n\n\n\n\n\n\n\n\n\n|Gemini 1.5 Pro GPT 4o Claude 3.5 Sonnet Gemma 27B Categorizing RAG Responses: Sufficient vs Insufficient Context FreshQA Musique HotpotQA|Col2|Col3|Col4|Col5|Col6|Gemma 27B HotpotQA|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Correct<br>Abstain<br>Halluc.<br>Gemini 1.5 Pro<br>Correct<br>Abstain<br>Halluc.<br>GPT 4o<br>Correct<br>Ab"," includes questions that are more likely to be answerable\nwithout context (e.g., yes or no questions, multiple choice questions, or questions with answers that\nmight be answerable due to pre-training). This explains the higher fraction of correct answers without\nRAG (e.g., 48.0% for GPT 4o).\n\n\n18\n\n\nPublished as a conference paper at ICLR 2025\n\n\nTable 4: **Performance Analysis of RAG Systems Using Human-Annotated Sufficient Context**\n**Labels.** These tables include results on a curated set of challenging context-dependent questions.\nTable (a) shows that while larger models generally achieve higher accuracy with sufficient context\n(present in 54.8% of cases), even top performers exhibit a 14-16% error rate. Table (b) reveals that\nwith insufficient context (45.2% of cases), models predominantly abstain from answering (50-73%\n\n- f instances), though significant hallucination rates (15-40%) persist. These patterns of contextdependent performance and hallucination risk are consistent with our analyses of HotpotQA, FreshQA,\nand Musique datasets, despite variations in absolute performance due to different task complexities.\n\n\n(a) Performance with Sufficient Context (54.8% of Dataset)\n\n\n**Model** **% Correct** **% Abstain** **% Hallucinate**\n\n\nGemini 1.5 Pro 84.1 1.6 14.3\n\nGPT 4o 82.5 4.8 12.7\n\nClaude 3.5 Sonnet 85.7 11.1 3.2\n\nGemini 1.5 Flash 77.8 4.8 17.5\n\nGemma 27B 71.4 3.2 25.4\n\n\n(b) Performance with Insufficient Context (45.2% of Dataset)\n\n\n**Model** **% Correct** **% Abstain** **% Hallucinate**\n\n\nGemini 1.5 Pro 9.6 50.0 40.4\n\nGPT 4o 23.1 61.5 15.4\n\nClaude 3.5 Sonnet 9.6 53.8 36.5\n\nGemini 1.5 Flash 7.7 73.1 19.2\n\nGemma 27B 9.6"],"reference":"The sufficient context autorater helps in reducing hallucinations in RAG systems by providing a binary label for whether the context is sufficient to answer the query. This label is then combined with model self-rated confidence scores to tune a selective accuracy-coverage trade-off, where 'coverage' denotes the portion of inputs on which the model does not abstain. By using these signals to train a simple logistic regression model, the approach leads to a better selective accuracy-coverage trade-off compared to using model confidence alone."}
{"user_input":"How does query rewriting improve the performance of retrieval-augmented language models in handling factual information?","response":"Query rewriting enhances the performance of retrieval-augmented language models (RALMs) by better aligning the search queries with the information needs of the model. The process involves generating refined queries that are more likely to retrieve relevant information from external knowledge bases.\n\nIn this approach, a small language model is utilized as a trainable rewriter, which adapts the original query based on feedback from a larger, black-box LLM reader. This method effectively closes the gap between the initial input text and the specific knowledge being sought in retrieval, leading to improved accuracy in acquiring factual information. Experiments indicate that this framework consistently demonstrates performance improvements in tasks such as open-domain and multiple-choice question answering <document id=\"2305.14283_query_rewriting_ma:chunk_0\" \/>. \n\nBy focusing on the refinement of the search query itself, query rewriting not only enhances the relevance of retrieved documents but also ensures that the LLM can utilize the retrieved content more effectively in its reasoning process.","retrieved_contexts":[" testing. This ability to maintain consistent performance despite variations in test-time document numbers further validates the robustness of\n\n- ur approach, RAFT . This finding underscores the importance of a well-calibrated training\nenvironment to prepare the model for a range of scenarios it may encounter in real-world.\n\n\n**6** **Related Works**\n\n\n**Retrieval-Augmented Language Models** Retrieval-Augmented Language Models (RALMs)\nenhance LLMs by integrating a retrieval module that sources relevant information from\nexternal knowledge bases, significantly improving performance across various NLP tasks,\nincluding language modeling (Guu et al., 2020; Borgeaud et al., 2022; Khandelwal et al.,\n2019; Shi et al., 2023d; Lin et al., 2023b; Shi et al., 2023c; Asai et al., 2023; Xu et al., 2023;\nWang et al., 2023) and open-domain question answering (Izacard et al., 2023; Lewis et al.,\n2020). For instance, Atlas (Izacard et al., 2023) fine-tunes T5 models with the retriever,\ntreating documents as latent variables, while RETRO (Borgeaud et al., 2022) modifies the\ndecoder-only architecture to include retrieved texts and conducts pre-training from scratch.\nkNN-LM (Khandelwal et al., 2019) interpolates between the LM\u2019s next token distribution\nand distributions computed from retrieved tokens at inference. (Shi et al., 2023d; Ram\net al., 2023) assume black-box access to an LLM, combining it with either off-the-shelf or\nfine-tuned retriever.\n\n\n**Memorization** A key question around large neural language models is whether they truly\n\u201cunderstand\u201d text (Feldman, 2020; Power et al., 2022) or simply rely on surface pattern\nmemorization (Carlini et al., 2019; T\u00e4nzer et al., 2022). (Feldman, 2020; Carlini et al., 2019;\n2022) develop methodologies to quantify the extent of memorization in neural models.\n(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored","che, J.-B. Lespiau, B. Damoc, A. Clark _et al._,\n\u201cImproving language models by retrieving from trillions of tokens,\u201d\nin _International conference on machine learning_ . PMLR, 2022, pp.\n2206\u20132240.\n\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray _et al._, \u201cTraining language\nmodels to follow instructions with human feedback,\u201d _Advances in_\n_neural information processing systems_, vol. 35, pp. 27 730\u201327 744,\n2022.\n\n[7] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, \u201cQuery rewriting for retrieval-augmented large language models,\u201d _arXiv preprint_\n_arXiv:2305.14283_, 2023.\n\n[8] I. ILIN, \u201cAdvanced rag techniques: an illustrated    - verview,\u201d [https:\/\/pub.towardsai.net\/](https:\/\/pub.towardsai.net\/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)\n[advanced-rag-techniques-an-illustrated-overview-04d193d8fec6,](https:\/\/pub.towardsai.net\/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6)\n2023.\n\n[9] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen _et al._,\n\u201cLarge language model based long-tail query rewriting in taobao\nsearch,\u201d _arXiv preprint arXiv:2311.03758_, 2023.\n\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V. Le,\nand D. Zhou, \u201cTake a step back: Evoking reasoning via abstraction in\nlarge language models,\u201d _arXiv preprint arXiv:2310.06117_, 2023.\n\n[11] L. Gao, X. Ma, J. Lin,","t_ .\n\n\n**3.2.2** **Confidence-based Query Formulation**\n\n\nOne way to perform retrieval is to directly use the\nnext sentence \u02c6 _**s**_ _t_ as the query _**q**_ _t_ . This shares a similar spirit with methods that use generated hypothetical titles or paragraphs from LMs as retrieval\nqueries or evidences (Gao et al., 2022; Sun et al.,\n2022; Yu et al., 2022; Mao et al., 2021). We generalize such techniques to long-form generation\nwhere active information access is essential.\n\nWe found retrieving with the next sentence\nachieves significantly better results than with the\nprevious context, as shown later in subsection 6.2.\nHowever, it has a risk of perpetuating errors contained in it. For example, if the LM produces the\nsentence \u201cJoe Biden attended the University of\nPennsylvania\u201d instead of the correct fact that he\nattended the University of Delaware, using this erroneous sentence as a query might retrieve mislead\n\nJoe Biden attended the University of Pennsylvania,\nwhere he earned a law degree.\n\n\n\nqueries _**q**_ _t_ are formulated based on \u02c6 _**s**_ _t_ as follows:\n\n\n\n\n\n_explicit query by_\n_question generation_\n\n\n\n_**q**_ _t_ =\n\n\n\n_\u2205_ if all tokens of \u02c6 _**s**_ _t_ have probs _\u2265_ _\u03b8_\n\n- mask(\u02c6 _**s**_ _t_ ) or qgen(\u02c6 _**s**_ _t_ ) - therwise\n\n\n\n\n\nAsk a question to which the answer is \u201cthe University of Pennsylvania\u201d\nAsk a question to which the answer is \u201ca law degree\u201d\n\n\nLM such as ChatGPT\n\n\n\n\n\nFigure 3: Implicit and explicit query formulation. Tokens with low probabilities are marked with underlines.\n\n\ning information. We propose two simple methods\nto overcome this issue as illustrated in Figure 3.\n\n\n**Masked sentences as implicit queries.** The first\nmethod masks out low-confidence tokens in \u02c6 _**s**_ _t_ with\nprobabilities below a threshold _\u03b2 \u2208_ [0 _,_ 1], where a\nhigher _\u03b2_ results in more aggressive masking. This\nremoves potential distractions from the sentence to\nimprove retrieval accuracy.\n\n\n**Generated questions as explicit queries","\nhave shown to be beneficial in many other tasks. In\nthe following, we highlight several key tasks that\napply retrieval-augmented generation approaches. [1]\n\n\n**Language Modelling** It has been shown that\nproperly leveraging information from retrieval\nmemory could improve the performance of large\npre-trained language model. To build a more accurate language model, Khandelwal et al. (2020b) propose to incorporate a soft memory module into the\nsystem. Specifically, an index is built by caching\nthe hidden states of the training corpus. Then, the\nlanguage model accesses the index via k-NN search\nand displays a greatly improved performance. As\nanother example, Guu et al. (2020) propose a new\nparadigm that applies retrieval-augmented technique into the pre-training of generative language\nmodel. During learning, they train a neural selector that dynamically samples a relevant text to\nguide the reconstruction of a corrupted input sequence. In this way, the pre-trained model delivers better results by explicitly grounding on the\nretrieval memory. Lewis et al. (2020a) combine\nlanguage model pre-training with a paraphrasing\n\n\n1Here, we focus on tasks other than question answering.\nWe refer readers interested in QA to Chen and Yih (2020).\n\n\n\napproach. During learning, an input sequence to\nthe model is first corrupted. In the meantime, a set\n\n- f multi-lingual texts are retrieved based on which\nthe model learns to reconstruct the original input\nsequence. Recently, Borgeaud et al. (2021) propose RETRO, a large pre-trained language model\nenhanced with retrieved documents, and obtained\ncomparable performances with GPT-3 using 25 _\u00d7_\nfewer parameters.\n\n\n**Summarization** Text summarization is another\n\nresearch area that benefits from retrievalaugmented text generation. Peng et al. (2019)\npropose an adaptive decoding framework which\nfirst retrieves an exemplar document given the\nsource document. Then, the summarization of the\nsource document is derived through an adaptive\ngeneration process based on the retrieved template.\nDifferent from Peng et al. (2019), Cao et al.\n(2018) and Hossain et al. (2020) introduce an\nintermediate re-ranking stage into the generation\npipeline. Specifically, before generating the\ndocument summary, the retrieval documents are\nfirst","## **Query Rewriting for Retrieval-Augmented Large Language Models**\n\n**Xinbei Ma** [1,2,] _[\u2217]_ **, Yeyun Gong** [3, #, \u2020] **, Pengcheng He** [4, #] **, Hai Zhao** [1,2,\u2020] **, Nan Duan** [3]\n\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\n3Microsoft Research Asia 4Microsoft Azure AI\n\nsjtumaxb@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn,\n{yegong, nanduan}@microsoft.com, Herbert.he@gmail.com\n\n\n\n**Abstract**\n\n\nLarge Language Models (LLMs) play powerful, black-box readers in the _retrieve-then-_\n_read_ pipeline, making remarkable progress\nin knowledge-intensive tasks. This work introduces a new framework, _Rewrite-Retrieve-_\n_Read_ instead of the previous _retrieve-then-read_\nfor the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior\nstudies focusing on adapting either the retriever\n\n  - r the reader, our approach pays attention to\nthe adaptation of the search query itself, for\nthere is inevitably a gap between the input text\nand the needed knowledge in retrieval. We\nfirst prompt an LLM to generate the query,\nthen use a web search engine to retrieve contexts. Furthermore, to better align the query\nto the frozen modules, we propose a trainable\nscheme for our pipeline. A small language\nmodel is adopted as a trainable rewriter to cater\nto the black-box LLM reader. The rewriter is\n\ntrained using the feedback of the LLM reader\nby reinforcement learning. Evaluation is conducted on downstream tasks, open-domain QA\nand multiple-choice QA. Experiments results\nshow consistent performance improvement, indicating that our framework is proven effective\nand scalable, and brings a new framework for\nretrieval-augmented LLM [1] .\n\n\n**1** **Introduction**\n\n\nLarge Language Models (LLMs) have shown remarkable abilities for human language processing\nand extraordinary scalability and adaptability in\nfew- or zero-shot settings.(Ouyang et al., 2022;\nBrown et al., 2020; Chowdh"],"reference":"Query rewriting improves the performance of retrieval-augmented language models by adapting the retrieval query to better match the input text, thus enhancing the reader's ability to retrieve relevant and accurate information. This leads to consistent improvements in the model's performance on downstream tasks such as open-domain QA and multiple-choice QA."}
{"user_input":"How does the combination of parametric memory with non-parametric memory in RAG models improve their performance on knowledge-intensive NLP tasks?","response":"The combination of parametric memory with non-parametric memory in RAG models significantly enhances their performance in knowledge-intensive NLP tasks. This hybrid approach addresses several limitations commonly found in purely parametric models, such as the inability to easily revise or expand knowledge and the tendency to produce \"hallucinations\" or inaccuracies in generated content.\n\n1. **Direct Access to Revised Knowledge**: By integrating non-parametric memory (retrieval-based components), RAG models can directly access updated information from external knowledge bases. This capability allows for real-time revisions and expansions of knowledge, improving the accuracy and relevance of the generated outputs compared to traditional parametric models, which rely solely on the fixed knowledge learned during training (<document id=\"2005.11401_rag_lewis:chunk_450\">).\n\n2. **Improved Specificity and Factual Accuracy**: RAG models have been shown to produce more specific, diverse, and factual responses than parametric-only seq2seq models. This is because they leverage both the strengths of a pre-trained generator and the contextual information retrieved from on-demand knowledge sources, leading to higher-quality language generation (<document id=\"2005.11401_rag_lewis:chunk_450\">).\n\n3. **Interpretability and Insight**: The non-parametric component allows for greater interpretability of the memory used in decision-making, as the retrieved documents can be inspected and their contributions to the final output can be understood more easily. This is in contrast to the more opaque nature of purely parametric models, where knowledge is embedded in parameters without direct accessibility (<document id=\"2005.11401_rag_lewis:chunk_450\">).\n\n4. **Reduction of Hallucinations**: By grounding generation in factual knowledge retrieved from external databases, RAG models mitigate the issue of hallucination typically associated with language models that generate text solely based on their internalized knowledge (<document id=\"2005.11401_rag_lewis:chunk_9000\">). \n\n5. **State-of-the-Art Performance**: The effectiveness of this hybrid memory approach is reflected in RAG models achieving state-of-the-art results on various open-domain question answering (QA) tasks, demonstrating their robustness across diverse applications (<document id=\"2005.11401_rag_lewis:chunk_9000\">).\n\nOverall, the synthesis of parametric and non-parametric memories in RAG models not only enhances accuracy and relevance in generated outputs but also significantly broadens their applicability in real-world knowledge-intensive scenarios.","retrieved_contexts":["une and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks,\n\n     - utperforming parametric seq2seq models and task-specific retrieve-and-extract\narchitectures. For language generation tasks, we find that RAG models generate\nmore specific, diverse and factual language than a state-of-the-art parametric-only\nseq2seq baseline.\n\n\n**1** **Introduction**\n\n\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\u2019t straightforwardly provide insight into\ntheir predictions, and may produce \u201challucinations\u201d [38]. Hybrid models that combine parametric\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\ninspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that\ncombine masked language models [8] with a differentiable retriever, have shown promising results,\n\n\n|Col1|Col2|Col3|z4<br>z3<br>z2|Col5|Col6|\n|---|---|---|---|---|---|\n||||**z3**<br>**z2**|**z3**<br>**z2**|**z3**<br>**z2**|\n||||**z2**|**z2**|**z2**|\n|||**z**|**z**|||\n|||**1**|**1**|||\n|||||||\n\n\n\nFigure 1: Overview of our approach. We combine a pre-trained retriever ( _Query Encoder_ + _Document_\n_Index_ ) with a pre-trained seq2seq model ( _Generator_ ) and fine-tune end-to-end. For query _x_, we use\nMaximum Inner Product Search (MIPS) to find the top-K documents _zi_ . For final prediction _y_, we\ntreat _z_ as a latent variable and marginalize over seq2seq predictions given different"," the retrieval and generation\nstages, achieving a 20\u201340% end-to-end performance gain over traditional textbased RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing\ntraining data and demonstrates strong generalization capability, positioning it as a\npromising solution for RAG on multi-modality documents. Our code and data are\n[available at https:\/\/github.com\/openbmb\/visrag.](https:\/\/github.com\/openbmb\/visrag)\n\n\n1 INTRODUCTION\n\n\nTrained on massive data, large language models (LLMs) have shown strong abilities in common\nNLP tasks using their parametric knowledge (Wei et al., 2022; Zhao et al., 2023; Achiam et al.,\n2023). However, the issue of hallucination (Ji et al., 2023; Bang et al., 2023) and the challenge of\nupdating the parametric knowledge limit their real-world application in specific domains. Retrievalaugmented generation (RAG) alleviates this problem by supplying the LLM with information retrieved from a custom outer knowledge base (Guu et al., 2020; Lewis et al., 2020; Yu et al., 2023).\nOpen-source RAG frameworks like llamaindex (Liu, 2022) have been developed to facilitate the\nresearch and deployment of RAG.\n\n\nTypical retrieval-augmented generation (RAG) pipelines are _text-based_, operating on segmented\ntexts as retrieval units (Yu et al., 2023; Asai et al., 2024; Yan et al., 2024), which we refer to as\nTextRAG. In real-world scenarios, knowledge is often presented in multi-modality documents such\nas textbooks and manuals, which may have texts and figures intersected together. To acquire texts\nfrom such data sources, a _parsing_ stage is required, which typically involves a cascade of processes,\nincluding layout recognition, optical character recognition (OCR), and post-processing steps like\ntext joining (Zhang et al., 2024; Liu, 2022). While effective in most scenarios, the parsing process\ninevitably introduces errors, which can negatively impact the retrieval and generation phases. More\n- ver, TextRAG utilizes only textual information, overlooking potential information present in other\nmodalities like images. Although research has been conducted on image retrieval and multi-modal\n\n\n_\ufffd",". These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content from several\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\ncould represent promising future work.\n\n\n**6** **Discussion**\n\n\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\nfound that people prefer RAG\u2019s generation over purely parametric BART, finding RAG more factual\nand specific. We conducted an thorough investigation of the learned retrieval component, validating\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components\ncan be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\nanother objective. Our work opens up new research directions on how parametric and non-parametric\nmemories interact and how to most effectively combine them, showing promise in being applied to a\nwide variety of NLP tasks.\n\n\n9\n\n\n**Broader Impact**\n\n\nThis work offers several positive societal benefits over previous work: the fact that it is more\nstrongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less\nwith generations that are more factual, and offers more control and interpretability. RAG could be\nemployed in a wide variety of scenarios with direct benefit to society, for example by endowing it\nwith a medical index and asking it open-domain questions on that topic, or by helping people be more\neffective at their jobs.\n\n\nWith these advantages also come potential downsides: Wikipedia, or any potential external knowledge\nsource, will probably never be entirely factual and completely devoid of bias. Since RAG can be\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\nto a lesser extent, including that it might be used to generate abuse, faked or misleading content in\nthe news or on social media; to impersonate others; or"," et al., 2019;\n2022) develop methodologies to quantify the extent of memorization in neural models.\n(Brown et al., 2020; Power et al., 2022; Liu et al., 2022) further explored how memorization\nimpacts the models\u2019 generalization capabilities. (Carlini et al., 2021; Shi et al., 2023b)\ndemonstrated the ability of language models to memorize and regurgitate training data,\nraising significant privacy concerns (Kandpal et al., 2022; Pan et al., 2020).\n\n\n**Finetuning for RAG** More recently, several papers have been exploring the idea of finetuning a pretrained LLM to be better at RAG tasks (Lin et al., 2023a; Wang et al., 2023; Xu\n\n\n9\n\n\nPreprint, Under Review\n\n\net al., 2023; Liu et al., 2024). These works focus on constructing a combination of finetuning\ndataset for RAG and train a model to perform well on these tasks. In particular, in their\nsettings, at test time, the domain or documents can be different than the training time;\nwhereas our paper studies a slightly opposite scenario where we only care about testing the\nLLM on the same set of documents.\n\n\n**7** **Conclusion**\n\n\nRAFT is a training strategy designed to enhance the model\u2019s performance in answering\nquestions within a specific domain, in \"open-book\" settings. We highlight several crucial\ndesign decisions, such as training the model alongside distractor documents, organizing the\ndataset so a portion lacks golden documents in their context, and formulating answers in a\nchain-of-thought manner with direct quotations from the relevant text. Our evaluations on\nPubMed, HotpotQA, and Gorilla API Bench underline RAFT\u2019s significant potential.\n\n\n**References**\n\n\nAnthropic. Prompt engineering for claude\u2019s long context window. 2023.\n\n\nAsai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Self-rag: Learning to retrieve, generate,\nand critique through self-reflection. _arXiv preprint arXiv:2310.11511_, 2023.\n\n\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican,","fect. Further analysis reveals that VisRAG possesses\nbetter training data efficiency and generalization ability than baseline models, and demonstrates robustness across both text-centric and vision-centric documents. VisRAG shows great promise in\nreplacing TextRAG as the next-generation standard for RAG pipelines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: TextRAG vs. VisRAG on final generation accuracy. In TextRAG, parsed text\nserves as the basis for both retrieval and generation processes. In contrast, VisRAG leverages the original document image directly by\nusing a VLM-based retriever and generator.\nDetails can be found in Sec. 5.1.\n\n\n\n2 RELATED WORK\n\n\n**Retrieval-augmented Generation (RAG).** RAG enhances large language models (LLMs) by\nincorporating retrieved information from external knowledge bases, which assists in addressing\nknowledge-intensive tasks (Guu et al., 2020), reducing hallucinations (Semnani et al., 2023), and\n\n\n2\n\n\nPublished as a conference paper at ICLR 2025\n\n\nacquiring new knowledge (Vu et al., 2023). An RAG pipeline typically comprises a text-based\nretriever that fetches relevant information from the knowledge base given the user query, and an\nLLM-based generator that reads the query along with the retrieved information to generate an answer (Shi et al., 2024b; Yu et al., 2023). Prior research on RAG primarily focuses on: a) improving\nthe retriever, which is typically a text encoder producing text embeddings, through generator feedback (Yu et al., 2023; Shi et al., 2024b); b) enhancing the generator via supervised fine-tuning (Lin\net al., 2024; Xu et al., 2024a), in-context pre-training (Shi et al., 2024a), or advanced prompting (Xu\net al., 2024c); and c) developing advanced RAG pipelines to handle long-form or multi-hop question answering (Jiang et al., 2023; Asai et al., 2024). However, research on RAG has predominantly\ntargeted cleaned text corpora like Wikipedia from an academic standpoint. Building effective RAG\npipelines for real-world, multi-modal documents remains a challenge.\n\n\n**Vision-language Models.** Recent advancements in vision-language models (VLMs"],"reference":"The combination of parametric memory with non-parametric memory in RAG models improves their performance on knowledge-intensive NLP tasks by allowing for the expansion and revision of knowledge, as well as direct access to and inspection of accessed knowledge. This hybrid approach addresses some limitations of pre-trained neural language models, which cannot easily expand or revise their memory and may produce 'hallucinations'. RAG models achieve state-of-the-art results on open-domain extractive question answering tasks such as Natural Questions, WebQuestions, and CuratedTrec."}
{"user_input":"What are some notable approaches for retrieval-augmented text generation tasks and how do they leverage deep learning technology?","response":"Notable approaches for retrieval-augmented text generation (RAG) tasks leverage deep learning technology in several ways:\n\n1. **General Framework**: RAG integrates traditional retrieval methods with modern deep learning architectures, allowing models to access an external memory of information. This method is formulated as \\( y = f(x, z) \\), where \\( z \\) includes relevant instances retrieved from training or external datasets, thus improving the generation process by leveraging similar input-output pairs (Li et al., 2022) <2202.01110_rag_survey_li:chunk_900>.\n\n2. **Dialogue Response Generation**: This task has seen implementations where exemplar\/template retrieval enhances the quality of generated responses. Models can utilize retrieved instances as a reference to generate more informative text (Li et al., 2022) <2202.01110_rag_survey_li:chunk_450>.\n\n3. **Machine Translation**: In this area, early approaches employed translation memory to improve statistical machine translation performance. Recent advancements integrate retrieval into neural machine translation (NMT) models, enabling more effective use of retrieved sentences (Li et al., 2022) <2202.01110_rag_survey_li:chunk_450>.\n\n4. **Memory-Based Architectures**: RAG can be viewed similarly to memory networks, using a document index as external memory that neural networks can attend to. This structure enables dynamic updates to the model\u2019s memory while maintaining human-readable content (Lewis et al., 2020) <2005.11401_rag_lewis:chunk_8550>.\n\n5. **Learning Retrieval Based on Tasks**: Some models focus on learning to retrieve relevant documents that are tailored for specific generative tasks like question answering, utilizing various optimization techniques to refine the retrieval module (Lewis et al., 2020) <2005.11401_rag_lewis:chunk_8550>.\n\n6. **Multi-Modal Applications**: The use of vision-language models (VLMs) in the VisRAG framework allows for the processing of complex multi-modal documents directly, helping maintain essential visual information in retrieval and generation (Yu et al., 2024) <2410.10594_vision_rag_yu:chunk_3150>.\n\nThese approaches highlight the potential of combining retrieval techniques with deep learning to achieve state-of-the-art results in various natural language processing tasks.","retrieved_contexts":[" generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n\n\n**2** **Retrieval-Augmented Paradigm**\n\n\nIn this section, we first give a general formulation\n\n- f retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrievalaugmented generation paradigm, including the re\n\nOutput\n\n\n\n\n\nFigure 1: The overview of this survey.\n\n\n\ntrieval source, retrieval metric and integration meth\n- ds.\n\n\n**2.1** **Formulation**\n\n\nMost text generation tasks can be formulated as a\nmapping from input sequence _**x**_ to output sequence\n_**y**_ : _**y**_ = _f_ ( _**x**_ ). For instance, _**x**_ and _**y**_ could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\n\n\n_**y**_ = _f_ ( _**x**_ _,_ _**z**_ ) (1)\n\n\nwhere _**z**_ = _{\u27e8_ _**x**_ _[r]_ _,_ _**y**_ _[r]_ _\u27e9}_ is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that _**y**_ _[r]_\n\nmay benefit the response generation, if _**x**_ _[r]_ (or _**y**_ _[r]_ )\nis similar (or relevant) to the input _**x**_ . It is worth\nnoting that _**x**_ _[r"," from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (Weston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n\n\n_\u2217_ All authors contributed equally.\n\n\n\nfirstly present the generic paradigm of retrievalaugmented generation as well as three key components under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are organized with respect to different tasks. Specifically,\n\n- n the dialogue response generation task, exemplar\/template retrieval as an intermediate step has\nbeen shown beneficial to informative response generation (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation exploring different forms of knowledge such as knowledge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summarize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we intensively highlight several popular methods to integrating translation memory to NMT models (Gu\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\nHe et al., 2021). We also review the applications\n\n- f retrieval-augmented generation in other generation tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b).","## **A Survey on Retrieval-Augmented Text Generation**\n\n**Huayang Li** _[\u2665][,][\u2217]_ **Yixuan Su** _[\u2660][,][\u2217]_ **Deng Cai** _[\u2666][,][\u2217]_ **Yan Wang** _[\u2663][,][\u2217]_ **Lemao Liu** _[\u2663][,][\u2217]_\n\n_\u2665_ Nara Institute of Science and Technology _\u2660_ University of Cambridge\n_\u2666_ The Chinese University of Hong Kong _\u2663_ Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\n\n\n\n**Abstract**\n\n\nRecently, retrieval-augmented text generation\nattracted increasing attention of the computational linguistics community. Compared\nwith conventional generation models, retrievalaugmented text generation has remarkable advantages and particularly has achieved state-ofthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrievalaugmented text generation. It firstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks including\ndialogue response generation, machine translation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval technology, has achieved state-of-the-art (SOTA) performance in many NLP tasks and attracted the attention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counterpart, this new paradigm has some remarkable advantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explicitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some retrieved human-written reference, which potentially\nalleviates the difficulty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation"," RETRIEVAL-AUGMENTED GENERATION\n\n\nA typical retrieval-augmented generation (RAG) pipeline consists of a retriever and a generator,\nboth built on large language models (LLMs) [1] . This pipeline operates on a knowledge corpus _D_,\n\n\n1In many cases, the retriever uses language models smaller than 1B parameters, which may not be considered \u201clarge\u201d, but we use the term LLM for simplicity.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\nFigure 2: TextRAG (left) vs. VisRAG (right). Traditional text-based RAG (TextRAG) relies on\nparsed texts for retrieval and generation, losing visual information in multi-modal documents. Our\nvision-based RAG (VisRAG) employs a VLM-based retriever and generator to directly process the\ndocument page\u2019s image, thereby preserving all information in the original page.\n\n\nwhich is processed into units for retrieval and generation, denoted as _D_ = _{d_ 1 _, . . ., dn}_, where\n_n_ is the number of retrieval units. Given a text query _q_ and the retrieval corpus _D_, the retriever\nfunctions as _R_ : ( _q, D_ ) _\u2192DR_, taking _q_ and _D_ as inputs and producing a candidate set _DR \u2282D_ .\nTo enable efficient search, the units in the knowledge corpus _D_ are pre-encoded into embeddings.\nDuring RAG pipeline inference, approximate nearest neighbor (ANN) search is applied to retrieve\n_DR_, which serves as the knowledge source for generation. The generation process can be defined as\na function _G_ : ( _q, DR_ ) _\u2192_ _a_, where _a_ represents the answer and _G_ denotes the LLM generator. This\nis achieved by prompting the LLM with the query and the retrieved units _DR_ to generate an answer.\n\n\nAs shown in Figure 2 (left), traditional RAG frameworks (TextRAG) typically utilize text-based\nunits for retrieval and generation. However, in real-world scenarios, data often appear in complex,\nmulti-modal documents, requiring an additional parsing step to obtain text. In this paper, we propose\nto use the _page_ as the fundamental unit for retrieval and generation, which is directly processed\nby vision language models (VLMs) as"," [51, 52] propose a single, pre-trained encoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nand generative tasks. Our work aims to expand the space of possible tasks with a single, unified\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\n\n\n**Learned Retrieval** There is significant work on learning to retrieve documents in information\nretrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\nwork optimizes the retrieval module to aid in a specific, downstream task such as question answering,\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\ncan be fine-tuned for strong performance on a variety of tasks.\n\n\n**Memory-based Architectures** Our document index can be seen as a large external memory for\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\nwork. Other work improves the ability of dialog models to generate factual text by attending over\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\u2019s\nmemory by editing the document index. This approach has also been used in knowledge-intensive\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\nrather than end-to-end learnt retrieval [9].\n\n\n**Retrieve-and-Edit approaches** Our method shares some similarities with retrieve-and-edit style\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\nto provide a final output. These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content"],"reference":"Notable approaches for retrieval-augmented text generation tasks include exemplar\/template retrieval for dialogue response generation, integrating translation memory into NMT models for machine translation, and applying retrieval-augmented generation in other tasks such as abstractive summarization, code generation, paraphrase, and knowledge-intensive generation. These approaches leverage deep learning technology to enhance the scalability and effectiveness of text generation."}
{"user_input":"How does VisRAG enhance the retrieval and generation stages of RAG by utilizing vision-language models?","response":"VisRAG enhances the retrieval and generation stages of RAG by utilizing vision-language models (VLMs) to directly process document images, rather than relying solely on parsed text. This approach preserves all visual information in multi-modal documents, which is often lost in traditional text-based retrieval-augmented generation (TextRAG) systems. \n\nIn the retrieval stage, VisRAG employs a VLM-based retriever that encodes both the query and document page images separately, thus enabling a more comprehensive understanding of the content. This is a shift from traditional text-centric methods, allowing for better efficiency and effectiveness in handling diverse document types (<document 2410.10594_vision_rag_yu:chunk_3600>).\n\nFor the generation stage, the VLM processes both the query and retrieved pages, ensuring that the model can generate responses that are informed by the entirety of the visual and textual context present in the original documents. This integration of visual data not only enhances the accuracy of the generated responses but also improves generalization ability and robustness (<document 2410.10594_vision_rag_yu:chunk_1800>).\n\nOverall, VisRAG positions itself as a next-generation standard for RAG pipelines by effectively combining retrieval and generation processes with advanced vision-language capabilities (<document 2410.10594_vision_rag_yu:chunk_3600>).","retrieved_contexts":["fect. Further analysis reveals that VisRAG possesses\nbetter training data efficiency and generalization ability than baseline models, and demonstrates robustness across both text-centric and vision-centric documents. VisRAG shows great promise in\nreplacing TextRAG as the next-generation standard for RAG pipelines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: TextRAG vs. VisRAG on final generation accuracy. In TextRAG, parsed text\nserves as the basis for both retrieval and generation processes. In contrast, VisRAG leverages the original document image directly by\nusing a VLM-based retriever and generator.\nDetails can be found in Sec. 5.1.\n\n\n\n2 RELATED WORK\n\n\n**Retrieval-augmented Generation (RAG).** RAG enhances large language models (LLMs) by\nincorporating retrieved information from external knowledge bases, which assists in addressing\nknowledge-intensive tasks (Guu et al., 2020), reducing hallucinations (Semnani et al., 2023), and\n\n\n2\n\n\nPublished as a conference paper at ICLR 2025\n\n\nacquiring new knowledge (Vu et al., 2023). An RAG pipeline typically comprises a text-based\nretriever that fetches relevant information from the knowledge base given the user query, and an\nLLM-based generator that reads the query along with the retrieved information to generate an answer (Shi et al., 2024b; Yu et al., 2023). Prior research on RAG primarily focuses on: a) improving\nthe retriever, which is typically a text encoder producing text embeddings, through generator feedback (Yu et al., 2023; Shi et al., 2024b); b) enhancing the generator via supervised fine-tuning (Lin\net al., 2024; Xu et al., 2024a), in-context pre-training (Shi et al., 2024a), or advanced prompting (Xu\net al., 2024c); and c) developing advanced RAG pipelines to handle long-form or multi-hop question answering (Jiang et al., 2023; Asai et al., 2024). However, research on RAG has predominantly\ntargeted cleaned text corpora like Wikipedia from an academic standpoint. Building effective RAG\npipelines for real-world, multi-modal documents remains a challenge.\n\n\n**Vision-language Models.** Recent advancements in vision-language models (VLMs","LM\nfine-tuning techniques.\nThe burgeoning field of RAG has experienced swift growth,\nyet it has not been accompanied by a systematic synthesis that\ncould clarify its broader trajectory. This survey endeavors to\nfill this gap by mapping out the RAG process and charting\nits evolution and anticipated future paths, with a focus on the\nintegration of RAG within LLMs. This paper considers both\ntechnical paradigms and research methods, summarizing three\nmain research paradigms from over 100 RAG studies, and\nanalyzing key technologies in the core stages of \u201cRetrieval,\u201d\n\u201cGeneration,\u201d and \u201cAugmentation.\u201d On the other hand, current\nresearch tends to focus more on methods, lacking analysis and\nsummarization of how to evaluate RAG. This paper comprehensively reviews the downstream tasks, datasets, benchmarks,\nand evaluation methods applicable to RAG. Overall, this\npaper sets out to meticulously compile and categorize the\nfoundational technical concepts, historical progression, and\nthe spectrum of RAG methodologies and applications that\nhave emerged post-LLMs. It is designed to equip readers and\nprofessionals with a detailed and structured understanding of\nboth large models and RAG. It aims to illuminate the evolution\n\n- f retrieval augmentation techniques, assess the strengths and\nweaknesses of various approaches in their respective contexts,\nand speculate on upcoming trends and innovations.\nOur contributions are as follows:\n\n\n_\u2022_ In this survey, we present a thorough and systematic\nreview of the state-of-the-art RAG methods, delineating\nits evolution through paradigms including naive RAG,\n\n\n2\n\n\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\nin the pre-training stage through retrieval-augmented techniques.\n\n\n\nadvanced RAG, and modular RAG. This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRet"," RETRIEVAL-AUGMENTED GENERATION\n\n\nA typical retrieval-augmented generation (RAG) pipeline consists of a retriever and a generator,\nboth built on large language models (LLMs) [1] . This pipeline operates on a knowledge corpus _D_,\n\n\n1In many cases, the retriever uses language models smaller than 1B parameters, which may not be considered \u201clarge\u201d, but we use the term LLM for simplicity.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\nFigure 2: TextRAG (left) vs. VisRAG (right). Traditional text-based RAG (TextRAG) relies on\nparsed texts for retrieval and generation, losing visual information in multi-modal documents. Our\nvision-based RAG (VisRAG) employs a VLM-based retriever and generator to directly process the\ndocument page\u2019s image, thereby preserving all information in the original page.\n\n\nwhich is processed into units for retrieval and generation, denoted as _D_ = _{d_ 1 _, . . ., dn}_, where\n_n_ is the number of retrieval units. Given a text query _q_ and the retrieval corpus _D_, the retriever\nfunctions as _R_ : ( _q, D_ ) _\u2192DR_, taking _q_ and _D_ as inputs and producing a candidate set _DR \u2282D_ .\nTo enable efficient search, the units in the knowledge corpus _D_ are pre-encoded into embeddings.\nDuring RAG pipeline inference, approximate nearest neighbor (ANN) search is applied to retrieve\n_DR_, which serves as the knowledge source for generation. The generation process can be defined as\na function _G_ : ( _q, DR_ ) _\u2192_ _a_, where _a_ represents the answer and _G_ denotes the LLM generator. This\nis achieved by prompting the LLM with the query and the retrieved units _DR_ to generate an answer.\n\n\nAs shown in Figure 2 (left), traditional RAG frameworks (TextRAG) typically utilize text-based\nunits for retrieval and generation. However, in real-world scenarios, data often appear in complex,\nmulti-modal documents, requiring an additional parsing step to obtain text. In this paper, we propose\nto use the _page_ as the fundamental unit for retrieval and generation, which is directly processed\nby vision language models (VLMs) as","multi-modal documents, requiring an additional parsing step to obtain text. In this paper, we propose\nto use the _page_ as the fundamental unit for retrieval and generation, which is directly processed\nby vision language models (VLMs) as an image without further processing during retrieval and\ngeneration. In subsequent sections, we use the terms \u201cpage\u201d and \u201cdocument\u201d interchangeably.\n\n\n3.2 VISRAG: VISION-BASED RETRIEVAL-AUGMENTED GENERATION\n\n\nIn this section, we present **Vis** ion-based **R** etrieval- **a** ugmented **G** eneration (VisRAG), as shown in\nFigure 2 (right). In contrast to traditional RAG frameworks which use text segments for both retrieval and generation, VisRAG leverages the image of the document to preserve all information.\n\n\n3.2.1 RETRIEVAL\n\n\nThe first stage of VisRAG, VisRAG-Ret, aims to retrieve a set of pages from the corpus _D_ given\n_q_ . We follow the dual-encoder paradigm in text-based dense retrieval models (Karpukhin et al.,\n2020) but employ a VLM rather than an LLM to encode the query and page. Specifically, the query\nand page are encoded separately as text and image in the VLM, producing in a sequence of hidden\nstates. To derive the final embedding, and given that we use generative VLMs with causual attention,\nwe adopt the position-weighted mean pooling over the last-layer VLM hidden states (Muennighoff,\n2022), giving higher weights to later tokens:\n\n\n\n**v** =\n\n\n\n_S_\n\n- _wi_ **h** _i,_ (1)\n\n\n_i_ =1\n\n\n\nwhere **h** _i_ is the _i_ - th hidden state, _S_ is the sequence length, _wi_ = ~~\ufffd~~ _Sji_ =1 _[j]_ [is the] _[ i]_ [-th weight, and] **[ v]** [ is]\n\nthe query or page embedding. The similarity score is calculated by the cosine similarity of the query\n\n\n4\n\n\nPublished as a conference paper at ICLR 2025\n\n\nand page embedding. VisRAG-Ret is optimized using the InfoNCE loss:\n\n\nexp( _s_ ( _q, d_ [+] ) _\/\u03c4",". This review contextualizes the broader scope of RAG research within the\nlandscape of LLMs.\n\n_\u2022_ We identify and discuss the central technologies integral\nto the RAG process, specifically focusing on the aspects\n\n   - f \u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, and\ndelve into their synergies, elucidating how these components intricately collaborate to form a cohesive and\neffective RAG framework.\n\n\n_\u2022_ We have summarized the current assessment methods of\n\nRAG, covering 26 tasks, nearly 50 datasets, outlining\nthe evaluation objectives and metrics, as well as the\ncurrent evaluation benchmarks and tools. Additionally,\nwe anticipate future directions for RAG, emphasizing\npotential enhancements to tackle current challenges.\n\n\nThe paper unfolds as follows: Section II introduces the\nmain concept and current paradigms of RAG. The following\nthree sections explore core components\u2014\u201cRetrieval\u201d, \u201cGeneration\u201d and \u201cAugmentation\u201d, respectively. Section III focuses\n\n- n optimization methods in retrieval,including indexing, query\nand embedding optimization. Section IV concentrates on postretrieval process and LLM fine-tuning in generation. Section V\nanalyzes the three augmentation processes. Section VI focuses\n\n- n RAG\u2019s downstream tasks and evaluation system. Section VII mainly discusses the challenges that RAG currently\n\n\n\nfaces and its future development directions. At last, the paper\nconcludes in Section VIII.\n\n\nII. OVERVIEW OF RAG\n\n\nA typical application of RAG is illustrated in Figure 2.\nHere, a user poses a question to ChatGPT about a recent,\nwidely discussed news. Given ChatGPT\u2019s reliance on pretraining data, it initially lacks the capacity to provide updates on recent developments. RAG bridges this information\ngap by sourcing and incorporating knowledge from external\ndatabases. In this case, it gathers relevant news articles related\nto the user\u2019s query. These articles, combined with the original\nquestion, form a comprehensive prompt that empowers LLMs\nto generate a well-informed answer.\nThe RAG research paradigm is continuously evolving, and\nwe categorize it into three stages: Naive RAG, Advanced\nRAG, and Modular RAG, as showed in Figure 3. Despite\nRAG method are cost-effective and surpass the performance\n\n- f the native LLM, they also exhibit several limitations.\nThe development of Advanced RAG and Modular RAG is\na response"],"reference":"VisRAG enhances the retrieval and generation stages of RAG by employing a vision-language model (VLM)-based retriever and generator. Instead of parsing documents to obtain text, VisRAG directly embeds the document page as an image using a VLM and retrieves it to enhance the generation process. This approach maximizes the retention and utilization of data information in the original documents, eliminating information loss introduced during the parsing process."}
{"user_input":"How does the SPLADE model address the limitations of previous sparse lexical representation models in terms of efficiency and effectiveness?","response":"The SPLADE model addresses the limitations of previous sparse lexical representation models by implementing several key changes that enhance both efficiency and effectiveness. \n\n1. **Sparsity Regularization**: SPLADE incorporates explicit sparsity regularization, which improves its performance compared to models like SparTerm that lack this feature. This regularization helps in emphasizing important terms while discarding those without significant information content <document id=\"2107.05720_splade_formal:chunk_1800\">.\n\n2. **Efficiency**: SPLADE is designed to operate on inverted indexes and its training process is straightforward, being end-to-end and in a single step. This simplification makes it less complex compared to advanced dense models such as ANCE, which often require more intricate setups and heavier computation <document id=\"2107.05720_splade_formal:chunk_5850\">.\n\n3. **Control over Efficiency-Effectiveness Trade-off**: The model allows explicit control over the trade-off between the number of floating-point operations (FLOPS) and its effectiveness by adjusting the sparsity regularization parameters. This adaptability ensures that SPLADE can be fine-tuned to meet specific requirements for efficiency or effectiveness <document id=\"2107.05720_splade_formal:chunk_6750\">.\n\n4. **Document Expansion**: SPLADE efficiently performs document expansion similar to standard expansion models, which helps in reducing vocabulary mismatch issues inherent in sparse representations, enhancing its overall performance <document id=\"2107.05720_splade_formal:chunk_900\">.\n\nThese enhancements make SPLADE a competitive alternative to existing dense retrieval models, combining the desirable properties of traditional sparse models with modern advancements.","retrieved_contexts":[" mechanism (roughly sum for\nSparTerm [1], max for EPIC [16] and SPARTA [29]), that allows to\ncompute an importance weight for each term of the vocabulary, for\nthe full document or query. However, EPIC and SPARTA (document)\nrepresentations are not sparse enough by construction \u2013 unless\nresorting on top- _\ud835\udc58_ pooling \u2013 contrary to SparTerm, for which fast\nretrieval is thus possible. Furthermore, the latter does not include\n(like SNRM) an _explicit_ sparsity regularization, which hinders its\nperformance. Our SPLADE model relies on such regularization, as\nwell as other key changes, that boost both the efficiency and the\neffectiveness of this type of models.\n\n\n**3** **SPARSE LEXICAL REPRESENTATIONS FOR**\n\n**FIRST-STAGE RANKING**\n\n\nIn this section, we first describe in details the SparTerm model [1],\nbefore presenting our model named SPLADE.\n\n\n**3.1** **SparTerm**\n\n\nSparTerm predicts term importance \u2013 in BERT WordPiece vocabulary (| _\ud835\udc49_ | = 30522) \u2013 based on the logits of the Masked Language Model (MLM) layer. More precisely, let us consider an input\nquery or document sequence (after WordPiece tokenization) _\ud835\udc61_ =\n( _\ud835\udc61_ 1 _,\ud835\udc61_ 2 _, ...,\ud835\udc61\ud835\udc41_ ), and its corresponding BERT embeddings ( _\u210e_ 1 _,\u210e_ 2 _, ...,\u210e\ud835\udc41_ ).\nWe consider the importance _\ud835\udc64\ud835\udc56\ud835\udc57_ - f the token _\ud835\udc57_ (vocabulary) for a\ntoken _\ud835\udc56_ (of the input sequence):\n\n\n_\ud835\udc64\ud835\udc56\ud835\udc57_ = transform( _\u210e\ud835\udc56_ ) _[\ud835\udc47]_ _\ud835\udc38_ _\ud835\udc57_ + _\ud835\udc4f_ _\ud835\udc57_ _\ud835\udc57_ \u2208{1 _, ...,_ | _\ud835\udc49_ |} (1)\n\n\n\nwhere _\ud835\udc38_ _\ud835\udc57_ denotes the BERT input embedding for token _\ud835\udc57_, _\ud835\udc4f_ _\ud835\udc57_ is a\ntoken-level bias, and transform(","ing by emphasizing on _important_ terms and discarding\nmost of the terms without information content. Expansion allows\nto enrich documents, either by implicitly adding stemming effects\n(legs \u2192 leg) or by adding relevant topic words (e.g. treatment).\n\n\n**5** **CONCLUSION**\n\n\nRecently, dense retrieval based on BERT has demonstrated its superiority for first-stage retrieval, questioning the competitiveness of\ntraditional sparse models. In this work, we have proposed SPLADE,\na sparse model revisiting query\/document expansion. Our approach\nrelies on in-batch negatives, logarithmic activation and FLOPS regularization to learn effective and efficient sparse representations.\nSPLADE is an appealing candidate for initial retrieval: it rivals the\nlatest state-of-the-art dense retrieval models, its training procedure\nis straightforward, its sparsity\/FLOPS can be controlled explicitly\nthrough the regularization, and it can operate on inverted indexes.\nIn reason of its simplicity, SPLADE is a solid basis for further improvements in this line of research.\n\n\n\n33\n\n\n32\n\n\n31\n\n\n30\n\n\n29\n\n\n28\n\n\n27\n\n\n\n\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||a<br>a|a<br>a|vg q len <br>vg d len|_ \u2243_15<br>_ \u2243_58||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||||\n|||||||||||||y-T5<br>|y-T5<br>|\n||||||||||||doc2quer<br>|y-T5<br>|y-T5<br>|\n||||~~a~~<br>a|~~a~~<br>a|~~vg q len~~ <br>vg d len|~~_ \u2243_6~~<br>_ \u2243_18||<br> <br>|||~~SparTerm~~<br>ST exp-_\u2113F_<br>SPLADE-|~~ lexical~~<br>_LOPS_<br>_\u2113_~~1~~||\n||||~~a~~<br>a|~~a~~<br>a||||||||||\n||||||||||||SPLADE-|_\u2113FLOPS_|_\u2113FLO"," with approximate nearest neighbors search\nhas shown impressive results [8, 15, 26], but is still combined with\nBOW models because of its inability to explicitly model term matching. Hence, there has recently been a growing interest in learning\n_sparse representations_ for queries and documents [1, 4, 19, 28, 29].\nBy doing so, models can inherit from the desirable properties of\nBOW models like exact-match of (possibly latent) terms, efficiency\n\n- f inverted indexes and interpretability. Additionally, by modeling\nimplicit or explicit (latent, contextualized) _expansion_ mechanisms \u2013\nsimilarly to standard expansion models in IR \u2013 these models can\nreduce the vocabulary mismatch.\nThe contributions of this paper are threefold: (1) we build upon\nSparTerm [1], and show that a mild tuning of hyperparameters\nbrings improvements that largely outperform the results reported\nin the original paper; (2) we propose the SParse Lexical AnD Expansion (SPLADE) model, based on a logarithmic activation and\nsparse regularization. SPLADE performs an efficient document expansion [1, 16], with competitive results with respect to complex\ntraining pipelines for dense models like ANCE [26]; (3) finally, we\nshow how the sparsity regularization can be controlled to influence the trade-off between efficiency (in terms of the number of\nfloating-point operations) and effectiveness.\n\n\n**2** **RELATED WORKS**\n\n\nDense retrieval based on BERT Siamese models [22] has become\nthe standard approach for candidate generation in Question Answering and IR [8, 10, 12, 15, 25]. While the backbone of these models remains the same, recent works highlight the critical aspects\n\n- f the training strategy to obtain state-of-the-art results, ranging\nfrom improved negative sampling [8, 25] to distillation [11, 15].\nColBERT [13] pushes things further: the postponed token-level\ninteractions allow to efficiently apply the model for first-stage retrieval, benefiting of the effectiveness of modeling fine-grained\ninteractions, at the cost of storing embeddings for each (sub)term\n\n- raising concerns about the actual scalability of the approach for\nlarge collections. To the best of our knowledge, very few studies\nhave discussed the impact of using _approximate_ nearest neighbors\n\n\n(ANN) search on IR metrics [","671 0.813 2.83\nSPLADE- _\u2113_ 1 0.322 0.954 0.667 0.792 0.88\n\nSPLADE- _\u2113_ FLOPS 0.322 0.955 0.665 0.813 0.73\n\n\n- f the FLOPS regularization over _\u2113_ 1 in order to decrease the computing cost. Note that in contrast to SparTerm, SPLADE is trained\nend-to end in a single step. It is also remarkably simple, compared\nto dense state-of-the-art baselines such as ANCE [25], and avoids\nresorting to approximate neighbors search, whose impact on IR\nmetrics has not been fully evaluated yet.\n\n\n**Effectiveness-efficiency trade-off.** Figure 1 illustrates the trade\n- ff between effectiveness (MRR@10) and efficiency (FLOPS), when\nwe vary _\ud835\udf06\ud835\udc5e_ and _\ud835\udf06\ud835\udc51_ (varying both implies that plots are not smooth).\nWe observe that ST exp- _\u2113_ FLOPS falls far behind BOW models and\n\n\n**Figure 1: Performance vs FLOPS for SPLADE models trained**\n**with different regularization strength** _\ud835\udf06_ **on MS MARCO**\n\n\n\n**Table 2: Document and expansion terms: between parenthesis**\n**is the weight associated with the term \u2013 omitted for the second oc-**\n**currence of the term in the document, and strike-through for zeros**\n\n\n**original document (doc ID: 7131647)**\n\n\nif (1.2) bow (2.56) legs (1.18) ~~is~~ caused (1.29) by (0.47) ~~the~~ bone (1.2) alignment\n(1.88) issue (0.87) ~~than you may be~~ able (0.29) ~~to~~ correct (1.37) through (0.43)\n_bow legs_ correction (1.05) ~~exercises. read more here..~~ _if bow legs is caused by_\n_the bone alignment issue than you may be able to correct through bow legs_\n\n_correction exercises._\n\n\n**expansion terms**\n\n\n(leg, 1.62) (arrow, 0.7) (exercise, 0.64) (",". 2018. Deep contextualized word representations. In _Proc. NAACL_ .\n\n\nFabio Petroni, Tim Rockt\u00a8aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowledge bases? In _Proc. EMNLP-IJCNLP_ .\n\n\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. _OpenAI_\n_Technical Report_ .\n\n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\n\n  - f transfer learning with a unified text-to-text transformer. _arXiv preprint arXiv:1910.10683_ .\n\n\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In _Proc. EMNLP_ .\n\n\nSiva Reddy, Danqi Chen, and Christopher D Manning.\n2019. CoQA: A conversational question answering\nchallenge. _TACL_ .\n\n\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parameters of a language model? _arXiv preprint_\n_arXiv:2002.08910_ .\n\n\nStephen E Robertson, Steve Walker, Susan Jones,\nMicheline M Hancock-Beaulieu, Mike Gatford, et al.\n1995. Okapi at TREC-3. _NIST Special Publication_\n_Sp_ .\n\n\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019.  - LMpics\u2013on what language model pre-training captures. _arXiv preprint_\n_arXiv:1912.13283_ .\n\n\nEllen M Voorhees et al. 1999. The TREC-8 question\nanswering track report. In _TREC_ .\n\n\nShuoh"],"reference":"The SPLADE model addresses the limitations of previous sparse lexical representation models by incorporating sparsity regularization, which boosts both the efficiency (in terms of the number of floating-point operations) and effectiveness of these types of models. Additionally, it relies on a logarithmic activation function and efficient document expansion, achieving competitive results with respect to complex training pipelines for dense models like ANCE."}
{"user_input":"How does ARES utilize prediction-powered inference (PPI) to improve model-based evaluation accuracy?","response":"ARES utilizes prediction-powered inference (PPI) to enhance model-based evaluation accuracy by integrating a human preference validation set, which includes around 300 datapoints. This approach rectifies the initial machine learning predictions and generates confidence intervals, thereby improving the accuracy of context relevance and answer relevance predictions. The results indicate that ARES, when combined with PPI, significantly outperforms other evaluation frameworks, such as RAGAS, in terms of accurately ranking RAG systems across various datasets. Specifically, ARES achieves higher Kendall\u2019s \u03c4 scores for both context and answer relevance, indicating improved ranking accuracy when PPI is applied <document id=\"2311.09476_ares_saad_falcon:chunk_5400\">.","retrieved_contexts":[" evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges). We compare against RAGAS (version 0.0.18) and a baseline few-shot prompted GPT3.5 judge ( _gpt-3.5-turbo-16k_ ). For the few-shot\nGPT-3.5 judge, we provide few-shot examples for\nguiding predictions; the prompts are included in\nAppendices A.2, A.3, and A.4. For both ARES\nand the GPT-3.5 judge baseline, we augment the\nLLM with PPI, using a 300-datapoint human preference validation set to rectify the ML predictions\nand produce confidence intervals.\n\n\n\nAcross almost all settings across the datasets\nfrom KILT and SuperGLUE, ARES provides a\nmore accurate ranking of RAG systems than RAGAS. ARES averages a Kendall\u2019s _\u03c4 0.065 higher_\n_for context relevance_ and _0.132 higher for answer_\n_relevance than RAGAS_ . Additionally, the LLMjudge is substantially more accurate than RAGAS\nat predicting context relevance and answer relevance of a query-passage-answer triple. For context relevance, ARES with a fine-tuned LLM-judge\nis _59.9 percentage points higher than RAGAS_ while\nfor answer relevance, our system is _14.4 percent-_\n_age points higher than RAGAS_ . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompting and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall\u2019s _\u03c4_ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall\u2019s _\u03c4_ for ARES is\n0.08 higher"," 5: **ARES Ranking on Real-World RAG Systems** : For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall\u2019s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\n\n\n**ARES Cross-Domain Ranking of Pseudo RAG Systems**\n\n\nNQ to FEVER to NQ to MultiRC to NQ to ReCoRD to\nFEVER NQ MultiRC NQ ReCoRD NQ\n\n\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\n\n\nKendall\u2019s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\n\n\nKendall\u2019s Tau of\n0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nIn-Domain LLM Judge\n\n\nAverage PPI Range 8.7% 7.2%"," predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach. We also find that ARES consistently\ndistinguishes competitive RAG systems that are\n\n- nly a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop\n\n\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\n[available on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**2** **Related Work**\n\n\nRAG (Guu et al., 2020; Lewis et al., 2020; Khattab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combining them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual information (Shuster et al., 2021; Huo et al., 2023), and\n\n- ffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essential for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\n\n- ut of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. However, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min","## **ARES: An Automated Evaluation Framework for Retrieval-Augmented** **Generation Systems**\n\n\n\n**Jon Saad-Falcon**\nStanford University _[\u2217]_\n\njonsaadfalcon@stanford.edu\n\n\n**Christopher Potts**\nStanford University\ncgpotts@stanford.edu\n\n\n**Abstract**\n\n\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an _Automated RAG Evaluation_\n_System_, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By creating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and\/or documents used in the evaluated RAG systems. We\nmake our code and datasets publicly available\n\n[on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**1** **Introduction**\n\n\nRetrieval-augmented generation (RAG) has become a prominent approach for building userfacing NLP applications, such as systems for question answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to generate a response. This formulation admits a multitude\n\n- f choices: what retrieval model to use, how to di\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n\n\n_\u2217_ Project started during research internship at Databricks\n\n\n\n**Omar Khattab**\n\nStanford University\n\n   - khattab","LM judges\n\n- n synthetically generated queries and answers.\nARES can evaluate each component of a RAG system separately to help improve system understanding and create targeted solutions, and it requires\n\n- nly minimal human annotations. For the eight different datasets in KILT, SuperGLUE, and AIS requiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\n\n- n context relevance, answer faithfulness, and answer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\n\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing\n\n\nlogits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n\n\n**7** **Limitations**\n\n\nARES relies on a small set of annotations in the\n\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\n\n- ften require an annotator familiar with the RAG\nsystem\u2019s domain application. While these annotations can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our evaluation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\n\n- ther languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\n\n\n**References**\n\n\nMubashara Ak"],"reference":"ARES utilizes prediction-powered inference (PPI) to improve model-based evaluation accuracy by leveraging a small set of human-annotated datapoints for computing its confidence intervals. This approach allows ARES to provide statistical confidence intervals for RAG scoring, enhancing the precision and reliability of its evaluations."}
{"user_input":"How did the introduction of dense passage retrieval improve open-domain question answering systems?","response":"The introduction of dense passage retrieval (DPR) has significantly improved open-domain question answering systems by enhancing the retrieval process beyond traditional sparse methods like TF-IDF or BM25. Specifically, DPR leverages learned dense representations that can capture semantic similarities between queries and passages more effectively than mere token matching. This approach allows for better identification of relevant passages, thus increasing the likelihood that the machine reader will find the correct answers.\n\nEmpirical results have shown that DPR outperforms BM25 by 9%-19% in top-20 passage retrieval accuracy across various QA datasets, leading to new state-of-the-art performance in end-to-end QA systems. Moreover, the effective pairing of dense retrieval with advanced reader models, trained on question-passage pairs, results in improved end-to-end QA accuracy, thereby validating the efficacy of dense representations in the context of open-domain question answering <document id=\"2004.04906_dpr_karpukhin:chunk_0\">.\n\nFurthermore, the use of dense passage retrieval facilitates a two-stage framework where a context retriever selects relevant passages, and a machine reader analyzes these passages for answers, streamlining the process and reducing performance degradation typically observed when handling larger passages <document id=\"2004.04906_dpr_karpukhin:chunk_0\">. \n\nOverall, the shift towards dense representations signifies a critical advancement in the capability of QA systems to process and understand large volumes of data, thereby improving both retrieval accuracy and answer extraction efficiency <document id=\"2004.04906_dpr_karpukhin:chunk_0\">.","retrieved_contexts":["## A Replication Study of Dense Passage Retriever\n\nXueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin\n\n\nDavid R. Cheriton School of Computer Science\nUniversity of Waterloo\n\n\n\nAbstract\n\n\nText retrieval using learned dense representations has recently emerged as a promising alternative to \u201ctraditional\u201d text retrieval using\nsparse bag-of-words representations. One recent work that has garnered much attention is\nthe dense passage retriever (DPR) technique\nproposed by Karpukhin et al. (2020) for endto-end open-domain question answering. We\npresent a replication study of this work, starting with model checkpoints provided by the\nauthors, but otherwise from an independent implementation in our group\u2019s Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results largely verify the claims of the original paper, we arrived at two important additional findings that\ncontribute to a better understanding of DPR:\nFirst, it appears that the original authors underreport the effectiveness of the BM25 baseline\nand hence also dense\u2013sparse hybrid retrieval\nresults. Second, by incorporating evidence\nfrom the retriever and an improved answer\nspan scoring technique, we are able to improve end-to-end question answering effectiveness using exactly the same models as in the\n\n  - riginal work.\n\n\n1 Introduction\n\n\nReplicability and reproducibility form the foundation of the scientific enterprise. Through such studies, we as a community gain increased confidence\nabout the veracity of previously published results.\nThese investigations are often under-valued, especially compared to work that proposes novel models, but they nevertheless make important contributions to advancing science.\nThis paper presents a replicability study of the\ndense passage retriever (DPR) technique proposed\nby Karpukhin et al. (2020) for end-to-end opendomain question answering (QA). To be precise,\nwe use the term replicability in the sense artic\n\n\nulated by the ACM, [1] characterized as \u201cdifferent\nteam, different experimental setup\u201d. We are able\nto achieve comparable measurements (i.e., effectiveness on different test collections) based on\nan independently developed computational artifact\n(i.e., a different implementation). Specifically, our\nexperiments rely on model checkpoints shared by\nthe original authors, but we have otherwise built an\nentirely different implementation (other than the\nevaluation scripts).\nDPR is worthy of detailed study because it represents an important exemplar","## **Dense Passage Retrieval for Open-Domain Question Answering**\n\n**Vladimir Karpukhin** _[\u2217]_ **, Barlas O\u02d8guz, Sewon Min** _[\u2217]_ _[\u2020]_ **, Patrick Lewis,**\n**Ledell Wu, Sergey Edunov, Danqi Chen** _[\u2021]_ **, Wen-tau Yih**\nFacebook AI _\u2020_ University of Washington _\u2021_ Princeton University\n_{_ vladk, barlaso, plewis, ledell, edunov, scottyih _}_ @fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu\n\n\n\n**Abstract**\n\n\nOpen-domain question answering relies on efficient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method. In this work, we show that\nretrieval can be practically implemented using _dense_ representations alone, where embeddings are learned from a small number\n\n  - f questions and passages by a simple dualencoder framework. When evaluated on a\n\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks. [1]\n\n\n**1** **Introduction**\n\n\nOpen-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions using a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), _inter alia_ ), the advances of reading\ncomprehension models suggest a much simplified\ntwo-stage framework: (1) a context _retriever_ first\nselects a small subset of passages where some\n\n- f them contain the answer to the question, and\nthen (2) a machine _reader_ can thoroughly examine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\n\n- pen-domain QA to machine reading is a very reasonable strategy, a huge performance degradation\nis often observed in practice ["," longer passages compared\nto DPR (288 word pieces compared to our 100\ntokens) and the computational complexity is superlinear in passage length. We also note that we\nfound _k_ = 50 to be optimal for NQ, and _k_ = 10\nleads to only marginal loss in exact match accuracy (40.8 vs. 41.5 EM on NQ), which should be\nroughly comparable to ORQA\u2019s 5-passage setup.\n\n\n**7** **Related Work**\n\n\nPassage retrieval has been an important component for open-domain QA (Voorhees, 1999). It\nnot only effectively reduces the search space for\nanswer extraction, but also identifies the support\ncontext for users to verify the answer. Strong sparse\nvector space models like TF-IDF or BM25 have\n\n\nbeen used as the standard method applied broadly\nto various QA tasks (e.g., Chen et al., 2017; Yang\net al., 2019a,b; Nie et al., 2019; Min et al., 2019a;\nWolfson et al., 2020). Augmenting text-based retrieval with external structured information, such\nas knowledge graph and Wikipedia hyperlinks, has\nalso been explored recently (Min et al., 2019b; Asai\net al., 2020).\n\n\nThe use of dense vector representations for retrieval has a long history since Latent Semantic\nAnalysis (Deerwester et al., 1990). Using labeled\npairs of queries and documents, discriminatively\ntrained dense encoders have become popular recently (Yih et al., 2011; Huang et al., 2013; Gillick\net al., 2019), with applications to cross-lingual\ndocument retrieval, ad relevance prediction, Web\nsearch and entity retrieval. Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically\nrelevant text pairs, even without exact token matching. The dense representation alone, however, is\ntypically inferior to the sparse one. While not the\nfocus of this work, dense representations from pretrained models, along with cross-attention mechanisms, have also been shown effective in passage\n\n- r dialogue re-ranking tasks (Nogueira and Cho,\n2019; Humeau et al., 2020). Finally, a concurrent\nwork (Khattab and Zaharia, 2020) demonstrates","Published as a conference paper at ICLR 2021\n\n\n4 RELATED WORK\n\n\n**Open-domain QA with Dense Retrieval** In contrast to sparse term-index IR methods that are\nwidely used by existing open-domain QA systems (Chen et al., 2017; Wang et al., 2018; Yang\net al., 2019), recent systems (Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) typically\nuses dense passage retrieval techniques that better capture the semantic matching beyond simple\nn-gram overlaps. To generate powerful dense question and passage representations, these methods\neither conduct large-scale pretraining with self-supervised tasks that are close to the underlying\nquestion-passage matching in retrieval, or directly use the human-labeled question-passage pairs\nto finetune pretrained masked language models. On single-hop information-seeking QA datasets\nsuch as NaturalQuestions (Kwiatkowski et al., 2019) or WebQuestions (Berant et al., 2013), these\ndense methods have achieved significant improvements over traditional IR methods. Prior to these\nmethods based on pretrained models, Das et al. (2019) use RNN encoder to get dense representations\n\n- f questions and passages. They also consider an iterative retrieval process and reformulate the query\nrepresentation based on reader model\u2019s hidden states. However, their method requires an initial round\n\n- f TF-IDF\/BM25 retrieval and a sophisticated RL-based training paradigm to work well. Finally, like\nthe aforementioned methods, only single-hop datasets are considered in their experiments. More akin\nto our approach, Feldman & El-Yaniv (2019) use a similar recursive dense retrieval formulation for\nmulti-hop QA. In contrast to their biattenional reformulation component, which is applied on top of\nthe token-level representations of the query and passages, we adopt a more straightforward query\nreformulation strategy, by simply concatenating the original query and previous retrieval as the inputs\nto the query encoder. Together with stronger pretrained encoders and more effective training methods\n(in-batch + memory bank negative sampling vs their binary ranking loss), MDR is able to double the\naccuracy of their system.\n\n\n**Query Expansion Techniques in IR** As our dense encoder augments the original question with\nthe initial retrieved results to form the updated query representation, our work is also relevant to query\n"," are twofold. First, we demonstrate that with the proper training setup, simply fine-tuning the question and passage encoders\n\n- n existing question-passage pairs is sufficient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\n\n- pen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve comparable or better results on multiple QA datasets in the\n\n- pen-retrieval setting, compared to several, much\ncomplicated systems.\n\n\n**2** **Background**\n\n\nThe problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as \u201c _Who first voiced Meg on Family_\n_Guy?_ \u201d or \u201c _Where was the 8th Dalai Lama born?_ \u201d, a\nsystem is required to answer it using a large corpus\n\n- f diversified topics. More specifically, we assume\n\n\n\nthe extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more passages in the corpus. Assume that our collection\ncontains _D_ documents, _d_ 1 _, d_ 2 _, \u00b7 \u00b7 \u00b7, dD_ . We first\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units [3] and get _M_\ntotal passages in our corpus _C_ = _{p_ 1 _, p_ 2 _, . . ., pM_ _}_,\nwhere each passage _pi_ can be viewed as a sequence\n\n- f tokens _w_ [(] _[i]_ [)]\n1 _[, w]_ 2 [(] _[i]_ [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _|_ [(] _p_ _[i]_ [)] _i|_ [. Given a question] _[ q]_ [,]\n\nthe task is to find a span _ws_ [(] _[i]_ [)] _[, w]_ _s_ [(] _[i]_ +1 [)] _[,][ \u00b7 \u00b7 \u00b7][, w]_ _e_ [(] _[i]_ [)] from\n\n- ne of the passages _pi_ that can"],"reference":"Dense Passage Retrieval for Open-Domain Question Answering introduced a method that uses dense representations alone to perform efficient passage retrieval, outperforming traditional sparse vector space models like TF-IDF or BM25. This was achieved by learning embeddings from a small number of questions and passages using a simple dual-encoder framework. When evaluated on various open-domain QA datasets, the dense retriever demonstrated significant improvements in terms of top-20 passage retrieval accuracy compared to strong Lucene-BM25 systems."}
{"user_input":"What is the core idea behind HyDE and how does it improve zero-shot dense retrieval?","response":"The core idea behind HyDE (Hypothetical Document Embeddings) is to enable effective zero-shot dense retrieval without the need for relevance supervision. It achieves this by utilizing a contrastive learning approach that focuses on document-document similarity, rather than requiring direct relevance evaluations between query-document pairs. Specifically, it employs a document encoder trained through unsupervised contrastive learning, allowing it to capture the necessary semantics for retrieval effectively.\n\nHyDE improves zero-shot dense retrieval in several ways:\n\n1. **Absence of Relevance Labels**: It can function without any relevance labels, making it applicable in situations where such data is not available or scarce\u2014this is a significant advantage over traditional models that rely on extensive labeled datasets (<document id=\"2212.10496_hyde_gao:chunk_2700\">).\n\n2. **Use of Instruction-following Language Models**: The model generates \"hypothetical\" documents based on input queries and specified instructions, leveraging the capabilities of generative language models to improve the quality of query representations (<document id=\"2212.10496_hyde_gao:chunk_2700\">).\n\n3. **Performance**: HyDE shows competitive performance compared to fine-tuned models on various retrieval tasks and is especially beneficial in low-resource settings, often outperforming other unsupervised methods like Contriever (<document id=\"2212.10496_hyde_gao:chunk_4950\">).\n\nThese features collectively enable HyDE to be a robust solution for zero-shot dense retrieval tasks across multiple domains, demonstrating its potential for greater generalizability and effectiveness in real-world scenarios.","retrieved_contexts":[" been shown\nsuccessful across tasks like web search, question\nanswering, and fact verification. A variety of meth\n- ds such as negative mining (Xiong et al., 2021; Qu\net al., 2021), distillation (Qu et al., 2021; Lin et al.,\n2021b; Hofst\u00e4tter et al., 2021) and task-specific\n\n\n_\u2217_ Equal contribution.\n1No models were trained or fine-tuned in making this pre[print. Our open source code is available at https:\/\/github.](https:\/\/github.com\/texttron\/hyde)\n[com\/texttron\/hyde.](https:\/\/github.com\/texttron\/hyde)\n\n\n\npre-training (Izacard et al., 2021; Gao and Callan,\n2021; Lu et al., 2021; Gao and Callan, 2022; Liu\nand Shao, 2022) have been proposed to improve the\neffectiveness of supervised dense retrieval models.\n\n\nOn the other hand, zero-shot dense retrieval still\nremains difficult. Many recent works consider the\nalternative transfer learning setup, where the dense\nretrievers are trained on a high-resource dataset and\nthen evaluated on queries from new tasks. The MSMARCO collection (Bajaj et al., 2016), a massive\njudged dataset with a large number of judged querydocument pairs, is arguably the most commonly\nused. As argued by Izacard et al. (2021), in practice, however, the existence of such a large dataset\ncannot always be assumed. Even MS-MARCO restricts commercial use and cannot be adopted in a\nvariety of real-world search scenarios.\n\nIn this paper, we aim to build effective fully\nzero-shot dense retrieval systems that require **no**\n**relevance** supervision, work out-of-box and generalize across tasks. As supervision is not available,\nwe start by examining self-supervised representation learning methods. Modern deep learning enables two distinct learning algorithms. At the token\nlevel, generative large language models (LLM) pretrained on large corpus have demonstrated strong\nnatural language understanding (NLU) and generation (NLG) capabilities (Brown et al., 2020;\nChen et al., 2021; Rae et al., 2021; Hoffmann\net al., 2022; Thoppilan et al.,",".2 **49.8** 54.0 46.8 44.7\n\nContriever 92.6 90.1 17.2 56.2 45.3 42.3\nHyDE **96.4** **97.9** 41.4 **62.1** **47.2** **50.9**\n\n\n_w\/ relevance judgement_\nDPR 72.7 75.1 21.2 34.2 34.9 21.5\n\nANCE 81.6 93.7 45.7 58.1 31.9 39.8\nContriever [FT] 94.7 97.7 40.7 **65.6** **54.1** 49.2\n\n\nTable 2: Low resource tasks from BEIR. Best performing w\/o relevance and overall system(s) are marked **bold** .\n\n\n\nboth precision-oriented and recall metrics. While\nunsupervised Contriever can underperform the\nclassical BM25 approach, HyDE outperforms BM25\nby large margins.\n\n\nHyDE remains competitive even when compared\nto fine-tuned models. Note that TREC DL19\/20\nare search tasks defined on MS-MARCO and\nthere, all the fine-tuned models are richly _super-_\n_vised_ . On TREC DL19, HyDE shows comparable\nmap and ndcg@10 to Contriever [FT] and best recall@1k. On DL20, HyDE gets around 10% lower\nmap and ndcg@10 than Contriever [FT] and similar recall@1k. The ANCE model shows better\n\nndcg@10 numbers than HyDE but lower recall, suggesting it may be biased to a subset of queries\nand\/or relevant documents.\n\n\n\n**4.3** **Low Resource Retrieval**\n\n\nIn Table 2, we show retrieval results on low\nresource tasks from BEIR. Similar to web\n\nsearch, HyDE again brings sizable improvements to\nContriever across the board in terms of both ndcg\nand recall. HyDE is only outperformed by BM25 on\n\n- ne dataset, TREC-Covid but with a tiny 0.2 margin; in comparison, the underlying Contriever\nunderperforms by more than 50%.\nWe also observe","triever [FT]\n\nw\/ Flan-T5 (11b) 60.2 62.1\nw\/ Cohere (52b) 61.4 63.1\nw\/ GPT (175b) **67.4** **63.5**\n\n\nTable 4: NDCG@10 on TREC DL19\/20. Effect\n\n- f changing different instruction LMs and using finetuned encoder. Best w\/o relevance and overall models\n\nare marked **bold** .\n\n\nmodels bring improvement to the unsupervised\nContriever, with larger models bringing larger\nimprovements. At the time when this paper is\nwritten, the Cohere model is still experimental\nwithout much detail disclosed. We can only\ntentatively hypothesize that training techniques\nmay have also played some role in the performance\ndifference.\n\n\n**5.2** **HyDE with Fine-tuned Encoder**\n\n\nTo begin with, HyDE with fine-tuned encoder is\n_not_ the intended usage: HyDE is more powerful\nand irreplaceable when few relevance labels are\npresent. Here we are interested to find out if\nand how HyDE embedding can affect fine-tuned encoders. In Table 4, we see that less powerful instruction LMs can negatively impact the overall performance of the fine-tuned retriever. (To remind our\nreaders, Contriever [FT] is in-domain supervisedly\nfine-tuned for TREC DL19\/20). The performance\ndegradations remain small. On the other hand, we\nalso observe the InstructGPT model able to fur\nther bring up the performance, especially on DL19.\nThis suggests that there may still exist certain factors not captured by the fine-tuned encoder but only\nby the generative model.\n\n\n**6** **Conclusion**\n\n\nAt the end of the paper, we encourage the readers\nto take a moment and reflect on the HyDE model.\nCompare it to some of the other recently seen retrievers or re-ranker. These other models probably\ndiffer in their architecture, training method, and\/or\ntask, but probably all of them involve modeling\nrelevance scores between a pair of query and docu\n\nment. Dense retrievers consider vector similarities\n\nwhile self-attentive re-rankers regression scores. In\ncomparison, the concept of relevance in HyDE is\ncaptured by an","d) _\u27e9_ = _\u27e8_ **vq** _,_ **vd** _\u27e9_ (1)\n\n\nFor zero-shot retrieval, we consider _L_ query sets\n_Q_ 1 _, Q_ 2 _, ..., QL_ and their corresponding search corpus, document sets _D_ 1 _, D_ 2 _, ..., DL_ . Denote the\n_j_ - th query from _i_ - th set query set _Qi_ as q _ij_ . We\nneed to fully define mapping _functions_ enc _q_ and\nenc _d_ without access to any query set _Qi_, document\nset _Di_, or any relevance judgment _rij_ .\nThe difficulty of zero-shot dense retrieval lies\nprecisely in Equation 1: it requires learning of two\nembedding functions (for query and document respectively) into the _same_ embedding space where\ninner product captures _relevance_ . Without relevance judgments\/scores to fit, learning becomes\nintractable.\n\n\n**3.2** **HyDE**\n\n\nHyDE circumvents the aforementioned learning\nproblem by performing search in document\n- nly embedding space that captures documentdocument similarity. This can be easily learned\nusing unsupervised contrastive learning (Izacard\net al., 2021; Gao et al., 2021; Gao and Callan,\n2022). We set document encoder enc _d_ directly as a\ncontrastive encoder enccon.\n\n\n_f_ = enc _d_ = enccon (2)\n\n\nThis function is also denoted as _f_ for simplicity. This unsupervised contrastive encoder will\nbe shared by all incoming document corpus.\n\n\n**vd** = _f_ ( _d_ ) _\u2200d \u2208_ _D_ 1 _\u222a_ _D_ 2 _\u222a_ _... \u222a_ _DL_ (3)\n\n\nTo build the query vector, we consider in addition\nan instruction following LM, InstructLM. It takes a\nquery _q_ and a textual instruction INST and follows\nthem to perform the task specified by INST. For\nsimplicity, denote,\n\n\n_g_ ( _q,_ INST) = InstructLM( _q,_ INST) (4)\n\n\nNow we can use _g_ to map queries to \"hypothetical\" documents by sampling from _g_, setting INST\n\n\nto be \u201cwrite a",".1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7 **81.3**\n\n\nDPR **79.4** 78.8 **75.0** **89.1** 51.6 **86.0** 84.7 **82.9** 93.9 67.6\nMulti\nBM25 + DPR 78.0 **79.9** 74.7 88.5 66.2 83.9 84.4 82.3 **94.1** 78.6\n\n\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20\/100 retrieved\npassages that contain the answer. _Single_ and _Multi_ denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.\n\n\n\ntraditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10 _[\u2212]_ [5] using Adam, linear\nscheduling with warm-up and dropout rate 0 _._ 1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be desirable to obtain a single retriever that works well\nacross the board. To this end, we train a _multi_ dataset encoder by combining training data from\nall datasets excluding SQuAD. [8] In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages"],"reference":"The core idea behind Hypothetical Document Embeddings (HyDE) is to pivot through generating a hypothetical document that captures relevance patterns but may contain false details. This document is then encoded into an embedding vector by an unsupervised contrastively learned encoder, which identifies a neighborhood in the corpus embedding space where similar real documents are retrieved based on vector similarity. HyDE improves zero-shot dense retrieval by grounding the generated document to the actual corpus and filtering out incorrect details with the encoder's dense bottleneck."}
{"user_input":"What is the purpose of constructing a Retrieval-Augmented Generation Benchmark (RGB) and what specific abilities does it evaluate?","response":"The purpose of constructing the Retrieval-Augmented Generation Benchmark (RGB) is to evaluate four fundamental capabilities of large language models (LLMs) in retrieval-augmented generation (RAG). These capabilities are critical for assessing how well LLMs can utilize retrieved information and handle challenges associated with RAG. The four specific abilities evaluated by RGB are:\n\n1. **Noise Robustness**: The ability to extract useful information from documents that contain irrelevant or misleading content.\n2. **Negative Rejection**: The capacity to identify when the required knowledge is not present in any retrieved documents and to appropriately signal insufficient information.\n3. **Information Integration**: The skill to answer complex questions by synthesizing information from multiple documents.\n4. **Counterfactual Robustness**: The capacity to handle false information during the retrieval or generation process.\n\nRGB was designed to systematically evaluate these capabilities, highlighting the current limitations of LLMs and suggesting directions for future improvements in the context of RAG <document id=\"2309.01431_rgb_benchmark_chen:chunk_0\">.","retrieved_contexts":[" question, LLMs often fail to provide accurate\nanswer. Finally, we found that even when the LLMs contain\nthe required knowledge and are given warnings about potential risks in the retrieved information through instruction,\nthey still tend to trust and prioritize the retrieved information\n\n- ver their own existing knowledge. The experimental results\nmentioned above highlight the need for further resolution of\nimportant issues in the existing RAG method. Therefore, it\nis crucial to exercise caution and carefully design its usage.\nGenerally speaking, the contributions of this paper are [1] :\n\n\n - We proposed to evaluate four capabilities for retrievalaugmented generation  - f LLMs and created the\nRetrieval-Augmented Generation Benchmark in both English and Chinese. To best of our knowledge, it is the first\nbenchmark designed to assess these four capabilities for\nretrieval-augmented generation of LLMs.\n\n - We evaluated the existing LLMs using RGB and found\nthe limitations of them in the four different abilities.\n\n - We analyzed the responses of LLMs in RGB and identified their current shortcomings as well as suggested directions for improvement.\n\n\n**Related work**\n\n**Retrieval-augmented models** The knowledge stored in\nlarge language models is commonly out-of-date (He, Zhang,\nand Roth 2022) and they also sometimes generate hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023) i.e., they may generate irrelevant or factually incorrect contents. By using external\nknowledge as guidance, retrieval-augmented models can\ngenerate more accurate and reliable responses (Guu et al.\n2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard\net al. 2022; Shi et al. 2023; Ren et al. 2023). Retrievalaugmented models have achieved remarkable results in various tasks such as open-domain QA (Izacard and Grave\n2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai\n\n\n1Our code&data: https:\/\/github.com\/chen700564\/RGB.\n\n\n|Retrieve using<br>search engine|Col2|\n|---|---|\n|**Retrieve using**<br>**search engine**|{\"link\": \"https",". These challenges result in LLMs being unable to\nconsistently generate reliable and accurate responses. Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how\ncould each model survives from these drawbacks and improvement their performance via information retrieval. As a\nresult, there is a pressing need for a comprehensive evaluation of LLMs on their ability to effectively utilize retrieved\ninformation, as well as their ability to withstand the various\ndrawbacks present in information retrieval.\nTo this end, this paper conducts a comprehensive evaluation of RAG for current LLMs. Specifically, we create a new\nRetrieval-Augmented Generation Benchmark, namely RGB,\nin both English and Chinese. In order to ensure that the internal knowledge of LLMs does not introduce bias into the\nevaluation results, RGB chooses to aggregate the latest news\ninformation and constructs queries based on the news information. Then, based on these queries, we use Search API to\nfetch relevant documents and select most relevant snippets\nfrom the content as external retrieved documents. Finally,\nbased on different compositions of query and document-set\npairs, we expand the corpus and divided it into 4 testbeds to\nevaluate the following basic abilities of LLMs according to\nthe common challenges in RAG, as shown in Figure 1:\n\n\n - **Noise Robustness**, which means a LLM can extract useful information from noisy documents. In this paper, we\ndefine noisy documents as those that are relevant to the\nquestion but do not contain any information of the answer. For the instance in Figure 1, the noisy documents\nrelated to the question \u201cWho was awarded the 2022 Nobel Prize in Literature\u201d include reports about the 2021\nNobel Prize in Literature. To this end, the testbed for\nnoise robustness contains instances whose external documents contain a certain number of noisy documents\nbased on the desired noise ratio.\n\n - **Negative Rejection**, which means that a LLM should reject to answer the question when the required knowledge\nis not present in any retrieved document. The testbed for\nnegative rejection contains instances whose external documents are only with noisy documents. LLMs are expected to indicate \u201cinsufficient information\u201d or other rejection signals.\n\n - **Information Integration**, which evaluates whether\nLLMs can answer complex questions that require integrating information from multiple documents. For the instance in Figure 1, for the question \u201cWhen were the ChatGPT app"," the future development of RAG in LLMs.\n\n\n**Conclusion**\n\nIn this paper, we evaluated four abilities of retrievalaugmented generation in LLMs: noise robustness, negative rejection, information integration, and counterfactual\nrobustness. To conduct the evaluation, we built RetrievalAugmented Generation Benchmark (RGB). The instances of\nRGB are generated from latest news articles and the external\ndocuments obtained from search engines. The experimental\nresults suggest that current LLMs have limitations in the 4\nabilities. This indicates that there is still a significant amount\n\n- f work needed to effectively apply RAG to LLMs. To ensure accurate and reliable responses from LLMs, it is crucial\nto exercise caution and carefully design for RAG.\n\n\n\n\n\n\n\n\n\n\n|Question|Answer|Response|Errors|\n|---|---|---|---|\n|What groupings are France and<br>Germany in World Cup 2022?|Group D<br>Group E|France and Germany are in<br>Group D of the 2022 FIFA<br>World Cup.|**Merging**<br>**Error**|\n|Who were the MVP of Super<br>Bowl 2022 and 2023?|Cooper Kupp<br>Patrick<br>Mahomes|According to the provided<br>document, the MVP of Super<br>Bowl LVI was Cooper Kupp<br>of the Los Angeles Rams.|**Ignoring**<br>**Error**|\n|What flms won the 2022 and<br>2023 Academy Awards for<br>Best Picture?|CODA<br>Everything<br>Everywhere<br>All at Once|The flm CODA won the<br>award for Best Picture at the<br>95th Academy Awards<br>ceremony held on 2023.|**Misalignment**<br>**Error**|\n\n\n\nTable 6: Error cases of information integration, the responses are generated by ChatGLM2-6B. The blue and red\ntexts represent the answers to two sub-questions.\n\n\n**(1) Merging Error (28% of the total).** The model sometimes merges the answers of the two sub-questions, resulting\nin an error. It mistakenly uses the answer from one question\nto address both two questions. At this point, the model will\ndisregard any documents related to one sub-question. For\nexample, in Table 6, it incorrectly states that Group","## **Benchmarking Large Language Models in Retrieval-Augmented Generation**\n\n**Jiawei Chen** [1,3] **, Hongyu Lin** [1,*] **, Xianpei Han** [1,2,*] **, Le Sun** [1,2]\n\n1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n_{_ jiawei2020,hongyu,xianpei,sunle _}_ @iscas.ac.cn\n\n\n\n**Abstract**\n\n\nRetrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language\nmodels (LLMs). However, existing research lacks rigorous\nevaluation of the impact of retrieval-augmented generation\n\n  - n different large language models, which make it challenging to identify the potential bottlenecks in the capabilities\n\n  - f RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance\n\n  - f different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative\nrejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in\nboth English and Chinese. RGB divides the instances within\nthe benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case.\nThen we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG.\nEvaluation reveals that while LLMs exhibit a certain degree\n\n  - f noise robustness, they still struggle significantly in terms of\nnegative rejection, information integration, and dealing with\nfalse information. The aforementioned assessment outcomes\nindicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n\n\n**Introduction**\n\n\nRecently, there have been impressive advancements in large\nlanguage models (LLMs) like ChatGPT (OpenAI 2022) and\nChatGLM (THUDM 2023a). Although these models have\nshown remarkable general abilities (Bang et al. 2023; Guo\net al. 2023), they still suffer severely from challenges including factual hallucination (Cao"," RETRIEVAL-AUGMENTED GENERATION\n\n\nA typical retrieval-augmented generation (RAG) pipeline consists of a retriever and a generator,\nboth built on large language models (LLMs) [1] . This pipeline operates on a knowledge corpus _D_,\n\n\n1In many cases, the retriever uses language models smaller than 1B parameters, which may not be considered \u201clarge\u201d, but we use the term LLM for simplicity.\n\n\n3\n\n\nPublished as a conference paper at ICLR 2025\n\n\nFigure 2: TextRAG (left) vs. VisRAG (right). Traditional text-based RAG (TextRAG) relies on\nparsed texts for retrieval and generation, losing visual information in multi-modal documents. Our\nvision-based RAG (VisRAG) employs a VLM-based retriever and generator to directly process the\ndocument page\u2019s image, thereby preserving all information in the original page.\n\n\nwhich is processed into units for retrieval and generation, denoted as _D_ = _{d_ 1 _, . . ., dn}_, where\n_n_ is the number of retrieval units. Given a text query _q_ and the retrieval corpus _D_, the retriever\nfunctions as _R_ : ( _q, D_ ) _\u2192DR_, taking _q_ and _D_ as inputs and producing a candidate set _DR \u2282D_ .\nTo enable efficient search, the units in the knowledge corpus _D_ are pre-encoded into embeddings.\nDuring RAG pipeline inference, approximate nearest neighbor (ANN) search is applied to retrieve\n_DR_, which serves as the knowledge source for generation. The generation process can be defined as\na function _G_ : ( _q, DR_ ) _\u2192_ _a_, where _a_ represents the answer and _G_ denotes the LLM generator. This\nis achieved by prompting the LLM with the query and the retrieved units _DR_ to generate an answer.\n\n\nAs shown in Figure 2 (left), traditional RAG frameworks (TextRAG) typically utilize text-based\nunits for retrieval and generation. However, in real-world scenarios, data often appear in complex,\nmulti-modal documents, requiring an additional parsing step to obtain text. In this paper, we propose\nto use the _page_ as the fundamental unit for retrieval and generation, which is directly processed\nby vision language models (VLMs) as"],"reference":"The purpose of constructing a Retrieval-Augmented Generation Benchmark (RGB) is to evaluate the retrieval-augmented generation of LLMs. It evaluates four specific abilities: Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness."}
{"user_input":"What challenges do language models face when retrieving information from their input contexts in open-domain question answering?","response":"Language models face several challenges when retrieving information from their input contexts in open-domain question answering:\n\n1. **Context Length and Information Position**: As the length of the input context increases, models must process more content, which can negatively impact their accuracy. Specifically, models often perform better when relevant information is located at the beginning or end of an input context, with significant performance degradation when such information is in the middle. This is referred to as a \"primacy bias\" when information is at the start and a \"recency bias\" when it's at the end. Performance can even be worse when retrieving information from the middle compared to not using any context at all (<document id=\"2307.03172_lost_in_middle_liu:chunk_900\">).\n\n2. **Inability to Effectively Utilize All Retrieved Information**: In open-domain settings, the retrieved documents may contain none, one, or multiple relevant answers, but models often do not effectively utilize the additional context from all retrieved documents. For instance, using more retrieved documents yields marginal performance improvements, suggesting that models are not leveraging the full potential of the information available (<document id=\"2307.03172_lost_in_middle_liu:chunk_1800\">).\n\n3. **Irrelevant Information**: The presence of irrelevant information can distract models from the relevant content, making it challenging to focus solely on context that helps answer the question correctly. Models may confidently produce incorrect answers even when relevant context is provided (<document id=\"2411.06037_sufficient_context_joren:chunk_450\">).\n\n4. **Variability in Information Relevance**: The relevance of information can vary widely, creating ambiguity in how models assess what is most pertinent to the question. This can lead to inconsistencies in answering, as different documents may contribute varying degrees of usefulness in terms of addressing the query (<document id=\"2411.06037_sufficient_context_joren:chunk_450\">).\n\nThese challenges highlight the need for advancements in how language models access and process long input contexts, particularly in dynamic information retrieval scenarios.","retrieved_contexts":[" in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f"," Computational Linguistics.\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. _Transactions of the_\n_Association for Computational Linguistics_ .\n\n\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot\nprompting for open-domain question answering.\n_arXiv preprint arXiv:2203.05115_ .\n\n\nHaejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paranjape, Christopher Manning, and Kyoung-Gu Woo.\n[2022. You only need one model for open-domain](https:\/\/aclanthology.org\/2022.emnlp-main.198)\n[question answering. In](https:\/\/aclanthology.org\/2022.emnlp-main.198) _Proceedings of the 2022 Con-_\n_ference on Empirical Methods in Natural Language_\n_Processing_, pages 3047\u20133060, Abu Dhabi, United\nArab Emirates. Association for Computational Linguistics.\n\n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\n[BART: denoising sequence-to-sequence pre-training](https:\/\/doi.org\/10.18653\/v1\/2020.acl-main.703)\n[for natural language generation, translation, and com-](https:\/\/doi.org\/10.18653\/v1\/2020.acl-main.703)\n[prehension. In](https:\/\/doi.org\/10.18653\/v1\/2020.acl-main.703) _Proceedings of the 58th Annual Meet-_\n_ing of the Association for Computational Linguistics,_\n_ACL 2020, Online, July 5-10, 2020_, pages 7871\u20137880.\nAssociation for Computational Linguistics.\n\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpuk"," question via\ncontrolled experiments with a variety of state-ofthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\nAnthropic\u2019s Claude-1.3) language models in settings that require accessing and using information\nwithin an input context. In particular, our experiments make controlled changes to the input context\nsize and the position of the relevant information\nwithin the input context and study their effects on\nlanguage model performance. If language models\ncan robustly use information within long input contexts, then their performance should be _minimally_\n_affected_ by the position of the relevant information\nin the input context.\n\n\nWe first experiment with multi-document question answering, which requires models to reason\n\n- ver provided documents to find relevant information and use it to answer a given question; this task\nmimics the retrieval-augmented generation setup\nunderlying many commercial generative search and\nquestion answering applications (e.g., Bing Chat).\nIn this setting, we control (i) the input context\nlength by changing the number of documents in\nthe input context (akin to retrieving more or less\ndocuments in retrieval-augmented generation), and\n(ii) control the position of the relevant information\nwithin the input context by changing the order of\nthe documents to place the relevant document at\nthe beginning, middle or end of the context.\n\n\nWe find that changing the position of relevant\ninformation in the input context can substantially\naffect model performance, indicating that current\nlanguage models do not robustly access and use\ninformation in long input contexts. Furthermore,\nwe observe a distinctive U-shaped performance\ncurve (Figure 1); language model performance is\nhighest when relevant information occurs at the\nvery beginning (primacy bias) or end of its input context (recency bias), and performance significantly degrades when models must access and\nuse information in the middle of their input context (\u00a72.3). For example, when relevant information is placed in the middle of its input context, GPT-3.5-Turbo\u2019s performance on the multi\n\n\ndocument question task is lower than its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily","2\u201310% for Gemini,\nGPT, and Gemma. Key findings and the prompts used in our autorater analysis are\n[available on our github.](https:\/\/github.com\/hljoren\/sufficientcontext)\n\n\n1 INTRODUCTION\n\n\nProviding Large Language Models (LLMs) with additional context, such as in Retrieval Augmented\nGeneration (RAG) systems, has led to major improvements in LLM factuality and verifiability when\nadapting to new domains (Lewis et al., 2020). In the case of open-domain question answering, a\nretrieval model provides context at inference time in the form of snippets or long-form text (Zhu\net al., 2021). Then, the model synthesizes the query along with this added context to generate the\nanswer. Unfortunately, current RAG-based LLMs exhibit many undesirable traits, such as confidently\npredicting the incorrect answer with retrieved evidence (Mishra et al., 2024; Niu et al., 2024; Ru\net al., 2024), being distracted by unrelated information (Cuconasu et al., 2024; Yoran et al., 2024),\nand failing to properly extract answers from long text snippets (Hsieh et al., 2024; Liu et al., 2024).\n\n\nThe ideal outcome is for the LLM to output the correct answer if the provided context contains\nenough information to answer the question when combined with the model\u2019s parametric knowledge.\nOtherwise, the model should abstain from answering and\/or ask for more information. One core\nchallenge in achieving this ideal outcome is building models that can use the provided context only\nwhen it helps answer the question correctly. Several works have investigated this issue by evaluating\n\n\n_\u2217_ Work done during an internship at Google.\n\n_\u2020_ Work done during an internship at Google.\n\n\n1\n\n\nPublished as a conference paper at ICLR 2025\n\n\nmodels in the presence of irrelevant information in the context (discussed in Section 2). However,\n\u201crelevant information\u201d can range from directly containing the answer to simply being topically related\nto the question. Even \u201cgolden\u201d or oracle documents in datasets vary in how much information they\nprovide about the query, and whether they directly inform the ground truth answer or not. In other\nwords, while the goal seems to be to understand how LLMs behave when they do or do","## **Lost in the Middle: How Language Models Use Long Contexts**\n\n**Nelson F. Liu** [1] _[\u2217]_ **Kevin Lin** [2] **John Hewitt** [1] **Ashwin Paranjape** [3]\n\n**Michele Bevilacqua** [3] **Fabio Petroni** [3] **Percy Liang** [1]\n\n1Stanford University 2University of California, Berkeley 3Samaya AI\n[nfliu@cs.stanford.edu](mailto:nfliu@cs.stanford.edu)\n\n\n\n**Abstract**\n\n\nWhile recent language models have the ability to take long contexts as input, relatively\nlittle is known about how well they _use_\nlonger context. We analyze the performance\n\n   - f language models on two tasks that require\nidentifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that\nperformance can degrade significantly when\nchanging the position of relevant information, indicating that current language models\ndo not robustly make use of information in\nlong input contexts. In particular, we observe\nthat performance is often highest when relevant information occurs at the beginning or\nend of the input context, and significantly\ndegrades when models must access relevant\ninformation in the middle of long contexts,\neven for explicitly long-context models. Our\nanalysis provides a better understanding of\nhow language models use their input context\nand provides new evaluation protocols for\nfuture long-context language models.\n\n\n**1** **Introduction**\n\n\nLanguage models have become an important and\nflexible building block in a variety of user-facing\nlanguage technologies, including conversational\ninterfaces, search and summarization, and collaborative writing (Shuster et al., 2022; Thoppilan et al.,\n2022; Lee et al., 2022, _inter alia_ ). These models\nperform downstream tasks primarily via prompting:\nall relevant task specification and data to process is\nformatted as a textual input context, and the model\nreturns a generated text completion. These input\ncontexts can contain thousands of tokens, especially when language models are used to process\nlong documents (e.g., legal or scientific documents,\nconversation histories, etc.) or when language models are augmented with external information (e.g.,\n\n\n*Work partially completed as an intern at Samaya AI.\n\n\n\n\n|Total|Col2|Ret|rievedDocuments( ~4"],"reference":"Language models struggle to retrieve and use relevant information in the multi-document question answering task, as evidenced by their performance on a synthetic key-value retrieval task. This challenge is particularly pronounced for models that rely on decoder-only architectures or do not employ query-aware contextualization techniques."}
{"user_input":"What is the trade-off between providing more context and its impact on language model performance in open-domain QA?","response":"The trade-off between providing more context and its impact on language model performance in open-domain question answering (QA) involves two main factors: the potential benefit of additional information and the risk of reduced accuracy due to increased reasoning complexity.\n\nProviding longer input contexts can help models perform downstream tasks better by offering more information. However, it also means that the model has to reason over more content, which can decrease its accuracy. This is particularly evident in cases where language models exhibit primacy (better performance with information at the start) and recency biases (better performance with information at the end), leading to a performance drop when relevant information is placed in the middle of an input context. For example, GPT-3.5-Turbo can actually perform worse when presented with a longer context containing relevant information in the middle than when it has no context at all (closed-book performance) <document id=\"2307.03172_lost_in_middle_liu:chunk_900\">.\n\nExperimental findings suggest that although model performance may saturate with longer contexts (e.g., adding more retrieved documents beyond a certain point yields minimal gains), the inaccuracies from having to navigate a more complex reasoning task can undermine the benefits of additional information. Thus, current models may not effectively utilize the potential richness of longer contexts in open-domain QA <document id=\"2307.03172_lost_in_middle_liu:chunk_1800\">.","retrieved_contexts":[" in the input context.\n\n\nOur results indicate that prompting language\n\n\nmodels with longer input contexts is a trade-off\u2014\nproviding the language model with more information may help it perform the downstream task, but\nit also increases the amount of content that the\n\nmodel must reason over, potentially decreasing accuracy. To better understand this trade-off in practice, we perform a case study with retriever-reader\nmodels on open-domain question answering (\u00a75).\nIn contrast to our controlled multi-document QA\ntask, where the context always contains exactly\n\n_one_ document that answers the question, none or\nmany of the top _k_ documents may contain the answer in the open-domain QA setting. When retrieving from Wikipedia to answer queries from\nNaturalQuestions-Open, we find that model performance saturates long before retriever recall saturates, indicating that current models fail to effectively use additional retrieved documents\u2014using\n50 documents instead of 20 retrieved documents\n\n- nly marginally improves performance ( _\u223c_ 1.5% for\nGPT-3.5-Turbo and _\u223c_ 1% for claude-1.3).\nOur analysis provides a better understanding of\nhow language models use their input context and\nintroduces new evaluation protocols for future longcontext models; to claim that a language model can\nrobustly use information within long input contexts, it is necessary to show that its performance\nis minimally affected by the position of the relevant information in the input context (e.g., minimal\ndifference in best- and worst-case performance).\nTo facilitate further work on understanding and\nimproving how language models use their input\ncontext, we release our code and evaluation data. [1]\n\n\n**2** **Multi-Document Question Answering**\n\n\nOur goal is to better understand how language models use their input context. To this end, we analyze\nmodel performance on multi-document question\nanswering, which requires models to find relevant\ninformation within an input context and use it to\nanswer the question. In particular, we make controlled changes to the length of the input context\nand the position of the relevant information and\nmeasure changes in task performance.\n\n\n**2.1** **Experimental Setup**\n\n\nIn the multi-document question answering task, the\nmodel inputs are (i) a question to answer and (ii) _k_\ndocuments (e.g., passages from Wikipedia), where\n_exactly one_ - f","5-Turbo 56.1% 88.3%\n\nGPT-3.5-Turbo (16K) 56.0% 88.6%\n\nClaude-1.3 48.3% 76.1%\n\nClaude-1.3 (100K) 48.2% 76.4%\n\n\nTable 1: Closed-book and oracle accuracy of language\nmodels on the multi-document question answering task.\n\n\nshaped performance curve\u2014models are often much\nbetter at using relevant information that occurs at\nthe very beginning (primacy bias) and very end of\ncontexts (recency bias), and suffer degraded performance when forced to use information within the\n\nmiddle of its input context. For example, GPT-3.5Turbo\u2019s multi-document QA performance can drop\nby more than 20%\u2014in the worst case, performance\nin 20- and 30-document settings is lower than performance without _any_ input documents (i.e., closedbook performance; 56.1%). These results indicate\nthat current models cannot effectively reason over\ntheir entire context window when prompted for\ndownstream tasks.\n\n\n**Extended-context models are not necessarily bet-**\n**ter at using input context.** When the input context fits in the context window of both a model\nand its extended-context counterpart, we see that\nperformance between them is nearly identical. For\nexample, the 10- and 20-document settings both\nfit in the context window of GPT-3.5-Turbo and\nGPT-3.5-Turbo (16K), and we observe that their\nperformance as a function of position of relative\ninformation is nearly superimposed (solid purple\nand dashed brown series in Figure 5). These results\n\n\nInput Context\n\n\nExtract the value corresponding to the specified key in the JSON object below.\n\n\nJSON data:\n\n{\"2a8d601d-1d69-4e64-9f90-8ad825a74195\": \"bb3ba2a5-7de8-434b-a86e-a88bb9fa7289\",\n\n\"a54e2eed-e625-4570-9f74-3624e77d6684\": \"d1ff29be-4e2a-4208-a182-0cea716be3d4\",\n\n\" **9f4a92b9","## **Lost in the Middle: How Language Models Use Long Contexts**\n\n**Nelson F. Liu** [1] _[\u2217]_ **Kevin Lin** [2] **John Hewitt** [1] **Ashwin Paranjape** [3]\n\n**Michele Bevilacqua** [3] **Fabio Petroni** [3] **Percy Liang** [1]\n\n1Stanford University 2University of California, Berkeley 3Samaya AI\n[nfliu@cs.stanford.edu](mailto:nfliu@cs.stanford.edu)\n\n\n\n**Abstract**\n\n\nWhile recent language models have the ability to take long contexts as input, relatively\nlittle is known about how well they _use_\nlonger context. We analyze the performance\n\n   - f language models on two tasks that require\nidentifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that\nperformance can degrade significantly when\nchanging the position of relevant information, indicating that current language models\ndo not robustly make use of information in\nlong input contexts. In particular, we observe\nthat performance is often highest when relevant information occurs at the beginning or\nend of the input context, and significantly\ndegrades when models must access relevant\ninformation in the middle of long contexts,\neven for explicitly long-context models. Our\nanalysis provides a better understanding of\nhow language models use their input context\nand provides new evaluation protocols for\nfuture long-context language models.\n\n\n**1** **Introduction**\n\n\nLanguage models have become an important and\nflexible building block in a variety of user-facing\nlanguage technologies, including conversational\ninterfaces, search and summarization, and collaborative writing (Shuster et al., 2022; Thoppilan et al.,\n2022; Lee et al., 2022, _inter alia_ ). These models\nperform downstream tasks primarily via prompting:\nall relevant task specification and data to process is\nformatted as a textual input context, and the model\nreturns a generated text completion. These input\ncontexts can contain thousands of tokens, especially when language models are used to process\nlong documents (e.g., legal or scientific documents,\nconversation histories, etc.) or when language models are augmented with external information (e.g.,\n\n\n*Work partially completed as an intern at Samaya AI.\n\n\n\n\n|Total|Col2|Ret|rievedDocuments( ~4","T:\n\n\n1. improves both retrieval and few-shot QA performance on several multi-step open-domain\nQA datasets, in both IID and OOD settings;\n2. reduces factual errors in generated CoTs; and\n3. improves performance with both large-scale\n(175B models) as well as smaller-scale models (Flan-T5-*, _\u2264_ 11B) without any training.\n\n\n**2** **Related Work**\n\n\n**Prompting for Open-Domain QA.** LLMs can\nlearn various tasks by simply using a few examples as prompts (Brown et al., 2020). They\u2019ve\nalso been shown to answer complex questions\nby producing step-by-step reasoning (chain-ofthoughts, or CoT) when prompted with a few or\nzero demonstrations (Wei et al., 2022; Kojima et al.,\n2022). Prompting has been applied to open-domain\nQA (Lazaridou et al., 2022; Sun et al., 2022; Yu\net al., 2023) but its value in improving retrieval and\nQA for multi-step open-domain questions remains\nrelatively underexplored.\n\n\n3We explain later (in the Metric section and Footnote 7)\nthe appropriateness of this metric in our setting as opposed to\nmore mainstream information recall metrics.\n\n\nRecently three approaches have been proposed\nfor multi-step open-domain QA. SelfAsk (Press\net al., 2022) prompts LLMs to decompose a question into subquestions and answers subquestions by\na call to Google Search API. DecomP (Khot et al.,\n2023) is a general framework that decomposes a\ntask and delegates sub-tasks to appropriate submodels. They also decompose questions but delegate retrieval to a BM25-based retriever. Both of\nthese approaches are not developed for CoT reasoning, do not focus on the retrieval problem, and require a single-hop QA model to answer the decomposed questions. Recently proposed ReAct (Yao\net al., 2022) system frames the problem as generating a sequence of reasoning and action steps. These\nsteps are much more complex, rely on much larger\nmodels (PaLM-540B), and require fine-tuning to\n\n- utperform CoT for multi-step ODQA. Furthermore, none of these works have been shown to be\neffective for smaller models without any training.\nWhile a direct comparison"," question via\ncontrolled experiments with a variety of state-ofthe-art open (MPT-30B-Instruct, LongChat-13B\n(16K)) and closed (OpenAI\u2019s GPT-3.5-Turbo and\nAnthropic\u2019s Claude-1.3) language models in settings that require accessing and using information\nwithin an input context. In particular, our experiments make controlled changes to the input context\nsize and the position of the relevant information\nwithin the input context and study their effects on\nlanguage model performance. If language models\ncan robustly use information within long input contexts, then their performance should be _minimally_\n_affected_ by the position of the relevant information\nin the input context.\n\n\nWe first experiment with multi-document question answering, which requires models to reason\n\n- ver provided documents to find relevant information and use it to answer a given question; this task\nmimics the retrieval-augmented generation setup\nunderlying many commercial generative search and\nquestion answering applications (e.g., Bing Chat).\nIn this setting, we control (i) the input context\nlength by changing the number of documents in\nthe input context (akin to retrieving more or less\ndocuments in retrieval-augmented generation), and\n(ii) control the position of the relevant information\nwithin the input context by changing the order of\nthe documents to place the relevant document at\nthe beginning, middle or end of the context.\n\n\nWe find that changing the position of relevant\ninformation in the input context can substantially\naffect model performance, indicating that current\nlanguage models do not robustly access and use\ninformation in long input contexts. Furthermore,\nwe observe a distinctive U-shaped performance\ncurve (Figure 1); language model performance is\nhighest when relevant information occurs at the\nvery beginning (primacy bias) or end of its input context (recency bias), and performance significantly degrades when models must access and\nuse information in the middle of their input context (\u00a72.3). For example, when relevant information is placed in the middle of its input context, GPT-3.5-Turbo\u2019s performance on the multi\n\n\ndocument question task is lower than its performance when predicting _without any documents_ (i.e.,\nthe closed-book setting; 56.1%). Furthermore, we\nfind that models often have identical performance\nto their extended-context counterparts, indicating\nthat extended-context models are not necessarily"],"reference":"The trade-off between providing more context and its impact on language model performance in open-domain QA is that while longer input contexts may help improve downstream task performance, they also increase the amount of content the model must reason over, potentially decreasing accuracy. The study found that reader model performance saturates long before retriever performance saturates, indicating that readers are not effectively using the extra context. Using more than 20 retrieved documents only marginally improves reader performance (\u223c1.5% for GPT-3.5-Turbo and \u223c1% for Claude-1.3), while significantly increasing the input context length (and thus latency and cost)."}
{"user_input":"What model did ARES use as its in-context learning baseline?","response":"ARES used a combination of a DPR retriever and a BART sequence-to-sequence model as its in-context learning baseline <document id=\"2311.09476_ares_saad_falcon:chunk_7650\">.","retrieved_contexts":[" Gemma has\nmuch more hallucinations (higher **red bars** ) than the other models, except for HotPotQA, where we\nattribute the higher accuracy to the smaller retrieved contexts.\n\n\n4.3 QUALITATIVELY ANALYZING RESPONSES WITH INSUFFICIENT CONTEXT\n\n\nOne curious observation in our analysis is the ability of models to sometimes provide correct answers\neven when presented with insufficient context. For example, from Figure 3, all three models are able\nto correctly answer upwards of 35% of instances with insufficient context on HotpotQA. A natural\nassumption is that the models already know the answer from pre-training, and they can generate a\ncorrect response from parametric memory. However, this only explains part of the story.\n\n\nLooking deeper, we provide a qualitative categorization in Table 2 of instance types where our\nautorater labels an instance as insufficient context, while the LLM evaluator marks the model answer\n\n\n7\n\n\nPublished as a conference paper at ICLR 2025\n\n\n**Instance type** **Why model may be correct** **Example**\n\n\nYes\/No question 50% chance of correct **Q:** Is there a total eclipse in the United States this year?\n\n\nLimited choice Some chance of correct **Q:** Which band has more members, Chvrches or\nGoodbye Mr. Mackenzie?\n\n\n\nMulti-hop: fragment Use parametric inference\n\n\nMulti-hop: partial Use parametric knowledge\n\n\nToo many hops Execute complex reasoning\n\n\nAmbiguous query Guess right interpretation\n\n\n\n**Q:** Who did the original voice for the character\nwhose series Mickey\u2019s Safari in Letterland is from?\n_Context says Mickey\u2019s Safari is a video game_\n_and Walt Disney voices Mickey Mouse in cartoons._\n_Must infer the game is in the Mickey Mouse series._\n\n\n**Q:** Claudine\u2019s Return starred the actress who played\nwhich role on \u201cMarried...with Children\u201d?\n\n_Context lists actresses but not their roles in_\n_\u201cMarried...with Children\u201d. Must know extra facts._\n\n\n**Q:** How many cyclists have won all three of women\u2019s\ncycling Grand Tours equivalents in the same year?\n_Context requires cross-referencing lists of events_\n_and lists of winners while tracking winners by year._\n\n\n**Q:** Who is the spouse of a cast member from King\n\n- f the Mountain?\n\n_Context has many cast members and query\/context do_\n_not specify which spouse to answer about._\n\n\n\nRater error Mislabel insuff. or correct","h Hajishirzi, and Daniel Khashabi. 2022.\nWhen not to trust language models: Investigating\neffectiveness and limitations of parametric and nonparametric memories. _arXiv preprint_ .\n\n\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy CampbellGillingham, Geoffrey Irving, et al. 2022. Teaching\nlanguage models to support answers with verified\nquotes. _arXiv preprint arXiv:2203.11147_ .\n\n\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle[moyer. 2022. Rethinking the role of demonstrations:](https:\/\/aclanthology.org\/2022.emnlp-main.759)\n[What makes in-context learning work? In](https:\/\/aclanthology.org\/2022.emnlp-main.759) _Proceed-_\n_ings of the 2022 Conference on Empirical Methods in_\n_Natural Language Processing_, pages 11048\u201311064,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\n\n\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering ambiguous open-domain questions. In _EMNLP_ .\n\n\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instructions with human feedback. _Advances in Neural_\n_Information Processing Systems_, 35:27730\u201327744.\n\n\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. _arXiv preprint arXiv:2210.03350_ .\n\n\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, X"," predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach. We also find that ARES consistently\ndistinguishes competitive RAG systems that are\n\n- nly a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop\n\n\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\n[available on Github.](https:\/\/github.com\/stanford-futuredata\/ARES)\n\n\n**2** **Related Work**\n\n\nRAG (Guu et al., 2020; Lewis et al., 2020; Khattab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combining them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual information (Shuster et al., 2021; Huo et al., 2023), and\n\n- ffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essential for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\n\n- ut of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. However, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min","\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-tosequence model (Lewis et al., 2019). During retrieval, each RAG system only retrieves one passage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applications, averaging a Kendall\u2019s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com\npared to RAGAS, ARES is 0.16 higher for context\nrelevance and 0.15 higher for answer relevance, on\naverage. ARES also provided accurate confidence\nbounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on average, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n\n\n**5.4** **Strengths and Limits of Cross-Domain**\n**Applications**\n\n\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are difficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in _query type_ from\ntraining to test (e.g. NQ to FEVER), change in\n_document type_ from training to test (e.g. NQ to\nMultiRC), and change in both _query and document_\n_type_ (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in crossdomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generalizability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Furthermore, we found that even when the LLM judge\u2019s accuracy suffered in cross-domain applications, PPI\nhelped"," Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReAct: Synergizing reasoning and acting in language\nmodels. In _International Conference on Learning_\n_Representations (ICLR)_ .\n\n\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In _International Confer-_\n_ence for Learning Representation (ICLR)_ .\n\n\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. 2023a. Tempera:\nTest-time prompt editing via reinforcement learning.\nIn _The Eleventh International Conference on Learn-_\n_ing Representations_ .\n\n\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023b. Automatic chain of thought prompting in large language models. In _The Eleventh In-_\n_ternational Conference on Learning Representations_\n_(ICLR 2023)_ .\n\n\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. _arXiv_\n_preprint arXiv:1909.08593_ .\n\n\n**A** **Warm-up Dataset**\n\n\nFor the warm-up training of the tuneable rewriter,\nwe construct a pseudo dataset for the query rewriting task. For benchmarks that provide official training and test splits (HotpotQA and AmbigNQ), we\nuse the whole training set. For those that have no\n\n- fficial splits (PopQA and MMLU), we randomly\nsplit the full dataset. In detail, PopQA contains 16\ntypes of questions, thus split into 13k for training\nand 714 for testing following stratified sampling.\nFor MMLU, each of the 4 categories is randomly\nsplit into 80% for the training set and 20% for\nthe test set. Then the training sets of each benchmark are used to derive the pseudo dataset for the\nquery rewriting, i.e"],"reference":"ARES used OpenAI\u2019s gpt-3.5-turbo-16k, version 10\/23, (Brown et al., 2020) as its in-context learning baseline."}
{"user_input":"What are some limitations of using FLAN-T5-XXL in the ARES evaluation framework?","response":"Some limitations of using FLAN-T5-XXL in the ARES evaluation framework include:\n\n1. **Relative Performance**: FLAN-T5-XXL exhibited lower performance in various settings compared to other models like GPT-3. For instance, while IRCoT (Interleaved Retrieval with Chain-of-Thought) significantly improved recall with FLAN-T5-XXL, the overall performance is still less competitive compared to some newer models (e.g., DSP outperformed IRCoT on certain benchmarks) <document id=\"2212.10509_cot_retrieval_trivedi:chunk_15750\">.\n\n2. **Generalization Issues**: Although FLAN-T5-XXL shows improvement through IRCoT in out-of-distribution settings, it remains sensitive to the prompt types used, indicating that direct prompting might suit it better than chain-of-thought prompting compared to other models <document id=\"2212.10509_cot_retrieval_trivedi:chunk_15750\">.\n\n3. **Factual Errors**: There is an indication that while IRCoT can improve the generation of outputs, FLAN-T5-XXL may still generate chain-of-thought responses with more factual errors compared to other models <document id=\"2212.10509_cot_retrieval_trivedi:chunk_5850\">.\n\n4. **Complexity and Limitations of Knowledge Sources**: The nature of the knowledge leveraged by FLAN-T5-XXL may limit its effectiveness, particularly in cases where relevant information is already embedded within other models like GPT-3, which could lead to diminished utility in certain tasks <document id=\"2212.10509_cot_retrieval_trivedi:chunk_5400\">.","retrieved_contexts":[" knowledge may already be present in GPT3, as also evidenced by its NoR QA score being similar. For\n\n- ther datasets and model combinations, NoR QA is\n\n\n\nmuch worse than IRCoT QA, indicating the limits\n\n- f the models\u2019 parametric knowledge.\n\n\n**IRCoT is effective in OOD setting.** Since CoT\nmay not always be easy to write for new datasets,\nwe evaluate NoR, OneR, and IRCoT on generalization to new datasets, i.e. OOD setting. To do so,\nwe use prompt demonstrations from one dataset to\nevaluate on another dataset. [9] For all pairs of the\ndatasets [10] and for both Flan-T5-XXL and GPT3, we\nfind the same trend as in the IID setting: IRCoT retrieval outperforms OneR (Fig. 5), and IRCoT QA\n\n- utperforms both OneR QA and NoR QA (Fig. 6).\n\n\n**IRCoT generates CoT with fewer factual errors.**\nTo assess whether our approach also improves the\nfactuality of generated CoTs, we manually annotated CoTs generated by NoR QA, OneR QA, and\nIRCoT QA using GPT3 for 40 randomly sampled\nquestions from each of the four datasets. We considered CoT to have a factual error if at least one\n\n\n9We use the evaluation dataset\u2019s corpus for retrieval.\n10We skip IIRC in this exploration as the task is structured\na bit differently and requires special handling (see App. B).\n\n\nFigure 5: Retrieval recall for OneR and IRCoT using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n||||||||||||\n"," 1.3 **60.7** _\u00b1_ **1.1** **68.0** _\u00b1_ **1.5** **36.5** _\u00b1_ **1.2** **49.9** _\u00b1_ **1.1**\n\n\nTable 4: Answer F1 for different ODQA models made from NoR, One and IRCoT retrievals, and Direct and\nCoT prompting readers. For Flan-T5-XXL, Direct prompting is a better choice for the reader, and for GPT3, CoT\nprompting is a better choice for the reader. Hence, we make different reader choices for Flan-T5 and GPT3 for the\nexperiments in the main paper. Note that IRCoT QA _>_ OneR QA _>_ ZeroR QA holds up regardless of this choice.\n\n\n\nDSP (Khattab et al., 2023) provides a way to programmatically define interactions between LLM\nand retrieval for ODQA (e.g., via question decomposition), bootstrap demonstrations for such a program, and use them to make the answer prediction.\nIt uses GPT3.5 LLM with ColBERT-based retrieval.\n\nSince most of these methods use different knowl\nedge sources or APIs and are built using different\nLLMs and retrieval models, it\u2019s difficult to make a\nfair scientific comparison across these systems. Additionally, the evaluations in the respective papers\nare on different random subsets (from the same\ndistribution) of test instances.\n\n\nDespite these differences, it is still informative to\nexplore, in a leaderboard-style fashion, how IRCoT\nperforms relative to the best numbers published\nfor these recent systems. Table 3 shows results\nfrom different systems, including contemporane\n- us and newer numbers. The two new systems in\nthis table (relative to Table 1) are DecomP (newer\nversion) and DSP. While IRCoT remains SOTA on\nMuSiQue, DSP outperforms it on HotpotQA by 2.0\npoints and the newer version of Decomp outperforms IRCoT on 2WikiMultihopQA by 2.8 points.\n\n\n\nWe speculate DecomP performs well on 2WikiMultihopQA because it has only a few easy-to-predict\ndecomposition patterns, which DecomP\u2019s question\ndecomposition can leverage. The lack of such patterns in Hotpot","- ther languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\n\n\n**References**\n\n\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. _[Proceedings of](https:\/\/aclanthology.org\/2023.fever-1.0)_\n_[the Sixth Fact Extraction and VERification Workshop](https:\/\/aclanthology.org\/2023.fever-1.0)_\n_[(FEVER)](https:\/\/aclanthology.org\/2023.fever-1.0)_ . Association for Computational Linguistics,\nDubrovnik, Croatia.\n\n\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\n[Prediction-powered inference.](http:\/\/arxiv.org\/abs\/2301.09633)\n\n\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\n\n\n\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n[2020. Language models are few-shot learners.](http:\/\/arxiv.org\/abs\/2005.14165)\n\n\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. _arXiv preprint_\n_arXiv:2309.01431_ .\n\n\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Z","|||\n|||||||||||\n|||||||||||\n|||||||||||\n|||||||||||\n\n\n\nFigure 4: Answer F1 for ODQA model made using (i) no retriever (NoR QA) (ii) one-step retriever (OneR QA) and\n(iii) IRCoT QA instantiated from Flan-T5-XXL (left) and GPT3 (right) models. IRCoT QA outperforms OneR QA\nand NoR QA for both models on all datasets, except for GPT3 on IIRC.\n\n\n\nFlan-T5-XXL and GPT3 LMs. For both models,\nIRCoT significantly outperforms one-step retrieval\nacross all datasets. For Flan-T5-XXL, IRCoT improves our recall metric relative to one-step retrieval, on HotpotQA by 7.9, on 2WikiMultihopQA\nby 14.3, on MuSiQue by 3.5, and on IIRC by 10.2\npoints. For GPT3, this improvement is by 11.3, 22.6,\n12.5, and 21.2 points, respectively.\n\n\n**IRCoT QA outperforms NoR and OneR QA.**\nFig. 4 compares ODQA performance using\nNoR, OneR and IRCoT retriever made from\n\nFlan-T5-XXL and GPT3 LMs. For Flan-T5-XXL,\nIRCoT QA outperforms OneR QA on HotpotQA\nby 9.4, on 2WikiMultihopQA by 15.3, on MuSiQue\nby 5.0 and IIRC by 2.5 F1 points. For GPT3, the\ncorresponding numbers (except for IIRC) are 7.1,\n13.2, and 7.1 F1 points. For GPT3, IRCoT doesn\u2019t\nimprove the QA score on IIRC, despite significantly improved retrieval (21 points as shown in\nFig. 3). This is likely because IIRC relevant knowledge may already be present in GPT3, as also evidenced by its NoR QA score being similar. For\n\n- ther datasets and model combinations, NoR QA is\n\n\n\nmuch worse than IRCoT QA, indicating the limits\n\n- f"," evaluation system.\n\n\n**5** **Results & Analysis**\n\n\n**5.1** **ARES Ranking**\n\n\nTable 1 summarizes our main evaluation of ARES\n\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges). We compare against RAGAS (version 0.0.18) and a baseline few-shot prompted GPT3.5 judge ( _gpt-3.5-turbo-16k_ ). For the few-shot\nGPT-3.5 judge, we provide few-shot examples for\nguiding predictions; the prompts are included in\nAppendices A.2, A.3, and A.4. For both ARES\nand the GPT-3.5 judge baseline, we augment the\nLLM with PPI, using a 300-datapoint human preference validation set to rectify the ML predictions\nand produce confidence intervals.\n\n\n\nAcross almost all settings across the datasets\nfrom KILT and SuperGLUE, ARES provides a\nmore accurate ranking of RAG systems than RAGAS. ARES averages a Kendall\u2019s _\u03c4 0.065 higher_\n_for context relevance_ and _0.132 higher for answer_\n_relevance than RAGAS_ . Additionally, the LLMjudge is substantially more accurate than RAGAS\nat predicting context relevance and answer relevance of a query-passage-answer triple. For context relevance, ARES with a fine-tuned LLM-judge\nis _59.9 percentage points higher than RAGAS_ while\nfor answer relevance, our system is _14.4 percent-_\n_age points higher than RAGAS_ . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompting and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall\u2019s _\u03c4_ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG system, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall\u2019s _\u03c4_ for ARES is\n0.08 higher"],"reference":"The use of FLAN-T5-XXL in the ARES evaluation framework has several limitations. It requires substantial GPU-based hardware with about 32GB of memory to run, taking several hours for fine-tuning and generation. Additionally, all of the datasets used in the evaluation are in English, which may limit its applicability to other languages."}
{"user_input":"What figure shows the overall performance of FLARE and baseline methods across all tasks\/datasets?","response":"I don't have enough information in the provided materials to answer this question.","retrieved_contexts":[".0 39.9 27.9 **34.3** 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\n\nFLARE (ours) **77.3** **41.3 28.2 34.3 31.1** **46.2 36.7 37.7 37.2** **53.4** **18.9 27.6**\n\n\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is\nDisambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\n\n\n\n**2WikiMultihopQA** **ASQA-hint**\n**EM** **F** 1 **Prec. Rec.** **EM D-F** 1 **R-L DR**\n\n\nPrevious 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\n\nNext 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\n\n\nTable 3: A head-to-head comparison between using the\nprevious sentence and the next sentence for retrieval.\n\n\n**#Tokens** **EM** **F** 1 **Prec.** **Rec.**\n\n\n16 43.2 52.3 51.7 54.5\n\n32 43.6 52.4 52.0 55.0\n\n48 40.0 49.3 49.0 52.0\n\nAll 39.0 48.5 48.2 51.1\n\n\nTable 4: Previous-window approaches using different\nnumbers of tokens as queries.\n\n\n- us window underperforms single-time retrieval\n\n- n ASQA, which we hypothesize is because the\nprevious window does not accurately reflect future\nintent. Since we focus on evaluating factuality, metrics with an emphasis on factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n\n\n**6.2** **Ablation Study**\n\n\n**Importance of forward-looking","6.1** **Limitations of evaluation approach**\n\n\nOur evaluation to date has focused on sensemaking questions specific to two corpora each containing\napproximately 1 million tokens. More work is needed to understand how performance generalizes to\ndatasets from various domains with different use cases. Comparison of fabrication rates, e.g., using\napproaches like SelfCheckGPT (Manakul et al., 2023), would also strengthen the current analysis.\n\n\n11\n\n\nTable 4: Average number of clusters across different distance thresholds, reported by condition and\ndataset type. Bolded values represent the highest score in each row.\n\n\n**Average Number of Clusters**\n**Dataset** **Distance Threshold**\n\n**C0** **C1** **C2** **C3** **TS** **SS**\n\n\n\nNews Articles\n\n\nPodcast Transcripts\n\n\n**6.2** **Future work**\n\n\n\n0.5 **23.42** 21.85 21.90 22.13 21.80 17.92\n\n0.6 **21.65** 20.38 20.30 20.52 20.13 16.78\n\n0.7 **20.19** 19.06 19.03 19.13 18.62 15.80\n\n0.8 **18.86** 17.78 17.82 17.79 17.30 14.80\n\n\n0.5 **23.16** 22.62 22.52 21.93 21.14 18.55\n\n0.6 **21.65** 21.33 21.21 20.62 19.70 17.39\n\n0.7 **20.41** 20.04 19.79 19.22 18.08 16.28\n\n0.8 **19.26** 18.77 18.46 17.89 16.66 15.07\n\n\n\nThe graph index, rich text annotations, and hierarchical community structure supporting the current\nGraphRAG approach offer many possibilities for refinement and adaptation. This includes RAG\napproaches that operate in a more local manner, via embedding-based matching of user queries and\ngraph annotations. In particular, we see potential in hybrid RAG schemes that combine embeddingbased matching with just-in-time community report generation before employing our map-reduce\nsummar"," study implicit query formation by\nmasking and explicit query formulation through\nquestion generation. In Table 5, we compare the\nperformance of FLARE with different masking\n\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n\n\n2WikiMultihopQA StrategyQA\n\n\n0.0 25.0 50.0 75.0 100.0\n\n\n%steps\/sentences with retrieval\n\n\n_\u03b2_ **EM** **F** 1 **Prec.** **Rec.**\n\n\n0.0 0.488 0.576 0.571 0.605\n\n0.2 0.498 0.588 0.582 0.616\n\n0.4 0.510 0.597 0.591 0.627\n\n0.6 0.506 0.593 0.586 0.622\n\n\nTable 5: Performance of FLARE with respect to the\nmasking threshold _\u03b2_ - n 2WikiMultihopQA.\n\n\n**ASQA-hint** **WikiAsp**\n**EM D-F** 1 **R-L DR** **UniEval E-F** 1 **R-L**\n\n\nImplicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7\nExplicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6\n\n\nTable 6: A comparison between implicit and explicit\nquery formulation methods in FLARE.\n\n\nthresholds _\u03b2_ . Retrieving directly with the complete\nsentence ( _\u03b2_ = 0) is worse than masking tokens\nwith low probabilities, confirming our hypothesis\nthat low-confidence erroneous tokens can distract\nretrievers. We compare implicit and explicit query\nformulation methods in Table 6. Performances of\n\nboth methods are similar, indicating that both meth\n- ds can effectively reflect information needs.\n\n\n**7** **Related Work**\n\n\nWe refer to subsection 2.2 and section 4 for ex\ntensively discussion on single-time and multi-time\nretrieval augmented LMs, which is the most relevant area to this paper.\n\n\n**Iterative and adaptive retrieval** Iterative retrieval and refinement has been studied in both\ntext and code generation tasks (Peng et al., 202"," engine, following Luo et al. (2023);\nfor ASQA, we use the author-provided top 5 documents by GTR-XXL (Ni et al., 2022) across all\nbaselines for a fair comparison.\n\n\n[5https:\/\/github.com\/princeton-nlp\/ALCE](https:\/\/github.com\/princeton-nlp\/ALCE)\n6We report numbers using the results reported in the paper as the implementations are not available.\n\n\n7\n\n\nPreprint.\n\n\nTable 2: Overall experiment results on six tasks. **Bold** numbers indicate the best performance among\nnon-proprietary models, and **gray-colored** bold text indicates the best proprietary model when\nthey outperforms all non-proprietary models. _[\u2217]_ indicates concurrent or recent results reported by\nconcurrent work. \u2013 indicates numbers that are not reported by the original papers or are not applicable.\nModels are sorted based on scale. FS, em, rg, mau, prec, rec denote FactScore (factuality); str-em,\nrouge (correctness); MAUVE (fluency); citation precision and recall, respectively.\n\n\nShort-form Closed-set Long-form generations (with citations)\nPopQA TQA Pub ARC Bio ASQA\nLM (acc) (acc) (acc) (acc) (FS) (em) (rg) (mau) (pre) (rec)\n\n\n_LMs with proprietary data_\nLlama2-c13B 20.0 59.3 49.4 38.4 55.9 22.4 29.6 28.6  -  Ret-Llama2-c13B 51.8 59.8 52.1 37.9 79.9 32.8 34.8 43.8 19.8 36.1\nChatGPT 29.3 **74.3** 70.1 **75.3** 71.8 35.3 36.2 68.8  -  \nRet-ChatGPT 50.8 65.7 54.7 **75.3**  - **40.7** **39.9** **79.7** 65.1 **76.6**\nPerplexity.ai  -  -  -  - 71.2  -  -  -  -  \n\n_Baselines without retrieval_\n\n","Que,\nsignificantly beyond all baseline methods.\n\n\n**5.3** **Further Analysis**\n\n\nWe report further analysis in StrategyQA with randomly selected 100 samples \u2013 we have similar findings in other datasets.\n\n\n**Ablation Study.** To validate the effectiveness of\n\n- ur proposed framework, we conduct an ablation\nanalysis of its key design elements. We design\nfour variants: (1) _w\/o Retrieval_ removes the retrieved documents in reward modeling; (2) _w\/o_\n_Refine_ does not refine the conflict answer with retrieved documents in reward modeling; (3) _w\/o_\n_Query Reward_ removes the query-aware reward _rq_\nfor scoring; and (4) _w\/o Answer Reward_ removes\nthe answer-aware reward _ra_ for scoring. We show\nthe results in Table 2. It is clear that all the vari\n\n\nFigure 2: Cover EM performance on the StrategyQA\n_w.r.t._ the number of simulations ( **Left** ) or the number of\ntraining data ( **Right** ).\n\n\nants perform worse than the original method, indicating the effectiveness of each component in\n\n- ur framework. Specifically, the performance of\n_w\/o Retrieval_ drops significantly for Llama-3.1-8B,\nindicating that using external knowledge for verification can be highly beneficial for the inherent\nreasoning of LLMs. Similarly, _w\/o Refine_ leads to a\ndecline in model performance, which highlights the\nimportance of repurposing external sources for correcting the errors in the model\u2019s reasoning process.\nMoreover, both _w\/o Query Reward_ and _w\/o Answer_\n_Reward_ variants lead to a substantial performance\ndecline, which suggests that the consistency and\nlogical plausibility of intermediate sub-queries and\nanswers are both critical for the model to plan the\ncorrect path towards the final answer.\n\n\n**Effect of Simulation Scaling.** Typically, scaling the simulation iterations will lead to a higher\n\n\n|Col1|Col2|\u2026|\n|---|---|---|\n|Life Hits is a 2006<br>drama film directe<br>by** Christian**\u2026|<br>  d|<br>  d|\n\n\n\nFigure 3: A qualitative example showing the deliberative reasoning process of RAG-Star in 2WikiMultihopQA.\n\n\n\nlevel of task-solving capability. To explore the relationship between simulation scaling and the"],"reference":"Figure 4 shows the overall performance of FLARE and baseline methods across all tasks\/datasets, demonstrating that FLARE outperforms all baselines on all tasks\/datasets."}
{"user_input":"What is GraphRAG and how does it differ from vector RAG in terms of its approach to knowledge graph extraction?","response":"**GraphRAG** is a variation of the traditional Retrieval-Augmented Generation (RAG) approach that focuses on knowledge graph extraction and uses graph-based community detection for thematic partitioning of data. Unlike conventional vector RAG, which relies on text embeddings to retrieve semantically similar records, GraphRAG builds a graph index from source data to enable global sensemaking over large datasets.\n\nKey differences between GraphRAG and vector RAG include:\n\n1. **Graph Indexing**: GraphRAG creates a graph index and utilizes community detection algorithms to partition the data into thematic communities, allowing for hierarchical summarization. This contrasts with vector RAG, which retrieves records based solely on their semantic similarity to the query without a structural perspective (<document 2404.16130_graphrag_edge:chunk_1800>).\n\n2. **Global Sensemaking**: GraphRAG is capable of answering queries that require understanding of the entire context of the data corpus, leveraging summaries from the hierarchical community structure. In contrast, vector RAG typically focuses on generating responses based on directly retrieved records, which may limit its ability to synthesize insights from the entire dataset (<document 2404.16130_graphrag_edge:chunk_2250>).\n\n3. **Community Summaries**: GraphRAG employs community summaries that can be independently useful and help users grasp overarching themes without specific queries. Vector RAG does not offer this hierarchical summary feature (<document 2404.16130_graphrag_edge:chunk_4050>).\n\nOverall, GraphRAG aims to enhance the RAG framework by exploiting the modularity of graphs for more complex and nuanced understanding in informational tasks.","retrieved_contexts":["2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao et al., 2023; Khattab et al., 2022; Shao et al., 2023; Su et al., 2020; Trivedi et al., 2022;\nWang et al., 2024). In particular, GraphRAG is similar to other approaches that use hierarchical\nindexing to create summaries (similar to Kim et al. 2023; Sarthi et al. 2024). GraphRAG contrasts\nwith these approaches by generating a graph index from the source data, then applying graph-based\ncommunity detection to create a thematic partitioning of the data.\n\n\n**2.2** **Using Knowledge Graphs with LLMs and RAG**\n\n\nApproaches to knowledge graph extraction from natural language text corpora include rulematching, statistical pattern recognition, clustering, and embeddings (Etzioni et al., 2004; Kim et al.,\n2016; Mooney and Bunescu, 2005; Yates et al., 2007). GraphRAG falls into a more recent body of\nresearch that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022;\nOpenAI, 2023; Tan et al., 2017; Trajanoska et al., 2023; Yao et al., 2023; Yates et al., 2007; Zhang\net al., 2024a). It also adds to a growing body of RAG approaches that use a knowledge graph as\nan index (Gao et al., 2023). Some techniques use subgraphs, elements of the graph, or properties\n\n- f the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023)\n\n- r as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n"," when using\nGPT-4 as the LLM.\n\n\nGraphRAG is available as open-source software at https:\/\/github _._ [com\/microsoft\/graphrag. In ad-](https:\/\/github.com\/microsoft\/graphrag)\ndition, versions of the GraphRAG approach are also available as extensions to multiple opensource libraries, including LangChain (LangChain, 2024), LlamaIndex (LlamaIndex, 2024), NebulaGraph (NebulaGraph, 2024), and Neo4J (Neo4J, 2024).\n\n\n**2** **Background**\n\n\n**2.1** **RAG Approaches and Systems**\n\n\nRAG generally refers to any system where a user query is used to retrieve relevant information from\nexternal data sources, whereupon this information is incorporated into the generation of a response\nto the query by an LLM (or other generative AI model, such as a multi-media model). The query and\nretrieved records populate a prompt template, which is then passed to the LLM (Ram et al., 2023).\nRAG is ideal when the total number of records in a data source is too large to include in a single\nprompt to the LLM, i.e. the amount of text in the data source exceeds the LLM\u2019s context window.\n\n\nIn canonical RAG approaches, the retrieval process returns a set number of records that are semantically similar to the query and the generated answer uses only the information in those retrieved\nrecords. A common approach to conventional RAG is to use text embeddings, retrieving records\nclosest to the query in vector space where closeness corresponds to semantic similarity (Gao et al.,\n2023). While some RAG approaches may use alternative retrieval mechanisms, we collectively refer\nto the family of conventional approaches as _vector RAG_ . GraphRAG contrasts with vector RAG in\nits ability to answer queries that require global sensemaking over the entire data corpus.\n\n\n2\n\n\nGraphRAG builds upon prior work on advanced RAG strategies. GraphRAG leverages summaries\n\n- ver large sections of the data source as a form of \u201dself-memory\u201d (described in Cheng et al. 2024),\nwhich are later used to answer queries as in Mao et al. 2020). These summaries are generated in\nparallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,\n2023; Gao","hi, 2023). Other\ntechniques (Wang et al., 2023b) use the knowledge graph to enhance retrieval, where at query time\nan LLM-based agent dynamically traverses a graph with nodes representing document elements\n(e.g., passages, tables) and edges encoding lexical and semantical similarity or structural relationships. GraphRAG contrasts with these approaches by focusing on a previously unexplored quality of\ngraphs in this context: their inherent _modularity_ (Newman, 2006) and the ability to partition graphs\ninto nested modular communities of closely related nodes (e.g., Louvain, Blondel et al. 2008; Leiden, Traag et al. 2019). Specifically, GraphRAG recursively creates increasingly global summaries\nby using the LLM to create summaries spanning this community hierarchy.\n\n\n**2.3** **Adaptive benchmarking for RAG Evaluation**\n\n\nMany benchmark datasets for open-domain question answering exist, including HotPotQA (Yang\net al., 2018), MultiHop-RAG (Tang and Yang, 2024), and MT-Bench (Zheng et al., 2024). However,\nthese benchmarks are oriented towards vector RAG performance, i.e., they evaluate performance\n\n- n explicit fact retrieval. In this work, we propose an approach for generating a set of questions\nfor evaluating global sensemaking over the entirety of the corpus. Our approach is related to LLM\nmethods that use a corpus to generate questions whose answers would be summaries of the corpus,\nsuch as in Xu and Lapata (2021). However, in order to produce a fair evaluation, our method avoids\ngenerating the questions directly from the corpus itself (as an alternative implementation, one can\nuse a subset of the corpus held out from subsequent graph extraction and answer evaluation steps).\n\n\n_Adaptive benchmarking_ refers to the process of dynamically generating evaluation benchmarks tailored to specific domains or use cases. Recent work has used LLMs for adaptive benchmarking\nto ensure relevance, diversity, and alignment with the target application or task (Yuan et al., 2024;\nZhang et al., 2024b). In this work, we propose an adaptive benchmarking approach to generating\nglobal sensemaking queries for the LLM. Our approach builds on prior work in LLM-based persona\ngeneration, where the LLM is used to generate diverse and authentic sets of personas ("," on scientific discovery: a preliminary study\nusing gpt-4.\n\n\nMooney, R. J. and Bunescu, R. (2005). Mining knowledge from text using information extraction.\n_SIGKDD Explor. Newsl._, 7(1):3\u201310.\n\n\nNebulaGraph (2024). Nebulagraph launches industry-first graph rag: Retrieval-augmented generation with llm based on knowledge graphs. https:\/\/www _._ [nebula-graph](https:\/\/www.nebula-graph.io\/posts\/graph-RAG) _._ io\/posts\/graph-RAG.\n\n\nNeo4J (2024). Get started with graphrag: Neo4j\u2019s ecosystem tools. https:\/\/neo4j _._ [com\/developer-](https:\/\/neo4j.com\/developer-blog\/graphrag-ecosystem-tools\/)\n[blog\/graphrag-ecosystem-tools\/.](https:\/\/neo4j.com\/developer-blog\/graphrag-ecosystem-tools\/)\n\n\nNewman, M. E. (2006). Modularity and community structure in networks. _Proceedings of the_\n_national academy of sciences_, 103(23):8577\u20138582.\n\n\nNi, J., Shi, M., Stammbach, D., Sachan, M., Ash, E., and Leippold, M. (2024). AFaCTA: Assisting\nthe annotation of factual claim detection with reliable LLM annotators. In Ku, L.-W., Martins, A.,\nand Srikumar, V., editors, _Proceedings of the 62nd Annual Meeting of the Association for Compu-_\n_tational Linguistics (Volume 1: Long Papers)_, pages 1890\u20131912, Bangkok, Thailand. Association\nfor Computational Linguistics.\n\n\nOpenAI (2023). Chatgpt: Gpt-4 language model.\n\n\nPadmakumar, V. and He, H. (2024). Does writing with language models reduce content diversity?\n_ICLR_ .\n\n\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,\nPerrot, M., and Duchesn"," single element because an element is typically detected and extracted multiple\ntimes across documents.\n\n\nIn the final step of the knowledge graph extraction process, these instances of entities and relationships become individual nodes and edges in the graph. Entity descriptions are aggregated and\nsummarized for each node and edge. Relationships are aggregated into graph edges, where the number of duplicates for a given relationship becomes edge weights. Claims are aggregated similarly.\n\n\nIn this manuscript, our analysis uses exact string matching for _entity matching_ - the task of reconciling different extracted names for the same entity (Barlaug and Gulla, 2021; Christen and Christen,\n2012; Elmagarmid et al., 2006). However, softer matching approaches can be used with minor adjustments to prompts or code. Furthermore, GraphRAG is generally resilient to duplicate entities\nsince duplicates are typically clustered together for summarization in subsequent steps.\n\n\n**3.1.4** **Knowledge Graph** _\u2192_ **Graph Communities**\n\n\nGiven the graph index created in the previous step, a variety of community detection algorithms\nmay be used to partition the graph into communities of strongly connected nodes (e.g., see the\nsurveys by Fortunato (2010) and Jin et al. (2021)). In our pipeline, we use Leiden community\ndetection (Traag et al., 2019) in a hierarchical manner, recursively detecting sub-communities within\neach detected community until reaching leaf communities that can no longer be partitioned.\n\n\n5\n\n\nEach level of this hierarchy provides a community partition that covers the nodes of the graph in a\nmutually exclusive, collectively exhaustive way, enabling divide-and-conquer global summarization.\nAn illustration of such hierarchical partitioning on an example dataset can be found in Appendix B.\n\n\n**3.1.5** **Graph Communities** _\u2192_ **Community Summaries**\n\n\nThe next step creates report-like summaries of each community in the community hierarchy, using\na method designed to scale to very large datasets. These summaries are independently useful as a\nway to understand the global structure and semantics of the dataset, and may themselves be used to\nmake sense of a corpus in the absence of a specific query. For example, a user may scan through\ncommunity summaries at one level looking for general themes of interest, then read linked reports\nat a lower level that provide additional details for each subtopic. Here, however, we focus on their\nutility as"],"reference":"GraphRAG is a method for knowledge graph extraction that uses advanced machine learning models, specifically leveraging summaries over large sections of the data source as a form of 'self-memory'. It contrasts with vector RAG by generating a graph index from the source data and applying graph-based community detection to create a thematic partitioning of the data. This approach allows GraphRAG to answer queries that require global sensemaking over the entire data corpus, making it more effective for complex knowledge extraction tasks."}
{"user_input":"How can the retrieval module be optimized to aid in a specific downstream task such as question answering?","response":"To optimize the retrieval module for a specific downstream task such as question answering, several strategies can be employed:\n\n1. **Iterative Retrieval**: Implement an iterative approach where the retrieval module processes passages in steps, conditioning on previously retrieved information. This is particularly important for complex questions that require multi-hop reasoning, as it allows the system to refine its queries based on the context built up from earlier retrievals <document id=\"2009.12756_multihop_qi:chunk_450\">.\n\n2. **Integration of Pre-trained Models**: Utilize pre-trained encoders to generate dense representations of questions and documents. This can facilitate maximum inner product search (MIPS) for effective retrieval and improve performance compared to traditional lexical methods, which may not capture nuanced semantic relationships <document id=\"2009.12756_multihop_qi:chunk_450\">.\n\n3. **Fine-tuning and Adaptive Modules**: The retrieval module can be fine-tuned for specific tasks based on feedback from the downstream process. Adjustments in module arrangement and interactions can enhance performance, such as using feedback loops to improve retrieval efficacy or integrating new modules as needed <document id=\"2312.10997_rag_survey_gao:chunk_3600\">.\n\n4. **Granularity of Retrieval**: Choosing the appropriate granularity for the retrieval units can affect the relevance of information provided to the downstream task. Optimal retrieval granularity ranges from coarse units (e.g., documents) to fine units (e.g., phrases or propositions), impacting both precision and semantic integrity <document id=\"2312.10997_rag_survey_gao:chunk_6300\">.\n\n5. **Multi-modal and Hybrid Retrieval Approaches**: Incorporating hybrid retrieval strategies that combine keyword, semantic, and vector searches can cater to varied question types and improve overall retrieval relevance <document id=\"2312.10997_rag_survey_gao:chunk_3600\">.\n\nThese combined strategies can significantly enhance the retrieval module's effectiveness in supporting question answering tasks by ensuring relevant and contextual information is efficiently accessed and utilized.","retrieved_contexts":["downstream models, our system can match the best published result while being 10x faster.\n\n\n2 METHOD\n\n\n2.1 PROBLEM DEFINITION\n\n\nThe retrieval task considered in this work can be described as follows (see also Figure 1). Given a\nmulti-hop question _q_ and a large text corpus _C_, the retrieval module needs to retrieve a sequence of\npassages _Pseq_ : _{p_ 1 _, p_ 2 _, ..., pn}_ that provide _sufficient_ information for answering _q_ . Practically, the\nretriever returns the _k_ best-scoring sequence candidates, _{Pseq_ [1] _[,][ P]_ _seq_ [2] _[, ...,][ P]_ _seq_ _[k]_ _[}]_ [ (] _[k][ \u226a|C|]_ [), with the]\nhope that at least one of them has the desired qualities. _k_ should be small enough for downstream\nmodules to process in a reasonable time while maintaining adequate recall. In general, retrieval also\nneeds to be efficient enough to handle real-world corpora containing millions of documents.\n\n\n2.2 MULTI-HOP DENSE RETRIEVAL\n\n\n**Model** Based on the sequential nature of the multi-hop retrieval problem, our system solves it in an\niterative fashion. We model the probability of selecting a certain passage sequence as follows:\n\n\n\n_P_ ( _Pseq|q_ ) =\n\n\n\n_n_\n\n- _P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) _,_\n\n\n_t_ =1\n\n\n2\n\n\nPublished as a conference paper at ICLR 2021\n\n\nwhere for _t_ = 1, we only condition on the original question for retrieval. At each retrieval step, we\nconstruct a new query representation based on previous results and the retrieval is implemented as\nmaximum inner product search over the dense representations of the whole corpus:\n\n\nexp ( _\u27e8_ _**p**_ _t,_ _**q**_ _t\u27e9_ )\n_P_ ( _pt|q, p_ 1 _, ..., pt\u2212_ 1) =\n\n~~\ufffd~~ _p\u2208C_ [exp (] _[\u27e8]_ _**[p]**_ _[,]_ _**[ q]**_ _[t][\u27e9]_ [)] _[,]_","Chen et al., 2017) tackles the\nproblem in two stages. Given a question, a _retriever_ first produces a list of _k_ candidate documents,\nand a _reader_ then extracts the answer from this set. Until recently, retrieval models were dependent\n\n- n traditional term-based information retrieval (IR) methods, which fail to capture the semantics of\nthe question beyond lexical matching and remain a major performance bottleneck for the task. Recent\nwork on dense retrieval methods instead uses pretrained encoders to cast the question and documents\ninto dense representations in a vector space and relies on fast maximum inner-product search (MIPS)\nto complete the retrieval. These approaches (Lee et al., 2019; Guu et al., 2020; Karpukhin et al.,\n2020) have demonstrated significant retrieval improvements over traditional IR baselines.\n\n\nHowever, such methods remain limited to _simple_ questions, where the answer to the question\nis explicit in a single piece of text evidence. In contrast, _complex_ questions typically involve\naggregating information from multiple documents, requiring logical reasoning or sequential (multihop) processing in order to infer the answer (see Figure 1 for an example). Since the process for\nanswering such questions might be sequential in nature, single-shot approaches to retrieval are\ninsufficient. Instead, iterative methods are needed to recursively retrieve new information at each\nstep, conditioned on the information already at hand. Beyond further expanding the scope of existing\ntextual open-domain QA systems, answering more complex questions usually involves _multi-hop_\n_reasoning_, which poses unique challenges for existing neural-based AI systems. With its practical\n\n\n_\u2217_ Equal Contribution\n1\n[https:\/\/github.com\/facebookresearch\/multihop_dense_retrieval.](https:\/\/github.com\/facebookresearch\/multihop_dense_retrieval)\n\n\n1\n\n\nPublished as a conference paper at ICLR 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: An overview of the multi-hop dense retrieval approach.\n\n\nand research values, multi-hop QA has been extensively studied recently (Talmor & Berant, 2018;\nYang et al., 2018; Welbl et al., 2018) and remains an active research area in NLP (Qi et al., 2019; Nie\net al., 2019; Min et al., 2019; Zhao et al., 2020; Asai"," [51, 52] propose a single, pre-trained encoder-decoder\nmodel that leverages bi-directional attention to achieve stronger performance on discriminative\nand generative tasks. Our work aims to expand the space of possible tasks with a single, unified\narchitecture, by learning a retrieval module to augment pre-trained, generative language models.\n\n\n**Learned Retrieval** There is significant work on learning to retrieve documents in information\nretrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some\nwork optimizes the retrieval module to aid in a specific, downstream task such as question answering,\nusing search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\nwork. These successes leverage different retrieval-based architectures and optimization techniques to\nachieve strong performance on a single task, while we show that a single retrieval-based architecture\ncan be fine-tuned for strong performance on a variety of tasks.\n\n\n**Memory-based Architectures** Our document index can be seen as a large external memory for\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\nwork. Other work improves the ability of dialog models to generate factual text by attending over\nfact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\ndistributed representations, which makes the memory both (i) human-readable, lending a form of\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\u2019s\nmemory by editing the document index. This approach has also been used in knowledge-intensive\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\nrather than end-to-end learnt retrieval [9].\n\n\n**Retrieve-and-Edit approaches** Our method shares some similarities with retrieve-and-edit style\napproaches, where a similar training input-output pair is retrieved for a given input, and then edited\nto provide a final output. These approaches have proved successful in a number of domains including\nMachine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,\nincluding less of emphasis on lightly editing a retrieved item, but on aggregating content"," Patterns:_ Modular RAG offers remarkable adaptability by allowing module substitution or reconfiguration\nto address specific challenges. This goes beyond the fixed\nstructures of Naive and Advanced RAG, characterized by a\nsimple \u201cRetrieve\u201d and \u201cRead\u201d mechanism. Moreover, Modular\nRAG expands this flexibility by integrating new modules or\nadjusting interaction flow among existing ones, enhancing its\napplicability across different tasks.\nInnovations such as the Rewrite-Retrieve-Read [7]model\nleverage the LLM\u2019s capabilities to refine retrieval queries\nthrough a rewriting module and a LM-feedback mechanism\nto update rewriting model., improving task performance.\nSimilarly, approaches like Generate-Read [13] replace traditional retrieval with LLM-generated content, while ReciteRead [22] emphasizes retrieval from model weights, enhancing the model\u2019s ability to handle knowledge-intensive tasks.\nHybrid retrieval strategies integrate keyword, semantic, and\nvector searches to cater to diverse queries. Additionally, employing sub-queries and hypothetical document embeddings\n(HyDE) [11] seeks to improve retrieval relevance by focusing\n\n- n embedding similarities between generated answers and real\ndocuments.\n\nAdjustments in module arrangement and interaction, such\nas the Demonstrate-Search-Predict (DSP) [23] framework\nand the iterative Retrieve-Read-Retrieve-Read flow of ITERRETGEN [14], showcase the dynamic use of module outputs to bolster another module\u2019s functionality, illustrating a\nsophisticated understanding of enhancing module synergy.\nThe flexible orchestration of Modular RAG Flow showcases\nthe benefits of adaptive retrieval through techniques such as\nFLARE [24] and Self-RAG [25]. This approach transcends\nthe fixed RAG retrieval process by evaluating the necessity\n\n- f retrieval based on different scenarios. Another benefit of\na flexible architecture is that the RAG system can more\neasily integrate with other technologies (such as fine-tuning\n\n- r reinforcement learning) [26]. For example, this can involve\nfine-tuning the retriever for better retrieval results, fine-tuning\nthe generator for more personalized outputs, or engaging in\ncollaborative fine-tuning [27].\n\n\n_D. RAG vs Fine-tuning_\n\nThe augmentation of LLMs has attracted considerable attention due to their growing prevalence. Among the optimization\n\n\n\n5\n\n\nmethods for LLMs, RAG is often compared with Fine-tuning\n(FT) and prompt engineering. Each method has distinct characteristics"," graph comprehension\nand question-answering capabilities through soft prompting\n\n- f the LLM, and employs the Prize-Collecting Steiner Tree\n(PCST) optimization problem for targeted graph retrieval. On\nthe contrary, it requires additional effort to build, validate,\nand maintain structured databases. On the contrary, it requires\nadditional effort to build, validate, and maintain structured\n\ndatabases.\n\n_LLMs-Generated Content._ Addressing the limitations of\nexternal auxiliary information in RAG, some research has\nfocused on exploiting LLMs\u2019 internal knowledge. SKR [58]\nclassifies questions as known or unknown, applying retrieval\nenhancement selectively. GenRead [13] replaces the retriever\nwith an LLM generator, finding that LLM-generated contexts\n\n- ften contain more accurate answers due to better alignment\nwith the pre-training objectives of causal language modeling.\nSelfmem [17] iteratively creates an unbounded memory pool\nwith a retrieval-enhanced generator, using a memory selector to choose outputs that serve as dual problems to the\n\n- riginal question, thus self-enhancing the generative model.\nThese methodologies underscore the breadth of innovative\ndata source utilization in RAG, striving to improve model\nperformance and task effectiveness.\n_2) Retrieval Granularity:_ Another important factor besides\nthe data format of the retrieval source is the granularity of\nthe retrieved data. Coarse-grained retrieval units theoretically\ncan provide more relevant information for the problem, but\nthey may also contain redundant content, which could distract\nthe retriever and language models in downstream tasks [50],\n\n[87]. On the other hand, fine-grained retrieval unit granularity\nincreases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge. Choosing\n\n\nthe appropriate retrieval granularity during inference can be\na simple and effective strategy to improve the retrieval and\ndownstream task performance of dense retrievers.\nIn text, retrieval granularity ranges from fine to coarse,\nincluding Token, Phrase, Sentence, Proposition, Chunks, Document. Among them, DenseX [30]proposed the concept of\nusing propositions as retrieval units. Propositions are defined\nas atomic expressions in the text, each encapsulating a unique\nfactual segment and presented in a concise, self-contained natural language format. This approach aims to enhance retrieval\nprecision and relevance. On the Knowledge Graph (KG),\nretrieval granularity includes Entity, Triplet, and sub-Graph"],"reference":"Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work."}
{"user_input":"What training methodology did the researchers use for their Dense Passage Retriever model?","response":"The researchers used a training methodology for their Dense Passage Retriever (DPR) model that includes an **in-batch negative training** setting. Specifically, each question in the batch is paired with one positive passage and one additional BM25 negative passage per question. They trained the question and passage encoders for up to 40 epochs for larger datasets (like NQ, TriviaQA, SQuAD) and 100 epochs for smaller datasets (such as TREC and WQ). The training process involved a batch size of 128, a learning rate of \\(10^{-5}\\) using Adam, with linear scheduling and a 0.1 dropout rate (<document id=\"2004.04906_dpr_karpukhin:chunk_4950\">).","retrieved_contexts":[" Special Publication Sp_ [, 109:109, 1995. URL https:\/\/www.](https:\/\/www.microsoft.com\/en-us\/research\/publication\/okapi-at-trec-3\/)\n[microsoft.com\/en-us\/research\/publication\/okapi-at-trec-3\/.](https:\/\/www.microsoft.com\/en-us\/research\/publication\/okapi-at-trec-3\/)\n\n\nDevendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil\nZaheer. Questions are all you need to train a dense passage retriever. _Transactions of the As-_\n_sociation for Computational Linguistics_, 11:600\u2013616, 2023. doi: 10.1162\/tacl ~~a 0~~ 0564. URL\n[https:\/\/aclanthology.org\/2023.tacl-1.35.](https:\/\/aclanthology.org\/2023.tacl-1.35)\n\n\nGideon Schwarz. Estimating the Dimension of a Model. _The annals of statistics_, pp. 461\u2013464,\n[1978. URL https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/](https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/aos\/1176344136.full)\n[volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/](https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/aos\/1176344136.full)\n[aos\/1176344136.full.](https:\/\/projecteuclid.org\/journals\/annals-of-statistics\/volume-6\/issue-2\/Estimating-the-Dimension-of-a-Model\/10.1214\/aos\/1176344136.full)\n\n\nKaren Sp\u00a8arck Jones. A Statistical Interpretation of Term Specificity and its Application in Retrieval. _Journal of documentation_ [, 28(1):11\u201321, 1972. URL https:\/\/doi.org\/10.1108\/](https:\/\/doi.org\/10.1108\/eb026526)\n[eb026526",".1 77.2\nSingle\nBM25 + DPR 76.6 79.8 71.0 85.2 **71.5** 83.8 84.5 80.5 92.7 **81.3**\n\n\nDPR **79.4** 78.8 **75.0** **89.1** 51.6 **86.0** 84.7 **82.9** 93.9 67.6\nMulti\nBM25 + DPR 78.0 **79.9** 74.7 88.5 66.2 83.9 84.4 82.3 **94.1** 78.6\n\n\nTable 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20\/100 retrieved\npassages that contain the answer. _Single_ and _Multi_ denote that our Dense Passage Retriever (DPR) was trained\nusing individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.\n\n\n\ntraditional retrieval methods, the effects of different\ntraining schemes and the run-time efficiency.\nThe DPR model used in our main experiments\nis trained using the in-batch negative setting (Section 3.2) with a batch size of 128 and one additional\nBM25 negative passage per question. We trained\nthe question and passage encoders for up to 40\nepochs for large datasets (NQ, TriviaQA, SQuAD)\nand 100 epochs for small datasets (TREC, WQ),\nwith a learning rate of 10 _[\u2212]_ [5] using Adam, linear\nscheduling with warm-up and dropout rate 0 _._ 1.\nWhile it is good to have the flexibility to adapt\nthe retriever to each dataset, it would also be desirable to obtain a single retriever that works well\nacross the board. To this end, we train a _multi_ dataset encoder by combining training data from\nall datasets excluding SQuAD. [8] In addition to DPR,\nwe also present the results of BM25, the traditional\nretrieval method [9] and BM25+DPR, using a linear\ncombination of their scores as the new ranking\nfunction. Specifically, we obtain two initial sets\n\n- f top-2000 passages"," generation models,\nnumerical comparison is still a bigger issue compared to extractive models.\n\n\nB MODEL DETAILS\n\n\nB.1 BEST MODEL HYPERPARAMETERS\n\n\nTable 11: Hyperparameters of Retriever\n\nlearning rate 2e-5\nbatch size 150\nmaximum passage length 300\nmaximum query length at initial hop 70\nmaximum query length at 2nd hop 350\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 50\nweight decay 0\n\n\nTable 12: Hyperparameters of Extractive Reader (ELECTRA)\n\n\nlearning rate 5e-5\nbatch size 128\nmaximum sequence length 512\nmaximum answer length 30\nwarmup ratio 0.1\ngradient clipping norm 2.0\ntraininig epoch 7\nweight decay 0\n# of negative context per question 5\nweight of SP sentence prediction loss 0.025\n\n\n16\n\n\n\nTable 10: Answer EM using top 50 retrieved passage chains\n\n\n\nComp Bridge\nModel Overall\n(20%) (80%)\n\n\n\nELECTRA 61.7 79.0 57.4\n\nFiD 61.7 75.3 58.3\n\n\nPublished as a conference paper at ICLR 2021\n\n\nB.2 FURTHER DETAILS ABOUT READER MODELS\n\n\nB.2.1 EXTRACTIVE READER\n\n\nThe extractive reader is trained with four loss functions. With the [CLS] token, we predict a\nreranking score based on whether the passage sequence match the groundtruth supporting passages.\nOn top of the representation of each token, we predict a answer start score and answer end score.\nFinally, we prepend each sentence with the [unused0] special token and predict whether the\nsentence is one of the supporting sentences using the representations of the special token. At training\ntime, we pair each question with 1 groundtruth passage sequence and 5 negative passage sequence\nwhich do not contain the answer. At inference time, we feed in the top 250 passage sequences from\nMDR. We rank the predicted answer for each sequence with a linear combination of the reranking\nscore and the answer span score. The combination weight is selected based on the dev results.\n\n\nB.2.2 FUSION-IN-DECODER\n\n\nThe FiD model uses T5-large as the underlying seq2seq model. It is","bo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022. Temporalwiki: A lifelong benchmark for training and evaluating ever-evolving language models.\n\n\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,\nZhiruo Wang, Jamie Callan, and Graham Neubig.\n2022. Retrieval as attention: End-to-end learning of\nretrieval and reading within a single transformer. In\n_Conference on Empirical Methods in Natural Lan-_\n_guage Processing (EMNLP)_, Abu Dhabi, UAE.\n\n\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\n[Wen-tau Yih. 2020. Dense passage retrieval for open-](https:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.550)\n[domain question answering. In](https:\/\/doi.org\/10.18653\/v1\/2020.emnlp-main.550) _Proceedings of the_\n_2020 Conference on Empirical Methods in Natural_\n_Language Processing (EMNLP)_, pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\n\n\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP. _arXiv preprint_\n_arXiv:2212.14024_ .\n\n\n\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\n\n[Internet-augmented dialogue generation. In](https:\/\/doi.org\/10.18653\/v1\/2022.acl-long.579) _Proceed-_\n_ings of the 60th Annual Meeting of the Association_\n_for Computational Linguistics (Volume 1: Long Pa-_\n_pers)_, pages 8460\u20138478, Dublin, Ireland. Association\nfor Computational Linguistics.\n\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee","-dataset settings.\n\nWe conjecture that the lower performance on\nSQuAD is due to two reasons. First, the annotators wrote questions after seeing the passage. As\na result, there is a high lexical overlap between\npassages and questions, which gives BM25 a clear\nadvantage. Second, the data was collected from\n\n- nly 500+ Wikipedia articles and thus the distribution of training examples is extremely biased, as\nargued previously by Lee et al. (2019).\n\n\n**5.2** **Ablation Study on Model Training**\n\n\nTo understand further how different model training\n\n- ptions affect the results, we conduct several additional experiments and discuss our findings below.\n\n\n\n\n\n\n**Sample efficiency** We explore how many training examples are needed to achieve good passage\nretrieval performance. Figure 1 illustrates the top- _k_\nretrieval accuracy with respect to different numbers of training examples, measured on the devel\n- pment set of Natural Questions. As is shown, a\ndense passage retriever trained using only 1,000 examples already outperforms BM25. This suggests\nthat with a general pretrained language model, it is\npossible to train a high-quality dense retriever with\na small number of question\u2013passage pairs. Adding\nmore training examples (from 1k to 59k) further\nimproves the retrieval accuracy consistently.\n\n\n**In-batch negative training** We test different\ntraining schemes on the development set of Natural\nQuestions and summarize the results in Table 3.\nThe top block is the standard 1-of- _N_ training setting, where each question in the batch is paired\nwith a positive passage and its own set of _n_ negative passages (Eq. (2)). We find that the choice\n\n- f negatives \u2014 random, BM25 or gold passages\n(positive passages from other questions) \u2014 does\nnot impact the top- _k_ accuracy much in this setting\nwhen _k \u2265_ 20.\n\nThe middle bock is the in-batch negative training\n(Section 3.2) setting. We find that using a similar\nconfiguration (7 gold negative passages), in-batch\nnegative training improves the results substantially.\nThe key difference between the two is whether the\ngold negative passages come from the same batch\n\n- r from the whole training set. Effectively, in-batch\nnegative training is an easy and memory"],"reference":"The researchers used the in-batch negative setting for training their Dense Passage Retriever model, with a batch size of 128 and one additional BM25 negative passage per question."}
{"user_input":"What method did Gao et al. introduce to improve LLMs' generation quality without hurting their versatility?","response":"Gao et al. introduced **Self-Reflective Retrieval-augmented Generation (SELF-RAG)** to improve the generation quality of language models (LLMs) without compromising their versatility. This method enhances factual accuracy through on-demand retrieval and self-reflection. In SELF-RAG, the model first determines if retrieving additional information would be beneficial, outputs a retrieval token if necessary, and processes the pertinent passages. It then generates critique tokens to evaluate its own output's quality before selecting the best response based on factuality and overall quality. This approach contrasts with traditional retrieval-augmented generation methods, which often indiscriminately retrieve passages and can negatively impact the quality of the generated content (<document id=\"2310.11511_self_rag_asai:chunk_450\">).","retrieved_contexts":[" still retains\nits fundamental role in compressing LLMs, making them\nmore efficient without significant loss in performance. (Gu\net al., 2024; Agarwal et al., 2024). More recently, the strategy\n\n- f employing open-source LLMs as teachers for their own\nself-improvement has emerged as a promising approach,\nenhancing their capabilities significantly (Yuan et al., 2024a;\nChen et al., 2024a). Figure 1 provides an illustration of these\nthree key roles played by KD in the context of LLMs.\nA key aspect of the knowledge distillation is the enhancement of skills such as advanced context following\n\n\n\n2\n\n\n\u2462\n\n\nSelf-Improvement\n\n\n\u2460 \u2461\n\nAdvance Compress\n\n\nClosed-Source LLMs Open-Source LLMs Smaller LMs\n\n\nDirection of KD\n\n\nFig. 1: KD plays three key roles in LLMs: 1) Primarily\nenhancing capabilities, 2) offering traditional compression\nfor efficiency, and 3) an emerging trend of self-improvement\nvia self-generated knowledge.\n\n\n(e.g., in-context learning (Huang et al., 2022a) and instruction following (Taori et al., 2023)), improved alignment with user intents (e.g., human values\/principles (Cui\net al., 2023a), and thinking patterns like chain-of-thought\n(CoT) (Mukherjee et al., 2023)), and NLP task specialization\n(e.g., semantic understanding (Ding et al., 2023a), and code\ngeneration (Chaudhary, 2023)). These skills are crucial for\nthe wide array of applications that LLMs are expected\nto perform, ranging from casual conversations to complex problem-solving in specialized domains. For instance,\nin vertical domains like healthcare (Wang et al., 2023a),\nlaw (LAW, 2023), or science (Zhang et al., 2024), where\naccuracy and context-specific knowledge are paramount,\nknowledge distillation allows open-source models to significantly improve their performance by learning from the\nproprietary models that have been extensively trained and\nfine-tuned in these areas.\nThe benefits of knowledge distillation in the era of\nLLMs are multifaceted and transformative (Gu et al., 2024).\nThrough"," et al., 2023b, 2024b), etc. For detailed\nprogress on trustworthy LLMs, please further refer to Yao\net al. (2023a); Liu et al. (2023i); Sun et al. (2024a).\nThough slightly under-explored, aligning LLMs with\nhuman values by distilling is still possible (Bai et al., 2022a;\n\n\nCui et al., 2023a; Yang et al., 2024; Sun et al., 2024b). For\ninstance, Bai et al. (2022a) propose RLAIF, utilizing AIgenerated labels to interactively improve both helpfulness\nand harmlessness. Sun et al. (2024b) prompt the student\nmodel with 16 principles as guidelines for generating helpful, ethical, and reliable responses. Similarly, both harmless\nand harmful generations could be elicited by modifying\nthe prompts, and then are used to train the preference\nmodel (Yang et al., 2024). Cui et al. (2023a) utilize GPT4 to rank generations regarding helpfulness, truthfulness,\nand honesty. Liu et al. (2023b) advance the alignment of\nLLMs with societal values by incorporating simulated social\ninteractions into the training process. This approach encompasses a range of elements, including demonstrations that\nare both in alignment and in conflict with social norms, as\nwell as collective ratings, in-depth feedback, and responses\nthat are revised iteratively.\n\n\n**4.3** **Agent**\n\n\n_4.3.1_ _Tool Using_\n\nWhile recent LLMs have shown proficiency in solving various tasks, they still tend to make mistakes when handling\nlarge numerical values or executing intricate mathematical\ncalculations (Qian et al., 2022; She et al., 2023; Manikandan\net al., 2023; Liang et al., 2023b; Mialon et al., 2023). Thus\nequipping LLM agents with the capability to utilize tools\nhas been increasingly focused on. Commonly used methods\nmainly relied on human-curated data for training (Parisi\net al., 2022; Nakano et al., 2022; Qin et al., 2023c; Song\net al., 2023b) or prompt designing(Cai et"," efficiency of research workflows and\nfostering interdisciplinary collaborations, LLMs are poised\nto accelerate the pace of scientific discovery and innovation\nacross various fields. To distill knowledge from an LLM,\nDARWIN Series (Xie et al., 2023a) utilizes a semi selfinstruct for instruction generation for science papers, which\nis then used to fine-tune an LLM. SciGLM (Zhang et al.,\n2024) proposes to train a scientific LLM, which prompts a\nteacher LLM to generate detailed answers for unlabelled\nscientific questions, as well as a self-reflective critic-andrevise to improve data quality. Besides the above knowledge\ndistillation methods to adapt LLMs to science, we will also\ndelve into how the distillation happens in sub-domains, e.g.,\nmathematics, astronautics, chemistry, etc.\n\n\n_**Mathematics.**_ The application of LLMs within the subdomain of mathematics heralds a transformative era in\nmathematical research, education, and problem-solving\n(Azerbayev et al., 2023; Yu et al., 2023b). LLMs in mathematics facilitate the exploration and understanding of complex\nmathematical theories and problems by providing intuitive\nexplanations, proofs, and solutions that can bridge the\ngap between advanced mathematical concepts and learners at various levels. These models have shown potential\nin conjecturing new mathematical theorems and patterns,\nthus opening new avenues for research and discovery that\nmight not have been readily accessible to humans alone.\nIn education, they serve as personalized tutors, offering\nstudents step-by-step guidance through mathematical problems and adapting explanations to the learner\u2019s level of understanding. This democratizes access to high-quality mathematical education and fosters a deeper appreciation and\nunderstanding of mathematics among a broader audience.\nBy enhancing collaborative efforts through the generation\n\n- f new ideas and the simplification of complex concepts,\nLLMs are poised to significantly advance the field of mathematics, making it more accessible, efficient, and innovative. WizardMath (Luo et al., 2023b) enhances the mathematical reasoning capabilities of Llama-2 by applying the\n\n\nnovel Reinforcement Learning from Evol-Instruct Feedback\n(RLEIF) method, significantly outperforming other opensource LLMs on the GSM8k and MATH benchmarks, as\nwell as surpassing several closed-source LLM"," in improving\nfactuality and citation accuracy for long-form generations relative to these models. [1]\n\n\n1 INTRODUCTION\n\n\nState-of-the-art LLMs continue to struggle with factual errors (Mallen et al., 2023; Min et al., 2023)\ndespite their increased model and data scale (Ouyang et al., 2022). Retrieval-Augmented Generation\n(RAG) methods (Figure 1 left; Lewis et al. 2020; Guu et al. 2020) augment the input of LLMs\nwith relevant retrieved passages, reducing factual errors in knowledge-intensive tasks (Ram et al.,\n2023; Asai et al., 2023a). However, these methods may hinder the versatility of LLMs or introduce\nunnecessary or off-topic passages that lead to low-quality generations (Shi et al., 2023) since they\nretrieve passages indiscriminately regardless of whether the factual grounding is helpful. Moreover,\nthe output is not guaranteed to be consistent with retrieved relevant passages (Gao et al., 2023) since\nthe models are not explicitly trained to leverage and follow facts from provided passages. This\nwork introduces **Self-Reflective Retrieval-augmented Generation (SELF-RAG)** to improve an\nLLM\u2019s generation quality, including its factual accuracy without hurting its versatility, via on-demand\nretrieval and self-reflection. We train an arbitrary LM in an end-to-end manner to learn to reflect on\nits own generation process given a task input by generating both task output and intermittent special\ntokens (i.e., _reflection tokens_ ). Reflection tokens are categorized into _retrieval_ and _critique_ tokens to\nindicate the need for retrieval and its generation quality respectively (Figure 1 right). In particular,\ngiven an input prompt and preceding generations, SELF-RAG first determines if augmenting the\ncontinued generation with retrieved passages would be helpful. If so, it outputs a **retrieval** token that\ncalls a retriever model on demand (Step 1). Subsequently, SELF-RAG concurrently processes multiple\nretrieved passages, evaluating their relevance and then **generating** corresponding task outputs (Step\n2). It then generates critique tokens to **criticize** its own output and choose best one (Step 3) in terms\n\n- f factuality and overall quality. This process differs from conventional RAG ("," (Guo\net al., 2023b), an LLM designed for IT operations, boosts\n\n- perational efficiency using the Owl-Instruct dataset, which\nis distilled from ChatGPT. By applying a mixture-of-adapter\nstrategy for domain-specific tuning, it enhances analysis and\nperformance in IT-related tasks.\n\n\n**6** **OPEN PROBLEMS**\n\n\n_**Further Data Selection**_ How much data is required for LLM\ndistillation and how to filter out the low-quality data remain\n\n- pen-domain questions. In the field of instruction tuning,\n\n- ne of the most commonly used methods for distillation,\nZhou et al. (2023a) propose that only 1000 human-curated\nhigh-quality data is enough for the alignment of LLMs,\nhypothesizing that LLMs have learned the required knowledge from pretraining and only a small amount of data is\nrequired for the alignment. Its finding further raises a new\nquestion, how to automatically select the data for better\ndistillation? Chen et al. (2023e) directly apply ChatGPT to\nrate each data sample together with explanations, and then\nthe data is selected based on the rating. Cao et al. (2023b)\nsplit the existing instruction-tuning datasets and trains a\n\n\n\n26\n\n\nlinear function to select the most effective data based on\ntheir statistical properties. Li et al. (2023j) propose a data\nselection pipeline similar to self-distillation, in which the\nLLM firstly learns from a small subset of the data to get the\nbasic ability, and then further uses this learned model to rate\nfor the original dataset. Du et al. (2023b) propose to consider\nthree aspects including quality, coverage, and necessity for\nthe filtering process. Li et al. (2023k) select instruction data\nby evaluating their one-shot improvement on a hold-out\nset. Li et al. (2024f) recently propose Superfiltering, which is\nable to utilize small language models like GPT2 to filter out\nthe high-quality subset from a given high-quality dataset.\nDespite the emergence of these works working on data filtering, How to efficiently select the optimal distillation data\nfor LLMs, and How much data is required for distillation\nare still unsolved.\n\n\n_**Reduce the Distillation Cost (Lightweight Methods)**_ Despite the remarkable"],"reference":"Gao et al. introduced Self-Reflective Retrieval-augmented Generation (SELF-RAG) to improve an LLM's generation quality, including its factual accuracy without hurting its versatility, via on-demand retrieval and self-reflection."}
{"user_input":"How does the effectiveness of FLARE in multihop QA tasks compare to other baselines, and what factors contribute to its performance?","response":"FLARE demonstrates significant effectiveness in multihop QA tasks compared to other baselines. According to the comparative results presented in Table 2 of the study, FLARE achieved an impressive score of **77.3** on the Evaluation Metric (EM) across various datasets, including StrategyQA and ASQA. This performance is considerably higher than the scores of previous baselines, which typically ranged from the 30s to low 50s, indicating FLARE's superior performance in handling complex retrieval tasks for QA systems <document id=\"2305.06983_active_rag_jiang:chunk_7650\">.\n\nFactors contributing to FLARE's performance include the utilization of masking strategies to form implicit queries, which proved effective in filtering out low-confidence tokens that could distract the retrievers. Experiments showed that completely retrieving with the original sentence (without masking) resulted in poorer outcomes than using masked queries, thus validating the importance of token probability thresholds in query formulation <document id=\"2305.06983_active_rag_jiang:chunk_8550\">. Furthermore, both implicit and explicit query formulation methods have shown similar levels of effectiveness in reflecting information needs, suggesting that flexible approaches in query construction are beneficial <document id=\"2305.06983_active_rag_jiang:chunk_8550\">.","retrieved_contexts":[".0 39.9 27.9 **34.3** 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2\n\nFLARE (ours) **77.3** **41.3 28.2 34.3 31.1** **46.2 36.7 37.7 37.2** **53.4** **18.9 27.6**\n\n\nTable 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is\nDisambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.\n\n\n\n**2WikiMultihopQA** **ASQA-hint**\n**EM** **F** 1 **Prec. Rec.** **EM D-F** 1 **R-L DR**\n\n\nPrevious 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5\n\nNext 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6\n\n\nTable 3: A head-to-head comparison between using the\nprevious sentence and the next sentence for retrieval.\n\n\n**#Tokens** **EM** **F** 1 **Prec.** **Rec.**\n\n\n16 43.2 52.3 51.7 54.5\n\n32 43.6 52.4 52.0 55.0\n\n48 40.0 49.3 49.0 52.0\n\nAll 39.0 48.5 48.2 51.1\n\n\nTable 4: Previous-window approaches using different\nnumbers of tokens as queries.\n\n\n- us window underperforms single-time retrieval\n\n- n ASQA, which we hypothesize is because the\nprevious window does not accurately reflect future\nintent. Since we focus on evaluating factuality, metrics with an emphasis on factual content (such as\nEM, Disambig-F1, UniEval) are more reliable than\nmetrics computed over all tokens (ROUGE-L).\n\n\n**6.2** **Ablation Study**\n\n\n**Importance of forward-looking"," study implicit query formation by\nmasking and explicit query formulation through\nquestion generation. In Table 5, we compare the\nperformance of FLARE with different masking\n\n\n\n80.0\n\n\n60.0\n\n\n40.0\n\n\n20.0\n\n\n0.0\n\n\n\n2WikiMultihopQA StrategyQA\n\n\n0.0 25.0 50.0 75.0 100.0\n\n\n%steps\/sentences with retrieval\n\n\n_\u03b2_ **EM** **F** 1 **Prec.** **Rec.**\n\n\n0.0 0.488 0.576 0.571 0.605\n\n0.2 0.498 0.588 0.582 0.616\n\n0.4 0.510 0.597 0.591 0.627\n\n0.6 0.506 0.593 0.586 0.622\n\n\nTable 5: Performance of FLARE with respect to the\nmasking threshold _\u03b2_ - n 2WikiMultihopQA.\n\n\n**ASQA-hint** **WikiAsp**\n**EM D-F** 1 **R-L DR** **UniEval E-F** 1 **R-L**\n\n\nImplicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7\nExplicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6\n\n\nTable 6: A comparison between implicit and explicit\nquery formulation methods in FLARE.\n\n\nthresholds _\u03b2_ . Retrieving directly with the complete\nsentence ( _\u03b2_ = 0) is worse than masking tokens\nwith low probabilities, confirming our hypothesis\nthat low-confidence erroneous tokens can distract\nretrievers. We compare implicit and explicit query\nformulation methods in Table 6. Performances of\n\nboth methods are similar, indicating that both meth\n- ds can effectively reflect information needs.\n\n\n**7** **Related Work**\n\n\nWe refer to subsection 2.2 and section 4 for ex\ntensively discussion on single-time and multi-time\nretrieval augmented LMs, which is the most relevant area to this paper.\n\n\n**Iterative and adaptive retrieval** Iterative retrieval and refinement has been studied in both\ntext and code generation tasks (Peng et al., 202","_ and _Multi-evidence FEVER_ . HotpotQA (Yang et al., 2018) includes 113k multi-hop questions. Unlike other multi-hop QA\ndatasets (Zhang et al., 2018; Talmor & Berant, 2018; Welbl et al., 2018), where the information\nsources of the answers are knowledge bases, HotpotQA uses documents in Wikipedia. Thus, its\nquestions are not restricted by the fixed KB schema and can cover more diverse topics. Each\nquestion in HotpotQA is also provided with ground truth support passages, which enables us to\nevaluate the intermediate retrieval performance. Multi-evidence FEVER includes 20k claims from\nthe FEVER (Thorne et al., 2018) fact verification dataset, where the claims can only be verified using\nmultiple documents. We use this dataset to validate the general applicability of our method.\n\n\n**Implementation Details** All the experiments are conducted on a machine with 8 32GB V100\nGPUs. Our code is based on Huggingface Transformers (Wolf et al., 2019). Our best retrieval results\nare predicted using the exact inner product search index (IndexFlatIP) in FAISS (Johnson et al., 2017).\n\n\n3\n\n\nPublished as a conference paper at ICLR 2021\n\n\nTable 1: Retrieval performance in recall at _k_ retrieved passages and precision\/recall\/F1.\n\n\nHotpotQA FEVER\nMethod\nR@2 R@10 R@20 Precision Recall F1\n\n\nTF-IDF 10.3 29.1 36.8 14.9 28.2 19.5\n\nTF-IDF + Linked 17.3 50.0 62.7 18.6 35.8 24.5\n\nDrKIT 38.3 67.2 71.0         -         -         Entity Linking          -          -          - 30.6 53.8 39.0\n\n\nMDR **65.9** **77.5** **80.2** **45.7** **69.1** **55.0**\n\n\nBoth datasets assume 2 hops, so we fix _n_ = 2 for all experiments. Since HotpotQA does not provide\nthe order of the passage sequences, as a heuristic, we consider the passage that includes the answer\n"," of the gold paragraphs among the retrieved set of paragraphs. We search for the hyperparameter _K_ (and _M_ for IRCoT) that maximizes\nthe recall on the dev set and use it on the test set.\n\n\n[6https:\/\/www.elastic.co\/](https:\/\/www.elastic.co\/)\n\n\n\nThe reported metric can thus be viewed as the _fixed-_\n_budget optimal recall_ for each system considered. [7]\n\n\n**QA Reader.** To implement the reader, we use\nthe same LMs as used in the reason-step of\nIRCoT Retriever. We found that QA readers implemented with Flan-T5-* perform better with the\nDirect Prompting strategy and GPT3 performs better with CoT Prompting strategy (see App. E).\nHence we use Direct prompting strategy for QA\nwith Flan-T5-* and CoT with GPT3 for the experiments. [8]\n\n\nThe QA reader has one hyperparameter _M_ : the\nnumber of distractor paragraphs in the in-context\ndemonstrations. We search for _M_ in _{_ 1 _,_ 2 _,_ 3 _}_ .\nWhen used in conjunction with IRCoT retriever\n_M_ is tied for the CoT generator and the reader.\n\n\n**Open-Domain QA (ODQA) Models.** Putting retrievers and readers together, we experiment with\nODQA models constructed from the various language models denoted as **OneR QA** and **IRCoT**\n**QA** . For IRCoT QA, the choice of LM for the CoT\ngenerator and the reader is kept the same. We also\nexperiment with retriever-less QA readers **NoR QA**\nto assess how well LMs can answer the question\nfrom their parametric knowledge alone. To select\nthe best hyperparameters for the ODQA model,\nwe search for the hyperparameters _K_ and _M_ that\nmaximize the answer F1 on the development set.\nIIRC is structured slightly differently from the\n\n- ther datasets, in that its questions are grounded\nin a main passage and other supporting paragraphs\ncome from the Wikipedia pages of entities mentioned in this passage. We slightly modify the retrievers and readers to account for this (see App. B).\n\n\n**5** **Results**\n\n\n**IRCoT retrieval is better than one-step.** Fig. 3\ncompares OneR with"," (7B) are solely recency-biased. Given\nthese results, we hypothesize that prior work (e.g.,\nKhandelwal et al., 2018; Sun et al., 2021) did not\npreviously observe any primacy bias in language\nmodels because the models they studied were too\nsmall (less than 1B parameters).\nComparing between Llama-2 models with and\nwithout additional supervised fine-tuning and reinforcement learning from human feedback, we\nsee that additional fine-tuning dramatically improves performance on the multi-document QA\ntask. The 7B models with and without additional\n\nfine-tuning show minimal primacy bias, and are\nlargely recency-biased. The 13B base model has\na dramatic primacy and recency bias\u2014there is a\n20-point accuracy disparity between the best- and\nworst-case performance. Applying additional finetuning to the 13B seems to slightly reduce this\nbias (10-point worst-case degradation), but the bias\nremains significant. However, the 70B models\nwith and without additional fine-tuning have largely\nsimilar trends (showing both primacy and recency\nbias), and additional fine-tuning minimally changes\nthe positional bias severity.\n\n\n|0 Tota|Col2|al Retr|rieved Documents (~4K token|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n|Tota|Tota|l Ret|rieved Documents(~4K token|rieved Documents(~4K token|rieved Documents(~4K token|rieved Documents(~4K token|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n\n\n\nFigure 16: Multi-document QA performance (20 total\ndocuments) of Llama-2 models of varying sizes (7B,\n13B, 70B parameters), with and without additional supervised fine-tuning and reinforcement learning from\nhuman feedback (\u201c-chat-\u201d models).\n\n\n\n\n\n\n\n\n**F** **Token Counts**\n\n\nTable 2, Table 3, and Table 4 present the average and maximum number of tokens in each of the input\ncontexts for all experimental settings. Note that MPT-30B and MPT-30B-Instruct use the same tokenizer,\nGPT-3.5-Turbo and GPT-3.5-Turbo"],"reference":"FLARE outperforms all baselines on multihop QA tasks by a large margin. This is largely due to the task's clear definition and specific objective of producing the final answer through a 2-hop reasoning process, which makes it easier for LMs to generate on-topic output. The improvement on ASQA-hint is larger than that of ASQA because identifying ambiguous aspects is challenging even for humans in many cases, and providing a generic hint helps LMs stay on topic."}
