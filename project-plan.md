# RAG Q&A System - Project Plan

**Created**: 2025-11-22 (Day 26, Week 4 Day 5)
**Timeline**: Option B - Full Project 4 (~12 hours over 2 weeks)
**Data Source**: ArXiv Papers (RAG/LLM domain)

---

## Problem Statement

Build a production-quality RAG system for question-answering over recent ArXiv papers on RAG and LLM techniques. Demonstrate:
- Hybrid retrieval (dense + sparse + RRF fusion)
- Automated evaluation with Ragas framework
- Docker deployment to cloud
- Complete senior MLE portfolio piece

---

## Architecture Overview

```
ArXiv Papers (PDF) ‚Üí Chunking (500 tokens, 50 overlap)
                           ‚Üì
              Sentence-BERT Embeddings (384-dim)
                           ‚Üì
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚Üì             ‚Üì
              FAISS Index      BM25 Index
             (Dense retrieval) (Sparse retrieval)
                    ‚Üì             ‚Üì
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚Üì
                  RRF Fusion (k=60)
               Score = Œ£ 1/(k + rank_i)
                           ‚Üì
                    Top-K Documents
                           ‚Üì
              GPT-3.5-turbo + Context
                           ‚Üì
                  Generated Answer + Citations
```

---

## Tech Stack Decisions

### Core Components
- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2)
  - Why: Free, fast, 384-dim works well, proven for RAG
  - Alternative considered: OpenAI embeddings (too expensive for portfolio)

- **Dense Retrieval**: FAISS (local index)
  - Why: Fast, battle-tested, good for <100K docs
  - Alternative: Chroma (heavier dependency)

- **Sparse Retrieval**: rank-bm25 library
  - Why: Pure Python, no server needed, sufficient for portfolio
  - Alternative: Elasticsearch (overkill for 20-30 papers)

- **Fusion**: Hand-coded RRF
  - Why: Simple (5 lines), demonstrates understanding
  - Formula: Score = Œ£ 1/(60 + rank_i)

- **LLM**: OpenAI API (gpt-3.5-turbo)
  - Why: Reliable, fast, cheap ($0.50 for 5K queries)
  - Alternative: Ollama (slower, local hassle for portfolio)

- **Evaluation**: Ragas + manual metrics
  - Ragas: Context precision, recall, faithfulness, answer relevance
  - Manual: Recall@K, MRR, NDCG for retrieval

- **Deployment**: Docker + Streamlit Cloud
  - Why: Free hosting, easy to share, professional
  - Alternative: AWS Lambda (more complex)

### Development Tools
- **Version Control**: Git + GitHub
- **Environment**: Python 3.10+ with venv
- **CI/CD**: GitHub Actions (linting + tests)
- **Monitoring**: Simple logging to file

---

## Data Source: ArXiv Papers

### Target Papers (20-30 papers on RAG/LLMs)

**Search queries on arxiv.org:**
1. "Retrieval Augmented Generation" (2023-2024)
2. "RAG evaluation" OR "RAG metrics"
3. "Hybrid retrieval" OR "dense sparse retrieval"
4. "Query rewriting" OR "Query decomposition"
5. "LLM hallucination" OR "Faithfulness"

**Recommended papers to download** (pick 20-30):
- FiD (Fusion-in-Decoder) - Izacard et al.
- Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020)
- Lost in the Middle (Liu et al., 2023)
- RAFT (Gorilla paper, 2024)
- ColBERT: Efficient and Effective Passage Search (Khattab & Zaharia, 2020)
- Ragas: Automated Evaluation of RAG (2023)
- Self-RAG (Asai et al., 2023)
- RAPTOR: Recursive Abstractive Processing (2024)
- GraphRAG (Microsoft, 2024)
- Recent survey papers on RAG (2024)
- Papers on query rewriting/decomposition
- Papers on reranking strategies
- Papers on long-context vs. RAG

**Download strategy**:
- Use arxiv.org search + filter by date (2023-2024)
- Download PDFs to `data/raw/`
- Total size: ~50-100 MB (acceptable)

---

## Implementation Timeline

### Weekend (Light Sessions)

**Day 26 (Sat, Nov 22) - 30 min** ‚úÖ
- [x] Create project structure
- [x] Write this project-plan.md
- [x] Decision: Option B confirmed

**Day 27 (Sun, Nov 23) - 30 min**
- [x] Download 20-30 ArXiv papers (PDFs to `data/raw/`)
- [x] Create folder structure:
  ```
  rag-qa-system/
  ‚îú‚îÄ‚îÄ data/
  ‚îÇ   ‚îú‚îÄ‚îÄ raw/              # PDFs
  ‚îÇ   ‚îú‚îÄ‚îÄ processed/        # Chunks (JSON)
  ‚îÇ   ‚îî‚îÄ‚îÄ eval/             # Test questions
  ‚îú‚îÄ‚îÄ src/
  ‚îú‚îÄ‚îÄ evaluation/
  ‚îú‚îÄ‚îÄ tests/
  ‚îî‚îÄ‚îÄ outputs/
  ```
- [x] Create `requirements.txt` stub (list libraries, don't install yet)

### Main Implementation

** Mon, Nov 24, Week 4 Day 7 - 2 hours**
- [x] `src/data_loader.py`: Parse PDFs, chunk by 500 tokens with 50 overlap
- [x] `src/vector_store.py`: Generate embeddings, build FAISS index, save to disk
- [x] Test: Search for 1 query, verify top-5 results
- [x] Commit: "Add data loading and embedding generation"

** Tue, Nov 25, Week 5 Day 1 (Day 29) - 2.5 hours** ‚úÖ **COMPLETE**
- [x] `src/sparse_retrieval.py`: BM25 with NLTK tokenization
- [x] `src/hybrid_search.py`: RRF fusion (k=60, retrieve 4√ók candidates)
- [x] Evaluation framework: Precision@5, MRR
- [x] Test: Compare dense vs BM25 vs hybrid on 2 query sets
  - General NLP queries: Hybrid 40% < Dense 60% (query-corpus mismatch)
  - RAG-focused queries: Hybrid 80% > Dense 67% ‚úÖ (aligned queries)
- [x] Key finding: Query-corpus alignment is critical for BM25 performance
- [x] Folder reorganization: Created `experiments/` for analysis scripts
- [x] Commit: "Add hybrid retrieval with RRF fusion"

**Decision**: Use **Hybrid (RRF)** for production - performs better (80% vs 67%) with RAG-focused queries
**Rationale**: User queries will be RAG-related (e.g., "How does ColBERT work?"), not general NLP
**See**: `references/day29-hybrid-retrieval-findings.md` for details

** Wed, Nov 26, Week 5 Day 2 - 2 hours** ‚úÖ **COMPLETE**
- [x] `src/generator.py`: OpenAI Responses API wrapper (gpt-4o-mini, prompt engineering)
- [x] `src/rag_pipeline.py`: End-to-end pipeline (RagAssistant with 4 modes)
- [x] Create test question set (10 questions in `data/eval/test_questions.json`)
  - 3 simple factual ‚úÖ
  - 3 complex reasoning ‚úÖ
  - 2 multi-hop ‚úÖ
  - 2 negative (not in corpus) ‚úÖ
- [x] Smoke test: 5 questions √ó 4 modes = 20 tests
  - Results: Citations excellent, token usage validated (2700 vs 50)
  - Issue discovered: Negative question handling (retrieval contamination)
- [x] Commit: "Add generation and end-to-end pipeline"

**Key Decision**: Used gpt-4o-mini ($0.15 input / $0.60 output) instead of gpt-3.5-turbo - 3√ó cheaper + better quality

** Thu, Nov 27, Week 5 Day 3 - 1 hour** ‚úÖ **PLANNING & COST ANALYSIS**
- [x] Add ArXiv metadata to chunks (title, authors, year, URL) - 30 min
- [x] Regenerate chunks with metadata (`scripts/build_index.py`)
- [x] Researched Ragas 0.3.9 API (`generate_with_langchain_docs`, gpt-4o-mini setup)
- [x] Investigated Ollama support (not reliable - missing `agenerate_prompt`)
- [x] Ragas cost underestimate
  - Test run: 200 chunks ‚Üí 200K tokens ‚Üí $0.70 (SummaryExtractor phase only) (but was using gpt-4o instead of gpt-4o-mini)
- [x] Analyzed manual vs Ragas test format differences
- [x] Discussed ground truth requirements for metrics
- [x] **Daily knowledge check**: 94% (A) - Excellent overdue item retention
- Implementation deferred to Day 5 (evaluation code, run metrics)
- Decision: Sample 250 representative chunks instead of all 1500 ‚Üí 8√ó cost savings ($1.25 vs $10-15)

** Fri, Nov 28, Week 5 Day 4 - 2 hours** ‚úÖ **RETRIEVAL EVALUATION**
- [x] Add reference filtering to `CorpusLoader.filter_reference_chunks()` (Ollama-based)
- [x] Rebuild index with filtered chunks (1395 remaining, 9.5% references removed)
- [x] Implement sampling: `_sample_chunks()` in generate_testset.py
- [x] Generate 42 Ragas questions with Ollama (free, exceeded target of 40)
- [x] Create `evaluation/evaluate_retrieval.py`: Recall@K, MRR, NDCG
- [x] Run retrieval evaluation on 41 questions (3 modes: sparse, dense, hybrid)
- [x] **Critical insight**: Sampled testset ‚Üí incomplete ground truth (metrics are lower bounds)
- RAG evaluation deferred to Day 6 (use LLM-based context_recall)
- Error analysis deferred to Day 6

**Total cost**: $0 (Ollama for filtering + generation)

** Sat, Nov 29, Week 5 Day 5 - 3 hours** ‚úÖ **RAG EVALUATION & ERROR ANALYSIS**
- [x] Run RAG evaluation on 10 manual + 41 Ragas questions (4 modes: sparse, dense, hybrid, none)
  - Initial results: answer_correctness 0.39-0.58 (unexpectedly low)
- [x] **Question quality analysis**: Discovered 46% of Ragas questions were low-quality
  - Created `experiments/analyze_question_quality.py` (citation pattern detection)
  - Found 19/41 suspicious questions (from bibliography/table/footnote chunks)
  - **Root cause identified**: Generated from 500-token chunks instead of whole documents
  - Ragas needs whole documents to build knowledge graph
- [x] Manual review and filtering
  - Created `experiments/review_suspicious_questions.py` (interactive review)
  - Categorized: 11 definitely bad, 4 moderate, 4 contaminated, 1 false positive
  - Filtered 13 low-quality questions ‚Üí 28 clean questions (68% retention)
  - Created `experiments/filter_ragas_testset.py` (generates filtered testset)
- [x] Metrics recalculation on filtered testset
  - Created `experiments/filter_and_recalculate.py`
  - ~13% average improvement across all metrics
  - SPARSE: 66.8% answer_correctness, HYBRID: 62.8%, DENSE: 53.0%
- [x] **Error analysis**: Categorize failure modes
  - Created `experiments/analyze_errors.py` (pattern-based categorization)
  - Key finding: **Dense 29.6% retrieval failures vs Sparse 10.7%** (3√ó worse!)
  - SPARSE success rate: 57.1% (best), HYBRID: 46.4%, DENSE: 25.9%
  - Failure patterns: retrieval failure, generation failure, hallucination, ranking issue
- [x] **Decision**: Default to SPARSE (best performance), keep HYBRID as option (highest recall 92%)

**Key Insights**:
1. Testset generation methodology matters: chunks vs whole documents is critical
2. SPARSE > DENSE for small technical corpus (keyword matching advantage)
3. Question quality filtering improved metrics by ~13%

**Status**: RAG evaluation complete, ready for UI + deployment

** Sun, Nov 30, Week 5 Day 6 - 2.5 hours** ‚úÖ **STREAMLIT UI + DOCKER DEPLOYMENT**
- [x] **Testset regeneration with whole documents**
  - Fixed Day 6 root cause: used PyMuPDFLoader for whole documents (not chunks)
  - Generated 40 questions with Ollama (qwen2.5-coder:7b), filtered to 32 clean
  - Quality: 5.5/10 ‚Üí 8.5/10, no questions from references (0% vs 46%)
  - Unique: 97.5% (vs 54% before), 1 duplicate removed
- [x] **Added reference answers to manual questions**
  - Updated `data/eval/test_questions.json` with comprehensive references
  - 10 manual questions now have LLM-gradable references
- [x] **RAG evaluation v2 (42 questions: 10 manual + 32 Ragas)**
  - HYBRID best: 66.9% answer_correctness, 52.4% success rate
  - SPARSE: 61.3% correctness, 87.3% recall (highest)
  - DENSE: 51.9% correctness, 23.8% success (worst)
  - Validated Day 6 findings: Dense 3.7√ó worse retrieval failures
- [x] **Error analysis v2**: Categorized 42 questions by failure patterns
  - Created `experiments/analyze_errors.py` (updated for new file structure)
  - HYBRID: 52% success, SPARSE: 43%, DENSE: 24%
  - Dense retrieval failures: 26% (confirmed 3-4√ó worse than SPARSE)
- [x] **File structure reorganization**
  - Created `outputs/eval_results/` for clean input/output separation
  - Updated `src/utils.py` with EVAL_OUTPUT_DIR
  - Updated all evaluation scripts to use new structure
- [x] **Streamlit UI complete**
  - Created `app.py` with mode selection, top-K config, example questions
  - Displays answer + retrieved chunks with scores
  - Fixed BM25 multiprocessing issue (pre-tokenize corpus)
  - Sidebar: retrieval mode, top K, model, system info
- [x] **Docker containerization**
  - Created `Dockerfile` (learned Docker basics: FROM, COPY, RUN, CMD, ENV)
  - Created `docker-compose.yml` for orchestration
  - Created `.dockerignore` (exclude venv/, __pycache__, raw PDFs)
  - Split `requirements-dev.txt`
  - Image size: 4.3GB ‚Üí 2.2GB (-49% optimized)
- [x] **Comprehensive README.md** created
  - Architecture, features, evaluation results, technical decisions
  - Quick start, usage examples, development setup
  - License note (MIT + PyMuPDF4LLM AGPL)
- [x] **Documentation updates**
  - Updated `experiments/README.md` with Day 34 experiments
  - Created `references/Day34-Quick-Reference.md`

**Key Insights**:
1. Whole documents vs chunks critical for testset generation (quality 5.5‚Üí8.5)
2. HYBRID best for production (67% correctness, 52% success)
3. Docker optimization: .dockerignore + slim image + dev deps split

**Status**: RAG project production-ready (90% complete)
**Next**: Push to GitHub, optional FastAPI + observability

---

## Code Structure (Full Project 4)

```
rag-qa-system/
‚îú‚îÄ‚îÄ README.md                      # Comprehensive documentation
‚îú‚îÄ‚îÄ requirements.txt              # All dependencies with versions
‚îú‚îÄ‚îÄ Dockerfile                    # Container definition
‚îú‚îÄ‚îÄ docker-compose.yml            # Optional: multi-service
‚îú‚îÄ‚îÄ .env.example                  # API keys template
‚îú‚îÄ‚îÄ .gitignore                    # Don't commit data, .env
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ ci.yml                # GitHub Actions (lint + test)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                      # 20-30 ArXiv PDFs
‚îÇ   ‚îú‚îÄ‚îÄ processed/                # Chunked docs (JSON lines)
‚îÇ   ‚îî‚îÄ‚îÄ eval/                     # Test question sets
‚îÇ       ‚îî‚îÄ‚îÄ test_questions.json   # 10 test questions with ground truth
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py            # PDF parsing, chunking
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py             # Sentence-BERT wrapper
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py           # FAISS operations
‚îÇ   ‚îú‚îÄ‚îÄ retriever.py              # Dense + BM25 + RRF fusion
‚îÇ   ‚îú‚îÄ‚îÄ generator.py              # OpenAI API wrapper
‚îÇ   ‚îú‚îÄ‚îÄ rag_pipeline.py           # End-to-end pipeline
‚îÇ   ‚îî‚îÄ‚îÄ api.py                    # FastAPI endpoint (optional)
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_retrieval.py    # Recall@K, MRR, NDCG
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_rag.py           # Ragas integration
‚îÇ   ‚îú‚îÄ‚îÄ error_analysis.py         # Failure categorization
‚îÇ   ‚îî‚îÄ‚îÄ cost_analysis.py          # API cost tracking
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_pipeline.py          # Unit tests
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py               # API tests
‚îú‚îÄ‚îÄ app.py                        # Streamlit UI
‚îú‚îÄ‚îÄ notebooks/                    # Optional
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_embedding_comparison.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 03_retrieval_tuning.ipynb
‚îî‚îÄ‚îÄ outputs/
    ‚îú‚îÄ‚îÄ eval_results/             # Evaluation metrics and reports
    ‚îÇ   ‚îú‚îÄ‚îÄ retrieval_metrics.json
    ‚îÇ   ‚îú‚îÄ‚îÄ ragas_scores.json
    ‚îÇ   ‚îî‚îÄ‚îÄ error_analysis.json
    ‚îî‚îÄ‚îÄ logs/                     # Query logs and monitoring
```

**Estimated lines of code**: ~800 lines (excluding notebooks)

---

## Evaluation Plan

### Test Question Set (10 questions)

**Simple Factual (3 questions)**:
1. "What is Retrieval-Augmented Generation?"
2. "Who proposed the FiD architecture?"
3. "What does RAFT stand for?"

**Complex Reasoning (3 questions)**:
4. "Why does hybrid retrieval (dense + sparse) outperform either approach alone?"
5. "How does ColBERT differ from traditional dense retrieval?"
6. "What are the trade-offs between long-context LLMs and RAG systems?"

**Multi-hop (2 questions)**:
7. "How do GraphRAG and FiD differ in their approach to multi-document reasoning?"
8. "What evaluation metrics are recommended for both retrieval and generation in RAG?"

**Negative (2 questions)**:
9. "What is the capital of France?" (not in corpus)
10. "How do you train a neural network?" (not in corpus)

### Metrics to Track

**Retrieval Metrics** (compare dense, BM25, hybrid):
- Recall@K (K=1,3,5,10): % of questions with correct doc in top-K
- MRR (Mean Reciprocal Rank): 1/rank of first correct doc
- NDCG: Normalized Discounted Cumulative Gain
- Precision@K: % of relevant docs in top-K

**Ragas Metrics** (automated LLM-as-judge):
- Context Precision: Are retrieved contexts relevant to question?
- Context Recall: Does retrieved context contain answer?
- Faithfulness: Is answer grounded in context (no hallucination)?
- Answer Relevance: Does answer address the question?
- Answer Correctness: Semantic similarity with ground truth

**Cost Metrics**:
- Total API calls (embeddings + generation + evaluation)
- Tokens used per query
- Cost per query, cost per 1K queries

### Expected Results

**Retrieval** (based on 99.2% RAG mastery):
- Dense-only: Recall@5 ‚âà 70-80%
- BM25-only: Recall@5 ‚âà 60-70%
- Hybrid+RRF: Recall@5 ‚âà 85-95% ‚≠ê (best)

**Ragas Scores** (target):
- Context Precision: >0.85
- Context Recall: >0.90
- Faithfulness: >0.90
- Answer Relevance: >0.85

---

## Interview Talking Points

After building this, you can say:

**"I built a production-ready hybrid RAG system with rigorous evaluation, error analysis, and Docker deployment for Q&A over ML research papers."**

**Architecture**:
- **Corpus**: 32 ArXiv papers on RAG/LLMs ‚Üí 1,395 chunks (500 tokens, 50 overlap)
- **Retrieval**: Hybrid with RRF fusion (k=60)
  - Dense: SentenceBERT (all-MiniLM-L6-v2) in FAISS
  - Sparse: BM25-Okapi with NLTK tokenization
- **Generation**: GPT-4o-mini with structured prompt for citations
- **Key insight**: On small technical corpus (1.4K chunks), BM25 achieved 87% context recall vs 69% dense-only. Hybrid fusion improved answer quality to 67%.

**Evaluation rigor**:
- **Testset**: 42 questions (10 manual + 32 Ragas-generated) with reference answers
- **Metrics**: 5 RAGAS metrics - answer_correctness, context_recall, faithfulness, answer_relevancy, context_precision
- **Results**: HYBRID 66.9% correctness (52.4% success), SPARSE 61.3%, DENSE 51.9%
- **Error analysis**: Categorized failures across modes
  - Retrieval failure: DENSE 3.7√ó worse (26%) vs SPARSE (7%)
  - Generation failure: 10-19% across all modes
  - Ranking issues: 7-10% (opportunity for cross-encoder reranking)
- **Key lesson**: Dense embeddings struggle on small technical corpora - keyword matching wins

**Production readiness**:
- **Docker**: Containerized with docker-compose, optimized from 4.3GB ‚Üí 2.2GB
- **UI**: Streamlit with mode selection (sparse/hybrid/dense), top-K config, OpenAI/Ollama support
- **Deployment**: Health checks, environment config, prebuilt indexes included
- **Documentation**: Comprehensive README, experiments log, quick reference sheets

**What I'd improve next**:
1. **Cross-encoder reranking**: Address 7-10% ranking issues (retrieve 20 ‚Üí rerank ‚Üí top 5)
2. **Semantic chunking**: Replace fixed 500-token chunks with paragraph-based splitting
3. **Adaptive routing**: Route keyword-heavy queries to BM25, conceptual to dense
4. **Scale corpus**: Test if 100+ papers reduce dense retrieval failures (26% ‚Üí <10%)

---

## Success Criteria

**Technical** (All Achieved ‚úÖ):
- ‚úÖ End-to-end RAG pipeline working (sparse/dense/hybrid/none modes)
- ‚úÖ Hybrid retrieval (dense + BM25 + RRF) implemented correctly with k=60
- ‚úÖ Ragas evaluation framework integrated with 5 metrics
- ‚úÖ Context recall ‚â• 85% (SPARSE: 87.3%, HYBRID: 83.3%)
- ‚úÖ Faithfulness score ‚â• 0.80 (SPARSE: 83.8%, HYBRID: 83.0%)
- ‚úÖ Dockerized with optimization (4.3GB ‚Üí 2.2GB, -49%)
- ‚úÖ Streamlit UI with mode selection and Ollama support
- ‚úÖ Comprehensive README with architecture, evaluation results, interview talking points

**Portfolio** (All Achieved ‚úÖ):
- ‚úÖ Demonstrates senior MLE skills (evaluation rigor, error analysis, production deployment)
- ‚úÖ Shows RAG mastery (99.2% from Week 4 studies + practical implementation)
- ‚úÖ Interview-ready: Can explain architecture, trade-offs, evaluation, testset quality issues
- ‚úÖ Meets all Project 4 requirements from Project-Ideas.md
- ‚úÖ Prebuilt indexes included for easy demo
- ‚úÖ Error analysis with failure categorization (retrieval/generation/ranking/partial)

**Timeline** (Completed ‚úÖ):
- ‚úÖ Day 1 (Nov 25): Data ingestion, chunking, vector store setup
- ‚úÖ Day 2 (Nov 26): Sparse retrieval (BM25), hybrid search (RRF)
- ‚úÖ Day 3 (Nov 27): Generation with GPT-4o-mini, end-to-end pipeline
- ‚úÖ Day 4 (Nov 28): Ragas testset generation (40 questions)
- ‚úÖ Day 5 (Nov 29): RAG evaluation (4 modes), retrieval-only evaluation
- ‚úÖ Day 6 (Nov 30): Testset regeneration (whole docs), re-evaluation (42 questions), error analysis v2, Streamlit UI, Docker deployment
- ‚úÖ **Total time**: ~15 hours (5 days, ~3 hours/day)

**Actual Results**:
- **Answer Correctness**: HYBRID 66.9% (best), SPARSE 61.3%, DENSE 51.9%
- **Context Recall**: SPARSE 87.3% (best), HYBRID 83.3%, DENSE 69.4%
- **Success Rate** (correctness >0.7): HYBRID 52.4%, SPARSE 42.9%, DENSE 23.8%
- **Testset Quality**: 8.5/10 after regeneration from whole documents (was 5.5/10 from chunks)

---

## Notes

- This is Option B: Full Project 4 with all mandatory components (all completed)
- Quality over features approach worked well - focused on rigorous evaluation before advanced features
- Documented everything for interview storytelling (README, experiments log, quick reference sheets)
- Prebuilt indexes and Docker deployment enable easy demo without rebuild
- Actual cost: <$2 for OpenAI API (testset generation + evaluation runs) - very cost-effective

**Key Learnings**:
1. **Testset generation ‚â† Retrieval chunking**: Ragas needs whole documents for knowledge graph, not isolated chunks
2. **Dense embeddings need scale**: On small corpus (1.4K chunks), BM25 outperforms (87% vs 69% recall)
3. **Hybrid helps when both contribute**: If one method dominates, fusion may not improve (or degrade)
4. **Error analysis drives improvements**: Identified 7-10% ranking issues ‚Üí cross-encoder reranking is clear next step

---

**Status**: ‚úÖ **PROJECT COMPLETE** (90% overall, production-ready)

**Remaining Optional Improvements**:
- [ ] Push to GitHub (separate portfolio repo)
- [ ] Optional: Deploy to Streamlit Cloud
- [ ] Optional: FastAPI backend + observability (LangSmith/Langfuse)
- [ ] Optional: Regenerate testset with GPT-4o-mini (higher quality than Ollama)

**Ready for interviews**: Can explain architecture, evaluation methodology, error analysis, and production deployment in 5-10 minutes. üöÄ
